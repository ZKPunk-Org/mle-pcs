{"version":"1","records":[{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis"},"type":"lvl1","url":"/analysis/basefold-analysis","position":0},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io","type":"content","url":"/analysis/basefold-analysis","position":1},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Protocol"},"type":"lvl2","url":"/analysis/basefold-analysis#protocol","position":2},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Protocol"},"content":"The following complexity analysis is based on the Basefold paper \n\nBasefold Protocol 4, \n\nBasefold Evaluation Argument Protocol Based on the Evaluation Form protocol description, and code implementation \n\nBasefold.py.\n\nThe protocol description for evaluation in the paper is as follows:","type":"content","url":"/analysis/basefold-analysis#protocol","position":3},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Parameters"},"type":"lvl2","url":"/analysis/basefold-analysis#parameters","position":4},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Parameters"},"content":"Let N = 2^d\n\nPublic Inputs\n\nCommitment of \\tilde{f}: \\pi_d = \\mathsf{Enc}_d(\\mathbf{a}). In practice, this is implemented using a Merkle Tree commitment, denoted as:\\pi_d = \\mathsf{Enc}_d(\\mathbf{a}) = \\mathsf{MT.commit}(\\mathsf{Enc}_d(\\mathbf{a}))\n\nEvaluation point \\mathbf{u}\n\nComputed value v = \\tilde{f}(\\mathbf{u})\n\nNumber of repeated queries in IOPP.query phase: l\n\nBlow-up factor in FRI protocol: \\mathcal{R}\n\nWitness\n\nEvaluation Form vector of MLE \\tilde{f}: \\mathbf{a} = (a_0, a_1, \\ldots, a_{N-1}), where a_i = \\tilde{f}(\\mathsf{bits}(i)),\n\nsatisfying\\tilde{f}(X_0, X_1, X_2, \\ldots, X_{d-1}) = \\sum_{\\mathbf{b}\\in\\{0,1\\}^d} \\tilde{f}(\\mathbf{b})\\cdot eq_{\\mathbf{b}}(X_0, X_1, X_2, \\ldots, X_{d-1})\n\nThe corresponding parameters passed in the code implementation are:proof = prove_basefold_evaluation_arg_multilinear_basis(f_code=ff_code, f_evals=ff, us=point, v=eval, k=log_n - log_k0, k0=2**log_k0, T=T, blowup_factor=blowup_factor, num_verifier_queries=4, transcript=transcript, debug=False); proof\nverify_basefold_evaluation_arg_multilinear_basis(len(ff_code), commit=commit, proof=proof, us=point, v=eval, d=2, k=log_n - log_k0, T=T, blowup_factor=blowup_factor, num_verifier_queries=4)","type":"content","url":"/analysis/basefold-analysis#parameters","position":5},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Prover"},"type":"lvl2","url":"/analysis/basefold-analysis#prover","position":6},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Prover"},"content":"","type":"content","url":"/analysis/basefold-analysis#prover","position":7},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Encoding","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#encoding","position":8},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Encoding","lvl2":"Prover"},"content":"Input: Values of \\tilde{f} on the boolean hypercube, \\mathbf{a} = (a_0, a_1, \\ldots, a_{N-1}). First encode it into a foldable code, then use basefold’s evaluation protocol.\n\nThe computational complexity of the encoding process is \\frac{\\mathcal{R}}{2} \\cdot dN ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/basefold-analysis#encoding","position":9},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 1","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#round-1","position":10},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 1","lvl2":"Prover"},"content":"Prover sends the values of h^{(d)}(X): (h^{(d)}(0), h^{(d)}(1), h^{(d)}(2))h^{(d)}(X) = \\sum_{b_1,b_2, \\ldots, b_d\\in\\{0,1\\}^2}f(X, b_1, b_2, \\ldots, b_d)\\cdot \\tilde{eq}((X, b_1, b_2, \\ldots, b_d), \\mathbf{u})","type":"content","url":"/analysis/basefold-analysis#round-1","position":11},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1","lvl2":"Prover"},"type":"lvl4","url":"/analysis/basefold-analysis#prover-cost-round-1","position":12},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1","lvl2":"Prover"},"content":"Analyzing the algorithmic complexity of Round 1:\n\nCompute \\vec{c} = \\tilde{eq}_{\\bf{u}}(\\vec{b}), where \\vec{b} = \\{0,1\\}^d, which means calculating values on the hypercube, a total of 2^d values.eq = MLEPolynomial.eqs_over_hypercube(us)\n\nThe specific implementation of this function is:@classmethod\ndef eqs_over_hypercube(cls, rs):\n    k = len(rs)\n    n = 1 << k\n    evals = [1] * n\n    half = 1\n    for i in range(k):\n        for j in range(half):\n            evals[j+half] = evals[j] * rs[i]\n            evals[j] = evals[j] - evals[j+half]\n        half *= 2\n    return evals\n\nThe complexity analysis here is consistent with the analysis in ph23, and borrowing the analysis result directly, the complexity is (2^d - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nProver computes h_d(X) and sends it to the Verifier.\n\nSince h_d(X) is a polynomial of degree 2, the Prover computes the values h_d(0), h_d(1), and h_d(2) and sends them to the Verifier.\n\nFor example, when d = 3:\\begin{split}\nh^{(d)}(0) &= a_0\\cdot e_0 + a_1\\cdot e_1 + a_2\\cdot e_2 + a_3\\cdot e_3 \\\\\nh^{(d)}(1) &= a_4\\cdot e_4 + a_5\\cdot e_5 + a_6\\cdot e_6 + a_7\\cdot e_7 \\\\\nh^{(d)}(2) &= \\sum_{i=0}^{3} (2\\cdot a_{i+4} - a_i)\\cdot (2\\cdot e_{i+4} - e_i) \\\\\n& = \\sum_{i=0}^{3} (4 a_{i+4} \\cdot e_{i+4}  + a_ie_i - 2 \\cdot a_{i}e_{i + 4} - 2 a_{i+4}e_i)\\\\\n& = 4 \\cdot h^{(d)}(1) + h^d(0) - 2 \\cdot \\sum_{i=0}^{3} a_{i}e_{i + 4} - 2 \\cdot \\sum_{i=0}^{3} a_{i + 4}e_{i}\n\\end{split}h_eval_at_0 = sum([f_low[j] * eq_low[j] for j in range(half)])\nh_eval_at_1 = sum([f_high[j] * eq_high[j] for j in range(half)])\nh_eval_at_2 = sum([ (2 * f_high[j] - f_low[j]) * (2 * eq_high[j] - eq_low[j]) for j in range(half)])\nh_poly_vec.append([h_eval_at_0, h_eval_at_1, h_eval_at_2])\n\nFor the calculation of h^{(d)}(X), the decomposition must satisfy:\\begin{align}\n & 1 - X = a \\cdot (1 - 1) + b \\cdot (1 - 0)  \\\\\n & X = a \\cdot 1 + b \\cdot (1 - 0)\n\\end{align}\n\nresulting in:a = X, \\quad b = 1 - X\n\nTherefore, for X = 2, we have a = 2, b = -1, so:\\begin{align}\n & \\tilde{eq}((u_0, u_1, u_2), (b_0, b_1, 2)) = 2 \\times \\tilde{eq}((u_0, u_1, u_2), (b_0, b_1, 1)) - \\tilde{eq}((u_0, u_1, u_2), (b_0, b_1, 0))  \\\\\n\\end{align}\n\nThe general formula is:h^{(d)}(X) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^{d - 1}} (X \\cdot f(\\mathsf{b},1) + (1 - X) \\cdot f(\\mathsf{b}, 0)) \\cdot  (X \\cdot \\tilde{eq}(\\mathsf{b},1) + (1 - X) \\cdot \\tilde{eq}(\\mathsf{b}, 0))\n\nComplexity of computing h^{(d)}(0) is 2^{d - 1} ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComplexity of computing h^{(d)}(1) is 2^{d - 1} ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComplexity of computing h^{(d)}(2) is (2 \\cdot 2^{d - 1} + 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTotal complexity:(4 \\cdot 2^{d - 1} + 3)  ~ \\mathbb{F}_{\\mathsf{mul}} = (2N + 3)  ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity for this round is:(3N + 2)  ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe Prover sends:(h^{(d)}(0), h^{(d)}(1), h^{(d)}(2))","type":"content","url":"/analysis/basefold-analysis#prover-cost-round-1","position":13},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 2","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#round-2","position":14},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 2","lvl2":"Prover"},"content":"For i = d-1, d-2, \\ldots, 1:\n\nVerifier sends challenge \\alpha_i \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p\n\nProver simultaneously conducts the Basefold-IOPP protocol and the Sumcheck protocol:\n\nProver sends the folded vector encoding: \\pi_i = \\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1}). In practice, the corresponding Merkle Tree commitment is sent:\\mathsf{cm}(\\pi_i) = \\mathsf{cm}(\\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1})) = \\mathsf{MT.commit}(\\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1}))\n\nProver computes h^{(i)}(\\alpha_i) as the sum for the next round of the Sumcheck protocol\n\nProver computes the Evaluations of f^{(i)}(X_0, X_1, \\ldots, X_{i-1}) as \\mathbf{a}^{(i)} = \\mathsf{fold}^*_{\\alpha_i}(\\mathbf{a}^{(i+1)})\n\nProver computes and sends h^{(i)}(X):h^{(i)}(X) = \\sum_{\\vec{b}\\in\\{0,1\\}^{i-1}}f(\\vec{b}, X, \\alpha_i, \\alpha_{i+1}, \\ldots, \\alpha_{d-1})\\cdot \\tilde{eq}((\\vec{b}, X, \\alpha_i, \\ldots, \\alpha_{d-1}), \\vec{u})\n\nThe right side of the equation is also a Univariate Polynomial of degree 2 in X, so the Prover can compute the values of h^{(i)}(X) at X=0,1,2: (h^{(i)}(0), h^{(i)}(1), h^{(i)}(2)) based on \\mathbf{a}^{(i)}.","type":"content","url":"/analysis/basefold-analysis#round-2","position":15},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 2","lvl3":"Round 2","lvl2":"Prover"},"type":"lvl4","url":"/analysis/basefold-analysis#prover-cost-round-2","position":16},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 2","lvl3":"Round 2","lvl2":"Prover"},"content":"Analyzing the complexity of the above process for the i-th iteration:\n\nProver computes and sends \\pi_i = \\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1}):f_code = basefold_fri_multilinear_basis(f_code, T[k-i-1], alpha, debug=debug)def basefold_fri_multilinear_basis(vs, table, c, debug=False):\n    assert len(table) == len(vs)/2, \"len(table) is not double len(vs), len(table) = %d, len(vs) = %d\" % (len(table), len(vs))\n    n = len(vs)\n    half = int(n / 2)\n    new_vs = []\n    left = vs[:half]\n    right = vs[half:]\n\n    for i in range(0, half):\n        if debug: print(\"(left[i] + right[i])/2=\", (left[i] + right[i])/2)\n        new_vs.append((1 - c) * (left[i] + right[i])/2 + (c) * (left[i] - right[i])/(2*table[i]))\n    return new_vs\n\nThe function parameter f_code represents \\pi_i, so the length of \\pi_i is 2^i \\cdot \\mathcal{R}, also denoted as n_i.\n\nThe loop for i in range(0, half) iterates n_i/2 times. In each iteration, the finite field operations involved are:2 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + 3 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} = 5 ~ \\mathbb{F}_{\\mathsf{mul}} + 2~\\mathbb{F}_{\\mathsf{inv}}\n\nTherefore, the overall computational complexity of this function is:(5 \\cdot \\frac{n_i}{2}) ~ \\mathbb{F}_{\\mathsf{mul}} + (2 \\cdot \\frac{n_i}{2} )~\\mathbb{F}_{\\mathsf{inv}} = \\frac{5n_i}{2} ~ \\mathbb{F}_{\\mathsf{mul}} + n_i~\\mathbb{F}_{\\mathsf{inv}}\n\nProver computes h^{(i)}(\\alpha_i) as the sum for the next round of the Sumcheck protocol:# compute the new sum = h(alpha)\nsumcheck_sum = UniPolynomial.uni_eval_from_evals([h_eval_at_0, h_eval_at_1, h_eval_at_2], alpha, [Fp(0),Fp(1),Fp(2)])@classmethod\ndef uni_eval_from_evals(cls, evals, z, D):\n    n = len(evals)\n    if n != len(D):\n        raise ValueError(\"Domain size should be equal to the length of evaluations\")\n    if z in D:\n        return evals[D.index(z)]\n    weights = cls.barycentric_weights(D)\n    # print(\"weights={}\".format(weights))\n    e_vec = [weights[i] / (z - D[i]) for i in range(n)]\n    numerator = sum([e_vec[i] * evals[i] for i in range(n)])\n    denominator = sum([e_vec[i] for i in range(n)])\n    return (numerator / denominator)\n\nNote\n\nThe value h^{(i)}(\\alpha_i) should be computed by the Verifier, not the Prover.\n\nProver computes the Evaluations of f^{(i)}(X_0, X_1, \\ldots, X_{i-1}) as \\mathbf{a}^{(i)} = \\mathsf{fold}^*_{\\alpha_i}(\\mathbf{a}^{(i+1)}):f = [(1 - alpha) * f_low[i] + alpha * f_high[i] for i in range(half)]\n\nHere, the length of half is 2^i, so the complexity is:2 \\cdot 2^{i} ~ \\mathbb{F}_{\\mathsf{mul}} = 2^{i+1} ~ \\mathbb{F}_{\\mathsf{mul}}\n\nProver computes and sends h^{(i)}(X):h^{(i)}(X) = \\sum_{\\vec{b}\\in\\{0,1\\}^{i-1}}f(\\vec{b}, X, \\alpha_i, \\alpha_{i+1}, \\ldots, \\alpha_{d-1})\\cdot \\tilde{eq}((\\vec{b}, X, \\alpha_i, \\ldots, \\alpha_{d-1}), \\vec{u})\n\nThe right side is a Univariate Polynomial of degree 2 in X, so the Prover can compute the values at X=0,1,2: (h^{(i)}(0), h^{(i)}(1), h^{(i)}(2)) based on \\mathbf{a}^{(i)}.eq_low = eq[:half]\neq_high = eq[half:]\n\neq = [(1 - alpha) * eq_low[i] + alpha * eq_high[i] for i in range(half)]\n\nh_eval_at_0 = sum([f_low[j] * eq_low[j] for j in range(half)])\nh_eval_at_1 = sum([f_high[j] * eq_high[j] for j in range(half)])\nh_eval_at_2 = sum([ (2 * f_high[j] - f_low[j]) * (2 * eq_high[j] - eq_low[j]) for j in range(half)])\nh_poly_vec.append([h_eval_at_0, h_eval_at_1, h_eval_at_2])\n\nThe complexity of computing eq = [(1 - alpha) * eq_low[i] + alpha * eq_high[i] for i in range(half)] is the same as computing f, which is 2^{i+1} ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe complexity of computing (h^{(i)}(0), h^{(i)}(1), h^{(i)}(2)) is similar to the analysis of h_d(X), which is:(2 \\cdot 2^{i} + 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThus, the complexity for this step is:2^{i+1} ~ \\mathbb{F}_{\\mathsf{mul}} + (4 \\cdot 2^{i-1} + 3)  ~ \\mathbb{F}_{\\mathsf{mul}} =(4 \\cdot 2^{i} + 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nAdding the complexities of all the steps:\\frac{5n_i}{2} ~ \\mathbb{F}_{\\mathsf{mul}} + n_i~\\mathbb{F}_{\\mathsf{inv}} + 2^{i+1} ~ \\mathbb{F}_{\\mathsf{mul}} + (4 \\cdot 2^i + 3)  ~ \\mathbb{F}_{\\mathsf{mul}} = (\\frac{5n_i}{2} + 6 \\cdot 2^{i} + 3) ~ \\mathbb{F}_{\\mathsf{mul}} +  n_i~\\mathbb{F}_{\\mathsf{inv}}\n\nSubstituting n_i = 2^i \\cdot \\mathcal{R}, the complexity becomes:(\\frac{5\\cdot 2^i \\cdot \\mathcal{R}}{2} + 6 \\cdot 2^{i} + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^i \\cdot \\mathcal{R})~\\mathbb{F}_{\\mathsf{inv}} = ((\\frac{5}{2} \\mathcal{R} + 6) \\cdot 2^i + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^i \\cdot \\mathcal{R})~\\mathbb{F}_{\\mathsf{inv}}\n\nSumming up the complexities for all i = d-1, \\ldots, 1:\\begin{align}\n & \\sum_{i = 1}^{d - 1}((\\frac{5}{2} \\mathcal{R} + 6) \\cdot 2^i + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^i \\cdot \\mathcal{R})~\\mathbb{F}_{\\mathsf{inv}} \\\\\n = & ((\\frac{5}{2} \\mathcal{R} + 6) (N - 2) + 3(d - 1)) ~ \\mathbb{F}_{\\mathsf{mul}} +  \\mathcal{R}(N - 2)~\\mathbb{F}_{\\mathsf{inv}} \\\\\n=  & ((\\frac{5}{2} \\mathcal{R} + 6) N + 3d - 5 \\mathcal{R} - 15) ~ \\mathbb{F}_{\\mathsf{mul}} +  (\\mathcal{R}N - 2 \\mathcal{R})~\\mathbb{F}_{\\mathsf{inv}}\n\\end{align}\n\nAdditionally, for the Merkle Tree computation, for i = d-1, \\ldots, 1, the Prover sends the Merkle Tree commitment of the folded vector encoding:\\mathsf{cm}(\\pi_i) = \\mathsf{cm}(\\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1})) = \\mathsf{MT.commit}(\\mathsf{fold}^*_{\\alpha_i}(\\pi_{i+1}))\n\nHere, the Merkle Tree has 2^i \\cdot \\mathcal{R} leaf nodes, denoted as \\mathsf{MT.commit}(2^i \\cdot \\mathcal{R}), totaling:\\sum_{i = 1}^{d - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})","type":"content","url":"/analysis/basefold-analysis#prover-cost-round-2","position":17},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 3","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#round-3","position":18},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 3","lvl2":"Prover"},"content":"Verifier sends challenge \\alpha_0 \\leftarrow \\mathbb{F}_p\n\nProver continues with the Basefold-IOPP protocol:\n\nProver sends the folded vector encoding \\pi_0 = \\mathsf{fold}^*_{\\alpha_0}(\\pi_1). Since the Verifier will check if \\pi_0 is a valid encoding, all values are sent to the Verifier instead of their Merkle commitment.","type":"content","url":"/analysis/basefold-analysis#round-3","position":19},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3","lvl2":"Prover"},"type":"lvl4","url":"/analysis/basefold-analysis#prover-cost-round-3","position":20},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3","lvl2":"Prover"},"content":"The complexity is consistent with the analysis of \\pi_i:\\frac{5n_0}{2} ~ \\mathbb{F}_{\\mathsf{mul}} + n_0~\\mathbb{F}_{\\mathsf{inv}} = \\frac{5}{2} \\mathcal{R}~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} ~ \\mathbb{F}_{\\mathsf{inv}}","type":"content","url":"/analysis/basefold-analysis#prover-cost-round-3","position":21},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 4","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#round-4","position":22},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Round 4","lvl2":"Prover"},"content":"This round involves the Verifier performing IOPP.query to check the correctness of the folding. The protocol description from the paper is:\n\nRepeated l times:\n\nVerifier randomly selects an index \\mu \\stackrel{\\$}{\\leftarrow} [1, n_{d-1}]\n\nFor i = d-1, \\ldots, 0:\n\nProver sends \\pi_{i+1}[\\mu], \\pi_{i+1}[\\mu + n_i] and the Merkle Path corresponding to \\pi_{i+1}[\\mu].\n\\{\\pi_{i+1}[\\mu], \\pi_{\\pi_{i+1}}(\\mu)\\} \\leftarrow \\mathsf{MT.open}(\\pi_{i+1}, \\mu)\n\nIf i > 0 and \\mu > n_{i-1}, Prover computes new \\mu, \\mu \\leftarrow \\mu - n_{i-1}","type":"content","url":"/analysis/basefold-analysis#round-4","position":23},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4","lvl2":"Prover"},"type":"lvl4","url":"/analysis/basefold-analysis#prover-cost-round-4","position":24},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4","lvl2":"Prover"},"content":"Since the Prover is only sending values already computed, there’s no additional computational cost in this round.","type":"content","url":"/analysis/basefold-analysis#prover-cost-round-4","position":25},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Prover Cost","lvl2":"Prover"},"type":"lvl3","url":"/analysis/basefold-analysis#prover-cost","position":26},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Prover Cost","lvl2":"Prover"},"content":"Summing up the computational complexities:\\begin{aligned}\n& (3N + 2)  ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + ((\\frac{5}{2} \\mathcal{R} + 6) N + 3d - 5 \\mathcal{R} - 15) ~ \\mathbb{F}_{\\mathsf{mul}} +  (\\mathcal{R}N - 2 \\mathcal{R})~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\frac{5 }{2}  \\mathcal{R}~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} ~ \\mathbb{F}_{\\mathsf{inv}}\\\\\n= & \\left((\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3d - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{aligned}\n\nAdding the complexity of Merkle Tree commitments:\\begin{aligned}\n\\left((\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3d - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{d - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \n\\end{aligned}\n\nIf we include the complexity of computing the encoding \\pi_d, the total complexity is:\\left(\\frac{\\mathcal{R}}{2} \\cdot dN + (\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3d - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{d - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})","type":"content","url":"/analysis/basefold-analysis#prover-cost","position":27},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Proof"},"type":"lvl2","url":"/analysis/basefold-analysis#proof","position":28},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Proof"},"content":"The proof sent by the Prover is:\\begin{aligned}\n    \\pi = & ((h^{(d)}(0), h^{(d)}(1), h^{(d)}(2)), \\mathsf{cm}(\\pi_{d - 1}), \\mathsf{cm}(\\pi_{d - 2}), \\ldots, \\mathsf{cm}(\\pi_{1}),\\\\\n    & \\quad  h^{(d-1)}(0), h^{(d-1)}(1), h^{(d-1)}(2), \\ldots, h^{(1)}(0), h^{(1)}(1), h^{(1)}(2), \\pi_0 \\\\\n    & \\quad \\{\\pi_{d}[\\mu^{(d)}], \\pi_{d}[\\mu^{(d)} + n_{d-1}], \\pi_{\\pi_{d}}(\\mu^{(d)}), \\ldots, \\pi_{1}[\\mu^{(1)}], \\pi_{1}[\\mu^{(1)} + n_{0}], \\pi_{\\pi_{1}}(\\mu^{(1)}) \\}^l ) \n\\end{aligned}\n\nIn the above representation, \\mu^{(d)}, \\ldots, \\mu^{(1)} denote the random indices in the IOPP.query phase, with their update process as described in Prover Round 4. \\{\\cdot\\}^l indicates repeating l rounds, where the proof may differ in each round.","type":"content","url":"/analysis/basefold-analysis#proof","position":29},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Proof Size","lvl2":"Proof"},"type":"lvl4","url":"/analysis/basefold-analysis#proof-size","position":30},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Proof Size","lvl2":"Proof"},"content":"\\mathsf{cm}(\\pi_{i}) represents a Merkle Tree commitment, which is the root node of the Merkle Tree (a hash value), denoted as H.\n\n\\pi_{\\pi_{i}}(\\mu^{(i)}) represents a Merkle Tree path. The height of this tree is \\log n_i, so the Merkle Path sends \\log n_i hash values, denoted as \\log n_i ~ H.\n\nThe proof size is:\\begin{aligned}\n    & \\quad 3 ~ \\mathbb{F} + d ~ H + (3 \\cdot (d - 1))~ \\mathbb{F} + n_0 ~ \\mathbb{F} + l \\cdot (2d ~ \\mathbb{F} + (\\log n_d + \\ldots + \\log n_1) ~ H) \\\\\n    & = (3d + \\mathcal{R} + 2dl) ~ \\mathbb{F} +  d ~ H + l (d + \\log \\mathcal{R} + \\ldots + 1 + \\log \\mathcal{R}) ~ H \\\\\n    & = (3d + \\mathcal{R} + 2dl) ~ \\mathbb{F} + \\left(d + l  \\cdot \\left(\\frac{d (d + 1)}{2} + d \\cdot \\log \\mathcal{R} \\right) \\right) ~ H \\\\\n\t& = ((2l + 3)d + \\mathcal{R}) ~ \\mathbb{F} + \\left( \\frac{l}{2} \\cdot d^2 + \\left(\\log \\mathcal{R} \\cdot l +\\frac{1}{2} \\cdot l + 1\\right) \\cdot d \\right) ~ H \n\\end{aligned}\n\nNote\n\nThe code sends each encoding \\pi_i, which is not actually needed. The code sends each computed encoding \\pi_0, \\pi_1, \\ldots, \\pi_d, but only needs to send the values corresponding to the IOPP.query and the commitments to these encodings.","type":"content","url":"/analysis/basefold-analysis#proof-size","position":31},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Verification"},"type":"lvl2","url":"/analysis/basefold-analysis#verification","position":32},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Verification"},"content":"The Verifier verifies the following equations:\n\nVerifier sends several rounds of Queries, Q=\\{q_i\\}\n\nVerifier checks the correctness of each folding step in Sumcheck:\n\nVerifyh^{(d)}(0) + h^{(d)}(1) \\overset{?}{=} v\n\nFor i = 1, \\ldots, d-1, verify\\begin{split}\nh^{(i)}(0) + h^{(i)}(1) &\\overset{?}{=} h^{(i+1)}(\\alpha_i) \\\\\n\\end{split}\n\nVerifier checks if the final encoding \\pi_0 is correct:\\pi_0 \\overset{?}{=} \\mathsf{enc}_0\\left(\\frac{h^{(1)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u})}\\right)","type":"content","url":"/analysis/basefold-analysis#verification","position":33},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Verifier Cost Analysis","lvl2":"Verification"},"type":"lvl4","url":"/analysis/basefold-analysis#verifier-cost-analysis","position":34},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl4":"Verifier Cost Analysis","lvl2":"Verification"},"content":"Analyzing the algorithmic complexity of the verification phase:\n\nVerifier verifies the IOPP.query, repeated l times:\n\nFor i = d-1,\\ldots, 0, Verifier checks the correctness of the folding:\n\nFirst verify the correctness of the sent \\pi_{i+1}[\\mu]:\\mathsf{MT.verify}(\\mathsf{cm}(\\pi_{i+1}), \\pi_{i+1}[\\mu], \\pi_{\\pi_{i+1}}(\\mu)) \\stackrel{?}{=} 1\n\nThe complexity is denoted as \\mathsf{MTV}(\\log n_{i+1}), where the parameter indicates the height of the Merkle Tree. The main computational cost is calculating hash values; the Verifier needs to compute as many hash values as the height of the tree, denoted as \\log n_{i+1} ~ H.\n\nVerifier computes the folded value:\\mathsf{fold} = (1 - \\alpha) \\cdot \\frac{\\pi_{i+1}[\\mu] + \\pi_{i+1}[\\mu + n_i]}{2} + \\alpha \\cdot \\frac{\\pi_{i+1}[\\mu] - \\pi_{i+1}[\\mu + n_i]}{2 \\cdot x[\\mu]}assert f_code_folded == ((1 - alpha) * (code_left + code_right)/2 + (alpha) * (code_left - code_right)/(2*table[x0])), \"failed to check multilinear base fri\"\n\nThe computational complexity is:5 ~ \\mathbb{F}_{\\mathsf{mul}} + 2~\\mathbb{F}_{\\mathsf{inv}}\n\nVerifier compares the computed folded value with the value sent by the Prover:\\mathsf{fold} \\stackrel{?}{=} \\pi_{i}[\\mu]\n\nFinally, Verifier verifies if \\pi_0 is a valid encoding, which for RS encoding means checking if \\pi_0 is an encoding of a constant polynomial by verifying that all values are equal.# check the final code\nfinal_code = f_code_vec[i]\nassert len(final_code) == blowup_factor, \"len(final_code) != blowup_factor, len(final_code) = %d, blowup_factor = %d\" % (len(final_code), blowup_factor)\nfor i in range(len(final_code)):\n    assert final_code[0] == final_code[i], \"final_code is not a repetition code\"\n\nSumming up the complexity for the IOPP.query step:l \\cdot \\sum_{i=1}^{d}\\mathsf{MTV}(\\log n_{i}) + 5dl ~ \\mathbb{F}_{\\mathsf{mul}} + 2dl~\\mathbb{F}_{\\mathsf{inv}} = l \\cdot\\sum_{i=1}^{d}\\mathsf{MTV}(i + \\log \\mathcal{R}) + 5dl ~ \\mathbb{F}_{\\mathsf{mul}} + 2dl~\\mathbb{F}_{\\mathsf{inv}}\n\nVerifier checks the correctness of each folding step in Sumcheck. This step’s computational complexity comes from the Verifier needing to compute h^{(i+1)}(\\alpha_i) for i = 1, \\ldots, d-1:sumcheck_sum = UniPolynomial.uni_eval_from_evals(h_evals, alpha, [Fp(0),Fp(1),Fp(2)])@classmethod\ndef uni_eval_from_evals(cls, evals, z, D):\n    n = len(evals)\n    if n != len(D):\n        raise ValueError(\"Domain size should be equal to the length of evaluations\")\n    if z in D:\n        return evals[D.index(z)]\n    weights = cls.barycentric_weights(D)\n    # print(\"weights={}\".format(weights))\n    e_vec = [weights[i] / (z - D[i]) for i in range(n)]\n    numerator = sum([e_vec[i] * evals[i] for i in range(n)])\n    denominator = sum([e_vec[i] for i in range(n)])\n    return (numerator / denominator)\n\nThis uses barycentric interpolation to compute the value of h^{(i+1)}(\\alpha_i).h(\\alpha) = \\frac{\\sum \\frac{\\omega_i \\cdot y_i}{\\alpha - x_i}}{\\sum \\frac{\\omega_i}{\\alpha - x_i}}\n\nwhere y_i = h(x_i), and \\omega_i are barycentric interpolation weights that can be precomputed:\\omega_i = \\frac{1}{\\prod_{j=1, j \\neq i}^n (x_i - x_j)}\n\nIf |D| = n, the computational complexity is:(2n + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 2) ~ \\mathbb{F}_{\\mathsf{inv}}\n\nThis conclusion is based on the ph23-analysis. The analysis method is similar to the first step of the verification process.\n\nWhen computing h^{(i+1)}(\\alpha_i), we know h^{(i+1)}(0), h^{(i+1)}(1), h^{(i+1)}(2), so substituting n = 3 in the above formula, the complexity of computing h^{(i+1)}(\\alpha_i) is:9 ~ \\mathbb{F}_{\\mathsf{mul}} + 5 ~ \\mathbb{F}_{\\mathsf{inv}}\n\nSince i = 1, \\ldots, d-1, there are d-1 times, so the total complexity is:9(d-1) ~ \\mathbb{F}_{\\mathsf{mul}} + 5(d-1) ~ \\mathbb{F}_{\\mathsf{inv}}\n\nVerifier checks if the final encoding \\pi_0 is correct:\\pi_0 \\overset{?}{=} \\mathsf{enc}_0\\left(\\frac{h^{(1)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u})}\\right)\n\nComputing \\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u}):\\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u}) = \\prod_{i=0}^{d-1} \\big( (1-\\alpha_i)(1-u_i) + \\alpha_i u_i\\big)\n\nEach term (1-\\alpha_i)(1-u_i) + \\alpha_i u_i requires 2 ~\\mathbb{F}_{\\mathsf{mul}}, so computing all d terms requires 2d ~\\mathbb{F}_{\\mathsf{mul}}. Then d numbers need to be multiplied, for a total complexity of:2d ~\\mathbb{F}_{\\mathsf{mul}} + (d-1) ~\\mathbb{F}_{\\mathsf{mul}} = (3d-1) ~\\mathbb{F}_{\\mathsf{mul}}\n\n[!bug]\nThe way the Verifier computes \\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u}) in the code can be improved:eq_evals = MLEPolynomial.eqs_over_hypercube(us)\nfor i in range(k):\n  alpha = challenge_vec[i]\n  eq_low = eq_evals[:half]\n  eq_high = eq_evals[half:]\n  if debug: print(\"eq_low={}, eq_high={}\".format(eq_low, eq_high))\n  eq_evals = [(1-alpha) * eq_low[i] + alpha * eq_high[i] for i in range(half)]\n\n# check f(alpha_vec)\nf_eval_at_random = sumcheck_sum/eq_evals[0]\n\nThis calculation method has a complexity of O(N).\n\nIt can be changed to use a product calculation method with a complexity of O(d):# use another way to compute eq_evals[0]\nchallenge_vec_test = challenge_vec[::-1]\nprint(f\"challenge_vec_test = {challenge_vec_test}\")\n\neq_evals_test = 1\nfor i in range(k):\n    eq_evals_test *= (1 - challenge_vec_test[i]) * (1 - us[i]) + challenge_vec_test[i] * us[i]\nprint(f\"eq_evals_test = {eq_evals_test}\")\n\nVerifier computes h^{(1)}(\\alpha_0):\n\nThe complexity is similar to the above analysis:9 ~ \\mathbb{F}_{\\mathsf{mul}} + 5 ~ \\mathbb{F}_{\\mathsf{inv}}\n\nComputing \\frac{h^{(1)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u})} involves division and multiplication in the finite field, with a complexity of \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}}.\n\nComputing the encoding \\mathsf{enc}_0\\left(\\frac{h^{(1)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\ldots,\\alpha_{d-1}), \\mathbf{u})}\\right):# check f(alpha_vec)\nf_eval_at_random = sumcheck_sum/eq_evals[0]\nif debug: print(\"f_eval_at_random={}\".format(f_eval_at_random))\nif debug: print(\"rs_encode([f_eval_at_random], k0=1, c=blowup_factor)=\", rs_encode([f_eval_at_random], k0=1, c=blowup_factor))\nassert rs_encode([f_eval_at_random], k0=1, c=blowup_factor) == f_code_folded, \"❌: Encode(f(rs)) != f_code_0\"\n\nSince this is already a constant polynomial, the encoding is just \\mathcal{R} copies of the constant, which doesn’t involve computation by the Verifier.","type":"content","url":"/analysis/basefold-analysis#verifier-cost-analysis","position":35},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Verification"},"type":"lvl3","url":"/analysis/basefold-analysis#verifier-cost","position":36},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Verification"},"content":"Summing up the Verifier’s computational cost:\\begin{aligned}\n    & l \\cdot\\sum_{i=1}^{d}\\mathsf{MTV}(i + \\log \\mathcal{R}) + 5dl ~ \\mathbb{F}_{\\mathsf{mul}} + 2dl~\\mathbb{F}_{\\mathsf{inv}} \\\\ \n    & \\quad + 9(d-1) ~ \\mathbb{F}_{\\mathsf{mul}} + 5(d-1) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & \\quad + (3d-1) ~ \\mathbb{F}_{\\mathsf{mul}} + 9 ~ \\mathbb{F}_{\\mathsf{mul}} + 5 ~ \\mathbb{F}_{\\mathsf{inv}} + \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} \\\\\n    = & l \\cdot\\sum_{i=1}^{d}\\mathsf{MTV}(i + \\log \\mathcal{R}) + (5dl + 12d) ~ \\mathbb{F}_{\\mathsf{mul}} + (2dl + 5d + 1) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    = & l \\cdot\\sum_{i=1}^{d}(i + \\log \\mathcal{R}) ~ H + (5dl + 12d) ~ \\mathbb{F}_{\\mathsf{mul}} + (2dl + 5d + 1) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    = &  l \\cdot \\left(\\frac{d(d+1)}{2} + d \\cdot \\log \\mathcal{R} \\right) ~ H + (5dl + 12d) ~ \\mathbb{F}_{\\mathsf{mul}} + (2dl + 5d + 1) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n\t= & \\left(\\frac{l}{2} \\cdot d^2 + (l\\log \\mathcal{R} + \\frac{l}{2})d \\right) ~ H + (5l + 12)d ~ \\mathbb{F}_{\\mathsf{mul}} + ((2l + 5)d + 1) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{aligned}","type":"content","url":"/analysis/basefold-analysis#verifier-cost","position":37},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Summary"},"type":"lvl2","url":"/analysis/basefold-analysis#summary","position":38},{"hierarchy":{"lvl1":"Basefold Protocol Algorithm Complexity Analysis","lvl2":"Summary"},"content":"Prover’s cost:\\begin{aligned}\n\\left((\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3d - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i=1}^{d-1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \n\\end{aligned}\n\nIf we include the complexity of computing the encoding \\pi_d, the total complexity is:\\left(\\frac{\\mathcal{R}}{2} \\cdot dN + (\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3d - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i=1}^{d-1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\n\nProof size:\\begin{align}\n((2l + 3)d + \\mathcal{R}) ~ \\mathbb{F} + \\left(\\frac{l}{2} \\cdot d^2 + \\left(\\log \\mathcal{R} \\cdot l +\\frac{1}{2} \\cdot l + 1\\right) \\cdot d \\right) ~ H \n\\end{align}\n\nVerifier’s cost:\\begin{align}\n\\left(\\frac{l}{2} \\cdot d^2 + (l\\log \\mathcal{R} + \\frac{l}{2})d \\right) ~ H + (5l + 12)d ~ \\mathbb{F}_{\\mathsf{mul}} + ((2l + 5)d + 1) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{align}","type":"content","url":"/analysis/basefold-analysis#summary","position":39},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR"},"type":"lvl1","url":"/analysis/basefold-deepfold-whir","position":0},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article introduces and compares three similar multilinear polynomial commitment schemes (PCS): BaseFold [ZCF23], DeepFold [GLHQTZ24], and WHIR [ACFY24b].\n\nThe FRI [BBHR18] protocol is an Interactive Oracle Proof of Proximity (IOPP) protocol used to determine if a univariate polynomial is close to a predefined Reed-Solomon code space, which can be used to perform a low degree test on a polynomial. The FRI protocol naturally supports univariate polynomials rather than multilinear polynomials, so constructing an efficient multilinear polynomial commitment scheme based on the FRI protocol is not easy. The Basefold protocol achieves this by utilizing the sumcheck protocol.\n\nFor a univariate polynomial, applying the FRI protocol is essentially a process of repeatedly folding the polynomial using random numbers until it is folded into a constant. The Basefold protocol cleverly discovered that this constant corresponds to the value of a multilinear polynomial at a random point. The last step of the Sumcheck protocol also requires an Oracle to obtain the value of a multilinear polynomial at a random point. If the FRI protocol and sumcheck protocol are performed synchronously, using the same random numbers during the protocol process, then the constant from the last step of the FRI protocol can naturally serve as the oracle function in the last step of the sumcheck protocol to complete the entire sumcheck protocol. The overall approach of the Basefold protocol is very concise and clever, combining sumcheck and FRI protocols to construct an efficient multilinear polynomial commitment scheme. The Basefold protocol actually applies not only to Reed-Solomon codes but to all codes that satisfy the definition of foldable code. In this article, for ease of comparison between these three protocols, we only consider Reed-Solomon codes.\n\nThe DeepFold protocol improves upon the Basefold protocol by replacing the FRI protocol in Basefold with the DEEP-FRI [BGKS20] protocol. Compared to the FRI protocol, the DEEP-FRI protocol sacrifices a small amount of Prover’s computation to reduce the number of Verifier’s queries, thereby reducing the overall proof size and Verifier’s computational cost.\n\nThe WHIR protocol goes a step further than the DeepFold protocol, replacing the FRI protocol in the Basefold protocol with the STIR [ACFY24a] protocol. Compared to the FRI and DEEP-FRI protocols, the STIR protocol requires fewer queries from the Verifier. Its idea is to reduce the code rate in each round of the protocol, thus increasing redundancy in the encoding. This allows the Verifier to have more information to determine whether the received message belongs to a code space, thereby reducing the number of queries.\n\nIn summary, the overall framework of the three protocols is shown in the following diagram.","type":"content","url":"/analysis/basefold-deepfold-whir","position":1},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"From Multilinear Polynomials to Univariate Polynomials"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#from-multilinear-polynomials-to-univariate-polynomials","position":2},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"From Multilinear Polynomials to Univariate Polynomials"},"content":"Before introducing these three multilinear polynomial commitment schemes in detail, let’s first discuss the correspondence between a multilinear polynomial and a univariate polynomial. Only after establishing this correspondence can we boldly invoke the FRI, DEEP-FRI, and STIR protocols that are only applicable to univariate polynomials.\n\nFor an n-variable linear polynomial \\tilde{f}(X_0, \\ldots, X_{n-1}), expressed in coefficient form as:\\tilde{f}(X_0,\\ldots,X_{n-1}) = a_0 + a_1 X_0 + a_2 X_1 + a_3 X_0X_1 + \\ldots + a_{2^n - 1} \\cdot X_0X_1 \\cdots X_{n-1}\n\nwhere the basis of the multilinear polynomial (1, X_0, X_1, X_0X_1, \\ldots, X_0X_1\\cdots X_{n-1}) is arranged in lexicographic order.\n\nBy directly corresponding the coefficients of the multilinear polynomial to the coefficients of a univariate polynomial, the univariate polynomial corresponding to \\tilde{f}(X_0, \\ldots, X_{n-1}) is:f(X) = a_0 + a_1 X + a_2 X^2 + a_3 X^3 + \\ldots + a_{2^n - 1} X^{2^n - 1}\n\nIf we let X_0 = X^{2^0}, X_1 = X^{2^1}, \\ldots , X_{n-1} = X^{2^{n - 1}} in the multilinear polynomial \\tilde{f}(X_0, \\ldots, X_{n-1}), we can find:\\begin{align}\n\\tilde{f}(X^{2^0},X^{2^1},\\ldots,X^{2^{n-1}})  & = a_0 + a_1 \\cdot X + a_2 \\cdot X^2 + a_3 \\cdot X \\cdot X^2 + \\ldots + a_{2^n - 1} \\cdot X^{2^0} \\cdot X^{2^1} \\cdot X^{2^{n - 1}} \\\\\n & = a_0 + a_1 X + a_2 X^2 + a_3 X^3 + \\ldots + a_{2^n - 1} X^{2^n - 1} \\\\\n & = f(X)\n\\end{align}\n\nThe above equation \\tilde{f}(X^{2^0},X^{2^1},\\ldots,X^{2^{n-1}}) = f(X) profoundly reveals the intrinsic connection between multilinear polynomials and univariate polynomials. This can also be seen as a conversion relationship between the basis of multilinear polynomials (1, X_0, X_1, \\ldots , X_0X_1\\cdots X_{n-1}) and the basis of univariate polynomials (1, X, X^2, X^3, \\ldots , X^{2^{n} - 1}). After establishing such a relationship, univariate polynomials and multilinear polynomials can be freely converted, essentially just differing in their basis.\n\nFor example, to find the value of a univariate polynomial f(X) at a point \\alpha, f(\\alpha), it is equal to \\tilde{f}(\\alpha, \\alpha^2, \\alpha^4, \\ldots, \\alpha^{2^n - 1}).\n\nNow let’s look at the folding process in the FRI protocol applicable to univariate polynomials. First, split f(X) into odd and even terms:\\begin{align}\nf(X)  & = a_0 + a_1 X + a_2 X^2 + a_3 X^3 +  \\ldots + a_{2^n - 1} X^{2^n - 1} \\\\\n & = (a_0 + a_2 X^2 + \\ldots + a_{2^{n}-2}X^{2^n - 2}) + X \\cdot (a_1 + a_3 X^2 + \\ldots + a_{2^n - 1} X^{2^n - 2}) \\\\\n & := f_{even}(X^2) + X \\cdot f_{odd}(X^2)\n\\end{align}\n\nAt this point, the degree bounds of f_{even}(X) and f_{odd}(X) are 2^{n - 1}, which is already half of the original polynomial f(X)'s degree bound 2^n. Use a random number \\alpha_1 to fold these two polynomials together:\\begin{align}\nf^{(1)}(X)  & = f_{even}(X) + \\alpha_1 \\cdot f_{odd}(X) \\\\\n & = (a_0 + a_2 X + \\ldots + a_{2^{n}-2}X^{2^{n - 1} - 1}) + \\alpha_1 \\cdot (a_1 + a_3 X + \\ldots + a_{2^n - 1} X^{2^{n - 1} - 1}) \n\\end{align}\n\nNow let’s look at this folding process from the perspective of multilinear polynomials. The process of splitting f(X) into odd and even terms is equivalent to factoring out the X term, which corresponds to factoring out terms containing X_0 in the multilinear polynomial:\\begin{align}\nf(X)   & = \\tilde{f}(X^{2^0},X^{2^1},\\ldots,X^{2^{n-1}})   \\\\\n & = \\tilde{f}(X_0,\\ldots,X_{n-1})  \\\\\n & = a_0 + a_1 X_0 + a_2 X_1 + \\ldots + a_{2^n - 1} X_0X_1\\cdots X_{n-1} \\\\\n& = (a_0 + a_2 X_1 + \\ldots + a_{2^{n}-2}X_1X_2\\cdots X_{n-1}) + X_0 \\cdot (a_1 + a_3 X_1 + \\ldots + a_{2^n - 1} X_1X_2\\cdots X_{n-1}) \\\\\n & := \\tilde{f}_{even}(X_1,\\ldots,X_{n-1}) + X_0 \\cdot \\tilde{f}_{odd}(X_1, \\ldots, X_{n-1})\n\\end{align}\n\nAt this point, \\tilde{f}_{even}(X_1, \\ldots, X_{n-1}) and \\tilde{f}_{odd}(X_1, \\ldots, X_{n-1}) have one fewer variable, only n - 1. Using the random number \\alpha_1 to fold the multilinear polynomial corresponds to:\\begin{align}\nf^{(1)}(X)  & = f_{even}(X) + \\alpha_1 \\cdot f_{odd}(X) \\\\\n & = \\tilde{f}_{even}(X_0,\\ldots,X_{n-2}) + \\alpha_1 \\cdot \\tilde{f}_{odd}(X_0, \\ldots, X_{n-2}) \\\\\n & = \\tilde{f}(\\alpha_1, X_0, X_1, \\ldots, X_{n - 2})\n\\end{align}\n\nThen the multilinear polynomial \\tilde{f}^{(1)}(X_0, X_1, \\ldots, X_{n- 2}) corresponding to the folded univariate polynomial f^{(1)}(X) satisfies:\\tilde{f}^{(1)}(X_0, X_1, \\ldots, X_{n- 2}) = \\tilde{f}(\\alpha_1, X_0, X_1, \\ldots, X_{n - 2})\n\nThe above equation shows that the process of folding a univariate polynomial corresponds to folding a multilinear polynomial using the same random number. The result is equivalent to variable substitution on the original n-variable linear polynomial, replacing (X_0, X_1, X_2, \\ldots, X_{n - 1}) with (\\alpha_1, X_0, X_1, \\ldots, X_{n-2}).\n\nIf we continue to fold f^{(1)}(X) using the above method, folding with random number \\alpha_2 to get f^{(2)}(X), then its corresponding multilinear polynomial \\tilde{f}^{(2)}(X_0, X_1, \\ldots, X_{n - 3}) should satisfy:\\tilde{f}^{(2)}(X_0, X_1, \\ldots, X_{n- 3}) = \\tilde{f}(\\alpha_1, \\alpha_2, X_0, \\ldots, X_{n - 3})\n\nAnd so on. After k folds, selecting k random numbers \\vec{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k), the folded polynomial f^{(k)}(X) corresponds to a multilinear polynomial satisfying:\\tilde{f}^{(k)}(X_0, X_1, \\ldots, X_{n- k - 1}) = \\tilde{f}(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k, X_0, \\ldots, X_{n - k - 1})\n\nSince the folding of univariate polynomials in the FRI protocol implies a similar folding of multilinear polynomials, the Reed-Solomon code space of univariate polynomials applicable to the FRI protocol can also be viewed from the perspective of multilinear polynomials. According to the description in the WHIR paper [ACFY24b], for the code space \\mathsf{RS}[\\mathbb{F}, \\mathcal{L}, n], it represents the set of evaluations on \\mathcal{L} of all univariate functions over finite field \\mathbb{F} with degree strictly less than 2^n. Then:\\begin{aligned}\n    \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, n] & := \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{g} \\in \\mathbb{F}^{< 2^n}[X] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{g}(x)\\} \\\\\n    & = \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\tilde{f} \\in \\mathbb{F}^{< 2}[X_0, \\ldots, X_{n - 1}] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\tilde{f}(x^{2^0}, x^{2^1},\\ldots, x^{2^{n-1}})\\}\n\\end{aligned}\n\nTo summarize, after establishing the coefficient correspondence between univariate linear polynomials and multilinear polynomials, they satisfy the equation \\tilde{f}(X^{2^0},X^{2^1},\\ldots,X^{2^{n-1}}) = f(X), from which we can derive:\n\nEvaluating a univariate polynomial f(X) at a point \\alpha: f(\\alpha) = \\tilde{f}(\\alpha, \\alpha^2, \\alpha^4, \\ldots, \\alpha^{2^n - 1}).\n\nAfter folding the univariate polynomial f(X) k times using random numbers (\\alpha_1, \\ldots, \\alpha_k), the multilinear polynomial corresponding to the folded polynomial f^{(k)}(X) satisfies:\\tilde{f}^{(k)}(X_0, X_1, \\ldots, X_{n- k - 1}) = \\tilde{f}(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k, X_0, \\ldots, X_{n - k - 1})\n\nThe Reed-Solomon code space \\mathsf{RS}[\\mathbb{F}, \\mathcal{L}, n] can be viewed from both univariate polynomial and multilinear polynomial perspectives:\\begin{aligned}\n    \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, n] & := \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{g} \\in \\mathbb{F}^{< 2^n}[X] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{g}(x)\\} \\\\\n    & = \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\tilde{f} \\in \\mathbb{F}^{< 2}[X_0, \\ldots, X_{n - 1}] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\tilde{f}(x^{2^0}, x^{2^1},\\ldots, x^{2^{n-1}})\\}\n\\end{aligned}","type":"content","url":"/analysis/basefold-deepfold-whir#from-multilinear-polynomials-to-univariate-polynomials","position":3},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Basefold: Leveraging Sumcheck"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#basefold-leveraging-sumcheck","position":4},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Basefold: Leveraging Sumcheck"},"content":"The Prover wants to prove to the Verifier that the value of an n-variable linear polynomial \\tilde{f}(X_0, \\ldots, X_{n-1}) at a public point \\vec{u} = (u_0, u_1, \\ldots, u_{n - 1}) is v, i.e.,\\tilde{f}(u_0, u_1, \\ldots, u_{n - 1}) = v \\tag{1}\n\nFor the multilinear polynomial \\tilde{f}(X_0, \\ldots, X_{n-1}), it can also be represented by its values on the boolean hypercube \\mathbf{B}^n = \\{0,1\\}^n:\\tilde{f}(X_0, \\ldots, X_{n-1}) = \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, (X_0, \\ldots, X_{n-1}))\n\nwhere \\tilde{eq}(\\vec{b}, (X_0, \\ldots, X_{n-1})) is the Lagrange basis function. The vector \\vec{b} is represented as \\vec{b} = (b_0, \\ldots, b_{n - 1}), so:\\tilde{eq}((b_0, \\ldots , b_{n - 1}), (X_0, \\ldots, X_{n-1})) = \\prod_{i = 0}^{n - 1} (b_i X_i + (1 - b_i)(1 - X_i))\n\nTherefore,\\tilde{f}(u_0, \\ldots, u_{n-1}) = \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, (u_0, \\ldots, u_{n-1}))\n\nProving \\tilde{f}(u_0, u_1, \\ldots, u_{n - 1}) = v can be transformed into proving a sum over the boolean hypercube \\mathbf{B}^n = \\{0,1\\}^n:\\sum_{\\vec{b} \\in \\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, (u_0, \\ldots, u_{n-1})) = v \\tag{2}\n\nThis can be proven using the sumcheck protocol.\n\nFirst, the Prover sends a univariate polynomial:h_{1}(X) = \\sum_{\\vec{b} \\in \\{0,1\\}^{n - 1}} \\tilde{f}(X,\\vec{b}) \\cdot \\tilde{eq}((X,\\vec{b}), (u_0, \\ldots, u_{n-1}))\n\nh_{1}(X) is obtained by replacing the first b_{0} in \\vec{b} = (b_0, \\ldots, b_{n - 1}) from equation (2) with a variable X. Therefore, proving equation (2) is transformed into verifying h_{1}(0) + h_{1}(1) = v. To believe that the h_{1}(X) sent by the Prover is correctly constructed, the Verifier selects a random number \\alpha_1 and sends it to the Prover. The Prover calculates:h_{1}(\\alpha_1) = \\sum_{\\vec{b} \\in \\{0,1\\}^{n - 1}} \\tilde{f}(\\alpha_1, \\vec{b}) \\cdot \\tilde{eq}((\\alpha_1,\\vec{b}), (u_0, \\ldots, u_{n-1})) \\tag{3}\n\nand sends h_1(\\alpha_1) to the Verifier. The Prover needs to prove the correctness of the sent h_1(\\alpha_1) to the Verifier, and equation (3) is again a sum over the boolean hypercube \\mathbf{B}^{n-1} = \\{0,1\\}^{n-1}. Therefore, proving equation (2) is transformed into proving equation (3). Continuing this process, the Verifier selects random numbers \\alpha_2,\\ldots, \\alpha_n in sequence, and finally, it is transformed into proving:h_{n}(\\alpha_{n}) =\\tilde{f}(\\alpha_1, \\ldots, \\alpha_n) \\cdot \\tilde{eq}((\\alpha_1,\\ldots, \\alpha_n), (u_0, \\ldots, u_{n-1}))\n\nAt this point, the Verifier needs to obtain the value of \\tilde{f}(\\alpha_1, \\ldots, \\alpha_n) to verify the above equation. If in the FRI protocol, the univariate polynomial f(X) corresponding to \\tilde{f}(X_0, \\ldots, X_{n-1}) is folded using the same random numbers (\\alpha_1,\\alpha_2,\\ldots, \\alpha_n) in sequence, then folding to the last step will result in a constant f^{(n)}. According to the relationship between multilinear polynomials and univariate polynomials obtained in the previous section, we know:f^{(n)} = \\tilde{f}(\\alpha_1, \\ldots, \\alpha_n)\n\nThis is the idea of the Basefold protocol. It synchronously uses the same random numbers for the sumcheck protocol and the FRI protocol. The last step of the FRI protocol provides the value of \\tilde{f}(\\alpha_1, \\ldots, \\alpha_n) that the last step of the sumcheck protocol wants to obtain. Therefore, it can complete the entire sumcheck protocol, thus proving the correctness of the opening value of a multilinear polynomial at a point, i.e., equation (1). For a more detailed description of the Basefold protocol, please refer to the \n\nBasefold series blog posts. This article only describes the key protocol ideas.","type":"content","url":"/analysis/basefold-deepfold-whir#basefold-leveraging-sumcheck","position":5},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"DeepFold: Introducing DEEP-FRI"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#deepfold-introducing-deep-fri","position":6},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"DeepFold: Introducing DEEP-FRI"},"content":"Since the Basefold protocol in the original paper [ZCF23] only proved soundness under unique decoding, the idea of the DeepFold protocol is to improve this by introducing the DEEP-FRI protocol in the Basefold protocol and proving that it is also secure under list decoding. In the FRI, DEEP-FRI, and STIR protocols, the Verifier needs to repeatedly query the Prover to achieve a certain level of security. If a larger bound can be proved in the security proof, such as reaching from unique decoding to list decoding, then the number of Verifier queries can be greatly reduced, thereby reducing the proof size and Verifier’s computational cost.\n\nIn each round of the FRI protocol, the Prover sends a Merkle commitment of the values of f^{(i)} on a domain \\mathcal{L}_i to the Verifier, i.e., a commitment of the Reed-Solomon encoding of f^{(i)}, denoted as \\vec{v}^{(i)} = f^{(i)}|_{\\mathcal{L}_i}. After receiving the Merkle commitment of \\vec{v}_i, the Verifier will judge whether it is close to the corresponding code space \\mathsf{RS}[\\mathbb{F}, \\mathcal{L}_i, 2^{n - i}]. Now considering the case of list decoding, since the decoding is not unique, in the i-th round, f^{(i)'} is also close to \\vec{v}^{(i)}. A malicious Prover might choose f^{(i)'} for subsequent protocols and still pass the subsequent checks of the FRI protocol, but the constant obtained in the last round of folding would become f^{(n)'}, which is not equal to \\tilde{f}(\\vec{\\alpha}) and cannot provide a correct value in the last round check of the sumcheck protocol. The DEEP method in the DEEP-FRI [BGKS20] protocol limits the malicious Prover to only send the correct f^{(i)} by making the Prover do a little more work, otherwise the Prover cannot pass subsequent checks. This converts list decoding to unique decoding.\n\nThe DEEP method is to randomly select a point \\beta_i from \\mathbb{F} \\setminus \\mathcal{L}_i, requiring the Prover to provide the value of f^{(i)}(\\beta_i) and ensure that this value is indeed equal to f^{(i)}(\\beta_i). This limits the Prover to only choose to send f^{(i)} rather than f^{(i)'}. The remaining problem now is to ensure the correctness of f^{(i)}(\\beta_i). The Verifier can require the Prover to also send f^{(i)}(-\\beta_i), so the Verifier can calculate f^{(i + 1)}(\\beta_i^2) by themselves using these two values. Through the conversion between multilinear polynomials and univariate polynomials, we know:f^{(i + 1)}(\\beta_i^2) = \\tilde{f}(\\alpha_1, \\alpha_2, \\ldots, \\alpha_{i + 1}, \\beta_i^2, \\beta_i^4, \\ldots, \\beta_i^{2^{n - i - 1}})\n\nTherefore, the Verifier can continue to have the Prover provide the value of f^{(i+ 1)}(-\\beta_i^2), and the Verifier calculates f^{(i + 2)}(\\beta_i^4) themselves. This continues until finally the Verifier can calculate f^{(n)}(\\beta_i^{2^{n-i-1}}) themselves, which should equal:f^{(n)}(\\beta_i^{2^{n-i-1}}) = \\tilde{f}(\\alpha_1, \\alpha_2, \\ldots, \\alpha_{i + 1}, \\ldots, \\alpha_{n})\n\nThis value is exactly the value of folding f(X) to the last step in the FRI protocol. The Verifier can check whether the f^{(n)}(\\beta_i^{2^{n-i-1}}) value they calculated is equal to the value from the last step of the FRI protocol, thus verifying the correctness of f^{(i)}(\\pm \\beta_i).\n\nTo summarize the idea of the DeepFold protocol: DeepFold borrows the framework of the BaseFold protocol, replacing the FRI protocol with the DEEP-FRI protocol to reduce the number of Verifier queries. The DeepFold protocol still synchronously performs the sumcheck protocol and the DEEP-FRI protocol. When using the DEEP technique, in each round, the Verifier selects a random number \\beta_i from \\mathbb{F} \\setminus \\mathcal{L}_i, and the Prover sends the following values to the Verifier:f^{(i)}(-\\beta_i),f^{(i + 1)}(- \\beta_i^2), \\ldots, f^{(n - 1)}(-\\beta_i^{2^{n - i - 2}})\n\nThe Verifier calculates f^{(n)}(\\beta_i^{2^{n-i-1}}) using these values to check if it equals the constant from the last step of FRI folding.\n\nFor a more detailed description of the DeepFold protocol, please refer to the blog post \n\nDeepFold Notes: Protocol Overview.","type":"content","url":"/analysis/basefold-deepfold-whir#deepfold-introducing-deep-fri","position":7},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"WHIR: Introducing STIR"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#whir-introducing-stir","position":8},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"WHIR: Introducing STIR"},"content":"For the FRI series protocols (including FRI, DEEP-FRI), the number of Verifier queries affects the proof size and Verifier’s computational cost. The STIR protocol [ACFY24a] is an improvement on the FRI series protocols. By reducing the code rate in each round, it increases redundancy in the encoding, thereby reducing the number of Verifier queries. It’s worth noting that the STIR protocol needs to perform 2 or more folds per round to show its advantages, thus lowering the code rate per round. For a more detailed introduction to the STIR protocol, see the blog post \n\nSTIR: Improving Code Rate to Reduce Query Complexity.\n\nThe WHIR protocol replaces the FRI protocol in the Basefold protocol framework with the STIR protocol, further reducing the number of Verifier queries. In the WHIR paper [ACFY24b], they introduced the concept of CRS (constrained Reed-Solomon code). CRS is actually a subcode of Reed-Solomon encoding. By introducing constraints similar to sumcheck in the CRS definition, the WHIR protocol becomes more general.\n\nDefinition 1 [ACFY24b, Definition 1] For a field \\mathbb{F}, smooth evaluation domain \\mathcal{L} \\subseteq \\mathbb{F}, number of variables n \\in \\mathbb{N}, weight polynomial \\hat{w} \\in \\mathbb{F}[Z, X_1, \\ldots, X_n], and target \\sigma \\in \\mathbb{F}, the constrained Reed-Solomon code is defined as:\\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, n, \\hat{w}, \\sigma] := \\left\\{ f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, n]: \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\hat{w}(\\tilde{f}(\\vec{b}), \\vec{b}) = \\sigma \\right\\}.\n\nFrom the definition, we can see that CRS is first an RS code, i.e., f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, n] in the definition, but on top of this, it needs to satisfy a sumcheck-like sum constraint \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\hat{w}(\\tilde{f}(\\vec{b}), \\vec{b}) = \\sigma.\n\nIf we want to prove \\tilde{f}(u_0, u_1, \\ldots, u_{n - 1}) = v, let the target \\sigma = v in the CRS definition, and the weight polynomial be:\\hat{w}(Z, \\vec{X}) = Z \\cdot \\tilde{eq}(\\vec{X}, \\vec{u})\n\nThen the sumcheck-like constraint in the CRS definition becomes:\\sum_{\\vec{b} \\in \\{0,1\\}^n} \\hat{w}(\\tilde{f}(\\vec{b}), \\vec{b}) = \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, \\vec{u}) = v\n\nAnd \\sum_{\\vec{b} \\in \\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, \\vec{u}) = \\tilde{f}(u_0, u_1, \\ldots, u_{n - 1}), so the sumcheck-like constraint above is exactly \\tilde{f}(u_0, u_1, \\ldots, u_{n - 1}) = v. Therefore, the WHIR protocol for CRS can be used as a commitment scheme for multilinear polynomials.\n\nThe overall framework of the WHIR protocol is still the same as the Basefold protocol, synchronously using the same random numbers for the sumcheck protocol and the STIR protocol. Since the STIR protocol needs to perform 2 or more folds per round to be more advantageous than the FRI series protocols, in the following protocol description, 2^k folds are performed on the univariate polynomial in each round.\n\nLet’s look deeper into one iteration of the WHIR protocol (from [ACFYb, 2.1.3 WHIR protocol]) to see how WHIR specifically combines BaseFold with the STIR protocol. After one iteration, the problem of testing the proximity of f \\in \\mathcal{C} := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, n, \\hat{w}, \\sigma] is transformed into testing f' \\in \\mathcal{C}' := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, n - k, \\hat{w}', \\sigma'], where the size of \\mathcal{L}^{(2)} is only half that of \\mathcal{L}. Let the code rate of \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, n, \\hat{w}, \\sigma] be \\rho, then:\\rho = \\frac{2^n}{|\\mathcal{L}|}\n\nAfter one iteration, the code rate of \\mathcal{C}' = \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, n - k, \\hat{w}', \\sigma'] is:\\rho' = \\frac{2^{n - k}}{|\\mathcal{L}^{(2)}|} = \\frac{2^{n - k}}{\\frac{|\\mathcal{L}|}{2}} = \\frac{2^{n - k + 1}}{|\\mathcal{L}|} = 2^{1 - k} \\cdot \\rho = \\left(\\frac{1}{2}\\right)^{k - 1} \\cdot \\rho\n\nWhen k \\ge 2, we can see that \\rho' is smaller than \\rho, the code rate decreases, which is the core idea of the STIR protocol. Although f is folded 2^k times in one iteration, the evaluation domain \\mathcal{L} is only reduced by half each time, rather than shrinking by 2^k times. This way, by increasing the evaluation domain, the code rate is reduced, thereby reducing the number of Verifier queries.\n\nOne iteration of the WHIR protocol is shown in the following diagram.\n\nSumcheck rounds. The Prover and Verifier interact for k rounds of Sumcheck for the constraint in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, n, \\hat{w}, \\sigma]:\\sum_{\\mathbf{b} \\in \\{0,1\\}^n} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma\n\nwhere \\tilde{f} is the multilinear polynomial corresponding to f.\n\na. The Prover sends a univariate polynomial \\hat{h}_1(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{n-1}} \\hat{w}(\\tilde{f}(X, \\mathbf{b}), X, \\mathbf{b}) to the Verifier. The Verifier checks \\hat{h}_1(0) + \\hat{h}_1(1) = \\sigma, selects a random number \\alpha_1 \\leftarrow \\mathbb{F} and sends it. The sumcheck claim becomes \\hat{h}_1(\\alpha_1) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{n-1}} \\hat{w}(\\tilde{f}(\\alpha_1, \\mathbf{b}), \\alpha_1, \\mathbf{b}).\nb. For the i-th round, i from 2 to k, the Prover sends a univariate polynomial:\\hat{h}_i(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{n-i}} \\hat{w}(\\tilde{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b})\n\nThe Verifier checks \\hat{h}_{i}(0) + \\hat{h}_{i}(1) = \\hat{h}_{i-1}(\\alpha_{i-1}), selects a random number \\alpha_i \\leftarrow \\mathbb{F}. The sumcheck claim becomes:\\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-i}} \\hat{w}(\\tilde{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}) = \\hat{h}_i(\\alpha_i)\n\nThus, after k rounds of sumcheck, the Prover has sent polynomials (\\hat{h}_1, \\ldots, \\hat{h}_k), and the Verifier has selected random numbers \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k) \\in \\mathbb{F}^k. The initial claim has become the following statement:\\sum_{\\mathbf{b} \\in \\{0,1\\}^{n-k}} \\hat{w}(\\tilde{f}(\\boldsymbol{\\alpha}, \\mathbf{b}), \\boldsymbol{\\alpha}, \\mathbf{b}) = \\hat{h}_k(\\alpha_k)\n\nSend folded function. The Prover sends a function g: \\mathcal{L}^{(2)} \\rightarrow \\mathbb{F}. In the case of an honest Prover, \\hat{g} \\equiv \\tilde{f}(\\boldsymbol{\\alpha}, \\cdot), and g is defined as the evaluation of \\hat{g} on the domain \\mathcal{L}^{(2)}.\n\nThis means first folding \\tilde{f} 2^k times using random numbers \\boldsymbol{\\alpha} to get \\hat{g} = \\tilde{f}(\\boldsymbol{\\alpha}, \\cdot). At this point, \\hat{g} : \\mathcal{L}^{(2^k)} \\rightarrow \\mathbb{F}, with its domain range being \\mathcal{L}^{(2^k)}. Since \\hat{g} is essentially a polynomial, we can change its variable’s domain to \\mathcal{L}^{(2)}. The function g is consistent with the evaluation of \\hat{g} on \\mathcal{L}^{(2)}.\n\nOut-of-domain sample. The Verifier selects a random number z_0 \\leftarrow \\mathbb{F} and sends it to the Prover. Let \\boldsymbol{z}_0 := (z_0^{2^0}, \\ldots, z_0^{2^{n-k - 1}}).\n\nOut-of-domain answers. The Prover sends y_0 \\in \\mathbb{F}. In the honest case, y_0 := \\hat{g}(\\boldsymbol{z}_0).\n\nShift queries and combination randomness. For the Verifier, for each i \\in [t], select a random number z_i \\leftarrow \\mathcal{L}^{(2^k)} and send it. By querying f, obtain y_i := \\mathrm{Fold}(f, \\boldsymbol{\\alpha})(z_i). Let \\boldsymbol{z}_i := (z_i^{2^0}, \\ldots, z_i^{2^{n- k - 1}}). The Verifier also selects a random number \\gamma \\leftarrow \\mathbb{F} and sends it.\n\nRecursive claim. The Prover and Verifier define a new weight polynomial and target value:\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\\sigma' := \\hat{h}_k(\\alpha_k) + \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i,\n\nThen, recursively test g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, n - k, \\hat{w}', \\sigma'].\n\nThe definition of the new weight polynomial \\hat{w}' is:\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\n\nIt consists of two parts:\n\nThe first part \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) constrains the correctness of k rounds of sumcheck in step 1 of the protocol.\n\nThe second part Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X}) constrains that the value of g at \\boldsymbol{z}_i is correct, and uses a random number \\gamma to linearly combine these t + 1 constraints.\na. The constraint g(\\boldsymbol{z}_0) = y_0 is actually verifying the correctness of out-of-domain answers.\nb. For i \\in [t], the constraint g(\\boldsymbol{z}_i) = y_i requires the correctness of shift queries.\n\nThis also shows the flexibility of the weight polynomial definition, which can achieve multiple constraints at once.\n\nIn the steps of one iteration described above, step 1 is k rounds of the sumcheck protocol, generating k random numbers \\alpha_1, \\ldots, \\alpha_k. Then steps 2-5 perform the STIR protocol using the same random numbers \\alpha_1, \\ldots, \\alpha_k. Finally, step 6 redefines the weight polynomial and target value to prepare for the next iteration.","type":"content","url":"/analysis/basefold-deepfold-whir#whir-introducing-stir","position":9},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Efficiency Comparison"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#efficiency-comparison","position":10},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Efficiency Comparison"},"content":"For the Basefold, DeepFold, and WHIR protocols based on FRI, DEEP-FRI, and STIR protocols respectively, their efficiency is closely related to the number of Verifier queries, which mainly affects the proof size and Verifier’s computational cost. The number of queries in these protocols is mainly determined by their soundness proofs.\n\nFor a potentially malicious Prover P^*, assume that the univariate function f(X) corresponding to the initially provided \\tilde{f} is \\delta > \\Delta far from the Reed Solomon code space \\mathsf{RS}[\\mathbb{F}, \\mathcal{L}, n], where \\Delta < \\Delta^*, and \\Delta^* is a bound that maintains distance to the corresponding code space during the folding process. If the number of Verifier queries in the protocol is l, then the probability that P^* can pass the protocol check is approximately no more than:(1 - \\Delta)^l\n\nThis probability is also called the soundness error.\n\nUnder the premise of requiring the entire protocol to achieve \\lambda-bit security, that is, requiring the soundness error to be less than 2^{-\\lambda}, we have:(1 - \\Delta)^l < 2^{-\\lambda}\n\nTaking the logarithm of both sides, we can get:l > \\frac{\\lambda}{- \\log_2(1 - \\Delta)}\n\nWhen \\Delta can take a larger value, the number of queries l can be smaller while still achieving \\lambda-bit security.\n\nIn the soundness proofs of these protocols, they all hope to increase the bound that \\Delta can take as much as possible. The bound that \\Delta can take is closely related to the unique decoding and list decoding of Reed Solomon codes. The common bounds, sorted from small to large, are:\n\nunique decoding bound: (1 - \\rho)/2\n\nJohnson bound: 1 - \\sqrt{\\rho}\n\nlist decoding bound: 1 - \\rho\n\nThe 2nd and 3rd items both enter the list decoding range.\n\nIn the original Basefold paper [ZCF23], its soundness was only proven to a maximum \\Delta of (1 - \\rho)/2. Subsequently, Ulrich Haböck in [H24] proved that the Basefold protocol can reach the Johnson bound 1 - \\sqrt{\\rho} for Reed Solomon codes. Hadas Zeilberger in the Khatam [Z24] paper proved that the Basefold protocol can reach 1 - \\rho^{\\frac{1}{3}} for general linear codes.\n\nFor the DeepFold protocol, in the original paper [GLHQTZ24], based on a List Decodability conjecture for Reed Solomon codes ([GLHQTZ24] Conjecture 1), it was proven that it can reach the bound of 1 - \\rho.\n\nFor the WHIR protocol, in the original paper [ACFY24b], the discussed soundness error is not about achieving \\lambda-bit security for the entire protocol, but discusses a stronger round-by-round soundness error, meaning that the Verifier needs to repeat queries to achieve \\lambda-bit security in each round. Under this premise, it was proven that the achievable bound is (1 - \\rho)/2, and it was conjectured that it could reach the Johnson bound 1 - \\sqrt{\\rho}. Ulrich Haböck in [H24] also pointed out that applying the method in his paper should be able to prove that the WHIR protocol can reach the Johnson bound.\n\nTo summarize the soundness proofs of these three protocols:\n\nProtocol\n\nSoundness\n\nOriginal Paper\n\nKhatam [Z24]\n\n[H24]\n\nBaseFold\n\nsoundness\n\n(1 - \\rho)/2\n\n1 - \\rho^{\\frac{1}{3}}\n\n1 - \\sqrt{\\rho} (only for RS codes)\n\nDeepFold\n\nsoundness\n\n1 - \\rho (based on list decoding conjecture)\n\n\n\n\n\nWHIR\n\nround-by-round soundness (stronger)\n\n(1 - \\rho)/2, conjectured to reach 1 - \\sqrt{\\rho}\n\n\n\nProvided proof idea, can reach 1 - \\sqrt{\\rho}\n\nThe DeepFold paper [GLHQTZ24] has already compared the BaseFold protocol with the DeepFold protocol. However, the bound that the BaseFold protocol could reach was the unique decoding bound (1 - \\rho)/2, not the Johnson bound 1 - \\sqrt{\\rho} that has now been proven in [H24]. Here, we borrow the analysis from the DeepFold paper [GLHQTZ24] to compare the BaseFold protocol with the DeepFold protocol.\n\nScheme\n\nSoundness\n\nSoundness bound\n\nCommit\n\nEvaluate\n\nVerify\n\nProof Size\n\nBaseFold\n\nsoundness\n\n(1 - \\rho)/2\n\nO(N \\log N) ~ \\mathbb{F} + O(N) ~ \\mathbb{H}\n\nO(N) ~ \\mathbb{H}\n\nO(s_{U} \\log^2N) ~ \\mathbb{H}\n\nO(s_{U} \\log^2N)\n\nBaseFold\n\nsoundness\n\n1 - \\sqrt{\\rho}\n\nO(N \\log N) ~ \\mathbb{F} + O(N) ~ \\mathbb{H}\n\nO(N) ~ \\mathbb{H}\n\nO(s_{J} \\log^2N) ~ \\mathbb{H}\n\nO(s_{J} \\log^2N)\n\nDeepFold\n\nsoundness\n\n1 - \\rho\n\nO(N \\log N) ~ \\mathbb{F} + O(N) ~ \\mathbb{H}\n\nO(N) ~ \\mathbb{H}\n\nO(s_{L} \\log^2N) ~ \\mathbb{H}\n\nO(s_{L} \\log^2N)\n\nIn the table above, N = 2^n, and s_{U}, s_{J}, and s_L represent the number of Verifier queries under the unique decoding bound, Johnson bound, and list decoding bound, respectively. According to the formula for calculating the number of queries derived earlier:l > \\frac{\\lambda}{- \\log_2(1 - \\Delta)}\n\nSubstituting \\Delta < (1 - \\rho)/2, \\Delta < 1 - \\sqrt{\\rho}, and \\Delta < 1 - \\rho, we can calculate s_U, s_J, and s_L as:s_U = \\frac{\\lambda}{-\\log_2 (\\frac{1 + \\rho}{2})}, \\qquad s_J = \\frac{\\lambda}{-\\log_2 \\sqrt{\\rho}}, \\qquad s_L = \\frac{\\lambda}{-\\log_2 \\rho}\n\nTaking different values of \\lambda and \\rho, and rounding up the calculated results, we get:\n\nNumber of queries\n\n\\lambda = 100, \\rho = 1/2\n\n\\lambda = 100, \\rho = 1/4\n\n\\lambda = 100, \\rho = 1/8\n\n\\lambda = 128, \\rho = 1/8\n\ns_U\n\n241\n\n148\n\n121\n\n155\n\ns_J\n\n200\n\n100\n\n67\n\n86\n\ns_L\n\n100\n\n50\n\n34\n\n43\n\nFrom the above table, we can find that:\n\nThe larger the achievable bound, the fewer the number of queries.\n\nThe smaller the code rate, the fewer the number of queries.\n\nThe higher the required security, the more queries are needed.\n\nFrom this, we can conclude that because the DeepFold protocol can reach a larger soundness bound, it requires fewer queries compared to the BaseFold protocol, resulting in smaller proof size and less Verifier computation. In terms of Prover computation, there is not much difference between these two protocols. The DeepFold protocol, due to its use of the DEEP-FRI protocol, will have slightly more Prover computation than the BaseFold protocol.\n\nWhen performing 2 or more folds, the STIR protocol has fewer queries compared to the FRI and DEEP-FRI protocols. According to the conclusions in the WHIR paper [ACFY24b], the query complexity and Verifier computational complexity comparison between the BaseFold protocol and the WHIR protocol is shown in the following table.\n\nProtocol\n\nSoundness\n\nQueries\n\nVerifier Time\n\nAlphabet\n\nBaseFold\n\nround-by-round soundness (stronger)\n\nq_{\\mathsf{BF}} := O_{\\rho}(\\lambda \\cdot n)\n\nO_{\\rho}(q_{\\mathsf{BF}})\n\n\\mathbb{F}^2\n\nWHIR\n\nround-by-round soundness (stronger)\n\nq_{\\mathsf{WHIR}} := O_{\\rho}(\\lambda + \\frac{\\lambda}{k} \\cdot \\log \\frac{n}{k})\n\nO_{\\rho}(q_{\\mathsf{WHIR}} \\cdot (2^k + n))\n\n\\mathbb{F}^{2^k}\n\nIn the WHIR protocol, the original univariate polynomial is folded 2^k times in each round, so the Verifier queries from \\mathbb{F}^{2^k}. In the BaseFold protocol, only 2 folds are performed in one round, so the Verifier queries from \\mathbb{F}^2.\n\nWhen k > 1, we can see that in the WHIR protocol, the number of Verifier queries is logarithmic in n/k, while in the BaseFold protocol, it is linear in n. Therefore, the WHIR protocol has fewer query complexities compared to the BaseFold protocol, and its Verifier computation is also smaller. For the DeepFold protocol, which uses the DEEP-FRI protocol, the number of queries is the same as the BaseFold protocol, linear in n, so the WHIR protocol also has fewer query complexities compared to the DeepFold protocol.\n\nTo summarize the efficiency of these three protocols in terms of Verifier query count:\n\nBecause the soundness proof of the DeepFold protocol has been proven to the bound of 1 - \\rho, its query count is smaller than that of the BaseFold protocol.\n\nBecause the WHIR protocol uses the STIR protocol, which has lower query complexity compared to both the FRI and DEEP-FRI protocols, the WHIR protocol has fewer queries compared to both the BaseFold and DeepFold protocols.\n\nIf we consider all three protocols under round-by-round soundness error, all reaching the Johnson bound 1 - \\sqrt{\\rho}, theoretically, when performing 2 or more folds, the WHIR protocol would be more advantageous than the BaseFold and DeepFold protocols in terms of Verifier computation and proof size.","type":"content","url":"/analysis/basefold-deepfold-whir#efficiency-comparison","position":11},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Summary"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#summary","position":12},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"Summary"},"content":"To summarize the BaseFold, DeepFold, and WHIR protocols, all three protocols follow the framework of the BaseFold protocol, synchronously performing the sumcheck protocol and FRI/DEEP-FRI/STIR protocol using the same random numbers. The main differences between them come from the differences between the FRI, DEEP-FRI, and STIR protocols.\n\nComparing the efficiency of these three protocols, the difference in Prover computation is not particularly noticeable. The main factor is the number of Verifier queries - more queries result in higher Verifier computation and larger proof size. Since the STIR protocol theoretically has lower query complexity than the FRI and DEEP-FRI protocols, the WHIR protocol has fewer queries compared to the BaseFold and DeepFold protocols.\n\nOn the other hand, the number of Verifier queries is related to the bound that can be achieved in the protocol’s soundness proof. According to current research progress:\n\nThe DeepFold protocol based on the DEEP-FRI protocol, under a simple conjecture, can reach the optimal bound of 1 - \\rho. If the FRI protocol wants to reach the 1 - \\rho bound, it would be based on a stronger conjecture (see [BCIKS20] Conjecture 8.4).\n\nThe BaseFold protocol can reach the Johnson bound 1 - \\sqrt{\\rho} for Reed Solomon codes.\n\nThe WHIR protocol was only proven to reach (1 - \\rho)/2 in the original paper, but according to the method in [H24], it is expected to be proven to reach the Johnson bound 1 - \\sqrt{\\rho}.","type":"content","url":"/analysis/basefold-deepfold-whir#summary","position":13},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"References"},"type":"lvl2","url":"/analysis/basefold-deepfold-whir#references","position":14},{"hierarchy":{"lvl1":"BaseFold vs DeepFold vs WHIR","lvl2":"References"},"content":"[ACFY24a] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[ACFY24b] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[BGKS20] Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. “DEEP-FRI: sampling outside the box improves soundness.” arXiv preprint arXiv:1903.12243 (2019).\n\n[GLHQTZ24] Yanpei Guo, Xuanming Liu, Kexi Huang, Wenjie Qu, Tianyang Tao, and Jiaheng Zhang. “DeepFold: Efficient Multilinear Polynomial Commitment from Reed-Solomon Code and Its Application to Zero-knowledge Proofs.” Cryptology ePrint Archive (2024).\n\n[H24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).\n\n[Z24] Zeilberger, Hadas. “Khatam: Reducing the Communication Complexity of Code-Based SNARKs.” Cryptology ePrint Archive (2024).\n\n[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.","type":"content","url":"/analysis/basefold-deepfold-whir#references","position":15},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm"},"type":"lvl1","url":"/analysis/gemini-analysis","position":0},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io","type":"content","url":"/analysis/gemini-analysis","position":1},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl3":"Optimized Version 1"},"type":"lvl3","url":"/analysis/gemini-analysis#optimized-version-1","position":2},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl3":"Optimized Version 1"},"content":"Protocol document: \n\nGemini-PCS (Part III)\n\nCorresponding Python code: \n\nbcho_pcs.py\n\nThe following protocol proves that a multilinear extension (MLE) polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) evaluates to v = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) at point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}). Here \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) is expressed in coefficient form as:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{n-1} f_i\\cdot X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}","type":"content","url":"/analysis/gemini-analysis#optimized-version-1","position":3},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Common Input","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#common-input","position":4},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Common Input","lvl3":"Optimized Version 1"},"content":"Commitment to the vector \\vec{f}=(f_0, f_1, \\ldots, f_{n-1}) as C_fC_f = \\mathsf{KZG10.Commit}(\\vec{f})\n\nEvaluation point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1})","type":"content","url":"/analysis/gemini-analysis#common-input","position":5},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Witness","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#witness","position":6},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Witness","lvl3":"Optimized Version 1"},"content":"Coefficients of polynomial f(X): f_0, f_1, \\ldots, f_{n-1}","type":"content","url":"/analysis/gemini-analysis#witness","position":7},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 1","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#round-1","position":8},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 1","lvl3":"Optimized Version 1"},"content":"Prover computes h_1(X), h_2(X), \\ldots, h_{n-1}(X) such that:h_{i+1}(X^2) = \\frac{h_i(X) + h_i(-X)}{2} + u_i\\cdot \\frac{h_i(X) - h_i(-X)}{2X}\n\nwhere h_0(X) = f(X)\n\nProver computes commitments (H_1, H_2, \\ldots, H_{n-1}) such that:H_{i+1} = \\mathsf{KZG10.Commit}(h_{i+1}(X))\n\nProver sends (H_1, H_2, \\ldots, H_{n-1})\n\nProver:\n\nFor i = 1, \\ldots, n-1, compute polynomial h_i using the formula:> h_{i}(X) = h_e^{(i-1)}(X) + u_{i-1} \\cdot h_o^{(i-1)}(X)\n>\n\n💡 The prover doesn’t need to compute and send h_n(X) because the final polynomial is a constant polynomial equal to the evaluation result v, which the Verifier can verify by querying h_{n-1}(X).\n\nTo compute h_i(X), we use the above formula. The coefficients of h_{i-1}(X) are known, and h_e^{(i-1)}(X) and h_o^{(i-1)}(X) are the even and odd coefficients of h_{i-1}(X) respectively. The main complexity lies in computing u_{i-1} \\cdot h_o^{(i-1)}(X), which involves multiplication in the finite field. h_{i-1}(X) has 2^{n-(i-1)} coefficients, so h_o^{(i-1)}(X) has 2^{n-i} coefficients. Thus, the complexity for multiplying u_{i-1} with these coefficients is 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the complexity for computing h_1(X), \\ldots, h_{n-1}(X) is:> \\sum_{i=1}^{n-1} 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}} = (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n>\n\nThe complexity for computing H_1 = [h_{1}(\\tau)]_1, \\ldots, H_{n-1} = [h_{n-1}(\\tau)]_1 is:> \\sum_{i=1}^{n-1} \\mathsf{msm}(2^{n-i}, \\mathbb{G}_1) = \\sum_{i=1}^{n-1} \\mathsf{msm}(2^i, \\mathbb{G}_1)\n>\n\nThus, the total computational complexity for this round is:> (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=1}^{n-1} \\mathsf{msm}(2^i, \\mathbb{G}_1)\n>","type":"content","url":"/analysis/gemini-analysis#round-1","position":9},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 2","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#round-2","position":10},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 2","lvl3":"Optimized Version 1"},"content":"Verifier sends random point \\beta \\in \\mathbb{F}_p\n\nProver computes h_0(\\beta), h_1(\\beta), \\ldots, h_{n-1}(\\beta)\n\nProver computes h_0(-\\beta), h_1(-\\beta), \\ldots, h_{n-1}(-\\beta)\n\nProver computes h_0(\\beta^2)\n\nProver sends \\{h_i(\\beta), h_i(-\\beta)\\}_{i=0}^{n-1} and h_0(\\beta^2)\n\nProver:\n\nComputing \\beta^2 has a complexity of \\mathbb{F}_{\\mathsf{mul}}, while computing -\\beta only involves addition/subtraction in the finite field, which we don’t count.\n\nUsing Horner’s method to evaluate a polynomial at a point requires as many field multiplications as there are coefficients. Thus, computing \\{h_i(\\beta), h_i(-\\beta)\\}_{i=0}^{n-1} has a complexity of:2 \\sum_{i=0}^{n-1} 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}} = (2^{n+1} - 4) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing h_0(\\beta^2) has a complexity of 2^n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity for this round is:\\mathbb{F}_{\\mathsf{mul}} + (2^{n+1} - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + 2^n ~ \\mathbb{F}_{\\mathsf{mul}} = (3 \\cdot 2^n - 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\n🎈 In the code, the Prover also computes \\{h_i(\\beta^2)\\}_{i=1}^{n-1}, which isn’t necessary and could save computation. In that case, the Verifier would need to compute these values during verification, increasing its computational load.# Compute evaluations of h_i(X) at beta, -beta, beta^2\nevals_pos = []\nevals_neg = []    \nevals_sq = []\nfor i in range(k):\n    poly = h_poly_vec[i]\n    poly_at_beta = UniPolynomial.evaluate_at_point(poly, beta)\n    poly_at_neg_beta = UniPolynomial.evaluate_at_point(poly, -beta)\n    poly_at_beta_sq = UniPolynomial.evaluate_at_point(poly, beta * beta)\n    evals_pos.append(poly_at_beta)\n    evals_neg.append(poly_at_neg_beta)\n    evals_sq.append(poly_at_beta_sq)","type":"content","url":"/analysis/gemini-analysis#round-2","position":11},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 3","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#round-3","position":12},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 3","lvl3":"Optimized Version 1"},"content":"Verifier sends random value \\gamma \\in \\mathbb{F}_p to aggregate multiple polynomials\n\nProver computes h(X):h(X) = h_0(X) + \\gamma \\cdot h_1(X) + \\gamma^2 \\cdot h_2(X) + \\cdots + \\gamma^{n-1} \\cdot h_{n-1}(X)\n\nDefine a new Domain D containing 3 elements:D = \\{\\beta, -\\beta, \\beta^2\\}\n\nProver computes a quadratic polynomial h^*(X) that interpolates h(X) at the points in D:h^*(X) = h(\\beta) \\cdot \\frac{(X+\\beta)(X-\\beta^2)}{2\\beta(\\beta-\\beta^2)} + h(-\\beta) \\cdot \\frac{(X-\\beta)(X-\\beta^2)}{2\\beta(\\beta^2+\\beta)} + h(\\beta^2) \\cdot \\frac{X^2-\\beta^2}{\\beta^4-\\beta^2}\n\nProver computes quotient polynomial q(X):q(X) = \\frac{h(X) - h^*(X)}{(X^2-\\beta^2)(X-\\beta^2)}\n\nProver computes commitment to q(X):C_q = \\mathsf{KZG10.Commit}(q(X))\n\nProver sends C_q\n\nProver:\n\nFirst, compute \\gamma^2, \\ldots, \\gamma^{n-1}, with complexity (n-2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing h(X) mainly involves multiplying \\gamma^i with the coefficients of h_i(X), with complexity:\\sum_{i=1}^{n-1} 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}} = (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing h^*(X):h^*(X) = h(\\beta) \\cdot \\frac{(X+\\beta)(X-\\beta^2)}{2\\beta(\\beta-\\beta^2)} + h(-\\beta) \\cdot \\frac{(X-\\beta)(X-\\beta^2)}{2\\beta(\\beta^2+\\beta)} + h(\\beta^2) \\cdot \\frac{X^2-\\beta^2}{\\beta^4-\\beta^2}\n\nIf computed as per the above expression:\n\nComputing \\beta^4 has complexity \\mathbb{F}_{\\mathsf{mul}}\n\nComputing denominators 2\\beta(\\beta-\\beta^2), 2\\beta(\\beta^2+\\beta), and \\beta^4-\\beta^2 has complexity 2 ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing inverses of these denominators has complexity 3 ~ \\mathbb{F}_{\\mathsf{inv}}\n\nMultiplying these inverses with h(\\beta), h(-\\beta), and h(\\beta^2) has complexity 3 ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe three numerator polynomials can be directly expanded:\\begin{aligned}\n        & X^2 + (\\beta - \\beta^2) X - \\beta^3 \\\\\n        & X^2 - (\\beta + \\beta^2) X + \\beta^3 \\\\\n        & X^2 - \\beta^2\n    \\end{aligned}\n\nComputing \\beta^3 has complexity \\mathbb{F}_{\\mathsf{mul}}.\n\nMultiplying these polynomials with the previously calculated coefficients has complexity2 ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~\\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{mul}} = 5 ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThus, the total complexity for computing h^*(X) is:(1 + 2 + 3 + 1 + 5) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} = 12 ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}}\n\n🎈 The code uses Barycentric interpolation for the three points (\\beta, h(\\beta)), (-\\beta, h(-\\beta)), and (\\beta^2, h(\\beta^2)).\n\nAnalyze the complexity of this interpolation, which involves polynomial multiplication and division.\n\nComputing quotient polynomial q(X) involves multiplying three linear polynomials for the denominator, and dividing h(X) - h^*(X) by this result. The complexity is:\\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) + \\mathsf{polydiv}(2^n - 1, 3)\n\nComputing C_q: The degree of q(X) is \\deg(q) = 2^n - 1 - 3 = 2^n - 4, so the complexity for computing C_q is:\\mathsf{msm}(2^n - 3, \\mathbb{G}_1)\n\nTherefore, the total complexity for this round is:\\begin{aligned}\n    & (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + 12 ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}}\\\\\n    &  + \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) +  \\mathsf{polydiv}(2^n - 1, 3) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) \\\\\n    = & (2^{n} + n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    &  + \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) +  \\mathsf{polydiv}(2^n - 1, 3) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) \\\\\n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#round-3","position":13},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 4","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#round-4","position":14},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 4","lvl3":"Optimized Version 1"},"content":"Verifier sends random point \\zeta \\in \\mathbb{F}_p\n\nProver computes linearization polynomial r(X) such that r(\\zeta) = 0:r(X) = h(X) - h^*(\\zeta) - (\\zeta^2-\\beta^2)(\\zeta-\\beta^2) \\cdot q(X)\n\nProver computes quotient polynomial w(X):w(X) = \\frac{r(X)}{(X-\\zeta)}\n\nProver computes commitment to w(X):C_w = \\mathsf{KZG10.Commit}(w(X))\n\nProver sends C_w\n\nProver:\n\nComputing \\zeta^2 has complexity \\mathbb{F}_{\\mathsf{mul}}\n\nComputing r(X):\n\nComputing h^*(\\zeta): Since \\deg(h^*) = 2, evaluating at a point has complexity 3 ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing (\\zeta^2-\\beta^2)(\\zeta-\\beta^2) has complexity \\mathbb{F}_{\\mathsf{mul}}\n\nComputing (\\zeta^2-\\beta^2)(\\zeta-\\beta^2) \\cdot q(X): With \\deg(q) = 2^n - 4, this has complexity \\mathsf{polymul}(0, 2^n - 4)\n\nThus, the total complexity for computing r(X) is:4 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^n - 4)\n\nComputing quotient polynomial w(X) using linear division: Since \\deg(r) = 2^n - 1, this has complexity (2^n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing C_w has complexity \\mathsf{msm}(2^n - 1, \\mathbb{G}_1).\n\nThe total complexity for this round is:\\begin{aligned}\n    & \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^n - 4) + (2^n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    = & (2^n + 4) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^n - 4) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#round-4","position":15},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Total Prover Complexity","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#total-prover-complexity","position":16},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Total Prover Complexity","lvl3":"Optimized Version 1"},"content":"Combining all the rounds:\\begin{aligned}\n    & (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) \\\\\n    & + (3 \\cdot 2^{n} - 3) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (2^{n} + n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    &  + \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) +  \\mathsf{polydiv}(2^n - 1, 3) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) \\\\\n    & + (2^n + 4) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^n - 4) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    = & (6 \\cdot 2^{n} + n + 7) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & +  \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) + \\mathsf{polymul}(0, 2^n - 4) +  \\mathsf{polydiv}(2^n - 1, 3) \\\\\n    & + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\\\\\n\\end{aligned}\n\nUsing polynomial long division:\\mathsf{polydiv}(N, k) = (N - k + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + (kN - k^2 + k) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nWe get:\\begin{align}\n\\mathsf{polydiv}(2^n - 1, 3)  & = (N - 1 - 3 + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + (3(N - 1) - 3^2 + 3) ~ \\mathbb{F}_{\\mathsf{mul}}  \\\\\n & = (N - 3) ~ \\mathbb{F}_{\\mathsf{inv}} + (3N - 9) ~ \\mathbb{F}_{\\mathsf{mul}} \n\\end{align}\n\nImportant\n\n\n\nThere should be a more efficient implementation for division.\n\nTherefore, the complexity is:\\begin{align}\n& (6 \\cdot 2^{n} + n + 7) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & +  \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) + \\mathsf{polymul}(0, 2^n - 4) +  \\mathsf{polydiv}(2^n - 1, 3) \\\\\n    & + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n= & (6 N + n + 7) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & +  4 ~ \\mathbb{F}_{\\mathsf{mul}} + 6 ~ \\mathbb{F}_{\\mathsf{mul}} + (N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}+  (N - 3) ~ \\mathbb{F}_{\\mathsf{inv}} + (3N - 9) ~ \\mathbb{F}_{\\mathsf{mul}}  \\\\\n    & + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n=  & (10 N + n + 5) ~ \\mathbb{F}_{\\mathsf{mul}} + N ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(N - 3, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1) \n\\end{align}","type":"content","url":"/analysis/gemini-analysis#total-prover-complexity","position":17},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Proof Representation","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#proof-representation","position":18},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Proof Representation","lvl3":"Optimized Version 1"},"content":"The proof consists of n+1 elements in \\mathbb{G}_1 and 2n+1 elements in \\mathbb{F}_p:\\pi=\\Big(H_1, H_2, \\ldots, H_{n-1}, C_q, C_w, \\{h_i(\\beta), h_i(-\\beta)\\}_{i=0}^{n-1}, h_0(\\beta^2) \\Big)\n\nProof size:(n + 1)~\\mathbb{G}_1 + (2n + 1) ~ \\mathbb{F}_p","type":"content","url":"/analysis/gemini-analysis#proof-representation","position":19},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verification","lvl3":"Optimized Version 1"},"type":"lvl4","url":"/analysis/gemini-analysis#verification","position":20},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verification","lvl3":"Optimized Version 1"},"content":"Verifier computes (h_1(\\beta^2), h_2(\\beta^2), \\ldots, h_{n-1}(\\beta^2)):h_{i+1}(\\beta^2) = \\frac{h_i(\\beta) + h_i(-\\beta)}{2} + u_i\\cdot \\frac{h_i(\\beta) - h_i(-\\beta)}{2\\beta}\n\nVerifier checks if h_n(\\beta^2) equals the claimed evaluation v = \\tilde{f}(\\vec{u}):h_n(\\beta^2) \\overset{?}{=} v\n\nVerifier computes commitment to h(X):C_h = C_f + \\gamma\\cdot H_1 + \\gamma^2\\cdot H_2 + \\cdots + \\gamma^{n-1}\\cdot H_{n-1}\n\nVerifier computes commitment to r_\\zeta(X):C_r = C_h - h^*(\\zeta)\\cdot[1]_1 - (\\zeta^2-\\beta^2)(\\zeta-\\beta^2)\\cdot C_q\n\nVerifier checks if C_w is a valid evaluation proof for C_r at X=\\zeta:\\mathsf{KZG10.Verify}(C_r, \\zeta, 0, C_w) \\overset{?}{=} 1\n\nOr directly expanded as a Pairing check:e\\Big(C_r + \\zeta\\cdot C_w, [1]_2\\Big) \\overset{?}{=} e\\Big(C_w, [\\tau]_2 \\Big)\n\nVerifier:\n\nComputing h_1(\\beta^2), \\ldots, h_{n-1}(\\beta^2):\n\nComputing \\beta^2 has complexity \\mathbb{F}_{\\mathsf{mul}}\n\nFor each h_{i+1}(\\beta^2), computing the inverses of 2 and 2\\beta and then multiplying with the numerators has complexity 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}}. Multiplying the second term with u_i adds another \\mathbb{F}_{\\mathsf{mul}}, for a total of 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 3 ~ \\mathbb{F}_{\\mathsf{mul}} per item, for n-1 items.\n\nThe total complexity for this step is:\\mathbb{F}_{\\mathsf{mul}} + (2n - 2) ~ \\mathbb{F}_{\\mathsf{inv}} + (3n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} = (3n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n - 2) ~ \\mathbb{F}_{\\mathsf{inv}}\n\nComputing C_h:\n\nComputing \\gamma^2, \\ldots, \\gamma^{n-1} has complexity (n-2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing \\gamma \\cdot H_1, \\ldots, \\gamma^{n-1} \\cdot H_{n-1} has complexity (n-1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nAdding n points in \\mathbb{G}_1 to get C_h has complexity (n-1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nThe total complexity for this step is:(n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (n - 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}","type":"content","url":"/analysis/gemini-analysis#verification","position":21},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl2":"3. Computation of C_r"},"type":"lvl2","url":"/analysis/gemini-analysis#id-3-computation-of-c-r","position":22},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl2":"3. Computation of C_r"},"content":"Computation of h^*(\\zeta) through bary-centric interpolation method, which the verifier calculates themselves\n\nThe analysis approach here is consistent with the ph23 protocol. Here, h^*(X) is derived by interpolation from 3 point-value pairs:h^*(X) = \\frac{h(\\beta) \\cdot \\frac{\\hat{\\omega_0}}{X- \\beta} + h(-\\beta) \\cdot \\frac{\\hat{\\omega_1}}{X+ \\beta} + h(\\beta^2) \\cdot \\frac{\\hat{\\omega_2}}{X- \\beta^2}}{\\frac{\\hat{\\omega_0}}{X- \\beta} + \\frac{\\hat{\\omega_1}}{X+ \\beta} + \\frac{\\hat{\\omega_2}}{X- \\beta^2}}\n\nwhere:\\begin{aligned}\n        & \\hat{\\omega_0} = (\\beta + \\beta)(\\beta - \\beta^2) \\\\\n        & \\hat{\\omega_1} = (-\\beta - \\beta)(-\\beta - \\beta^2) \\\\\n        & \\hat{\\omega_2} = (\\beta^2 - \\beta)(\\beta^2 - (-\\beta))\n    \\end{aligned}\n\nSince \\beta is randomly generated, it cannot be pre-computed, resulting in a complexity of 3 ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTo calculate h^*(\\zeta), we substitute \\zeta into the expression for h^*(X):h^*(\\zeta) = \\frac{h(\\beta) \\cdot \\frac{\\hat{\\omega_0}}{\\zeta- \\beta} + h(-\\beta) \\cdot \\frac{\\hat{\\omega_1}}{\\zeta+ \\beta} + h(\\beta^2) \\cdot \\frac{\\hat{\\omega_2}}{\\zeta- \\beta^2}}{\\frac{\\hat{\\omega_0}}{\\zeta- \\beta} + \\frac{\\hat{\\omega_1}}{\\zeta+ \\beta} + \\frac{\\hat{\\omega_2}}{\\zeta- \\beta^2}}\n\nWithout detailing the method, the complexity analysis is consistent with ph23. For a set of n point-value pairs, this step has a complexity of (2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}}. Therefore, the computation complexity here is 7 ~ \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{inv}}.\n\nThus, the total complexity for calculating h^*(\\zeta) is:10 ~ \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{inv}}\n\nCalculation of [h^*(\\zeta)]_1, with a complexity of \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nComputing (\\zeta^2 - \\beta^2)(\\zeta - \\beta^2) \\cdot C_q: Computing \\zeta^2 has complexity \\mathbb{F}_{\\mathsf{mul}}, computing (\\zeta^2 - \\beta^2)(\\zeta - \\beta^2) has complexity \\mathbb{F}_{\\mathsf{mul}}, and multiplying this value with C_q has complexity \\mathsf{EccMul}^{\\mathbb{G}_1}. The total complexity is2 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nComputing C_r by adding/subtracting three points in \\mathbb{G}_1 has complexity 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nThe total complexity for this step is:\\begin{aligned}\n    & 10 ~ \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{inv}}\\\\\n    &  + \\mathsf{EccMul}^{\\mathbb{G}_1} \\\\\n    & + 2 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    = & 12 ~ \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{inv}}  + 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\\end{aligned}\n\nVerification using Pairing:e(C_r + \\zeta \\cdot C_w, [1]_2) \\overset{?}{=} e(C_w, [\\tau]_2)\n\nComputing C_r + \\zeta \\cdot C_w has complexity \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nComputing two pairings has complexity 2 ~ P\n\nThe total complexity for this step is:\\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P","type":"content","url":"/analysis/gemini-analysis#id-3-computation-of-c-r","position":23},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verifier Complexity","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#verifier-complexity","position":24},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verifier Complexity","lvl2":"3. Computation of C_r"},"content":"Combining all steps:\\begin{aligned}\n    & (3n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n - 2) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & + (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (n - 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    & + 12 ~ \\mathbb{F}_{\\mathsf{mul}} + 4 ~ \\mathbb{F}_{\\mathsf{inv}}  + 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    & + \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P \\\\\n    = & (4n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#verifier-complexity","position":25},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Summary","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#summary","position":26},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Summary","lvl2":"3. Computation of C_r"},"content":"Prover’s cost:\\begin{aligned}\n    & (6 \\cdot 2^{n} + n + 7) ~ \\mathbb{F}_{\\mathsf{mul}} + 3 ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & +  \\mathsf{polymul}(1, 1) + \\mathsf{polymul}(2, 1) + \\mathsf{polymul}(0, 2^n - 4) +  \\mathsf{polydiv}(2^n - 1, 3) \\\\\n    & + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 3, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\\\\\n\\end{aligned}\n\nSimplified:(10 N + n + 5) ~ \\mathbb{F}_{\\mathsf{mul}} + N ~ \\mathbb{F}_{\\mathsf{inv}} \n+ \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(N - 3, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\nVerifier’s cost:\\begin{aligned}\n    & (4n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\\end{aligned}\n\nProof size:(2n + 1)  \\mathbb{F}_p + (n + 1) \\cdot \\mathbb{G}_1","type":"content","url":"/analysis/gemini-analysis#summary","position":27},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl3","url":"/analysis/gemini-analysis#optimized-version-2","position":28},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Protocol document: \n\nGemini-PCS-4\n\nOptimization technique:\n\nThis version uses an approach inspired by the query phase of FRI protocol. Instead of evaluating h_0(X) at challenge point X=\\beta, then evaluating the folded polynomial h_1(X) at X=\\beta^2, and so on until h_{n-1}(\\beta^{2^{n-1}}). This allows reuse of evaluation points when verifying the folding of h_i(X), saving n evaluation points in total.\n\nProving goal: A multilinear extension polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) evaluates to v = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) at point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}).\n\nThe MLE polynomial is expressed in coefficient form:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{n-1} c_i\\cdot X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}","type":"content","url":"/analysis/gemini-analysis#optimized-version-2","position":29},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Common Input","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#common-input-1","position":30},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Common Input","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Commitment to the vector \\vec{c}=(c_0, c_1, \\ldots, c_{n-1}) as C_f:C_f = \\mathsf{KZG10.Commit}(\\vec{c})\n\nEvaluation point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1})","type":"content","url":"/analysis/gemini-analysis#common-input-1","position":31},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Witness","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#witness-1","position":32},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Witness","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Coefficients of polynomial f(X): \\vec{c}=(c_0, c_1, \\ldots, c_{n-1})","type":"content","url":"/analysis/gemini-analysis#witness-1","position":33},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 1","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#round-1-1","position":34},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 1","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Prover defines h_0(X) = f(X) and computes folded polynomials h_1(X), h_2(X), \\ldots, h_{n-1}(X) such that:h_{i+1}(X^2) = \\frac{h_i(X) + h_i(-X)}{2} + u_i\\cdot \\frac{h_i(X) - h_i(-X)}{2X}\n\nProver computes commitments (C_{h_1}, C_{h_2}, \\ldots, C_{h_{n-1}}) such that:C_{h_{i+1}} = \\mathsf{KZG10.Commit}(h_{i+1}(X))\n\nProver sends (C_{h_1}, C_{h_2}, \\ldots, C_{h_{n-1}})\n\nThe computation in this round is the same as in Optimized Version 1, so the complexity is:(2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1)","type":"content","url":"/analysis/gemini-analysis#round-1-1","position":35},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 2","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#round-2-1","position":36},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 2","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Verifier sends random point \\beta \\in \\mathbb{F}_p\n\nProver computes h_0(\\beta)\n\nProver computes h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\n\nProver sends \\big(h_0(\\beta), h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\\big)\n\nProver:\n\nComputing \\beta^2, \\ldots, \\beta^{2^{n-1}} has complexity (n-1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing h_0(\\beta), h_0(-\\beta), \\ldots, h_{n-1}(-\\beta^{2^{n-1}}) has complexity:2^{n} ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=0}^{n-1} 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}} = (3 \\cdot 2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity for this round is:(n-1) ~ \\mathbb{F}_{\\mathsf{mul}} + (3 \\cdot 2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} = (3 \\cdot 2^{n} + n - 3) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/gemini-analysis#round-2-1","position":37},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 3","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#round-3-1","position":38},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 3","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Verifier sends random value \\gamma \\in \\mathbb{F}_p to aggregate multiple polynomials\n\nProver computes q(X) satisfying:q(X) = \\frac{h_0(X)-h_0(\\beta)}{X-\\beta} + \\sum_{i=0}^{n-1} \\gamma^{i+1} \\cdot \\frac{h_i(X)-h_i(-\\beta^{2^i})}{X+\\beta^{2^i}}\n\nDefine a new Domain D containing the points:D = \\{\\beta, -\\beta, -\\beta^2, \\ldots, -\\beta^{2^{n-1}}\\}\n\nProver computes and sends commitment C_q = \\mathsf{KZG10.Commit}(q(X))\n\nProver:\n\nComputing \\gamma^2, \\ldots, \\gamma^n has complexity (n-1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing \\gamma^{i+1} \\cdot (h_i(X) - h_i(-\\beta^{2^i})) has complexity \\mathsf{polymul}(0, 2^{n-i}), dividing this by (X + \\beta^{2^i}) using linear division has complexity 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}}. The total complexity for computing q(X) including the first term is:2^n ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=0}^{n-1} (\\mathsf{polymul}(0, 2^{n-i}) + 2^{n-i} ~ \\mathbb{F}_{\\mathsf{mul}}) = (3 \\cdot 2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1})\n\nComputing C_q has complexity \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\n\nTherefore, the total complexity for this round is:\\begin{aligned}\n    & (n-1) ~ \\mathbb{F}_{\\mathsf{mul}} + (3 \\cdot 2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1}) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    & = (3 \\cdot 2^n + n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1}) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#round-3-1","position":39},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 4","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#round-4-1","position":40},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Round 4","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Verifier sends random point \\zeta \\in \\mathbb{F}_q\n\nProver computes linearization polynomial L_\\zeta(X) such that L_\\zeta(\\zeta) = 0:L_\\zeta(X) = v_D(\\zeta) \\cdot q(X) - \\frac{v_D(\\zeta)}{\\zeta-\\beta} \\cdot (h_0(X)-h_0(\\beta)) - \\sum_{i=0}^{n-1} \\gamma^{i+1} \\cdot \\frac{v_D(\\zeta)}{\\zeta+\\beta^{2^i}} \\cdot (h_i(X)-h_i(-\\beta^{2^i}))\n\nProver computes quotient polynomial w(X):w(X) = \\frac{L_\\zeta(X)}{(X-\\zeta)}\n\nProver computes and sends commitment to w(X):C_w = \\mathsf{KZG10.Commit}(w(X))\n\nComplexity analysis:\n\nComputing L_\\zeta(X):\n\nComputation of \\gamma^2, \\ldots, \\gamma^{n}, with complexity of (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputation of v_D(\\zeta),v_D(\\zeta) = (\\zeta - \\beta)(\\zeta - (-\\beta)) \\cdots (\\zeta - (-\\beta^{2^{n - 1}}))\n\nSince D contains a total of n + 1 elements, this involves multiplying n + 1 elements, resulting in a complexity of n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nThe total complexity for computing L_\\zeta(X) is:\\begin{aligned}\n    & \\mathsf{polymul}(0, 2^n - 2) + \\mathbb{F}_{\\mathsf{inv}} + \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^n) \\\\\n    & + \\sum_{i=0}^{n-1} (\\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^{n-i})) \\\\\n    & = (2n+1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1}) + \\mathsf{polymul}(0, 2^n-2) + \\mathsf{polymul}(0, 2^n)\n\\end{aligned}\n\nComputing w(X) using linear division: With \\deg(L_\\zeta) = 2^n, this has complexity 2^n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing C_w has complexity \\mathsf{msm}(2^n-1, \\mathbb{G}_1)\n\nThe total complexity for this round is:\\begin{aligned}\n    & (2n+1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1}) + \\mathsf{polymul}(0, 2^n-2) \\\\\n    & + \\mathsf{polymul}(0, 2^n) + 2^n ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(2^n-1, \\mathbb{G}_1) \\\\\n    = & (2^n+2n+1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i=0}^{n-1} \\mathsf{polymul}(0, 2^{i+1}) + \\mathsf{polymul}(0, 2^n-2) \\\\\n    & + \\mathsf{polymul}(0, 2^n) + \\mathsf{msm}(2^n-1, \\mathbb{G}_1)\n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#round-4-1","position":41},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Proof Representation","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#proof-representation-1","position":42},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Proof Representation","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"The proof consists of n+1 elements in \\mathbb{G}_1 and n+1 elements in \\mathbb{F}_q:\\pi=\\Big(C_{f_1}, C_{f_2}, \\ldots, C_{f_{n-1}}, C_q, C_w, h_0(\\beta), h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\\Big)\n\nProof size:(n+1) \\cdot \\mathbb{G}_1 + (n+1) \\cdot \\mathbb{F}_p","type":"content","url":"/analysis/gemini-analysis#proof-representation-1","position":43},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verification","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#verification-1","position":44},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verification","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Verifier computes (h_1(\\beta^2), h_2(\\beta^{2^2}), \\ldots, h_{n-1}(\\beta^{2^{n-1}}), h_n(\\beta^{2^n})):h_{i+1}(\\beta^{2^{i+1}}) = \\frac{h_i(\\beta^{2^i}) + h_i(-\\beta^{2^i})}{2} + u_i \\cdot \\frac{h_i(\\beta^{2^i}) - h_i(-\\beta^{2^i})}{2\\beta^{2^i}}\n\nVerifier checks if h_n(\\beta^{2^n}) equals the claimed evaluation v = \\tilde{f}(\\vec{u}):h_n(\\beta^{2^n}) \\overset{?}{=} v\n\nVerifier computes commitment to L_\\zeta(X):C_L = v_D(\\zeta) \\cdot C_q - e_0 \\cdot (C_{h_0} - h_0(\\beta) \\cdot [1]_1) - \\sum_{i=0}^{n-1} e_{i+1} \\cdot (C_{h_i} - h_i(-\\beta^{2^i}) \\cdot [1]_1)\n\nwhere e_0, e_1, \\ldots, e_n are defined as:\\begin{aligned}\ne_0 &= \\frac{v_D(\\zeta)}{\\zeta-\\beta} \\\\\ne_{i+1} &= \\gamma^{i+1} \\cdot \\frac{v_D(\\zeta)}{\\zeta+\\beta^{2^i}}, \\quad i=0,1,\\ldots,n-1\n\\end{aligned}\n\nVerifier checks if C_w is a valid evaluation proof for C_L at X=\\zeta:\\mathsf{KZG10.Verify}(C_L, \\zeta, 0, C_w) \\overset{?}{=} 1\n\nOr directly as a Pairing check:e\\Big(C_L + \\zeta \\cdot C_w, [1]_2\\Big) \\overset{?}{=} e\\Big(C_w, [\\tau]_2 \\Big)\n\nVerifier:\n\nComputing h_1(\\beta^2), \\ldots, h_n(\\beta^{2^n}):\n\nComputing \\beta^2, \\beta^{2^2}, \\ldots, \\beta^{2^n} has complexity n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nFor each h_{i+1}(\\beta^{2^{i+1}}), computing the inverses of 2 and 2\\beta^{2^i} and then multiplying with the numerators has complexity 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 3 ~ \\mathbb{F}_{\\mathsf{mul}} per item, for n items.\n\nThe total complexity for this step is:n ~ \\mathbb{F}_{\\mathsf{mul}} + 2n ~ \\mathbb{F}_{\\mathsf{inv}} + 3n ~ \\mathbb{F}_{\\mathsf{mul}} = 4n ~ \\mathbb{F}_{\\mathsf{mul}} + 2n ~ \\mathbb{F}_{\\mathsf{inv}}\n\nComputing C_L:\n\nFirst, calculate v_D(\\zeta),v_D(\\zeta) = (\\zeta - \\beta)(\\zeta - (-\\beta)) \\cdots (\\zeta - (-\\beta^{2^{n - 1}}))\n\nSince D contains a total of n + 1 elements, this involves multiplying n + 1 elements, resulting in a complexity of n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing e_0 has complexity \\mathbb{F}_{\\mathsf{inv}} + \\mathbb{F}_{\\mathsf{mul}}\n\nComputing \\gamma^2, \\ldots, \\gamma^n has complexity (n-1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing each e_{i+1} has complexity \\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}}, for n items, giving a total of n ~ \\mathbb{F}_{\\mathsf{inv}} + 2n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing\\sum_{i = 0}^{n - 1} e_{i + 1} \\cdot (C_{h_i} - h_i(- \\beta^{2^{i}}) \\cdot [1]_1)\n\nComputing e_{i + 1} \\cdot (C_{h_i} - h_i(- \\beta^{2^{i}}) \\cdot [1]_1) has a complexity of 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}. The total complexity is 2n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + n ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}, which equals 2n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nComputing e_0 \\cdot (C_{h_0} - h_0(\\beta) \\cdot [1]_1), with a complexity of 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nComputing v_D(\\zeta) \\cdot C_q has complexity \\mathsf{EccMul}^{\\mathbb{G}_1}, and adding the three terms to compute C_L has complexity 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nThe total complexity for this step is:\\begin{aligned}\n    & n ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + \\mathbb{F}_{\\mathsf{mul}} + (n-1) ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathbb{F}_{\\mathsf{inv}} + 2n ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + 2n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n-1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    = & 4n ~ \\mathbb{F}_{\\mathsf{mul}} + (n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + (2n+3) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n+2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\\end{aligned}\n\nFinal verification using Pairing:e(C_L + \\zeta \\cdot C_w, [1]_2) \\overset{?}{=} e(C_w, [\\tau]_2)\n\nComputing C_L + \\zeta \\cdot C_w has complexity \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nComputing two pairings has complexity 2 ~ P\n\nThe total complexity for this step is:\\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P","type":"content","url":"/analysis/gemini-analysis#verification-1","position":45},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verifier Complexity","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#verifier-complexity-1","position":46},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Verifier Complexity","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Combining all steps:\\begin{aligned}\n    & 4n ~ \\mathbb{F}_{\\mathsf{mul}} + 2n ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & + 4n ~ \\mathbb{F}_{\\mathsf{mul}} + (n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + (2n+3) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n+2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    & + \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P \\\\\n    & = 8n ~ \\mathbb{F}_{\\mathsf{mul}} + (3n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + (2n+4) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n+3) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n\\end{aligned}","type":"content","url":"/analysis/gemini-analysis#verifier-complexity-1","position":47},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Summary","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"type":"lvl4","url":"/analysis/gemini-analysis#summary-1","position":48},{"hierarchy":{"lvl1":"Complexity Analysis of Gemini-PCS Algorithm","lvl4":"Summary","lvl3":"Optimized Version 2","lvl2":"3. Computation of C_r"},"content":"Prover’s cost:\\begin{aligned}\n    & (2^n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) \\\\\n    & + (3 \\cdot 2^{n} + n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (3 \\cdot 2^{n} + n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^{i + 1}) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    & + (2^n + 2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}}  + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^{i + 1}) + \\mathsf{polymul}(0, 2^n - 2) \\\\\n    & + \\mathsf{polymul}(0, 2^n) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    = & (8 \\cdot 2^{n} + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + 2 \\cdot \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^{i + 1}) + \\mathsf{polymul}(0, 2^n - 2) \\\\\n    & + \\mathsf{polymul}(0, 2^n) + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\\\\\n\\end{aligned}\n\nThis simplifies to:\\begin{aligned}\n    & (8 N + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + 2 \\cdot \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^{i + 1}) + \\mathsf{polymul}(0, 2^n - 2) \\\\\n    & + \\mathsf{polymul}(0, N) + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)\\\\\n\\end{aligned}\n\nSubstituting \\mathsf{polymul}(0, N) = (N+1) ~ \\mathbb{F}_{\\mathsf{mul}}:\\begin{aligned}\n    & (8 N + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + 2 \\cdot \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^{i + 1}) + \\mathsf{polymul}(0, 2^n - 2) \\\\\n    & + \\mathsf{polymul}(0, N) + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)\\\\\n\t= & (8 N + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + 2 \\cdot \\sum_{i = 0}^{n - 1} (2^{i + 1} + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1) \\\\\n\t= & (8 N + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + 2 \\cdot \\sum_{i = 0}^{n - 1} (2^{i + 1} + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2^n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1) \\\\\n\t= & (8 N + 4n - 7) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + (4N + 2n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1) \\\\\n\t= & (14 N + 6n - 11) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)   \\\\\n\\end{aligned}\n\nVerifier’s cost:8n ~ \\mathbb{F}_{\\mathsf{mul}} + (3n+1) ~ \\mathbb{F}_{\\mathsf{inv}} + (2n+4) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n+3) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n\nProof size:(n+1) \\cdot \\mathbb{F}_p + (n+1) \\cdot \\mathbb{G}_1","type":"content","url":"/analysis/gemini-analysis#summary-1","position":49},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis"},"type":"lvl1","url":"/analysis/ph23-analysis","position":0},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nPH23+KZG10 Protocol (Optimized Version) Protocol Description Document: \n\nPH23+KZG10 Protocol (Optimized Version)\n\nFor the KZG10 protocol, its Commitment has additive homomorphism.","type":"content","url":"/analysis/ph23-analysis","position":1},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Precomputation"},"type":"lvl2","url":"/analysis/ph23-analysis#precomputation","position":2},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Precomputation"},"content":"Precompute s_0(X),\\ldots, s_{n-1}(X) and v_H(X)v_H(X) = X^N - 1s_i(X) = \\frac{v_H(X)}{v_{H_i}(X)} = \\frac{X^N-1}{X^{2^i}-1}\n\nPrecompute Bary-Centric Weights \\{\\hat{w}_i\\} on D=(1, \\omega, \\omega^2, \\ldots, \\omega^{2^{n-1}}). This can accelerate\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nPrecompute KZG10 SRS for Lagrange Basis A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1","type":"content","url":"/analysis/ph23-analysis#precomputation","position":3},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Common inputs"},"type":"lvl2","url":"/analysis/ph23-analysis#common-inputs","position":4},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Common inputs"},"content":"C_a=[\\hat{f}(\\tau)]_1: the (uni-variate) commitment of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1})\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1}): evaluation point\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1}): computed value of MLE polynomial \\tilde{f} at \\vec{X}=\\vec{u}","type":"content","url":"/analysis/ph23-analysis#common-inputs","position":5},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Commit calculation process"},"type":"lvl2","url":"/analysis/ph23-analysis#commit-calculation-process","position":6},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Commit calculation process"},"content":"Prover constructs a univariate polynomial a(X) such that its Evaluation form equals \\vec{a}=(a_0, a_1, \\ldots, a_{N-1}), where a_i = \\tilde{f}(\\mathsf{bits}(i)), which is the value of \\tilde{f} on the Boolean Hypercube \\{0,1\\}^n.a(X) = a_0\\cdot L_0(X) + a_1\\cdot L_1(X) + a_2\\cdot L_2(X) + \\cdots + a_{N-1}\\cdot L_{N-1}(X)\n\nProver: In this step, the Prover does not need to derive the coefficient form, but directly uses the evaluation form for computation, which does not involve computational complexity.\n\nProver calculates the commitment C_a of \\hat{f}(X) and sends C_aC_{a} = a_0\\cdot A_0 + a_1\\cdot A_1 + a_2\\cdot A_2 + \\cdots + a_{N-1}\\cdot A_{N-1} = [\\hat{f}(\\tau)]_1\n\nwhere A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1 were obtained during the precomputation process.\n\nProver: The algorithm complexity is \\mathsf{msm}(N, \\mathbb{G}_1), representing the commitment of a vector of length N.\n\n","type":"content","url":"/analysis/ph23-analysis#commit-calculation-process","position":7},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Commit Phase Complexity","lvl2":"Commit calculation process"},"type":"lvl4","url":"/analysis/ph23-analysis#commit-phase-complexity","position":8},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Commit Phase Complexity","lvl2":"Commit calculation process"},"content":"In the commit phase, the total complexity for the prover is:> \\mathsf{msm}(N, \\mathbb{G}_1)\n>","type":"content","url":"/analysis/ph23-analysis#commit-phase-complexity","position":9},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Evaluation proof protocol"},"type":"lvl2","url":"/analysis/ph23-analysis#evaluation-proof-protocol","position":10},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Evaluation proof protocol"},"content":"Recall the constraint of the polynomial operation to be proven:\\tilde{f}(u_0, u_1, u_2, \\ldots, u_{n-1}) = v\n\nHere \\vec{u}=(u_0, u_1, u_2, \\ldots, u_{n-1}) is a publicly known challenge point.","type":"content","url":"/analysis/ph23-analysis#evaluation-proof-protocol","position":11},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Memory","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#prover-memory","position":12},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Memory","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([z_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\}","type":"content","url":"/analysis/ph23-analysis#prover-memory","position":13},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#round-1","position":14},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Prover:","type":"content","url":"/analysis/ph23-analysis#round-1","position":15},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-1-1","position":16},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Calculate vector \\vec{c}, where each element c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})","type":"content","url":"/analysis/ph23-analysis#round-1-1","position":17},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-1","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-1-1","position":18},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-1","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Prover:\n\nThe algorithm for calculating vector \\vec{c} is:@classmethod\ndef eqs_over_hypercube(cls, rs):\n    k = len(rs)\n    n = 1 << k\n    evals = [Field(1)] * n\n    half = 1\n    for i in range(k):\n        for j in range(half):\n            evals[j+half] = evals[j] * rs[i]\n            evals[j] = evals[j] - evals[j+half]\n        half *= 2\n    return evals\n\nFor example, when k = 2, the calculation result should be:> \\begin{aligned}\n>   00 & \\quad (1 - u_0) & (1- u_1)  \\\\\n>   10 & \\quad u_0 & (1- u_1) \\\\\n>   01 & \\quad (1 - u_0) & u_1 \\\\\n>   11 & \\quad u_0 & u_1\n> \\end{aligned}\n>\n\nThis algorithm first calculates based on the binary position of u_0, and then updates all elements if an additional bit u_1 is added.\n\nInside the for j in range(1) loop, evals[1] and evals[0] are calculated:\n\nevals[1] = u_0, corresponding to the bit position 1 of u_0\n\nevals[0] = 1 - u_0, corresponding to the binary bit position 0 of u_0\n\nInside for j in range(2), update the position where u_1 is located.\n\nj = 0, update evals[0] and evals[2]\n\nj = 1, update evals[1] and evals[3]\n\nIn each loop of for j in range(half), there is 1 multiplication in the finite field, namely evals[j+half] = evals[j] * rs[i], and half changes as 1, 2, 2^2, \\ldots, 2^{k-1}, so the total number of finite field multiplications is:>   1 + 2 + 2^2 + \\ldots + 2^{k - 1} = \\frac{1(1 - 2^k)}{1 - 2} = 2^k - 1 = N - 1\n>\n\nTherefore, the computational complexity here is (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#prover-cost-1-1","position":19},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 1-1","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-1-1","position":20},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 1-1","lvl4":"Round 1-1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\}","type":"content","url":"/analysis/ph23-analysis#prover-memory-1-1","position":21},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-2","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-1-2","position":22},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-2","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Construct polynomial c(X) such that its evaluation results on H are exactly \\vec{c}.c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)","type":"content","url":"/analysis/ph23-analysis#round-1-2","position":23},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-2","lvl4":"Round 1-2","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-1-2","position":24},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-2","lvl4":"Round 1-2","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Prover: This step does not require calculating c(X), the calculation proceeds directly with \\vec{c}.","type":"content","url":"/analysis/ph23-analysis#prover-cost-1-2","position":25},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-3","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-1-3","position":26},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 1-3","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Calculate the commitment of c(X) as C_c= [c(\\tau)]_1, and send C_cC_c = \\mathsf{KZG10.Commit}(\\vec{c}) = [c(\\tau)]_1","type":"content","url":"/analysis/ph23-analysis#round-1-3","position":27},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-3","lvl4":"Round 1-3","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-1-3","position":28},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 1-3","lvl4":"Round 1-3","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"The commitment calculation method for C_c is:C_c = c_0 \\cdot A_0 + c_1 \\cdot A_1 + \\ldots + c_{N - 1} \\cdot A_{N - 1}\n\nThe algorithm complexity here is \\mathsf{msm}(N, \\mathbb{G}_1)","type":"content","url":"/analysis/ph23-analysis#prover-cost-1-3","position":29},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#prover-cost-round-1","position":30},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"Prover complexity is:> (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N, \\mathbb{G}_1)\n>","type":"content","url":"/analysis/ph23-analysis#prover-cost-round-1","position":31},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Memory Round 1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#prover-memory-round-1","position":32},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Memory Round 1","lvl3":"Round 1.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\}","type":"content","url":"/analysis/ph23-analysis#prover-memory-round-1","position":33},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#round-2","position":34},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Verifier: Send challenge number \\alpha\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:","type":"content","url":"/analysis/ph23-analysis#round-2","position":35},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-1","position":36},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Construct constraint polynomials p_0(X),\\ldots, p_{n}(X) regarding \\vec{c}\\begin{split}\np_0(X) &= s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\np_k(X) &= s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big) , \\quad k=1\\ldots n\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#round-2-1","position":37},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-1","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-1","position":38},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-1","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"📝 Note: First, let’s introduce how to quickly perform polynomial multiplication and division in finite fields. LetH' = \\{g^0, g^1, \\ldots, g^{2N - 1}\\} = \\langle g \\rangle\n\nTake>H = \\langle g^2 \\rangle = \\langle \\omega \\rangle = \\{\\omega^0, \\omega^1, \\ldots, \\omega^{N - 1} \\}\n>\n\nThen>gH = \\{g\\omega^0, g\\omega^1, \\ldots, g\\omega^{N - 1} \\} =\\{ g^1, g^3, \\ldots, g^{2N - 1} \\}\n>\n\nIf we want to calculate>a(X) = a_1 + a_2 X + \\ldots + a_{N - 1}X^{N - 1}\n>\n\nand>b(X) = b_1 + b_2 X + \\ldots + b_{N - 1}X^{N - 1}\n>\n\nthe product polynomial c(X) = a(X)\\cdot b(X). If what we have is the evaluation form of a(X) and b(X) on H, i.e.,[a(x)|_{x \\in H}], \\quad [b(x)|_{x \\in H}]\n\nWe want to calculate the quotient polynomial>q(X) = \\frac{a(X) \\cdot b(X)}{z_H(X)}\n>\n\nSince \\deg(q) < N, storing q(X) can still use the evaluation form. Since z_H(X) is 0 on H, we can calculate>[(a(x) \\cdot b(x))|_{x \\in gH}], \\quad [z_H(x)|_{x \\in gH}]\n>\n\nThen divide element-wise to calculate [q(x)|_{x \\in gH}].\n\nCalculate [a(x))|_{x \\in gH}]: First perform IFFT on [a(x)|_{x \\in H}] to get the coefficients of a(X), then perform FFT to calculate its values on gH. In actual implementation, this can be done simultaneously, but the complexity should remain unchanged, which is N \\log N ~ \\mathbb{F}_{\\mathsf{mul}}, also denoted as \\mathsf{FFT}(N) + \\mathsf{IFFT}(N).\n\nCalculate [b(x))|_{x \\in gH}]: First perform IFFT on [b(x)|_{x \\in H}] to get the coefficients of b(X), then perform FFT to calculate its values on gH. The complexity of this step is \\mathsf{FFT}(N) + \\mathsf{IFFT}(N), which is N \\log N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [(a(x) \\cdot b(x))|_{x \\in gH}]: Multiply N elements, with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [z_H(x)|_{x \\in gH}]: Since z_H(X) = X^N - 1, we have\nz_H(x) = z_H(g\\omega^i) = (g\\omega^i)^N - 1 = g^N \\cdot (\\omega^N)^i - 1 = g^N - 1\n\n\nThe value of z_H(X) on gH is always a constant, so its inverse (g^N - 1)^{-1} can be precalculated. This step does not involve Prover’s complexity.\n\nCalculate [q(x)|_{x \\in gH}]: Multiply each value in [(a(x) \\cdot b(x))|_{x \\in gH}] by (g^N - 1)^{-1} to get [q(x)|_{x \\in gH}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the overall complexity for calculating the division is> 2~ \\mathsf{FFT}(N) + 2~\\mathsf{IFFT}(N) + 2N ~ \\mathbb{F}_{\\mathsf{mul}}\n>\n\nNow analyzing the algorithm complexity. The Prover needs to calculate [p_i(x)|_{x \\in gH}] to facilitate the subsequent calculation of the quotient polynomial’s evaluation form.\n\nProver calculates [s_0(x)|_{x \\in gH}], [s_1(x)|_{x \\in gH}], \\ldots, [s_{n - 1}(x)|_{x \\in gH}]. The values of s_0(x), \\ldots, s_{n - 1}(x) at any point can be obtained using an O(n) algorithm, as shown in Round 3, with complexity (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}. Since |gH| = N, the complexity to obtain all values on gH is (n - 1)N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [c(x)|_{x \\in gH}] by first using IFFT on [c(x)|_{x \\in H}] to get its coefficients, then using FFT to find its values on gH, with complexity \\mathsf{FFT}(N) + \\mathsf{IFFT}(N).\n\nCalculate [(c(x) - (1 - u_0)(1 - u_1) \\ldots (1 - u_{n - 1}))|_{x \\in gH}]: (1 - u_0)(1 - u_1) \\ldots (1 - u_{n - 1}) is actually c_0, and we can directly subtract for calculation.\n\nCalculate [p_0(x)|_{x \\in gH}], involving N multiplications, with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor k = 1, \\ldots, n, calculate [( u_{n-k}\\cdot c(x) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot x))|_{x \\in gH}]: For each k and x \\in gH, each calculation involves 2 finite field multiplications, so the complexity to calculate all values for all k is 2nN ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor k = 1, \\ldots, n, calculate [p_k(x)|_{x \\in gH}], for each k, involving N multiplications, so the total complexity is nN ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nIn summary, the total complexity for this step is:\\begin{aligned}\n  & (n - 1)N ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{FFT}(N) + \\mathsf{IFFT}(N) + N ~ \\mathbb{F}_{\\mathsf{mul}} + 3nN ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n  = & \\mathsf{FFT}(N) + \\mathsf{IFFT}(N) + 4nN ~ \\mathbb{F}_{\\mathsf{mul}}\n\\end{aligned}","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-1","position":39},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-1","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-1","position":40},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-1","lvl4":"Round 2-1","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\nCoefficients of c(X)\n\n[c(x)|_{x \\in gH}]\n\n\\{[p_k(x)|_{x \\in gH}]\\}_{k = 0}^n","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-1","position":41},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-2","position":42},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Aggregate \\{p_i(X)\\} into a single polynomial p(X)p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)","type":"content","url":"/analysis/ph23-analysis#round-2-2","position":43},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-2","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-2","position":44},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-2","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"This step actually calculates [p(x)|_{x \\in gH}] rather than the polynomial coefficients.\n\nThe prover calculates \\alpha^2, \\alpha^3, \\ldots, \\alpha^n from \\alpha, involving a total of n - 1 multiplications in the finite field, so the complexity is (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor each x \\in gH, directly calculatep(x) = p_0(x) + \\alpha\\cdot p_1(x) + \\alpha^2\\cdot p_2(x) + \\cdots + \\alpha^{n}\\cdot p_{n}(x)\n\nThis involves n finite field multiplications, and with a total of |gH| = N values of x, the complexity is nN ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nIn summary, the complexity for this step is:(nN + n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-2","position":45},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-2","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-2","position":46},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-2","lvl4":"Round 2-2","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-2","position":47},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-3","position":48},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Construct accumulator polynomial z(X) that satisfies:\\begin{split}\nz(1) &= a_0\\cdot c_0 \\\\\nz(\\omega_{i}) - z(\\omega_{i-1}) &= a(\\omega_{i})\\cdot c(\\omega_{i}), \\quad i=1,\\ldots, N-1 \\\\ \nz(\\omega^{N-1}) &= v \\\\\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#round-2-3","position":49},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-3","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-3","position":50},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-3","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Having already obtained [a(x)|_{x \\in H}] and [c(x)|_{x \\in H}], calculating [z(x)|_{x \\in H}] is straightforward.\\begin{aligned}\n  & z(1) = a_0 \\cdot c_0 \\\\\n  & z(\\omega_1) = z(1) + a(\\omega_1) \\cdot c(\\omega_1) \\\\\n  & \\cdots \\\\\n  & z(\\omega_{N - 1}) = z(\\omega_{N - 2}) + a(\\omega_{N - 1}) \\cdot c(\\omega_{N - 1}) \\\\\n  & z(\\omega^{N - 1}) = v\n\\end{aligned}\n\nThe complexity involved is N ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-3","position":51},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-3","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-3","position":52},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-3","lvl4":"Round 2-3","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([z_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n[L_0(x)|_{x \\in gH}]\n\n[L_{N - 1}(x)|_{x \\in gH}]\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]\n\n[z(x)|_{x \\in H}]","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-3","position":53},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-4","position":54},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Construct constraint polynomials h_0(X), h_1(X), h_2(X) that satisfy:\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - c_0\\cdot a(X) \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#round-2-4","position":55},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-4","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-4","position":56},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-4","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"To calculate [h_0(x)|_{x \\in gH}], [h_1(x)|_{x \\in gH}], [h_2(x)|_{x \\in gH}]:\n\nFirst calculate [z(x)|_{x \\in gH}], with complexity \\mathsf{FFT}(N) + \\mathsf{IFFT}(N).\n\nCalculate [a(x)|_{x \\in gH}], with complexity \\mathsf{FFT}(N) + \\mathsf{IFFT}(N).\n\nCalculate [h_0(x)|_{x \\in gH}], with complexity 2N ~ \\mathbb{F}_{\\mathsf{mul}}\n\nCalculate [(a(x) \\cdot c(x))|_{x \\in gH}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [h_1(x)|_{x \\in gH}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}\n\nCalculate [h_2(x)|_{x \\in gH}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe total complexity for this step is:2~ \\mathsf{FFT}(N) + 2~ \\mathsf{IFFT}(N) + 5N ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-4","position":57},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-4","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-4","position":58},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-4","lvl4":"Round 2-4","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"This round adds [h_0(x)|_{x \\in gH}], [h_1(x)|_{x \\in gH}], [h_2(x)|_{x \\in gH}].\n\nKZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([z_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n[L_0(x)|_{x \\in gH}]\n\n[L_{N - 1}(x)|_{x \\in gH}]\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]\n\n[z(x)|_{x \\in H}]\n\nCoefficients of z(X) (Round 2-4)\n\n[h_0(x)|_{x \\in gH}], [h_1(x)|_{x \\in gH}], [h_2(x)|_{x \\in gH}]","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-4","position":59},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-5","position":60},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Aggregate p(X) and h_0(X), h_1(X), h_2(X) into a single polynomial h(X) such that:\\begin{split}\nh(X) &= p(X) + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#round-2-5","position":61},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-5","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-5","position":62},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-5","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"This round calculates [h(x)_{x \\in gH}].\n\nIn the previous steps of this round, \\alpha^2, \\ldots, \\alpha^n have already been calculated. Now to calculate \\alpha^{n + 1}, \\alpha^{n + 2}, \\alpha^{n + 3}, this involves 3 multiplications in the finite field, so the complexity is 3 ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [h(x)|_{gH}], with complexity 3N ~ \\mathbb{F}_{\\mathsf{mul}}\nThe total complexity for this round is:\n(3N + 3) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-5","position":63},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-5","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-5","position":64},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-5","lvl4":"Round 2-5","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([v_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n[L_0(x)|_{x \\in gH}]\n\n[L_{N - 1}(x)|_{x \\in gH}]\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]\n\n[z(x)|_{x \\in H}]\n\n[h_0(x)|_{x \\in gH}], [h_1(x)|_{x \\in gH}], [h_2(x)|_{x \\in gH}]\n\n[h(x)|_{x \\in gH}]","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-5","position":65},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-6","position":66},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Calculate the Quotient polynomial t(X) such that:h(X) =t(X)\\cdot v_H(X)","type":"content","url":"/analysis/ph23-analysis#round-2-6","position":67},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-6","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-6","position":68},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-6","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Calculate [t(x)|_{x \\in gH}] for all x \\in gH:t(x) = h(x) \\cdot (v_H(x))^{-1} = h(x) \\cdot (g^N - 1)^{-1}\n\nThe complexity is N ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-6","position":69},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-6","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-2-6","position":70},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 2-6","lvl4":"Round 2-6","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([v_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n[L_0(x)|_{x \\in gH}]\n\n[L_{N - 1}(x)|_{x \\in gH}]\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]\n\n[z(x)|_{x \\in H}]\n\n[t(x)|_{x \\in gH}]","type":"content","url":"/analysis/ph23-analysis#prover-memory-2-6","position":71},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-7","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-2-7","position":72},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 2-7","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Calculate C_t=[t(\\tau)]_1, C_z=[z(\\tau)]_1, and send C_t and C_z\\begin{split}\nC_t &= \\mathsf{KZG10.Commit}(t(X)) = [t(\\tau)]_1 \\\\\nC_z &= \\mathsf{KZG10.Commit}(z(X)) = [z(\\tau)]_1\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#round-2-7","position":73},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-7","lvl4":"Round 2-7","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-2-7","position":74},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 2-7","lvl4":"Round 2-7","lvl3":"Round 2.","lvl2":"Evaluation proof protocol"},"content":"Calculate C_t:\n\nFrom [t(x)|_{x \\in gH}] calculate [t(x)|_{x \\in H}], with complexity \\mathsf{FFT}(N) + \\mathsf{IFFT}(N),\n\nCalculateC_t = t_0 A_0 + \\ldots + t_{N - 1}A_{N - 1}\n\nwith complexity \\mathsf{msm}(N, \\mathbb{G}_1)\n\nCalculate C_z: \\mathsf{msm}(N, \\mathbb{G}_1)\n\nTherefore, the total complexity for this step is:\\mathsf{FFT}(N) + \\mathsf{IFFT}(N) + 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1)\n\n💡 Option\n\nIf memory constraints are not a concern, another set of KZG10 SRS can be stored in memory in advance: B_0 =[L'_0(\\tau)]_1, B_1= [L'_1(\\tau)]_1, B_2=[L'_2(\\tau)]_1, \\ldots, B_{N-1} = [L'_{2^{n} - 1}(\\tau)]_1, where L_0', \\ldots, L_{N - 1}' are the Lagrange interpolation polynomials on gH.\n\nCalculate C_t:C_t = t_0 B_0 + \\ldots + t_{N - 1}B_{N - 1}\n\nwhere [t_0, \\ldots, t_{N-1}] is [t(x)|_{x \\in gH}]. The complexity for this step would be \\mathsf{msm}(N, \\mathbb{G}_1).\n\nCalculate C_z: \\mathsf{msm}(N, \\mathbb{G}_1)\n\nTotal complexity would be:2 ~ \\mathsf{msm}(N, \\mathbb{G}_1)\n\nThis approach would save one FFT and one IFFT, reducing the computation by N \\log N ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#prover-cost-2-7","position":75},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Cost Round 2","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#prover-cost-round-2","position":76},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Cost Round 2","lvl2":"Evaluation proof protocol"},"content":"Summarizing the Prover computational complexity for all steps above:\\begin{aligned}\n& \\mathsf{FFT}(N) + \\mathsf{IFFT}(N) + 4nN ~ \\mathbb{F}_{\\mathsf{mul}} + (nN + n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + N ~ \\mathbb{F}_{\\mathsf{mul}} + 2~ \\mathsf{FFT}(N) + 2~ \\mathsf{IFFT}(N) + 5N ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + (3N + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + N ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{FFT}(N) + \\mathsf{IFFT}(N) + 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) \\\\\n= & 4 ~ \\mathsf{FFT}(N) + 4 ~ \\mathsf{IFFT}(N) + (5nN + 10N + n + 2) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1)\n\\end{aligned}","type":"content","url":"/analysis/ph23-analysis#prover-cost-round-2","position":77},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#round-3","position":78},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Verifier: Send random evaluation point \\zeta\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:","type":"content","url":"/analysis/ph23-analysis#round-3","position":79},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-1","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-1","position":80},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-1","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Calculate the values of s_i(X) at point \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)","type":"content","url":"/analysis/ph23-analysis#round-3-1","position":81},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-1","lvl4":"Round 3-1","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-1","position":82},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-1","lvl4":"Round 3-1","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Here the Prover can efficiently calculate s_i(\\zeta). From the formula for s_i(X) we get:\\begin{aligned}\n  s_i(\\zeta) & = \\frac{\\zeta^N - 1}{\\zeta^{2^i} - 1} \\\\\n  & = \\frac{(\\zeta^N - 1)(\\zeta^{2^i} +1)}{(\\zeta^{2^i} - 1)(\\zeta^{2^i} +1)} \\\\\n  & = \\frac{\\zeta^N - 1}{\\zeta^{2^{i + 1}} - 1} \\cdot (\\zeta^{2^i} +1) \\\\\n  & = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1)\n\\end{aligned}\n\nThus the value of s_i(\\zeta) can be calculated from s_{i + 1}(\\zeta), ands_{n-1}(\\zeta) = \\frac{\\zeta^N - 1}{\\zeta^{2^{n-1}} - 1} = \\zeta^{2^{n-1}} + 1\n\nThis gives us an O(n) algorithm to calculate s_i(\\zeta), which does not involve division operations. The calculation process is: s_{n-1}(\\zeta) \\rightarrow s_{n-2}(\\zeta) \\rightarrow \\cdots \\rightarrow s_0(\\zeta).\n\nFirst calculate \\zeta^2, \\zeta^4, \\ldots, \\zeta^{2^{n - 1}} from the random number \\zeta. This requires one finite field multiplication for \\zeta^2 = \\zeta \\times \\zeta, then one more for \\zeta^4 = \\zeta^2 \\times \\zeta^2, and so on, each requiring one finite field multiplication. In total, this involves n - 1 finite field multiplications, with complexity (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate s_{n-1}(\\zeta) = \\zeta^{2^{n-1}} + 1, which only involves addition in the finite field and is not counted in the complexity.\n\nCalculate s_{i}(\\zeta) (for i = 0, \\ldots, n - 2), where s_{i}(\\zeta) = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1) requires one finite field multiplication, so the operation needed is \\mathbb{F}_{\\mathsf{mul}}. For all i = 0, \\ldots, n - 2, the total complexity is (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the total complexity is:>   (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} = 2(n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}\n>","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-1","position":83},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-2","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-2","position":84},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-2","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Define the evaluation Domain D' containing n+1 elements:D'=D\\zeta = \\{\\zeta, \\omega\\zeta, \\omega^2\\zeta,\\omega^4\\zeta, \\ldots, \\omega^{2^{n-1}}\\zeta\\}","type":"content","url":"/analysis/ph23-analysis#round-3-2","position":85},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-3","position":86},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Calculate and send the values of c(X) on D':c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})","type":"content","url":"/analysis/ph23-analysis#round-3-3","position":87},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-3","lvl4":"Round 3-3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-3","position":88},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-3","lvl4":"Round 3-3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"The values (1, \\omega, \\omega^2, \\ldots, \\omega^{2^{n - 1}}) can be precomputed, so calculating the points (\\zeta, \\zeta \\cdot \\omega, \\zeta \\cdot \\omega^2, \\ldots, \\zeta \\cdot \\omega^{2^{n - 1}}) involves n finite field multiplications, with complexity n ~\\mathbb{F}_{\\mathsf{mul}}.\n\nTo calculate [c(x)|_{x \\in D'}], in Round 2-1 we obtained the coefficients of c(X). Using the FFT method, we can find [c(x)|_{x \\in D^{(2)}}] in a subgroup D' \\subset D^{(2)} of size N, where |D'| = n, |D^{(2)}| = N. This naturally gives us [c(x)|_{x \\in D'}], with complexity \\mathsf{FFT}(N).\n\n💡 Since we’ve calculated many more values than needed (only values on D' are required, but we calculated values on D^{(2)}), there’s room for optimization:\n\nCalculating on the D’ sub-tree has complexity n\\log^2n\n\nIs there an optimized algorithm?\n\nThe complexity for this step is:n ~\\mathbb{F}_{\\mathsf{mul}} + \\mathsf{FFT}(N)","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-3","position":89},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-4","position":90},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Calculate and send z(\\omega^{-1}\\cdot\\zeta)","type":"content","url":"/analysis/ph23-analysis#round-3-4","position":91},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-4","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-4","position":92},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-4","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"In Round 2-4, the coefficients of z(X) were already calculated, so we can directly use these to find the value of z(X) at a specific point.\n\nProver:\n\nComputing \\omega^{-1}\\cdot\\zeta has complexity \\mathbb{F}_{\\mathsf{mul}}, and computing z(\\omega^{-1}\\cdot\\zeta) has complexity N ~\\mathbb{F}_{\\mathsf{mul}}, giving a total complexity of:>   (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}}\n>","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-4","position":93},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 3-4","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-memory-3-4","position":94},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Memory 3-4","lvl4":"Round 3-4","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"KZG10 SRS: A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1\n\nBary-Centric Weights: \\{\\hat{w}_i\\}\n\n([v_H(x)|_{x \\in gH}])^{-1} = (g^N - 1)^{-1}\n\n[L_0(x)|_{x \\in gH}]\n\n[L_{N - 1}(x)|_{x \\in gH}]\n\nCoefficients of L_0(X) and L_{N - 1}(X)\n\n\\omega^{-1} (Precomputed)\n\n\\vec{a} = \\{a_0, \\ldots, a_{N-1}\\} = [a(x)|_{x \\in H}]\n\nC_a=[\\hat{f}(\\tau)]_1\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1})\n\n\\vec{c} = \\{c_0, \\ldots, c_{N-1}\\} = [c(x)|_{x \\in H}]\n\n[c(x)|_{x \\in gH}]\n\n[p(x)|_{x \\in gH}]\n\n[z(x)|_{x \\in H}]\n\n[z(x)|_{x \\in gH}]\n\n[t(x)|_{x \\in gH}]\n\nCoefficients of c(X)\n\nCoefficients of z(X)\n\nc(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})\n\ns_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nz(\\omega^{-1}\\cdot\\zeta)\n\n\\alpha, \\alpha^2, \\ldots, \\alpha^{n + 3} (Round 2-5)","type":"content","url":"/analysis/ph23-analysis#prover-memory-3-4","position":95},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-5","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-5","position":96},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-5","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Calculate the Linearized Polynomial l_\\zeta(X):\\begin{split}\nl_\\zeta(X) =& \\Big(s_0(\\zeta) \\cdot (c(\\zeta) - c_0) \\\\\n& + \\alpha\\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\\\\n  & + \\alpha^2\\cdot s_1(\\zeta) \\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta)) \\\\\n  & + \\cdots \\\\\n  & + \\alpha^{n-1}\\cdot s_{n-2}(\\zeta)\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\\\\n  & + \\alpha^n\\cdot s_{n-1}(\\zeta)\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta)) \\\\\n  & + \\alpha^{n+1}\\cdot (L_0(\\zeta)\\cdot\\big(z(X) - c_0\\cdot a(X))\\\\\n  & + \\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot a(X) ) \\\\\n  & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(X) - v) \\\\\n  & - v_H(\\zeta)\\cdot t(X)\\ \\Big)\n\\end{split}\n\nClearly, l_\\zeta(\\zeta)= 0, so this evaluation value does not need to be sent to the Verifier, and [l_\\zeta(\\tau)]_1 can be constructed by the Verifier themselves.","type":"content","url":"/analysis/ph23-analysis#round-3-5","position":97},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-5","lvl4":"Round 3-5","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-5","position":98},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-5","lvl4":"Round 3-5","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Calculate [l_{\\zeta}(x)|_{x \\in H}].\n\nL_0(\\zeta) and L_{N - 1}(\\zeta): The coefficients of L_0(X) and L_{N - 1}(X) can be precomputed, so calculating L_0(\\zeta) and L_{N - 1}(\\zeta) has complexity 2N ~\\mathbb{F}_{\\mathsf{mul}}.\n\ns_0(\\zeta) \\cdot (c(\\zeta) - c_0) involves one finite field multiplication, with complexity \\mathbb{F}_{\\mathsf{mul}}\n\n\\alpha \\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta)) has complexity 4 ~ \\mathbb{F}_{\\mathsf{mul}}. This is similar for terms 2 through n+1, so the complexity is 4n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nFor x \\in H, calculate [\\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot\\big(z(x) - c_0\\cdot a(x))\\big)]. Each term involves 3 finite field multiplications, so the total complexity is 3N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor x \\in H, calculate [\\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(x)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot a(x)]. Computing \\omega^{-1}\\cdot\\zeta involves one finite field multiplication, and for each x involves 3 finite field multiplications, so the total complexity is (3N + 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor x \\in H, calculate [\\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(x) - v)]. Each calculation involves 2 finite field multiplications, with complexity 2N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate v_H(\\zeta), with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor x \\in H, calculate [v_H(\\zeta)\\cdot t(x)], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nThe total complexity for this step is:(12N + 4n + 2) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-5","position":99},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-6","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-6","position":100},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-6","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Construct polynomial c^*(X), which is the interpolation polynomial of the following vector on D\\zeta:\\vec{c^*} = \\Big(c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), c(\\zeta)\\Big)\n\nProver can use the precomputed Bary-Centric Weights \\{\\hat{w}_i\\} on D to quickly calculate c^*(X):c^*(X) = \\frac{c^*_0 \\cdot \\frac{\\hat{w}_0}{X-\\omega\\zeta} + c^*_1 \\cdot \\frac{\\hat{w}_1}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}}{\n   \\frac{\\hat{w}_0}{X-\\omega\\zeta} + \\frac{\\hat{w}_1}{X-\\omega^2\\zeta} + \\cdots + \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}\n  }\n\nHere \\hat{w}_j are precomputed values:\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}","type":"content","url":"/analysis/ph23-analysis#round-3-6","position":101},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-6","lvl4":"Round 3-6","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-6","position":102},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-6","lvl4":"Round 3-6","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"📝 Notes\n\nThe coefficients of c(X) have already been calculated\n\nHere c(X) is calculated to get the coefficient form, then \\vec{c^*} is obtained\n\nHere we obtain the coefficient form of c^*(X)\n\nproduct fast interpolation\n\nComputing c^*(X) has complexity (n + 1) \\log^2(n + 1)\n\n📝 Previous analysis process:\n\nProver:\n\n\\vec{c^*} and the values of \\omega^{2^i}\\zeta have already been calculated in step 3 of this round.\n\nCalculate \\frac{\\hat{w}_i}{X-\\omega^{2^i}\\zeta}. The numerator is a constant, the denominator is a first-degree polynomial, so the complexity is denoted as \\mathsf{polydiv}(0, 1), and the result is a fraction.\n\nCalculate c_i^* \\cdot \\frac{\\hat{w}_i}{X-\\omega^{2^i}\\zeta}, where the complexity is denoted as \\mathsf{polymul}(0, -1).\n\nFinally, calculate c^*(X). After bringing to a common denominator, both the numerator and denominator are polynomials of degree n, so the complexity of their division is denoted as \\mathsf{polydiv}(n, n), and the resulting c^*(X) is also of degree n.\n\nPolynomial addition only involves addition in the finite field, which is not counted, so the complexity for this step c^*(X) is:>  n ~ \\mathsf{polymul}(0, -1) + n ~\\mathsf{polydiv}(0, 1) + \\mathsf{polydiv}(n, n) \n>\n\nComplexity is:(n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-6","position":103},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-7","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-7","position":104},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-7","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Since l_\\zeta(\\zeta)= 0, there exists a Quotient polynomial q_\\zeta(X) such that:q_\\zeta(X) = \\frac{1}{X-\\zeta}\\cdot l_\\zeta(X)","type":"content","url":"/analysis/ph23-analysis#round-3-7","position":105},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-7","lvl4":"Round 3-7","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-7","position":106},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-7","lvl4":"Round 3-7","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"This step’s calculation uses the algorithm below, with code:def division_by_linear_divisor(self, d):\n    \"\"\"\n    Divide a polynomial by a linear divisor (X - d) using Ruffini's rule.\n\n    Args:\n        coeffs (list): Coefficients of the polynomial, from lowest to highest degree.\n        d (Scalar): The constant term of the linear divisor.\n\n    Returns:\n        tuple: (quotient coefficients, remainder)\n    \"\"\"\n    assert len(self.coeffs) > 0, \"Polynomial degree must be at least 1\"\n\n    n = len(self.coeffs)\n    quotient = [0] * (n - 1)\n    \n    # Start with the highest degree coefficient\n    current = self.coeffs[-1]\n    \n    # Iterate through coefficients from second-highest to lowest degree\n    for i in range(n - 2, -1, -1):\n        # Store the current value in the quotient\n        quotient[i] = current\n        \n        # Compute the next value\n        current = current * d + self.coeffs[i]\n    \n    # The final current value is the remainder\n    remainder = current\n\n    return UniPolynomial(quotient), remainder\n\nFor an n-degree polynomial> f(X) = f_0 + f_1 X + f_2 X^2 + \\cdots + f_{n-1} X^{n-1} + f_n X^n\n>\n\ndivided by a linear polynomial X - d, to find its quotient polynomial and remainder, i.e., satisfying f(X) = q(X)(X - d) + r(X), we can decompose as follows:> \\begin{aligned}\n>   & f_0 + f_1 X + f_2 X^2 + \\cdots + f_{n-1} X^{n-1} + f_n X^n  \\\\\n>   = & (X -  d)(f_n \\cdot X^{n - 1}) + d \\cdot f_n \\cdot X^{n - 1} + f_{n - 1} X^{n - 1} + \\cdots + f_1 X + f_0 \\\\\n>   = & (X -  d)(f_n \\cdot X^{n - 1}) + (X - d)((df_n + f_{n - 1}) \\cdot X^{n - 2}) \\\\\n>   & + d \\cdot (df_n + f_{n - 1}) + f_{n - 2} X^{n - 2} + \\cdots + f_1 X + f_0 \\\\\n> \\end{aligned}\n>\n\nFrom the above equation, we find:> \\begin{aligned}\n>   & q_{n - 1} = f_n \\\\\n>   & q_i = d \\cdot q_{i + 1} + f_{i + 1} , \\quad i = n - 2, \\ldots, 0 \\\\\n> \\end{aligned}\n>\n\nSo the final remainder is:> r(X) = d \\cdot q_0 + f_0\n>\n\nHere i runs from n - 2, \\ldots, 0, each involving one finite field multiplication, and calculating r(X) also involves one multiplication, so the complexity is n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComing back to analyzing the complexity of calculating q_\\zeta(X), we need to analyze the degree of l_\\zeta(X).> \\begin{split}\n> l_\\zeta(X) =& \\Big(s_0(\\zeta) \\cdot (c(\\zeta) - c_0) \\\\\n> & + \\alpha\\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\\\\n>   & + \\alpha^2\\cdot s_1(\\zeta) \\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta)) \\\\\n>   & + \\cdots \\\\\n>   & + \\alpha^{n-1}\\cdot s_{n-2}(\\zeta)\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\\\\n>   & + \\alpha^n\\cdot s_{n-1}(\\zeta)\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta)) \\\\\n>   & + \\alpha^{n+1}\\cdot (L_0(\\zeta)\\cdot\\big(z(X) - c_0\\cdot a(X))\\\\\n>   & + \\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot a(X) \\big) \\\\\n>   & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(X) - v) \\\\\n>   & - v_H(\\zeta)\\cdot t(X)\\ \\Big)\n> \\end{split}\n>\n\nThe first few terms are constants.\n\n\\alpha^{n+1}\\cdot (L_0(\\zeta)\\cdot\\big(z(X) - c_0\\cdot a(X)) has degree N - 1 + N - 1 = 2N - 2.\n\n\\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot a(X) \\big) has degree N - 1.\n\n\\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(X) - v) has degree N - 1 + N - 1 = 2N - 2. v_H(\\zeta)\\cdot t(X) has degree 2N - 1.\n\nSo l_\\zeta(X) has degree 2N - 1. Therefore, calculating q_\\zeta(X) has complexity (2N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\n📝 Efficient Inversion Algorithm\n\nIn general, for N arbitrary points a_0, \\ldots, a_{N - 1}, to find their inverses a_0^{-1}, \\ldots, a_{N-1}^{-1}, direct inversion is computationally expensive. We want to convert inversion operations to multiplications in the finite field. The specific algorithm is:\n\nFirst calculate the N products:\\begin{aligned}\n& b_0 = a_0 \\\\\n& b_1 = b_0 \\cdot a_1 = a_0 \\cdot a_1 \\\\\n& b_2 = b_1 \\cdot a_2 = a_0 \\cdot a_1 \\cdot a_2 \\\\\n& \\ldots \\\\\n& b_{N - 2} = b_{N - 3} \\cdot a_{N - 2} = a_0 \\cdot a_1 \\cdots a_{N - 2} \\\\\n& b_{N - 1} = b_{N - 2} \\cdot a_{N - 1} = a_0 \\cdot a_1 \\cdots a_{N - 2} \\cdot a_{N - 1}\\\\\n\\end{aligned}\n\nCalculate b_1, \\ldots, b_{N - 1}, each involving one finite field multiplication, with complexity (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate b_{N-1}^{-1}, with complexity \\mathbb{F}_{\\mathsf{inv}}.\n\nCalculate:\\begin{aligned}\n  & b_{N-2}^{-1} = (a_0 \\cdot a_1 \\cdots a_{N - 2} )^{-1} = a_{N - 1} \\cdot b_{N-1}^{-1} \\\\\n  & b_{N-3}^{-1} = (a_0 \\cdot a_1 \\cdots a_{N - 3} )^{-1} = a_{N - 2} \\cdot b_{N-2}^{-1} \\\\\n  & \\ldots \\\\\n  & b_{1}^{-1} = (a_0 \\cdot a_1 )^{-1} = a_{2} \\cdot b_{2}^{-1} \\\\\n\\end{aligned}\n\nThe complexity for this step is (N - 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nNow calculate a_0^{-1}, \\ldots, a_{N-1}^{-1} from the beginning by multiplication:\\begin{aligned}\n  & a_0^{-1} = \\frac{1}{a_0 \\cdot a_1} \\cdot a_1 = b_1^{-1} \\cdot a_1 \\\\\n  & a_1^{-1} = \\frac{1}{a_0 \\cdot a_1} \\cdot a_0 = b_1^{-1} \\cdot b_0\\\\\n  & a_2^{-1} = \\frac{1}{a_0 \\cdot a_1 \\cdot a_2} \\cdot (a_0 \\cdot a_1) = b_2^{-1} \\cdot b_1\\\\\n  & \\ldots \\\\\n  & a_{N - 2}^{-1} = \\frac{1}{a_0 \\cdot a_1 \\cdot a_2 \\cdots a_{N - 3} \\cdot a_{N - 2}} \\cdot (a_0 \\cdot a_1 \\cdot a_2 \\cdots a_{N - 3} ) = b_{N - 2}^{-1} \\cdot b_{N - 3} \\\\\n  & a_{N - 1}^{-1} = \\frac{1}{a_0 \\cdot a_1 \\cdot a_2 \\cdots a_{N - 2} \\cdot a_{N - 1}} \\cdot (a_0 \\cdot a_1 \\cdot a_2 \\cdots a_{N - 2} ) = b_{N - 1}^{-1} \\cdot b_{N - 2} \n\\end{aligned}\n\nThe complexity is N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the total complexity to calculate the inverses a_0^{-1}, \\ldots, a_{N-1}^{-1} is \\mathbb{F}_{\\mathsf{inv}} + (3N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nAnalyzing Prover Cost\n\nIn Round 3-5, we already calculated [l_{\\zeta}(x)|_{x \\in H}]. Now we’ll calculate [q_{\\zeta}(x)|_{x \\in H}].\n\nFirst calculate N inverses, [(x - \\zeta)^{-1}|_{x \\in H}], using the efficient inversion algorithm described above, with complexity \\mathbb{F}_{\\mathsf{inv}} + (3N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [q_{\\zeta}(x)|_{x \\in H}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}\n\nSo the total complexity for this step is:\\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-7","position":107},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-8","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-8","position":108},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-8","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Step 8. Construct the vanishing polynomial z_{D_{\\zeta}}(X) on D\\zeta:z_{D_{\\zeta}}(X) = (X-\\zeta\\omega)\\cdots (X-\\zeta\\omega^{2^{n-1}})(X-\\zeta)","type":"content","url":"/analysis/ph23-analysis#round-3-8","position":109},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-8","lvl4":"Round 3-8","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-8","position":110},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-8","lvl4":"Round 3-8","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"In Round 3-6, the coefficient form of the vanishing polynomial z_{D_{\\zeta}}(X) has already been calculated.","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-8","position":111},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-9","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-9","position":112},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-9","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Step v,. Construct the Quotient polynomial q_c(X):q_c(X) = \\frac{(c(X) - c^*(X))}{(X-\\zeta)(X-\\omega\\zeta)(X-\\omega^2\\zeta)\\cdots(X-\\omega^{2^{n-1}}\\zeta)}","type":"content","url":"/analysis/ph23-analysis#round-3-9","position":113},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-9","lvl4":"Round 3-9","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-9","position":114},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-9","lvl4":"Round 3-9","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Since the degree of the polynomial in the denominator is relatively high, it’s more efficient to perform the calculation using point-value form.\n\nWe already have the coefficient forms of c^*(X) and z_{D_{\\zeta}}(X), calculated in Round 3-6.\n\nCalculate [c^*(x)|_{x \\in H}] using one FFT to evaluate c^*(X) on H, with complexity \\mathsf{FFT}(N).\n\nCalculate [z_{D_{\\zeta}}(x)|_{x \\in H}] using one FFT to evaluate z_{D_{\\zeta}}(X) on H, with complexity \\mathsf{FFT}(N).\n\nCalculate the inverse [(z_{D_{\\zeta}}(x))^{-1}|_{x \\in H}] using the efficient inversion algorithm described earlier, with complexity \\mathbb{F}_{\\mathsf{inv}} + (3N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [q_c(x)|_{x \\in H}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the total complexity for this step is:2 ~ \\mathsf{FFT}(N) + \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-9","position":115},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-10","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-10","position":116},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-10","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Step 10. Construct the Quotient polynomial q_{\\omega\\zeta}(X):q_{\\omega\\zeta}(X) = \\frac{z(X) - z(\\omega^{-1}\\cdot\\zeta)}{X - \\omega^{-1}\\cdot\\zeta}","type":"content","url":"/analysis/ph23-analysis#round-3-10","position":117},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-10","lvl4":"Round 3-10","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-10","position":118},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-10","lvl4":"Round 3-10","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Method 1: Division using coefficient form.\n\nIn Round 2-4, we already calculated the coefficient form of z(X), so the coefficients of the polynomial z(X) - z(\\omega^{-1}\\cdot\\zeta) are easy to obtain - just change the constant term. The denominator polynomial is a linear polynomial, so its coefficient form can also be directly obtained. This involves division of a univariate polynomial by a linear polynomial, with complexity (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nMethod 2: Calculation using point-value form.\n\nCalculate [q_{\\omega\\zeta}(x)|_{x \\in H}],\n\nFirst calculate [(x - \\omega^{-1} \\cdot \\zeta)^{-1}|_{x \\in H}] using the efficient inversion algorithm, with complexity \\mathbb{F}_{\\mathsf{inv}} + (3N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [q_{\\omega\\zeta}(x)|_{x \\in H}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nThe total complexity for this method is:\\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nWe can see that since the denominator is only a linear polynomial, Method 1 would be more efficient.","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-10","position":119},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-11","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-3-11","position":120},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 3-11","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Step 11. Send \\big(Q_c = [q_c(\\tau)]_1, Q_\\zeta=[q_\\zeta(\\tau)]_1, Q_{\\omega\\zeta}=[q_{\\omega\\zeta}(\\tau)]_1, \\big)","type":"content","url":"/analysis/ph23-analysis#round-3-11","position":121},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-11","lvl4":"Round 3-11","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-3-11","position":122},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 3-11","lvl4":"Round 3-11","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Using [q_c(x)|_{x \\in H}] obtained in Round 3-9:Q_c = q_c(\\omega^0) \\cdot A_0 + \\ldots q_c(\\omega^{N - 1}) \\cdot A_{N - 1}\n\nwith complexity \\mathsf{msm}(N, \\mathbb{G}_1).\n\nUsing [q_{\\zeta}(x)|_{x \\in H}] obtained in Round 3-7:Q_\\zeta = q_\\zeta(\\omega^0) \\cdot A_0 + \\ldots q_\\zeta(\\omega^{N - 1}) \\cdot A_{N - 1}\n\nwith complexity \\mathsf{msm}(N, \\mathbb{G}_1).\n\nFrom Round 3-10:\n\nIf using Method 1, we get the coefficient form of q_{\\omega\\zeta}(X): q_{\\omega\\zeta}^{(0)}, q_{\\omega\\zeta}^{(1)}, \\ldots, q_{\\omega\\zeta}^{(N - 2)}, then:Q_{\\omega\\zeta} = q_{\\omega\\zeta}^{(0)} \\cdot G + q_{\\omega\\zeta}^{(1)} \\cdot (\\tau \\cdot G) + \\cdots + q_{\\omega\\zeta}^{(N - 2)} \\cdot (\\tau^{N - 2} \\cdot G)\n\nwhere G is the generator of the elliptic curve \\mathbb{G}_1, and (G, \\tau G, \\ldots, \\tau^{N - 2}G) is the KZG10 SRS. The complexity for this method is \\mathsf{msm}(N - 1, \\mathbb{G}_1).\n\nIf using Method 2, we get [q_{\\omega\\zeta}(x)|_{x \\in H}], then:Q_\\zeta = q_{\\omega\\zeta}(\\omega^0) \\cdot A_0 + \\ldots q_{\\omega \\zeta}(\\omega^{N - 1}) \\cdot A_{N - 1}\n\nwith complexity \\mathsf{msm}(N, \\mathbb{G}_1).\n\nSummarizing the complexities above:\n\nUsing Method 1 for Round 3-10 (coefficient form), the complexity is:2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\nUsing Method 2 for Round 3-10 (point-value form), the complexity is:3 ~ \\mathsf{msm}(N, \\mathbb{G}_1)","type":"content","url":"/analysis/ph23-analysis#prover-cost-3-11","position":123},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#prover-cost-round-3","position":124},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3.","lvl2":"Evaluation proof protocol"},"content":"Adding up all the computational complexities for this round:\n\nUsing Method 1 for Round 3-10 (coefficient form), the complexity is:\\begin{aligned}\n& 2(n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + n ~\\mathbb{F}_{\\mathsf{mul}} + \\mathsf{FFT}(N) + (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (12N + 4n + 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{FFT}(N) + \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + {\\color{red} (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}} + {\\color{red} 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} \\\\\n= & 3 ~ \\mathsf{FFT}(N) + (21N + 7n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + 2~ \\mathbb{F}_{\\mathsf{inv}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } \\\\\n& + {\\color{red} (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}} + {\\color{red} 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} \\\\\n= & 3 ~ \\mathsf{FFT}(N) + (22N + 7n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + 2~ \\mathbb{F}_{\\mathsf{inv}} + {\\color{} 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} }\n\\end{aligned}\n\nThis method requires storing the SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory to facilitate polynomial commitment in coefficient form.\n\nUsing Method 2 for Round 3-10 (point-value form), the complexity is:\\begin{aligned}\n& 2(n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + n ~\\mathbb{F}_{\\mathsf{mul}} + \\mathsf{FFT}(N) + (N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (12N + 4n + 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{FFT}(N) + \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + {\\color{red} \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}} + {\\color{red} 3 ~ \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n= & 3 ~ \\mathsf{FFT}(N) + (21N + 7n - 3) ~ \\mathbb{F}_{\\mathsf{mul}} + 2~ \\mathbb{F}_{\\mathsf{inv}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } \\\\\n& + {\\color{red} \\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}} + {\\color{red} 3 ~ \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n= & 3 ~ \\mathsf{FFT}(N) + (25N + 7n - 6) ~ \\mathbb{F}_{\\mathsf{mul}} + 3~ \\mathbb{F}_{\\mathsf{inv}} + 3 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } \\\\\n\\end{aligned}","type":"content","url":"/analysis/ph23-analysis#prover-cost-round-3","position":125},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#round-4","position":126},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"","type":"content","url":"/analysis/ph23-analysis#round-4","position":127},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-1","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-4-1","position":128},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-1","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"The Verifier sends a second random challenge \\xi\\leftarrow_{\\$}\\mathbb{F}_p","type":"content","url":"/analysis/ph23-analysis#round-4-1","position":129},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-2","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-4-2","position":130},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-2","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"Prover constructs the third Quotient polynomial q_\\xi(X):q_\\xi(X) = \\frac{c(X) - c^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot q_c(X)}{X-\\xi}","type":"content","url":"/analysis/ph23-analysis#round-4-2","position":131},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 4-2","lvl4":"Round 4-2","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-4-2","position":132},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 4-2","lvl4":"Round 4-2","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"c^*(X) has degree N - 1, so calculating c^*(\\xi) has complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nz_{D_\\zeta}(X) has degree n + 1, so calculating z_{D_\\zeta}(\\xi) has complexity (n + 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nIn Round 3-9, we already obtained [q_c(x)|_{x \\in H}]. First calculate [(z_{D_\\zeta}(\\xi)\\cdot q_c(x))|_{x \\in H}], with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate the coefficient form of z_{D_\\zeta}(\\xi)\\cdot q_c(X) using one IFFT, with complexity \\mathsf{IFFT}(N).\n\nCalculate \\frac{c(X) - c^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot q_c(X)}{X-\\xi} using linear division. The numerator polynomial has degree N - 1, so the complexity is (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity for this step is:\\mathsf{IFFT}(N) + (3N + n + 1) ~ \\mathbb{F}_{\\mathsf{mul}}","type":"content","url":"/analysis/ph23-analysis#prover-cost-4-2","position":133},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-3","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#round-4-3","position":134},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Round 4-3","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"Prover calculates and sends Q_\\xi:Q_\\xi = \\mathsf{KZG10.Commit}(q_\\xi(X)) = [q_\\xi(\\tau)]_1","type":"content","url":"/analysis/ph23-analysis#round-4-3","position":135},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 4-3","lvl4":"Round 4-3","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl5","url":"/analysis/ph23-analysis#prover-cost-4-3","position":136},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl5":"Prover Cost 4-3","lvl4":"Round 4-3","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"The previous step calculated the coefficient form of q_\\xi(X), so the complexity of the commitment mainly depends on the polynomial’s degree. \\deg(q_\\xi) = N - 2, so the complexity is \\mathsf{msm}(N - 1, \\mathbb{G}_1).\n\nThis method requires storing the SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory.","type":"content","url":"/analysis/ph23-analysis#prover-cost-4-3","position":137},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"type":"lvl4","url":"/analysis/ph23-analysis#prover-cost-round-4","position":138},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4.","lvl2":"Evaluation proof protocol"},"content":"Summarizing the complexity for this round:\\mathsf{IFFT}(N) + (3N + n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N - 1, \\mathbb{G}_1)","type":"content","url":"/analysis/ph23-analysis#prover-cost-round-4","position":139},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Cost","lvl2":"Evaluation proof protocol"},"type":"lvl3","url":"/analysis/ph23-analysis#prover-cost","position":140},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Prover Cost","lvl2":"Evaluation proof protocol"},"content":"Summarizing the Prover Cost for all rounds:\n\nUsing Method 1 for Round 3-10 (coefficient form), the complexity is:\\begin{align}\n & {\\color{blue} (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n& + {\\color{red} 4 ~ \\mathsf{FFT}(N) + 4 ~ \\mathsf{IFFT}(N) + (5nN + 10N + n + 2) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n & + 3 ~ \\mathsf{FFT}(N) + (22N + 7n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + 2~ \\mathbb{F}_{\\mathsf{inv}} + {\\color{} 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } \\\\\n & + {\\color{purple}\\mathsf{IFFT}(N) + (3N + n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} \\\\\n= & (17nN + 36N + 9n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + 2~ \\mathbb{F}_{\\mathsf{inv}} + 5 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}\n\nThis method requires storing the SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory for polynomial commitment in coefficient form.\n\nUsing Method 2 for Round 3-10 (point-value form), the complexity is:\\begin{align}\n & {\\color{blue} (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n& + {\\color{red} 4 ~ \\mathsf{FFT}(N) + 4 ~ \\mathsf{IFFT}(N) + (5nN + 10N + n + 2) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{msm}(N, \\mathbb{G}_1)} \\\\\n & + 3 ~ \\mathsf{FFT}(N) + (25N + 7n - 6) ~ \\mathbb{F}_{\\mathsf{mul}} + 3~ \\mathbb{F}_{\\mathsf{inv}} + 3 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } \\\\\n& + {\\color{purple}\\mathsf{IFFT}(N) + (3N + n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N - 1, \\mathbb{G}_1)} \\\\\n= & (17nN + 39N + 9n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + 3~ \\mathbb{F}_{\\mathsf{inv}} + 6 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}","type":"content","url":"/analysis/ph23-analysis#prover-cost","position":141},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Proof Representation"},"type":"lvl2","url":"/analysis/ph23-analysis#proof-representation","position":142},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Proof Representation"},"content":"7\\cdot\\mathbb{G}_1, (n+1)\\cdot\\mathbb{F}_{p}\\begin{aligned}\n\\pi_{eval} &= \\big(z(\\omega^{-1}\\cdot\\zeta), c(\\zeta)，c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), \\\\\n& \\qquad C_{c}, C_{t}, C_{z}, Q_c, Q_\\zeta, Q_\\xi, Q_{\\omega\\zeta}\\big)\n\\end{aligned}","type":"content","url":"/analysis/ph23-analysis#proof-representation","position":143},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Verification Process"},"type":"lvl2","url":"/analysis/ph23-analysis#verification-process","position":144},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Verification Process"},"content":"","type":"content","url":"/analysis/ph23-analysis#verification-process","position":145},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 1","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-1","position":146},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 1","lvl2":"Verification Process"},"content":"Verifier calculates c^*(\\xi) using the precomputed Barycentric Weights \\{\\hat{w}_i\\}:c^*(\\xi)=\\frac{\\sum_i c_i^*\\frac{\\hat{w}_i}{\\xi-x_i}}{\\sum_i \\frac{\\hat{w}_i}{\\xi-x_i}}\n\nThen calculates the corresponding commitment C^*(\\xi)=[c^*(\\xi)]_1.","type":"content","url":"/analysis/ph23-analysis#step-1","position":147},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 1","lvl3":"Step 1","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-1","position":148},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 1","lvl3":"Step 1","lvl2":"Verification Process"},"content":"Verifier:\n\nFirst analyzing the complexity of each calculation: To calculate \\frac{\\hat{w}_i}{\\xi-x_i}, the numerator \\hat{w}_i can be obtained from precomputation, and after calculating the denominator \\xi-x_i, we need to compute its inverse, then multiply with \\hat{w}_i. The complexity is therefore \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}}.\n\nCalculate c_i^*\\frac{\\hat{w}_i}{\\xi-x_i}, with complexity \\mathbb{F}_{\\mathsf{mul}}.\n\nFinally, dividing the numerator by the denominator is equivalent to inverting the denominator and multiplying with the numerator, with complexity \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}}.\n\nAfter obtaining c^*(\\xi), calculate its commitment C^*(\\xi), with complexity \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nSo the total complexity for this step is:> \\begin{aligned}\n>   & (n + 1) ~ (\\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}}) + (n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + \\mathsf{EccMul}^{\\mathbb{G}_1} \\\\\n>  = & \\color{orange}{(2n + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + \\mathsf{EccMul}^{\\mathbb{G}_1}}\n> \\end{aligned}\n>","type":"content","url":"/analysis/ph23-analysis#verifier-cost-1","position":149},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 2","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-2","position":150},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 2","lvl2":"Verification Process"},"content":"Verifier calculates v_H(\\zeta), L_0(\\zeta), L_{N-1}(\\zeta):v_H(\\zeta) = \\zeta^N - 1L_0(\\zeta) = \\frac{1}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-1}L_{N-1}(\\zeta) = \\frac{\\omega^{N-1}}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-\\omega^{N-1}}","type":"content","url":"/analysis/ph23-analysis#step-2","position":151},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 2","lvl3":"Step 2","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-2","position":152},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 2","lvl3":"Step 2","lvl2":"Verification Process"},"content":"Verifier:\n\nv_H(\\zeta): \\zeta^N can be calculated using \\log N finite field multiplications, with complexity \\log N ~ \\mathbb{F}_{\\mathsf{mul}}\n\nL_0(\\zeta): 1/N can be provided in precomputation. Calculating the inverse of \\zeta-1 involves one inversion operation in the finite field, with complexity \\mathbb{F}_{\\mathsf{inv}}. Multiplying the inverse of \\zeta-1 with v_{H}(\\zeta) involves one multiplication in the finite field, with complexity \\mathbb{F}_{\\mathsf{mul}}. The result is then multiplied with 1/N, with complexity \\mathbb{F}_{\\mathsf{mul}}. So the total complexity for this step is \\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nL_{N-1}(\\zeta): \\omega^{N-1}/N can be provided in precomputation. Calculating the inverse of \\zeta-\\omega^{N-1} involves one inversion operation in the finite field, with complexity \\mathbb{F}_{\\mathsf{inv}}. Multiplying the inverse of \\zeta-\\omega^{N-1} with v_{H}(\\zeta) involves one multiplication in the finite field, with complexity \\mathbb{F}_{\\mathsf{mul}}. The result is then multiplied with \\omega^{N-1}/N, with complexity \\mathbb{F}_{\\mathsf{mul}}. So the total complexity for this step is \\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTherefore, the total complexity for this step is 2 ~ \\mathbb{F}_{\\mathsf{inv}} + (\\log N + 4) ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#verifier-cost-2","position":153},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 3","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-3","position":154},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 3","lvl2":"Verification Process"},"content":"Verifier calculates s_0(\\zeta), \\ldots, s_{n-1}(\\zeta) using the recursive method mentioned earlier.","type":"content","url":"/analysis/ph23-analysis#step-3","position":155},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 3","lvl3":"Step 3","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-3","position":156},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 3","lvl3":"Step 3","lvl2":"Verification Process"},"content":"\\zeta^2, \\zeta^4, \\ldots, \\zeta^{2^{n - 1}} can be obtained while calculating \\zeta^N in Step 2.\n\nThe remaining calculation is consistent with the analysis in Round 3-1, with complexity (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#verifier-cost-3","position":157},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 4","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-4","position":158},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 4","lvl2":"Verification Process"},"content":"Verifier calculates z_{D_\\zeta}(\\xi):z_{D_{\\zeta}}(\\xi) = (\\xi-\\zeta\\omega)\\cdots (\\xi-\\zeta\\omega^{2^{n-1}})(\\xi-\\zeta)","type":"content","url":"/analysis/ph23-analysis#step-4","position":159},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 4","lvl3":"Step 4","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-4","position":160},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 4","lvl3":"Step 4","lvl2":"Verification Process"},"content":"Verifier:\n\nThe calculation of \\xi-\\zeta\\omega^i was already done in the previous steps, so the complexity here mainly involves multiplying n numbers in the finite field, with complexity (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/ph23-analysis#verifier-cost-4","position":161},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 5","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-5","position":162},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 5","lvl2":"Verification Process"},"content":"Verifier calculates the commitment to the linearized polynomial C_l:\\begin{split}\nC_l & = \n\\Big( \\Big((c(\\zeta) - c_0)s_0(\\zeta) \\\\\n& + \\alpha \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\cdot s_0(\\zeta)\\\\\n  & + \\alpha^2\\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta))\\cdot s_1(\\zeta)  \\\\\n  & + \\cdots \\\\\n  & + \\alpha^{n-1}\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\cdot s_{n-2}(\\zeta)\\\\\n  & + \\alpha^n\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta))\\cdot s_{n-1}(\\zeta) \\Big) \\cdot [1]_1 \\\\\n  & + \\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot(C_z - c_0\\cdot C_a)\\\\\n  & + \\alpha^{n+2}\\cdot (\\zeta-1)\\cdot\\big(C_z - z(\\omega^{-1}\\cdot \\zeta)\\cdot [1]_1-c(\\zeta)\\cdot C_{a} ) \\\\\n  & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(C_z - v \\cdot [1]_1) \\\\\n  & - v_H(\\zeta)\\cdot C_t \\Big)\n\\end{split}","type":"content","url":"/analysis/ph23-analysis#step-5","position":163},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 5","lvl3":"Step 5","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-5","position":164},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 5","lvl3":"Step 5","lvl2":"Verification Process"},"content":"Verifier:\n\nFirst calculate \\alpha^2, \\ldots, \\alpha^{n+3}, involving n + 2 finite field multiplications, with complexity (n + 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\ns_0(\\zeta) \\cdot (c(\\zeta) - c_0) involves one finite field multiplication, with complexity \\mathbb{F}_{\\mathsf{mul}}\n\n\\alpha \\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta)) has complexity 4 ~ \\mathbb{F}_{\\mathsf{mul}}. This applies to terms 2 through n+1, so the total complexity is 4n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe results above are summed to get a finite field value, which is then multiplied with [1]_1, with complexity \\mathsf{EccMul}^{\\mathbb{G}_1}\n\n\\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot(C_z - c_0\\cdot C_a):\n\nc_0\\cdot C_a has complexity \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nC_z - c_0\\cdot C_a involves elliptic curve subtraction, which is calculated as addition with the negative point. Since subtraction on elliptic curves is essentially addition with the negative point, and if P_2 = (x_2, y_2), then -P_2 = (x_2, -y_2), the complexity is essentially the same as addition. So the complexity is \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n📝 For the Python implementation of addition or subtraction on elliptic curves, refer to \n\npy_ecc.\n\n\\alpha^{n+1}\\cdot L_0(\\zeta) has complexity \\mathbb{F}_{\\mathsf{mul}}\n\n\\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot(C_z - c_0\\cdot C_a) involves multiplying the results from above, with complexity \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nSo the total complexity for this calculation is \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n\\alpha^{n+2}\\cdot (\\zeta-1)\\cdot\\big(C_z - z(\\omega^{-1}\\cdot \\zeta)\\cdot [1]_1-c(\\zeta)\\cdot C_{a} \\big):\n\nc(\\zeta)\\cdot C_{a}: \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nz(\\omega^{-1}\\cdot \\zeta)\\cdot [1]_1: \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nC_z - z(\\omega^{-1}\\cdot \\zeta)\\cdot [1]_1-c(\\zeta)\\cdot C_{a}: 2 ~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n\\alpha^{n+2}\\cdot (\\zeta-1): \\mathbb{F}_{\\mathsf{mul}}\n\n\\alpha^{n+2}\\cdot (\\zeta-1)\\cdot\\big(C_z - z(\\omega^{-1}\\cdot \\zeta)\\cdot [1]_1-c(\\zeta)\\cdot C_{a} \\big): \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nTotal: \\mathbb{F}_{\\mathsf{mul}} + 3~\\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n\\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(C_z - v \\cdot [1]_1):\n\nv \\cdot [1]_1: \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nC_z - v \\cdot [1]_1: \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n\\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(C_z - v \\cdot [1]_1): \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nTotal: \\mathbb{F}_{\\mathsf{mul}} + 2~\\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nv_H(\\zeta)\\cdot C_t: \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nSum all the results above, involving 4 additions on the elliptic curve, with complexity 4 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nTherefore, the total complexity for calculating l_{\\zeta}(X) in this step is:> \\begin{aligned}\n>   & (n + 2 + 1 + 4n) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1} \\\\\n>   & + \\mathbb{F}_{\\mathsf{mul}} + 2~\\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + \\mathbb{F}_{\\mathsf{mul}} + 3~\\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~\\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + \\mathbb{F}_{\\mathsf{mul}} + 2~\\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + \\mathsf{EccMul}^{\\mathbb{G}_1} + 4 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   = & (5n + 6) ~ \\mathbb{F}_{\\mathsf{mul}} + 9 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 8 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n> \\end{aligned}\n>","type":"content","url":"/analysis/ph23-analysis#verifier-cost-5","position":165},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 6","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#step-6","position":166},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Step 6","lvl2":"Verification Process"},"content":"Verifier generates a random number \\eta to merge the following Pairing verifications:\\begin{split}\ne(C_l + \\zeta\\cdot Q_\\zeta, [1]_2)\\overset{?}{=}e(Q_\\zeta, [\\tau]_2)\\\\\ne(C_c - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi, [1]_2) \\overset{?}{=} e(Q_\\xi, [\\tau]_2)\\\\\ne(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1, [1]_2) \\overset{?}{=} e(Q_{\\omega\\zeta}, [\\tau]_2)\\\\\n\\end{split}\n\nThe merged verification only requires two Pairing operations:\n\n$$\n\\begin{split}\n\nP &= \\Big(C_l + \\zeta\\cdot Q_\\zeta\\Big) \\\n\n& + \\eta\\cdot \\Big(C_c - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi\\Big) \\\n\n& + \\eta^2\\cdot\\Big(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1\\Big)\n\n\\end{split}\n$$e\\Big(P, [1]_2\\Big) \\overset{?}{=} e\\Big(Q_\\zeta + \\eta\\cdot Q_\\xi + \\eta^2\\cdot Q_{\\omega\\zeta}, [\\tau]_2\\Big)","type":"content","url":"/analysis/ph23-analysis#step-6","position":167},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 6","lvl3":"Step 6","lvl2":"Verification Process"},"type":"lvl4","url":"/analysis/ph23-analysis#verifier-cost-6","position":168},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl4":"Verifier Cost 6","lvl3":"Step 6","lvl2":"Verification Process"},"content":"Verifier:\n\nFirst calculate \\eta^2, with complexity \\mathbb{F}_{\\mathsf{mul}}\n\n\\Big(C_l + \\zeta\\cdot Q_\\zeta\\Big): \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\n\\eta\\cdot \\Big(C_c - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi\\Big):>   2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 3 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_1} = 3 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 3 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n>\n\n\\eta^2\\cdot\\Big(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1\\Big): 3 ~\\mathsf{EccMul}^{\\mathbb{G}_1} + 2~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nCalculate P by adding the results above, with complexity 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\ne\\Big(P, [1]_2\\Big) involves one pairing operation on the elliptic curve, denoted as P\n\nQ_\\zeta + \\eta\\cdot Q_\\xi + \\eta^2\\cdot Q_{\\omega\\zeta}: 2 ~\\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\ne\\Big(Q_\\zeta + \\eta\\cdot Q_\\xi + \\eta^2\\cdot Q_{\\omega\\zeta}, [\\tau]_2\\Big) involves one pairing operation on the elliptic curve, with complexity P\n\nAdding up all the results above, the total complexity for this step is:> \\begin{aligned}\n>   & \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + 3 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 3 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + 3 ~\\mathsf{EccMul}^{\\mathbb{G}_1} + 2~\\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + P + 2 ~\\mathsf{EccMul}^{\\mathbb{G}_1} + P \\\\\n>   = & \\mathbb{F}_{\\mathsf{mul}} + 9 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 8 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n> \\end{aligned}\n>","type":"content","url":"/analysis/ph23-analysis#verifier-cost-6","position":169},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Verification Process"},"type":"lvl3","url":"/analysis/ph23-analysis#verifier-cost","position":170},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Verification Process"},"content":"\\begin{aligned}\n  & {\\color{orange} (2n + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + \\mathsf{EccMul}^{\\mathbb{G}_1}} \\\\\n  & + 2 ~ \\mathbb{F}_{\\mathsf{inv}} + (\\log N + 4) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n  & + 2(n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n  & + (n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n  & + (5n + 6) ~ \\mathbb{F}_{\\mathsf{mul}} + 9 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 8 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n  & + \\mathbb{F}_{\\mathsf{mul}} + 9 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 8 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P \\\\\n  = & (9n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 18 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 16 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P \\\\\n  & + {\\color{orange} (2n + 3) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + \\mathsf{EccMul}^{\\mathbb{G}_1}} \\\\\n  = & (11n + 11) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 4) ~ \\mathbb{F}_{\\mathsf{inv}} + 19 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 16 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n\\end{aligned}","type":"content","url":"/analysis/ph23-analysis#verifier-cost","position":171},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Protocol Complexity Summary"},"type":"lvl2","url":"/analysis/ph23-analysis#protocol-complexity-summary","position":172},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl2":"Protocol Complexity Summary"},"content":"","type":"content","url":"/analysis/ph23-analysis#protocol-complexity-summary","position":173},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Commit Phase","lvl2":"Protocol Complexity Summary"},"type":"lvl3","url":"/analysis/ph23-analysis#commit-phase","position":174},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Commit Phase","lvl2":"Protocol Complexity Summary"},"content":"Prover’s cost:N\\log N ~\\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(N, \\mathbb{G}_1)","type":"content","url":"/analysis/ph23-analysis#commit-phase","position":175},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Evaluation Protocol","lvl2":"Protocol Complexity Summary"},"type":"lvl3","url":"/analysis/ph23-analysis#evaluation-protocol","position":176},{"hierarchy":{"lvl1":"PH23 Protocol Complexity Analysis","lvl3":"Evaluation Protocol","lvl2":"Protocol Complexity Summary"},"content":"Prover’s cost:\n\nUsing Method 1 for Round 3-10 (coefficient form), the complexity is:\\begin{align}\n (17nN + 36N + 9n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + 2~ \\mathbb{F}_{\\mathsf{inv}} + 5 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}\n\nThis method requires storing the SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory for polynomial commitment in coefficient form.\n\nUsing Method 2 for Round 3-10 (point-value form), the complexity is:\\begin{align}\n  (17nN + 39N + 9n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + {\\color{orange} (n + 1) \\log^2(n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} } + 3~ \\mathbb{F}_{\\mathsf{inv}} + 6 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}\n\nVerifier’s cost:(11n + 11) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 4) ~ \\mathbb{F}_{\\mathsf{inv}} + 19 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 16 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n\nProof size:(n + 1) \\cdot \\mathbb{F}_p + 7 ~ \\mathbb{G}_1","type":"content","url":"/analysis/ph23-analysis#evaluation-protocol","position":177},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series"},"type":"lvl1","url":"/analysis/zeromorph-anlysis","position":0},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io","type":"content","url":"/analysis/zeromorph-anlysis","position":1},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl2","url":"/analysis/zeromorph-anlysis#complexity-analysis-of-the-evaluation-proof-protocol-naive-version","position":2},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Protocol description doc: \n\nEvaluation Proof Protocol (Naive Version)\n\nBelow we present a simple naive implementation of the protocol for ease of understanding.","type":"content","url":"/analysis/zeromorph-anlysis#complexity-analysis-of-the-evaluation-proof-protocol-naive-version","position":3},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#common-inputs","position":4},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Commitment to the MLE polynomial \\tilde{f}: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/analysis/zeromorph-anlysis#common-inputs","position":5},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#witness","position":6},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Point value vector of the MLE polynomial \\tilde{f} on the n-dimensional HyperCube: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/analysis/zeromorph-anlysis#witness","position":7},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-1","position":8},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Prover sends the commitments to the remainder polynomials:\n\nComputes n remainder MLE polynomials: \\{\\tilde{q}_k\\}_{k=0}^{n-1}\n\nConstructs univariate polynomials mapped from the remainder MLE polynomials: \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n\n\nComputes and sends their commitments: \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1})\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{k=0}^{n-1} (X_k-u_k) \\cdot \\tilde{q}_k(X_0,X_1,\\ldots, X_{k-1})\n\nProver computes \\pi_k=\\mathsf{cm}(X^{D_{max}-2^k+1}\\cdot \\hat{q}_k), \\quad 0\\leq k<n, as a degree bound proof for \\deg(\\hat{q}_k)<2^k, and sends them to the Verifier.\n\nProver:\n\nUsing the algorithm from \n\nZeromorph paper Appendix A.2, the values of \\tilde{q}_k on the Hypercube can be computed to obtain the coefficients of Q_k. According to the paper, the overall algorithm complexity is (2^{n+1} - 3) ~ \\mathbb{F}_{\\mathsf{add}} and (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}. Ignoring the addition complexity, the complexity of computing \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n is (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1}) involves the MSM algorithm. For each \\mathsf{cm}(\\hat{q}_{k}), polynomial q_{k} has 2^k coefficients, with complexity \\mathsf{msm}(2^k,\\mathbb{G}_1). Therefore, the total complexity for this step is:\n\\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1)\n\nComputing \\pi_k=\\mathsf{cm}(X^{D_{max}-2^k+1}\\cdot \\hat{q}_k), \\quad 0\\leq k<n:\n\nComputing X^{D_{max}-2^k+1}\\cdot \\hat{q}_k involves polynomial multiplication. Since \\deg(\\hat{q}_k) = 2^k - 1, the complexity is \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1), giving a total complexity of:\n\\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1)\n\nComputing the commitments \\pi_k=\\mathsf{cm}(X^{D_{max}-2^k+1}\\cdot \\hat{q}_k), \\quad 0\\leq k<n has complexity:\n\\sum_{k=0}^{n-1} \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) = n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1)\n\n\nTherefore, the total complexity for this step is:\n\\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1) + n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1)\n\nThus, the total complexity for this round is:>   (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1) + n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1)\n>\n\n💡 Note: The polynomial multiplication calculation X^{D_{max}-2^k+1}\\cdot \\hat{q}_k could be optimized by simply shifting the coefficients of \\hat{q}_k.","type":"content","url":"/analysis/zeromorph-anlysis#round-1","position":9},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-2","position":10},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^*\n\nProver computes the auxiliary polynomial r(X) and quotient polynomial h(X), and sends \\mathsf{cm}(h):\n\nComputes r(X):r(X) = [[\\tilde{f}]]_{n} - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{k=0}^{n-1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot \\Phi_{n-k}(\\zeta^{2^{k}})\\Big)\\cdot \\hat{q}_k(X)\n\nComputes h(X) and its commitment \\mathsf{cm}(h) as a proof that r(X) evaluates to zero at X=\\zeta:h(X) = \\frac{r(X)}{X-\\zeta}\n\nProver:\n\nFirst calculates the powers of \\zeta: \\zeta^2, \\ldots, \\zeta^{2^{n}}, involving n finite field multiplications with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing r(X):\n\nCalculates \\Phi_n(\\zeta):\\Phi_n(\\zeta) = \\sum_{i=0}^{n-1} \\zeta^{2^i}\n\nThis can be directly computed using the previously calculated powers of \\zeta, involving finite field additions which we don’t count in our complexity analysis.\n\nCalculates v \\cdot \\Phi_n(\\zeta), involving one finite field multiplication, with complexity \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculates \\Phi_{n-k-1}(\\zeta^{2^{k+1}}). Since:\\Phi_{n-k-1}(X^{2^{k+1}}) = 1 + X^{2^{k + 1}} + X^{2 \\cdot 2^{k + 1}} + \\ldots + X^{(2^{n - k - 1} - 1) \\cdot 2^{k + 1}}\n\nTherefore:\\Phi_{n-k-1}(\\zeta^{2^{k+1}}) = 1 + \\zeta^{2^{k + 1}} + \\zeta^{2 \\cdot 2^{k + 1}} + \\ldots + \\zeta^{(2^{n - k - 1} - 1) \\cdot 2^{k + 1}}\n\nThis can also be calculated directly through finite field additions. Similarly, \\Phi_{n-k}(\\zeta^{2^{k}}) can be calculated the same way.\n\nCalculating \\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot \\Phi_{n-k}(\\zeta^{2^{k}}) involves two finite field multiplications with complexity 2 ~ \\mathbb{F}_{\\mathsf{mul}}. For all k, this gives 2n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculating \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot \\Phi_{n-k}(\\zeta^{2^{k}})\\Big)\\cdot \\hat{q}_k(X) involves polynomial multiplication with complexity \\mathsf{polymul}(0, 2^k - 1). The total complexity after summing over all k is:\\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n\nTherefore, the total complexity for computing r(X) is:>       \\mathbb{F}_{\\mathsf{mul}} + 2n ~ \\mathbb{F}_{\\mathsf{mul}} +  \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) = (2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n>\n\nComputing \\mathsf{cm}(h): First calculate h(X) using polynomial division, with complexity related to the degree of r(X). Since \\deg(r) = 2^{n - 1} - 1, the complexity of polynomial division is (2^{n - 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}}. Then compute the commitment with complexity \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1). The total complexity for this step is:(2^{n - 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1)\n\nThe total complexity for this round is:(3n + 2^{n - 1}) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1)","type":"content","url":"/analysis/zeromorph-anlysis#round-2","position":11},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#verification","position":12},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Verifier verifies the following equations:\n\nConstructs the commitment to \\mathsf{cm}(r):\\mathsf{cm}(r) = \\mathsf{cm}([[\\tilde{f}]]_{n}) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nVerifies that r(\\zeta) = 0:e(\\mathsf{cm}(r), \\ [1]_2) = e(\\mathsf{cm}(h), [\\tau]_2 - \\zeta\\cdot [1]_2)\n\nVerifies that (\\pi_0, \\pi_1, \\ldots, \\pi_{n-1}) are correct, i.e., verifies the degree bounds \\deg(\\hat{q}_i)<2^i for all 0\\leq i<n:e(\\mathsf{cm}(\\hat{q}_i), [\\tau^{D_{max}-2^i+1}]_2) = e(\\pi_i, [1]_2), \\quad 0\\leq i<n\n\nVerifier:\n\nConstructing \\mathsf{cm}(r):\n\nCalculates the powers of \\zeta: \\zeta^2, \\ldots, \\zeta^{2^{n}} with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputes \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) with complexity \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nComputes \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i) with complexity:\nn( 2 ~ \\mathbb{F}_{\\mathsf{mul}}  + \\mathsf{EccMul}^{\\mathbb{G}_1}) + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} = 2n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nComputes \\mathsf{cm}([[\\tilde{f}]]_{n}) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i) by adding three points on the elliptic curve, with complexity 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nThe total complexity for constructing \\mathsf{cm}(r) is:(3n + 1)~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nVerifying r(\\zeta) = 0:\n\nComputes [\\tau]_2 - \\zeta\\cdot [1]_2 with complexity \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2}\n\nComputes e(\\mathsf{cm}(r), \\ [1]_2) and e(\\mathsf{cm}(h), [\\tau]_2 - \\zeta\\cdot [1]_2), involving two pairing operations, denoted as 2~P.\n\nThe total complexity for this step is:\\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n\nVerifying (\\pi_0, \\pi_1, \\ldots, \\pi_{n-1}):e(\\mathsf{cm}(\\hat{q}_i), [\\tau^{D_{max}-2^i+1}]_2) = e(\\pi_i, [1]_2), \\quad 0\\leq i<n\n\nEach verification involves 2 ~ P, so the total complexity is 2n ~ P.\n\nThe total complexity for this round is:> (3n + 1)~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} +  (2n + 2)~P\n>","type":"content","url":"/analysis/zeromorph-anlysis#verification","position":13},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#summary","position":14},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Complexity Analysis of the Evaluation Proof Protocol (Naive Version)"},"content":"Prover Computational Complexity:> \\begin{aligned}\n>     & (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1) + n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) \\\\\n>     & + (3n + 2^{n - 1}) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n>     = & ( 3 \\cdot 2^{n - 1} + 3n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1) + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) \\\\\n>     & + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) +  n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1)\n> \\end{aligned}\n>\n\nVerifier Computational Complexity:> (3n + 1)~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} +  (2n + 2)~P\n>\n\nProof Size:\n\nThe proof sent by the Prover consists of:> (\\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1}), \\pi_0, \\ldots, \\pi_{n - 1}, \\mathsf{cm}(h))\n>\n\nFor a total of (2n + 1) \\mathbb{G}_1 elements.\n\nSubstituting the polynomial multiplication complexity \\mathsf{polymul}(a, b) = (a + 1) (b + 1) ~ \\mathbb{F}_{\\mathsf{mul}} = (ab + a + b + 1) ~ \\mathbb{F}_{\\mathsf{mul}}, we get:\n\nProver complexity:\\begin{align}\n& ( 3 \\cdot 2^{n - 1} + 3n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{polymul}(D_{max} - 2^k + 1, 2^k - 1) + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) \\\\\n& + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) +  n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n=  & ( \\frac{3}{2}  N + 3n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + ((D_{max} + 2)(N - 1) - \\frac{1}{3}(N^2 - 1))~ \\mathbb{F}_{\\mathsf{mul}} + (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}\\\\\n& + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) +  n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n= & ((D_{max} + \\frac{9}{2}) N - \\frac{1}{3} N^2 + 3n - D_{max} - \\frac{14}{3}) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) +  n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n\\end{align}\n\nVerifier complexity:(3n + 1)~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} +  (2n + 2)~P\n\nProof size:(2n + 1) \\mathbb{G}_1","type":"content","url":"/analysis/zeromorph-anlysis#summary","position":15},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl2","url":"/analysis/zeromorph-anlysis#evaluation-proof-protocol-optimized-version-degree-bound-aggregation","position":16},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Protocol description doc: \n\nEvaluation Proof Protocol (Optimized Version)","type":"content","url":"/analysis/zeromorph-anlysis#evaluation-proof-protocol-optimized-version-degree-bound-aggregation","position":17},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#common-inputs-1","position":18},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Commitment to the MLE polynomial \\tilde{f} mapped to univariate polynomial f(X)=[[\\tilde{f}]]_n: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/analysis/zeromorph-anlysis#common-inputs-1","position":19},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#witness-1","position":20},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Point value vector of the MLE polynomial \\tilde{f}: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/analysis/zeromorph-anlysis#witness-1","position":21},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-1-1","position":22},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"First round: Prover sends the commitments to the remainder polynomials\n\nComputes n remainder MLE polynomials: \\{q_i\\}_{i=0}^{n-1}\n\nConstructs univariate polynomials mapped from the remainder MLE polynomials: \\hat{q}_i=[[q_i]]_i, \\quad 0 \\leq i < n\n\nComputes and sends their commitments: \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1})\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{i=0}^{n-1} (X_k-u_k) \\cdot q_i(X_0,X_1,\\ldots, X_{k-1})\n\nProver:\n\nUsing the \n\nZeromorph paper’s algorithm from Appendix A.2, values of q_i on the Hypercube can be computed to obtain Q_i coefficients. This has complexity (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing \\mathsf{cm}(q_0), \\mathsf{cm}(q_1), \\ldots, \\mathsf{cm}(q_{n-1}) with total complexity:>   \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1)\n>\n\nTherefore, the total complexity for this round is:>   (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1)\n>","type":"content","url":"/analysis/zeromorph-anlysis#round-1-1","position":23},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-2-1","position":24},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Verifier sends a random number \\beta\\in \\mathbb{F}_p^* to aggregate multiple degree bound proofs\n\nProver constructs \\bar{q}(X) as an aggregation polynomial for \\{\\hat{q}_i(X)\\}, and sends its commitment \\mathsf{cm}(\\bar{q}):\\bar{q}(X) = \\sum_{i=0}^{n-1} \\beta^i \\cdot X^{2^n-2^i}\\hat{q}_i(X)\n\nProver:\n\nFirst calculates powers of \\beta: \\beta^2, \\ldots, \\beta^{n - 1}, with complexity (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputing \\beta^i \\cdot X^{2^n-2^i}\\hat{q}_i(X) is polynomial multiplication with complexity \\mathsf{polymul}(2^n - 2^i, 2^i - 1) + \\mathsf{polymul}(0, 2^n - 1). The total complexity for the sum is:\n\\begin{aligned}\n      & \\sum_{i = 0}^{n - 1} \\big( \\mathsf{polymul}(2^n - 2^i, 2^i - 1) + \\mathsf{polymul}(0, 2^n - 1) \\big) \\\\\n      = & n ~ \\mathsf{polymul}(0, 2^n - 1) + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(2^n - 2^i, 2^i - 1)\n  \\end{aligned}\n\nComputing \\mathsf{cm}(\\bar{q}) has complexity \\mathsf{msm}(2^n , \\mathbb{G}_1)\n\nThe total complexity for this round is:> (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{polymul}(0, 2^n - 1) + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(2^n - 2^i, 2^i - 1) + \\mathsf{msm}(2^n , \\mathbb{G}_1)\n>","type":"content","url":"/analysis/zeromorph-anlysis#round-2-1","position":25},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-3","position":26},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^* to challenge polynomial evaluation at X=\\zeta\n\nProver computes h_0(X) and h_1(X):\n\nComputes r(X):r(X) = \\hat{f}(X) - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot\\hat{q}_i(X)\n\nComputes s(X):s(X) = \\bar{q}(X) - \\sum_{k=0}^{n-1} \\beta^k \\cdot \\zeta^{2^n-2^k}\\cdot \\hat{q}_k(X)\n\nComputes quotient polynomials h_0(X) and h_1(X):h_0(X) = \\frac{r(X)}{X-\\zeta}, \\qquad h_1(X) = \\frac{s(X)}{X-\\zeta}\n\nProver:\n\nFirst calculates powers of \\zeta: \\zeta^2, \\ldots, \\zeta^{2^{n}}, with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing r(X) has complexity:>     (2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n>\n\nComputing s(X):\n\n\\beta^k \\cdot \\zeta^{2^n-2^k} has complexity \\mathbb{F}_{\\mathsf{mul}}\n\n\\beta^k \\cdot \\zeta^{2^n-2^k}\\cdot \\hat{q}_k(X) has complexity \\mathsf{polymul}(0, 2^k - 1)\n\nThus computing s(X) = \\bar{q}(X) - \\sum_{k=0}^{n-1} \\beta^k \\cdot \\zeta^{2^n-2^k}\\cdot \\hat{q}_k(X) has complexity:>     n ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n>\n\nComputing the quotient polynomials h_0(X) and h_1(X) can be done using polynomial long division. Since \\deg(r) = 2^n - 1 and \\deg(s(X)) = 2^n - 1, the complexity for this step is:>         (2^{n + 1} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n>\n\nThe total complexity for this round is:> \\begin{aligned}\n>     & n ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n>     & + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) +  n ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) +  (2^{n + 1} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n>      = & (4n + 2^{n + 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n> \\end{aligned}\n>","type":"content","url":"/analysis/zeromorph-anlysis#round-3","position":27},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-4","position":28},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Verifier sends a random number \\alpha\\in \\mathbb{F}_p^* to aggregate h_0(X) and h_1(X)\n\nProver computes h(X) and sends its commitment \\mathsf{cm}(h):h(X)=(h_0(X) + \\alpha\\cdot h_1(X))\\cdot X^{D_{max}-2^n+2}\n\nProver:\n\nComputing h_0(X) + \\alpha\\cdot h_1(X) has complexity \\mathsf{polymul}(0, 2^n - 2).\n\nComputing (h_0(X) + \\alpha\\cdot h_1(X))\\cdot X^{D_{max}-2^n+1} has complexity \\mathsf{polymul}(2^n - 2, D_{max}-2^n+1).\n\nComputing \\mathsf{cm}(h) has complexity \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})\n\nThe total complexity for this round is:> \\mathsf{polymul}(0, 2^n - 2) + \\mathsf{polymul}(2^n - 2, D_{max}-2^n+1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})\n>","type":"content","url":"/analysis/zeromorph-anlysis#round-4","position":29},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#verification-1","position":30},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Verifier:\n\nConstructs the commitment to \\mathsf{cm}(r):\\mathsf{cm}(r) = \\mathsf{cm}(f) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nConstructs the commitment to \\mathsf{cm}(s):\\mathsf{cm}(s) = \\mathsf{cm}(\\bar{q}) - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{2^n-2^i}\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nVerifies that r(\\zeta) = 0 and s(\\zeta) = 0:e(\\mathsf{cm}(r) + \\alpha\\cdot \\mathsf{cm}(s), \\ [\\tau^{D-2^n+2}]_2) = e(\\mathsf{cm}(h),\\ [\\tau]_2 - \\zeta\\cdot [1]_2)\n\nProof size:\n\nBefore verification, the Verifier has received the following proof:> \\pi = (\\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1}), \\mathsf{cm}(\\bar{q}), \\mathsf{cm}(h))\n>\n\nThus, the proof size is (n + 2) ~ \\mathbb{G}_1.\n\nVerifier:\n\nConstructing \\mathsf{cm}(r):\n\nComputes powers of \\zeta: \\zeta^2, \\zeta^4, \\ldots, \\zeta^{2^n}, with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputes \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) with complexity \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nComputes \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i):\n\nEach term has complexity 2 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1}\n\nThe total complexity is 2n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~\\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nAdding the three \\mathbb{G}_1 points has complexity 2~\\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nThe total complexity for computing \\mathsf{cm}(r) is:>     \\begin{aligned}\n>         & n ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1} + 2n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~\\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~\\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~\\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>         = & (3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n>     \\end{aligned}\n>\n\nComputation of the commitment \\mathsf{cm}(s)\n\nCalculate \\beta^2, \\ldots, \\beta^{n-1} with complexity of (n-2)~\\mathbb{F}_{\\mathsf{mul}}\n\nCalculate \\beta^i \\cdot \\zeta^{2^n-2^i}\\cdot \\mathsf{cm}(\\hat{q}_i) with complexity of \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{EccMul}^{\\mathbb{G}_1} for each term. Computing the sum \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{2^n-2^i}\\cdot \\mathsf{cm}(\\hat{q}_i) requires not only calculating each term, but also adding n points on the elliptic curve. Therefore, the total complexity is:>     n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n>\n\nCalculate \\mathsf{cm}(\\bar{q}) - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{2^n-2^i}\\cdot \\mathsf{cm}(\\hat{q}_i) with complexity of \\mathsf{EccAdd}^{\\mathbb{G}_1} for adding two points on the elliptic curve \\mathbb{G}_1.\n\nTherefore, the total complexity for computing \\mathsf{cm}(s) is:>   (2n-2)~\\mathbb{F}_{\\mathsf{mul}} + n~\\mathsf{EccMul}^{\\mathbb{G}_1} + n~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n>\n\nVerification that r(\\zeta) = 0 and s(\\zeta) = 0\n\nCalculate \\mathsf{cm}(r) + \\alpha\\cdot \\mathsf{cm}(s) and [\\tau]_2 - \\zeta\\cdot [1]_2 with complexity of \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2}\n\nCalculate the pairings e(\\mathsf{cm}(r) + \\alpha\\cdot \\mathsf{cm}(s), [\\tau^{D-2^n+2}]_2) and e(\\mathsf{cm}(h), [\\tau]_2 - \\zeta\\cdot [1]_2), involving two pairing operations on elliptic curves, denoted as 2~P\n\nThe total complexity for this step is:>   \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n>\n\nThe total complexity for the Verification phase is:> \\begin{aligned}\n>   & (3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & + (2n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}  + n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + n ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n>   & +   \\mathsf{EccMul}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P \\\\\n>   = & (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n> \\end{aligned}\n>","type":"content","url":"/analysis/zeromorph-anlysis#verification-1","position":31},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#summary-1","position":32},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Evaluation Proof Protocol (Optimized Version - Degree Bound Aggregation)"},"content":"Prover Computational Complexity:> \\begin{aligned}\n>   & (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) \\\\\n>   & + (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{polymul}(0, 2^n - 1) + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(2^n - 2^i, 2^i - 1) + \\mathsf{msm}(2^n , \\mathbb{G}_1) \\\\\n>   & + (4n + 2^{n + 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) \\\\\n>   & + \\mathsf{polymul}(0, 2^n - 2) + \\mathsf{polymul}(2^n - 2, D_{max}-2^n+1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})\\\\\n>   = & (3 \\cdot 2^n + 5n - 5) ~ \\mathbb{F}_{\\mathsf{mul}} +  n ~ \\mathsf{polymul}(0, 2^n - 1) + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(2^n - 2^i, 2^i - 1) \\\\\n>   & + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{polymul}(0, 2^n - 2) + \\mathsf{polymul}(2^n - 2, D_{max}-2^n+1)\\\\\n>   & + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})\n> \\end{aligned}\n>\n\nVerifier Computational Complexity:> (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n>\n\nProof Size:> (n + 2) ~ \\mathbb{G}_1\n>\n\nSubstituting the polynomial multiplication complexity \\mathsf{polymul}(a, b) = (a + 1) (b + 1) ~ \\mathbb{F}_{\\mathsf{mul}} = (ab + a + b + 1) ~ \\mathbb{F}_{\\mathsf{mul}}, we get:\n\nProver complexity:\\begin{align}\n& (3 \\cdot 2^n + 5n - 5) ~ \\mathbb{F}_{\\mathsf{mul}} +  n ~ \\mathsf{polymul}(0, 2^n - 1) + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(2^n - 2^i, 2^i - 1) \\\\\n& + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{polymul}(0, 2^n - 2) + \\mathsf{polymul}(2^n - 2, D_{max}-2^n+1)\\\\\n& + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1}) \\\\\n=  & (3 N + 5n - 5) ~ \\mathbb{F}_{\\mathsf{mul}} +  nN ~ \\mathbb{F}_{\\mathsf{mul}} + (\\frac{2}{3}N^2 - \\frac{2}{3}) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + (2N - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((D_{max} + 3)N - N^2 - D_{max} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})  \\\\\n=  & (-\\frac{1}{3}N^2 +nN +(D_{max} +9)N + 5n - \\frac{32}{3} - D_{max}) ~ \\mathbb{F}_{\\mathsf{mul}}  \\\\\n& + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})  \\\\\n\\end{align}\n\n**Verifier complexity: **> (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n>\n\nProof size:> (n + 2) ~ \\mathbb{G}_1\n>","type":"content","url":"/analysis/zeromorph-anlysis#summary-1","position":33},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl2","url":"/analysis/zeromorph-anlysis#zeromorph-pcs-degree-bound-optimized","position":34},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Protocol description doc: \n\nZeromorph-PCS (Part II)","type":"content","url":"/analysis/zeromorph-anlysis#zeromorph-pcs-degree-bound-optimized","position":35},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#common-inputs-2","position":36},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Common Inputs","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Commitment to the MLE polynomial \\tilde{f} mapped to univariate polynomial f(X)=[[\\tilde{f}]]_n: \\mathsf{cm}(f)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/analysis/zeromorph-anlysis#common-inputs-2","position":37},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#witness-2","position":38},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Witness","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Point value vector of the MLE polynomial \\tilde{f}: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/analysis/zeromorph-anlysis#witness-2","position":39},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-1-2","position":40},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 1","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Prover computes n remainder MLE polynomials: \\{\\tilde{q}_i\\}_{i=0}^{n-1}\n\nProver constructs univariate polynomials mapped from the remainder MLE polynomials: q_i=[[\\tilde{q}_i]]_i, \\quad 0 \\leq i < n\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{i=0}^{n-1} (X_i-u_i) \\cdot \\tilde{q}_i(X_0,X_1,\\ldots, X_{i-1})\n\nProver computes and sends their commitments: \\mathsf{cm}(q_0), \\mathsf{cm}(q_1), \\ldots, \\mathsf{cm}(q_{n-1})\n\nProver Cost:\n\nThis round has the same complexity as the previous batched degree bound protocol:(2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1)","type":"content","url":"/analysis/zeromorph-anlysis#round-1-2","position":41},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-2-2","position":42},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 2","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Verifier sends a random number \\beta\\in \\mathbb{F}_q^*\n\nProver constructs g(X) as an aggregated polynomial for \\{q_i(X)\\}, satisfying:g(X^{-1}) = \\sum_{i=0}^{n-1} \\beta^i \\cdot X^{-2^i+1}\\cdot q_i(X)\n\nProver computes and sends the commitment to g(X): \\mathsf{cm}(g)\n\nProver Cost:\n\nFirst calculates powers of \\beta: \\beta^2, \\ldots, \\beta^{n - 1}, with complexity (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nComputes g(X) using:g(X) = \\sum_{i = 0}^{n - 1} \\beta^i \\cdot X^{2^i-1}{q}_i(X^{-1})\n\nComputing X^{2^i-1}{q}_i(X^{-1}) involves reversing the coefficients of q_i(X) and then multiplying by \\beta^i, with complexity \\mathsf{polymul}(0, 2^i - 1). The total complexity for the sum is:\\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^i - 1)\n\nComputing \\mathsf{cm}(g) has complexity \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1) since \\deg(g) = 2^{n - 1} - 1.\n\nThe total complexity for this round is:(n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^i - 1) + \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1)","type":"content","url":"/analysis/zeromorph-anlysis#round-2-2","position":43},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 3","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-3-1","position":44},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 3","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^* to challenge polynomial evaluation at X=\\zeta\n\nProver computes g(\\zeta^{-1}), and computes quotient polynomial q_g(X):q_g(X) = \\frac{g(X) - g(\\zeta^{-1})}{X-\\zeta^{-1}}\n\nProver constructs linearization polynomials r_\\zeta(X) and s_\\zeta(X):\n\nComputes r_\\zeta(X):r_\\zeta(X) = f(X) - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot q_i(X)\n\nComputes s_\\zeta(X) which evaluates to zero at X=\\zeta:s_\\zeta(X) = g(\\zeta^{-1}) - \\sum_i\\beta^i\\zeta^{2^i-1}\\cdot q_i(X)\n\nComputes quotient polynomials w_r(X) and w_s(X):w_r(X) = \\frac{r_\\zeta(X)}{X-\\zeta}, \\qquad w_s(X) = \\frac{s_\\zeta(X)}{X-\\zeta}\n\nComputes and sends the commitment \\mathsf{cm}(q_g)\n\nProver Cost:\n\nFirst calculates powers of \\zeta: \\zeta^2, \\ldots, \\zeta^{2^{n}}, with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing \\zeta^{-1} has complexity \\mathbb{F}_{\\mathsf{inv}}.\n\nComputing g(\\zeta^{-1}) using Horner’s method has complexity 2^{n - 1} ~ \\mathbb{F}_{\\mathsf{mul}} since \\deg(g) = 2^{n - 1} - 1.\n\nComputing q_g(X) using polynomial division has complexity (2^{n - 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing r_{\\zeta}(X) has the same complexity as in the optimized protocol:(2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1)\n\nComputing s_{\\zeta}(X): Each \\beta^i \\cdot \\zeta^{2^i - 1} \\cdot q_i(X) has complexity \\mathbb{F}_{\\mathsf{mul}} + \\mathsf{polymul}(0, 2^i - 1), so the total complexity is:n ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 0}^{n - 1}\\mathsf{polymul}(0, 2^i - 1)\n\nComputing w_r(X) and w_s(X) using polynomial division has complexity (3 \\cdot 2^{n - 1} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} since \\deg(r_{\\zeta}) = 2^n - 1 and \\deg(s_{\\zeta}) = 2^{n - 1} - 1.\n\nComputing \\mathsf{cm}(q_g) has complexity \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1).\n\nThe total complexity for this round is:\\begin{aligned}\n    & n ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + 2^{n - 1} ~ \\mathbb{F}_{\\mathsf{mul}} + (2^{n - 1} - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + (2n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) \\\\\n    & + n ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 0}^{n - 1}\\mathsf{polymul}(0, 2^i - 1) + (3 \\cdot 2^{n - 1} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n    = & (5 \\cdot 2^{n - 1} + 4n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1)\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-anlysis#round-3-1","position":45},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 4","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#round-4-1","position":46},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Round 4","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Verifier sends a random number \\alpha\\in \\mathbb{F}_p^* to aggregate w_r(X) and w_s(X)\n\nProver computes w(X) and sends its commitment \\mathsf{cm}(w):w(X) = w_r(X) + \\alpha\\cdot w_s(X)\n\nProver Cost:\n\nComputing w_r(X) + \\alpha \\cdot w_s(X) has complexity \\mathsf{polymul}(0, 2^{n - 1} - 2).\n\nComputing \\mathsf{cm}(w) has complexity \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) since \\deg(w(X)) = 2^n - 2.\n\nThe total complexity for this round is:\\mathsf{polymul}(0, 2^{n - 1} - 2) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)","type":"content","url":"/analysis/zeromorph-anlysis#round-4-1","position":47},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Proof","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#proof","position":48},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Proof","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"The complete proof consists of n+3 elements in \\mathbb{G}_1 and 1 element in \\mathbb{F}_q:\\pi= \\Big( \\mathsf{cm}(q_0), \\mathsf{cm}(q_1), \\ldots, \\mathsf{cm}(q_{n-1}), \\mathsf{cm}(g), \\mathsf{cm}(q_g), \\mathsf{cm}(w), g(\\zeta^{-1})\\Big)\n\nProof size:\\begin{aligned}\n    (n + 3) \\cdot \\mathbb{G}_1 + \\mathbb{F}_q\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-anlysis#proof","position":49},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#verification-2","position":50},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Verification","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Verifier:\n\nConstructs the commitment to \\mathsf{cm}(r_\\zeta):\\mathsf{cm}(r_\\zeta) = \\mathsf{cm}(f) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(q_i)\n\nConstructs the commitment to \\mathsf{cm}(s_\\zeta):\\mathsf{cm}(s_\\zeta) = g(\\zeta^{-1})\\cdot[1]_1 - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{-2^i+1}\\cdot \\mathsf{cm}(q_i)\n\nVerifies that r_\\zeta(\\zeta) = 0 and s_\\zeta(\\zeta) = 0:e(\\mathsf{cm}(r_\\zeta) + \\alpha\\cdot \\mathsf{cm}(s_\\zeta), \\ [1]_2) = e(\\mathsf{cm}(w),\\ [\\tau]_2 - \\zeta\\cdot [1]_2)\n\nThis can be rewritten as:e(\\mathsf{cm}(r_\\zeta) + \\alpha\\cdot \\mathsf{cm}(s_\\zeta) + \\zeta\\cdot\\mathsf{cm}(w), \\ [1]_2) = e(\\mathsf{cm}(w),\\ [\\tau]_2)\n\nVerifies the correctness of g(\\zeta^{-1}):e(\\mathsf{cm}(g) - g(\\zeta^{-1})\\cdot [1]_1 + \\zeta^{-1}\\cdot\\mathsf{cm}(q_g),\\  [1]_2) = e(\\mathsf{cm}(q_g), \\ [\\tau]_2)\n\nVerifier Cost:\n\nConstructing \\mathsf{cm}(r_{\\zeta}) has the same complexity as in the optimized protocol:(3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nConstructing \\mathsf{cm}(s_{\\zeta}):\n\nComputing \\zeta^{-1} has complexity \\mathbb{F}_{\\mathsf{inv}}.\n\nComputing g(\\zeta^{-1}) \\cdot [1]_1 has complexity \\mathsf{EccMul}^{\\mathbb{G}_1}.\n\nComputing (\\zeta^{-1})^{2^2 - 1}, \\ldots, (\\zeta^{-1})^{2^{n -1} - 1} has complexity (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nThe computation of \\sum_{i=0}^{n-1} \\beta^i \\cdot (\\zeta^{-1})^{2^i - 1} \\cdot \\mathsf{cm}(\\hat{q}_i) has total complexity:n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nAdding the two \\mathbb{G}_1 points has complexity \\mathsf{EccAdd}^{\\mathbb{G}_1}.\n\nThe total complexity for constructing \\mathsf{cm}(s_{\\zeta}) is:\\begin{aligned}\n    & \\mathbb{F}_{\\mathsf{inv}} + \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n    & + n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n - 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    = & (2n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + n ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\\end{aligned}\n\nVerification that r_{\\zeta}(\\zeta) = 0 and s_{\\zeta}(\\zeta) = 0\n\nCalculate \\mathsf{cm}(r_{\\zeta}) + \\alpha \\cdot \\mathsf{cm}(s_{\\zeta}) + \\zeta \\cdot \\mathsf{cm}(w) with complexity of 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~\\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nCalculate the pairings e(\\mathsf{cm}(r_{\\zeta}) + \\alpha \\cdot \\mathsf{cm}(s_{\\zeta}) + \\zeta \\cdot \\mathsf{cm}(w), [1]_2) and e(\\mathsf{cm}(w), [\\tau]_2) with complexity of 2~P\n\nTherefore, the total complexity for this step is 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~\\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\nVerification of the correctness of g(\\zeta^{-1})\n\nCalculate \\mathsf{cm}(g) - g(\\zeta^{-1}) \\cdot [1]_1 + \\zeta^{-1} \\cdot \\mathsf{cm}(q_g) with complexity of 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}\n\nCalculate the pairings e(\\mathsf{cm}(g) - g(\\zeta^{-1}) \\cdot [1]_1 + \\zeta^{-1} \\cdot \\mathsf{cm}(q_g), [1]_2) and e(\\mathsf{cm}(q_g), [\\tau]_2) with complexity of 2~P\n\nTherefore, the total complexity for this step is 2 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 2 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\nThe pairing checks in steps 3 and 4 can be combined into a single check:e(\\mathsf{cm}(r_{\\zeta}) + \\alpha \\cdot \\mathsf{cm}(s_{\\zeta}) + \\zeta \\cdot \\mathsf{cm}(w) + \\mathsf{cm}(g) - g(\\zeta^{-1}) \\cdot [1]_1 + \\zeta^{-1} \\cdot \\mathsf{cm}(q_g), [1]_2) \\overset{?}{=} e(\\mathsf{cm}(w) + \\mathsf{cm}(q_g), [\\tau]_2)\n\nThis has complexity 4 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 6 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\nTherefore, the total complexity for the Verification phase is:\\begin{aligned}\n    & (3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 1) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    & + (2n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + n ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} \\\\\n    & + 4 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 6 ~\\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P  \\\\\n    = & (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + (2n + 6) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 7) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}  + 2~P\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-anlysis#verification-2","position":51},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"type":"lvl3","url":"/analysis/zeromorph-anlysis#summary-2","position":52},{"hierarchy":{"lvl1":"Complexity Analysis of the Zeromorph Protocol Series","lvl3":"Summary","lvl2":"Zeromorph-PCS (Degree Bound Optimized)"},"content":"Prover Computational Complexity:\\begin{aligned}\n    & (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n-1} \\mathsf{msm}(2^k,\\mathbb{G}_1) \\\\\n    & + (n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{i = 0}^{n - 1} \\mathsf{polymul}(0, 2^i - 1) + \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1)\\\\\n    & + (5 \\cdot 2^{n - 1} + 4n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + 2 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) \\\\\n    & + \\mathsf{polymul}(0, 2^{n - 1} - 2) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1) \\\\\n    = & (\\frac{7}{2} \\cdot 2^n + 5n - 6) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & + 3 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{polymul}(0, 2^{n - 1} - 2) \\\\\n    & + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\n\\end{aligned}\n\nOr simply:\\begin{aligned}\n    & (\\frac{7}{2} \\cdot 2^n + 5n - 6) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & + 3 \\sum_{k = 0}^{n - 1} \\mathsf{polymul}(0, 2^k - 1) + \\mathsf{polymul}(0, 2^{n - 1} - 2) \\\\\n    & + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\n\\end{aligned}\n\nVerifier Computational Complexity:\\begin{aligned}\n    (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + (2n + 6) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 7) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}  + 2~P\n\\end{aligned}\n\nProof Size:\\begin{aligned}\n    (n + 3) \\cdot \\mathbb{G}_1 + \\mathbb{F}_q\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-anlysis#summary-2","position":53},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis"},"type":"lvl1","url":"/analysis/zeromorph-fri-analysis","position":0},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io","type":"content","url":"/analysis/zeromorph-fri-analysis","position":1},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl2":"Evaluation Proof Protocol"},"type":"lvl2","url":"/analysis/zeromorph-fri-analysis#evaluation-proof-protocol","position":2},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl2":"Evaluation Proof Protocol"},"content":"Protocol description document: \n\nZeromorph-PCS: Integration with FRI","type":"content","url":"/analysis/zeromorph-fri-analysis#evaluation-proof-protocol","position":3},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Public Inputs","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#public-inputs","position":4},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Public Inputs","lvl2":"Evaluation Proof Protocol"},"content":"Commitment to the MLE polynomial \\tilde{f}: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})\n\nCode rate parameter: \\rho\n\nParameter for the number of repetitive queries in the query phase of the FRI protocol low degree test: l\n\nMultiplication subgroups in the FRI protocol encoding: D, D^{(0)}, \\ldots, D^{(n - 1)}","type":"content","url":"/analysis/zeromorph-fri-analysis#public-inputs","position":5},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Witness","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#witness","position":6},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Witness","lvl2":"Evaluation Proof Protocol"},"content":"Point value vector of MLE polynomial \\tilde{f} on the n-dimensional HyperCube: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/analysis/zeromorph-fri-analysis#witness","position":7},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#round-1","position":8},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol"},"content":"Prover sends commitments to remainder polynomials\n\nCalculate n remainder MLE polynomials, \\{\\tilde{q}_k\\}_{k=0}^{n-1}, satisfying\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{k=0}^{n-1} (X_k-u_k) \\cdot \\tilde{q}_k(X_0,X_1,\\ldots, X_{k-1})\n\nConstruct univariate polynomials mapping from remainder MLE polynomials \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n\n\nCompute and send their commitments. Using the mmcs structure to commit to the values of these n polynomials on the same tree. First, calculate the values of these polynomials on the corresponding D^{(k)}:\\{[\\hat{q}_k(x)|_{x \\in D^{(k)}}]\\}_{k = 0}^{n - 1}\n\nwhere |D^{(k)}| = 2^k / \\rho, then use mmcs to commit to these (2^{n - 1} + 2^{n - 2} + \\ldots + 2^0)/\\rho values at once, denoted as:\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0) = \\mathsf{MMCS.commit}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0)","type":"content","url":"/analysis/zeromorph-fri-analysis#round-1","position":9},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-1","position":10},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 1","lvl3":"Round 1","lvl2":"Evaluation Proof Protocol"},"content":"Using the algorithm from Appendix A.2 of the \n\nZeromorph paper, we can compute the values of q_k on the Hypercube, thus obtaining the coefficients of \\hat{q}_k. According to the paper, the overall algorithm complexity is (2^{n+1} - 3) ~ \\mathbb{F}_{\\mathsf{add}} and (2^{n} - 2) ~ \\mathbb{F}_{\\mathsf{mul}}. Without counting addition complexity, the complexity of computing \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n is (N - 2) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nTo calculate \\{[\\hat{q}_k(x)|_{x \\in D^{(k)}}]\\}_{k = 0}^{n - 1}, since we already have the coefficients of \\hat{q}_k(X), we directly substitute D^{(k)} for evaluation. For evaluation at one point, using FFT method:\n\nSince |D^{(k)}| = 2^k \\cdot \\mathcal{R}, the complexity of calculating [\\hat{q}_k(x)|_{x \\in D^{(k)}}] is 2^k\\mathcal{R} \\cdot \\log(2^k\\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{mul}} =2^k \\mathcal{R}(k + \\log \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{mul}}. The complexity of calculating \\{[\\hat{q}_k(x)|_{x \\in D^{(k)}}]\\}_{k = 0}^{n - 1} is:\\sum_{k = 0}^{n - 1} 2^k \\mathcal{R}(k + \\log \\mathcal{R})  ~ \\mathbb{F}_{\\mathsf{mul}}\n\nSince\\sum_{k = 0}^{n - 1} 2^k = 2^0 + \\ldots + 2^{n - 1} = \\frac{2^0(1- 2^n)}{1- 2} = 2^n - 1\\sum_{k = 0}^{n - 1} k \\cdot 2^k = (n - 2) \\cdot 2^n + 2\n\nTherefore:\\begin{align}\n \\sum_{k = 0}^{n - 1} 2^k \\mathcal{R}(k + \\log \\mathcal{R})     & = \\mathcal{R} \\cdot\\sum_{k = 0}^{n - 1} k \\cdot 2^k  + \\mathcal{R}\\log \\mathcal{R}  \\cdot \\sum_{k = 0}^{n - 1} 2^k  \\\\\n &  = \\mathcal{R} \\cdot nN + (\\mathcal{R} \\log \\mathcal{R} - 2 \\mathcal{R}) N + 2\\mathcal{R}  - \\mathcal{R}\\log \\mathcal{R}\n\\end{align}\n\nSo the complexity for this round is (\\mathcal{R} \\cdot nN + (\\mathcal{R} \\log \\mathcal{R} - 2 \\mathcal{R}) N + 2\\mathcal{R}  - \\mathcal{R}\\log \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}}.\n\n[!summary]\nIn this step, there are n polynomials \\hat{q}_k(X) that need to be evaluated on D^{(k)}, using FFT method, with complexity O(N \\log N).\n\nComputing the commitment \\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0) = \\mathsf{MMCS.commit}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0). The tree height is 2 \\cdot \\log (2^{n - 1} \\cdot \\mathcal{R}), involving (2^{n - 1} + \\cdots + 2^0) \\cdot \\mathcal{R} hash calculations, with one hash operation complexity denoted as H. The Compress operations are (2^{n - 2} \\cdot \\mathcal{R} + \\ldots + 2^{0} \\cdot \\mathcal{R} + 1), denoted as (2^{n - 2} \\cdot \\mathcal{R} + \\ldots + 2^{0} \\cdot \\mathcal{R} + 1) ~ C. Thus, this step’s complexity is:\\begin{aligned}\n  & \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R})\\\\\n  = & ((2^{n - 1} + \\cdots + 2^0) \\cdot \\mathcal{R}) ~ H + (2^{n - 2} \\cdot \\mathcal{R} + \\ldots + 2^{0} \\cdot \\mathcal{R} + 1) ~ C \\\\\n  = & (N - 1) \\cdot \\mathcal{R} ~ H  + ((N/2 - 1) \\cdot \\mathcal{R} + 1) ~ C\n\\end{aligned}\n\nThe total complexity for this round is:\\begin{align}\n & (N - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot nN + (\\mathcal{R} \\log \\mathcal{R} - 2 \\mathcal{R}) N + 2\\mathcal{R}  - \\mathcal{R}\\log \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}} \\\\\n & + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) \\\\\n=  & (\\mathcal{R}\\cdot nN + (\\mathcal{R} \\log \\mathcal{R} - 2 \\mathcal{R} + 1) N + 2\\mathcal{R}  - \\mathcal{R}\\log \\mathcal{R}  - 2) ~\\mathbb{F}_{\\mathsf{mul}}  \\\\\n & + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) \n\\end{align}","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-1","position":11},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#round-2","position":12},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol"},"content":"Verifier sends a random number \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F} \\setminus D\n\nProver computes and sends \\hat{f}(\\zeta)\n\nProver computes and sends \\{\\hat{q}_k(\\zeta)\\}_{k = 0}^{n - 1}.","type":"content","url":"/analysis/zeromorph-fri-analysis#round-2","position":13},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 2","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-2","position":14},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 2","lvl3":"Round 2","lvl2":"Evaluation Proof Protocol"},"content":"Computing \\hat{f}(\\zeta): Prover has the coefficient form of \\hat{f}, and uses Horner’s method to compute its value at a point, with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nNote\n\nUsing Lagrange interpolation would have complexity (2N + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (N + 1) ~ \\mathbb{F}_{\\mathsf{inv}}, which would be more computationally intensive.\n\nComputing \\{\\hat{q}_k(\\zeta)\\}_{k = 0}^{n - 1}, with complexity:\\sum_{k = 0}^{n - 1} 2^k ~ \\mathbb{F}_{\\mathsf{mul}} = (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nThe total complexity for this round is:\\begin{align}\nN ~ \\mathbb{F}_{\\mathsf{mul}} + (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} = (2N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}\n\\end{align}","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-2","position":15},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#round-3","position":16},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol"},"content":"Verifier sends a random number \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver computes:q_{f_\\zeta}(X) = \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta}\n\non D, i.e.:[q_{f_\\zeta}(x)|_{x \\in D}] = \\big[\\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta} + \\lambda \\cdot x \\cdot \\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta}\\big|_{x \\in D} \\big]\n\nFor 0 \\le k < n, Prover computes:q_{\\hat{q}_k}(X) = \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta}\n\non D^{(k)}.","type":"content","url":"/analysis/zeromorph-fri-analysis#round-3","position":17},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-3","position":18},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 3","lvl3":"Round 3","lvl2":"Evaluation Proof Protocol"},"content":"Computing [q_{f_\\zeta}(x)|_{x \\in D}]:\n\nFirst use FFT to compute [f(x)|_{x \\in D}] from the coefficient form of f(X). Since |D| = N \\cdot \\mathcal{R}, the complexity is (\\mathcal{R}N \\cdot\\log(\\mathcal{R}N)) ~ \\mathbb{F}_{\\mathsf{mul}} = (\\mathcal{R} \\cdot nN + \\mathcal{R}\\log\\mathcal{R} \\cdot N) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFFT can be avoided as we already computed [f(x)|_{x \\in D}] during the commit phase.\n\nThen calculate [q_{f_\\zeta}(x)|_{x \\in D}], with each value involving \\mathbb{F}_{\\mathsf{inv}} + \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathbb{F}_{\\mathsf{mul}} = 3 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}}, for a total of 3\\mathcal{R} \\cdot N ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} \\cdot N\\mathbb{F}_{\\mathsf{inv}}.\n\nThe overall computation complexity is:\\begin{align}\n& (\\mathcal{R} \\cdot nN + \\mathcal{R}\\log\\mathcal{R} \\cdot N) ~ \\mathbb{F}_{\\mathsf{mul}} + 3\\mathcal{R} \\cdot N ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} \\cdot N ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n= & (\\mathcal{R} \\cdot nN + (\\mathcal{R}\\log\\mathcal{R} + 3 \\mathcal{R}) \\cdot N) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} \\cdot N ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{align}\n\n[!summary]\nComputing [f(x)|_{x \\in D}] has complexity O(N \\log N).\n\nComputing [q_{\\hat{q}_k}(x)|_{x \\in D^{(k)}}]: Round 1 already calculated [\\hat{q}_k(x)|_{x \\in D^{(k)}}]. Since |D_k| = 2^k \\cdot \\mathcal{R}, this step’s complexity is:\\begin{align}\n\\sum_{k = 0}^{n-1} 2^k \\cdot \\mathcal{R} \\cdot (3 ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}})  & = (3 \\mathcal{R}~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} ~\\mathbb{F}_{\\mathsf{inv}}) \\cdot \\sum_{k = 0}^{n-1} 2^k = (3 \\mathcal{R}~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} ~\\mathbb{F}_{\\mathsf{inv}}) \\cdot (N-1) \\\\\n& = (3 \\mathcal{R} \\cdot N - 3 \\mathcal{R} )~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{inv}}\n\\end{align}\n\nThe total complexity for Round 3 is:\\begin{aligned}\n  & (\\mathcal{R} \\cdot nN + (\\mathcal{R}\\log\\mathcal{R} + 3 \\mathcal{R}) \\cdot N) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathcal{R} \\cdot N ~ \\mathbb{F}_{\\mathsf{inv}} + (3 \\mathcal{R} \\cdot N - 3 \\mathcal{R} )~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n  = & (\\mathcal{R} \\cdot nN + (\\mathcal{R}\\log\\mathcal{R} + 6 \\mathcal{R}) \\cdot N - 3 \\mathcal{R} ) ~ \\mathbb{F}_{\\mathsf{mul}} + (2\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-3","position":19},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#round-4","position":20},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol"},"content":"Prover and Verifier engage in the FRI protocol’s low degree test interaction, using the rolling batch technique to optimize. For k = 0, \\ldots, n - 1, they simultaneously prove that all q_{\\hat{q}_k}(X) have degree less than 2^k, and that q_{f_\\zeta}(X) has degree less than 2^n. For convenience, let’s denote:q_{\\hat{q}_n}(X) := q_{f_\\zeta}(X)\n\nThe low degree test proof is then:\\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}} \\leftarrow \\mathsf{OPFRI.LDT}(q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}, 2^{n})\n\nThis involves n + 1 rounds of interaction until the final fold into a constant polynomial. Using i to represent round i, the specific interaction process is:\n\nInitialize i = n, set D^{(n)} := D. For x \\in D^{(n)}, initialize:\\mathsf{fold}^{(i)}(x) = q_{\\hat{q}_{n}}(x)\n\nFor i = n - 1, \\ldots, 0:\n\nVerifier sends random number \\beta^{(i)}\n\nFor y \\in D^{(i)}, find x \\in D^{(i + 1)} such that x^2 = y. Prover computes:\\mathsf{fold}^{(i)}(y) = \\frac{\\mathsf{fold}^{(i + 1)}(x) + \\mathsf{fold}^{(i + 1)}(-x)}{2} + \\beta^{(i)} \\cdot \\frac{\\mathsf{fold}^{(i + 1)}(x) - \\mathsf{fold}^{(i + 1)}(-x)}{2x}\n\nFor x \\in D^{(i)}, Prover updates \\mathsf{fold}^{(i)}(x):\\mathsf{fold}^{(i)}(x) = \\mathsf{fold}^{(i)}(x) + q_{\\hat{q}_{i}}(x)\n\nWhen i > 0:\n\nProver sends commitment to \\mathsf{fold}^{(i)}(x):\\mathsf{cm}(\\mathsf{fold}^{(i)}(X)) = \\mathsf{MT.commit}([\\mathsf{fold}^{(i)}(x)|_{x \\in D^{(i)}}])\n\nWhen i = 0, since the final fold is a constant polynomial, Prover selects any point y_0 \\in D^{(0)} and sends the final folded value \\mathsf{fold}^{(0)}(y_0).","type":"content","url":"/analysis/zeromorph-fri-analysis#round-4","position":21},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-4","position":22},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 4","lvl3":"Round 4","lvl2":"Evaluation Proof Protocol"},"content":"For i = n-1, \\ldots, 0:\n\nProver computes:\\mathsf{fold}^{(i)}(y) = \\frac{\\mathsf{fold}^{(i + 1)}(x) + \\mathsf{fold}^{(i + 1)}(-x)}{2} + \\beta^{(i)} \\cdot \\frac{\\mathsf{fold}^{(i + 1)}(x) - \\mathsf{fold}^{(i + 1)}(-x)}{2x}\n\nThis can also be written as:\\mathsf{fold}^{(i)}(y) = (\\frac{1}{2} + \\frac{\\beta^{(i)}}{2x}) \\cdot \\mathsf{fold}^{(i + 1)}(x) + (\\frac{1}{2} - \\frac{\\beta^{(i)}}{2x}) \\cdot \\mathsf{fold}^{(i + 1)}(-x)\n\nplonky3 uses this computation approach in \n\nfold_even_odd.rs.#[instrument(skip_all, level = \"debug\")]\npub fn fold_even_odd<F: TwoAdicField>(poly: Vec<F>, beta: F) -> Vec<F> {\n    // We use the fact that\n    //     p_e(x^2) = (p(x) + p(-x)) / 2\n    //     p_o(x^2) = (p(x) - p(-x)) / (2 x)\n    // that is,\n    //     p_e(g^(2i)) = (p(g^i) + p(g^(n/2 + i))) / 2\n    //     p_o(g^(2i)) = (p(g^i) - p(g^(n/2 + i))) / (2 g^i)\n    // so\n    //     result(g^(2i)) = p_e(g^(2i)) + beta p_o(g^(2i))\n    //                    = (1/2 + beta/2 g_inv^i) p(g^i)\n    //                    + (1/2 - beta/2 g_inv^i) p(g^(n/2 + i))\n    let m = RowMajorMatrix::new(poly, 2);\n    let g_inv = F::two_adic_generator(log2_strict_usize(m.height()) + 1).inverse();\n    let one_half = F::TWO.inverse();\n    let half_beta = beta * one_half;\n\n    // TODO: vectorize this (after we have packed extension fields)\n\n    // beta/2 times successive powers of g_inv\n    let mut powers = g_inv\n        .shifted_powers(half_beta)\n        .take(m.height())\n        .collect_vec();\n    reverse_slice_index_bits(&mut powers);\n\n    m.par_rows()\n        .zip(powers)\n        .map(|(mut row, power)| {\n            let (r0, r1) = row.next_tuple().unwrap();\n            (one_half + power) * r0 + (one_half - power) * r1\n        })\n        .collect()\n}\n\nFirst compute \n\n2-1 with complexity \\mathbb{F}_{\\mathsf{inv}}.\n\nIn round i, calculate \\frac{\\beta^{(i)}}{2} = \\beta^{(i)} \\cdot 2^{-1} with complexity \\mathbb{F}_{\\mathsf{mul}}.\n\nFor each y \\in D_i, computing \\mathsf{fold}^{(i)}(y) involves calculating x^{-1} then multiplying, with complexity \\mathbb{F}_{\\mathsf{inv}} + 3 ~\\mathbb{F}_{\\mathsf{mul}}. Since |D_i| = 2^{i} \\cdot \\mathcal{R}, this step’s complexity is 2^{i} \\cdot \\mathcal{R} ~\\mathbb{F}_{\\mathsf{inv}} + 3 \\cdot 2^{i} \\cdot \\mathcal{R} ~\\mathbb{F}_{\\mathsf{mul}}.\n\nFor each i, the total computation complexity is:(2^{i} \\cdot \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{inv}} + (3 \\cdot 2^{i} \\cdot \\mathcal{R}+ 1) ~\\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity is:\\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 0}^{n - 1}((2^{i} \\cdot \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{inv}} + (3 \\cdot 2^{i} \\cdot \\mathcal{R}+ 1) ~\\mathbb{F}_{\\mathsf{mul}}) = (\\mathcal{R}N - \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} +  (3\\mathcal{R}N + n - 3\\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}}\n\nIf i > 0, Prover sends Merkle Tree commitment to [\\mathsf{fold}^{(i)}(x)|_{x \\in D_{i}}]. This mainly involves Hash operations. The tree height is \\log (2^{i} \\cdot \\mathcal{R})= i + \\log(\\mathcal{R}). If the tree has 2^k leaf nodes, the number of hash operations required is 2^{k - 1} + 2^{k - 2} + \\ldots + 2 + 1 = 2^k - 1. That is, \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) = (2^{i} \\cdot \\mathcal{R} - 1) ~ H.\n\nSummarizing all calculations above, the folding process happens n times with complexity (\\mathcal{R}N - \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} +  (3\\mathcal{R}N + n - 3\\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}}. The Merkle Tree commitment complexity is:\\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\n\nTherefore, the total complexity for this round is:\\begin{aligned}\n  &    (3\\mathcal{R}N + n - 3\\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}}+ (\\mathcal{R}N - \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} +\\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \\\\\n\\end{aligned}\n\nNote\n\nGenerally, to prove that a polynomial has degree less than 2^n, the complexity in the FRI low degree test phase is as shown above.","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-4","position":23},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 5","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#round-5","position":24},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Round 5","lvl2":"Evaluation Proof Protocol"},"content":"This round continues the FRI protocol’s low degree test interaction in the query phase. Verifier repeats queries l times:\n\nVerifier randomly selects a number t^{(n)} \\stackrel{\\$}{\\leftarrow} D^{(n)}\n\nProver sends values \\hat{f}(t^{(n)}), \\hat{f}(- t^{(n)}) with their Merkle Paths:\\{(\\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], t^{(n)})\\{(\\hat{f}(-t^{(n)}), \\pi_{\\hat{f}}(-t^{(n)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], -t^{(n)})\n\nFor i = n - 1, \\ldots, 1:\n\nProver calculates t^{(i)} = (t^{(i + 1)})^2\n\nProver sends \\hat{q}_{i}(t^{(i)}) and its Merkle Path\\{(\\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)}))\\} \\leftarrow \\mathsf{MMCS.open}(\\hat{q}_{i}, t^{(i)})\n\nProver sends \\mathsf{fold}^{(i)}(-t^{(i)}) and its Merkle Path\\{(\\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)}))\\} \\leftarrow \\mathsf{MT.open}(\\mathsf{fold}^{(i)}, -t^{(i)})\n\nFor i = 0:\n\nProver calculates t^{(0)} = (t^{(1)})^2\n\nProver sends \\hat{q}_0(s^{(0)}) and its Merkle Path\n\\{(\\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)}))\\} \\leftarrow \\mathsf{MMCS.open}(\\hat{q}_0, t^{(0)})\n\n📝 Notes\n\nFor example, when querying 3 polynomials, if the query selects the last element \\omega_2^7 in q_{\\hat{q}_2}(X), then Prover needs to send the values and their Merkle Paths for the green parts in the figure below. The orange-bordered parts indicate that what’s being sent isn’t the value of the quotient polynomial itself and its corresponding Merkle Path, but the Merkle Path of \\hat{q}_k(X). That is, Prover will send:> \\{\\hat{q_2}(\\omega_2^7), \\hat{q_2}(\\omega_2^3), \\hat{q}_1(\\omega_1^3), \\mathsf{fold}^{(1)}(\\omega_1^1),  \\hat{q}_0(\\omega_0^1)\\}\n>\n\nand the Merkle Paths corresponding to these values.","type":"content","url":"/analysis/zeromorph-fri-analysis#round-5","position":25},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 5","lvl3":"Round 5","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-5","position":26},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Prover Cost Round 5","lvl3":"Round 5","lvl2":"Evaluation Proof Protocol"},"content":"In the query phase, Prover’s computational complexity mainly comes from calculating s^{(i + 1)} = (s^{(i)})^2, but these numbers are all elements from D_i and don’t need additional calculation - they can be obtained through index values.","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost-round-5","position":27},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Prover Cost","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#prover-cost","position":28},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Prover Cost","lvl2":"Evaluation Proof Protocol"},"content":"Summarizing the Prover Cost:\\begin{aligned}\n& (\\mathcal{R}\\cdot nN + (\\mathcal{R} \\log \\mathcal{R} - 2 \\mathcal{R} + 1) N + 2\\mathcal{R}  - \\mathcal{R}\\log \\mathcal{R}  - 2) ~\\mathbb{F}_{\\mathsf{mul}}  + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) \\\\\n& + (2N - 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n& + (\\mathcal{R} \\cdot nN + (\\mathcal{R}\\log\\mathcal{R} + 6 \\mathcal{R}) \\cdot N - 3 \\mathcal{R} ) ~ \\mathbb{F}_{\\mathsf{mul}} + (2\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n& + (3\\mathcal{R}N + n - 3\\mathcal{R}) ~\\mathbb{F}_{\\mathsf{mul}}+ (\\mathcal{R}N - \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} +\\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \\\\\n& + (\\frac{3}{2} \\mathcal{R} \\cdot N + n - 3 \\mathcal{R} - 1) ~\\mathbb{F}_{\\mathsf{mul}} + (\\frac{1}{2} \\mathcal{R} \\cdot N - \\mathcal{R}) ~\\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 2}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \\\\\n= & (2\\mathcal{R}\\cdot nN + (2\\mathcal{R} \\log \\mathcal{R} + 7 \\mathcal{R} + 3) \\cdot  N + n -  \\mathcal{R}\\log \\mathcal{R} - 4 \\mathcal{R} - 3) ~\\mathbb{F}_{\\mathsf{mul}} + (3 \\mathcal{R} \\cdot N - 2 \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) + \\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\\\\\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-fri-analysis#prover-cost","position":29},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Proof","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#proof","position":30},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Proof","lvl2":"Evaluation Proof Protocol"},"content":"The proof sent by the Prover is:\\begin{aligned}\n  \\pi = \\left(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{f}(\\zeta), \\hat{q}_0(\\zeta), \\ldots, \\hat{q}_{n - 1}(\\zeta), \\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}}\\right)\n\\end{aligned}\n\nUsing the symbol \\{\\cdot\\}^l to represent the proofs generated by repeating queries l times in the FRI low degree test query phase. Since each query is randomly selected, the proof in braces is also random. The low degree test proof from the FRI protocol is:\\begin{aligned}\n  \\pi_{q_{\\hat{q}_{n}}, \\ldots, q_{\\hat{q}_{0}}} = &  ( \\mathsf{cm}(\\mathsf{fold}^{(n - 1)}(X)), \\ldots, \\mathsf{cm}(\\mathsf{fold}^{(1)}(X)),\\mathsf{fold}^{(0)}(y_0),  \\\\\n  & \\, \\{\\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)}), \\hat{f}(- t^{(n)}), \\pi_{\\hat{f}}(- t^{(n)}),\\\\\n  & \\quad \\hat{q}_{n - 1}(t^{(n - 1)}), \\pi_{\\hat{q}_{n - 1}}(t^{(n - 1)}), \\mathsf{fold}^{(n - 1)}(-t^{(n - 1)}), \\pi_{\\mathsf{fold}^{(n - 1)}}(-t^{(n - 1)}), \\ldots, \\\\\n  & \\quad \\hat{q}_{1}(t^{(1)}), \\pi_{\\hat{q}_{1}}(t^{(1)}), \\mathsf{fold}^{(1)}(-t^{(1)}), \\pi_{\\mathsf{fold}^{(1)}}(-t^{(1)}), \\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)})\\}^l)\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-fri-analysis#proof","position":31},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Proof Size","lvl3":"Proof","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#proof-size","position":32},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Proof Size","lvl3":"Proof","lvl2":"Evaluation Proof Protocol"},"content":"\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0) is an mmcs structure commitment, sending a hash value denoted as H.\n\n\\hat{f}(\\zeta), \\hat{q}_0(\\zeta), \\ldots, \\hat{q}_{n - 1}(\\zeta) are values in the finite field, with size (n + 1) ~ \\mathbb{F}.\n\nCalculating the size of \\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}}\n\nThe size of \\mathsf{cm}(\\mathsf{fold}^{(n - 1)}(X)), \\mathsf{cm}(\\mathsf{fold}^{(n - 2)}(X)), \\ldots, \\mathsf{cm}(\\mathsf{fold}^{(1)}(X)),\\mathsf{fold}^{(0)}(y_0) is n ~ H + \\mathbb{F}.\n\nRepeat l times:\n\n\\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)}), \\hat{f}(- t^{(n)}), \\pi_{\\hat{f}}(-t^{(n)}): \\pi_{\\hat{f}}(t^{(n)}) and \\pi_{\\hat{f}}(-t^{(n)}) are Merkle Paths. Here \\hat{f} is committed using a Merkle Tree structure with 2^n \\cdot \\mathcal{R} leaf nodes. To reduce the Merkle Path sent when opening points, when committing with the Merkle Tree, \\hat{f}(s^{(0)}), \\hat{f}(- s^{(0)}) can be placed on adjacent leaf nodes, sending \\log(2^n \\cdot \\mathcal{R} - 1) ~H hash values, or (n + \\log \\mathcal{R} - 1) ~H.\n\nHere \\mathsf{MT.open}(x) represents the number of hash values in the Merkle Path when committing x leaf nodes in a Merkle Tree:\\mathsf{MT.open}(x) = \\log x\n\nSo the proof size for this step is:2 ~ \\mathbb{F} + (\\mathsf{MT.open}(2^n \\cdot \\mathcal{R}) - 1) ~ H = 2 ~ \\mathbb{F} + (n + \\log \\mathcal{R} - 1) ~ H\n\nFor i = n-1, \\ldots, 1, sending \\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)}), \\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)}).\n\nFor \\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)}), the Merkle path in the mmcs structure needs to be sent.\n\nFor example, to open \\hat{q}_{1}(\\omega_1^0), the purple hash values in the figure are the proof sent by Prover. During verification, Verifier can calculate the values in the green part and finally compare whether the value of the topmost green square in the tree is equal to the root node value sent by Prover during commitment.\n\nThe \\hat{q}_{i}(t^{(i)}) corresponds to \\hat{q}_i(X) with 2^i \\cdot \\mathcal{R} values during commitment. So \\pi_{\\hat{q}_{i}}(t^{(i)}) needs to send 2 (\\log(2^{i} \\cdot \\mathcal{R})) =2i + 2 \\log \\mathcal{R} hash values.\n\nThe Merkle Tree for \\mathsf{fold}^{(i)}(x) has height i + \\log \\mathcal{R}. For \\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)}), using Merkle tree commitment, the tree height is i + \\log \\mathcal{R}, so i + \\log \\mathcal{R} hash values are sent.\n\nTherefore, for each round i, the size is 2 ~ \\mathbb{F} +(3i + 3\\log \\mathcal{R}) ~ H. The total complexity is:\\begin{aligned}\n  & \\sum_{i = 1}^{n - 1} (2 ~ \\mathbb{F} +(3i + 3\\log \\mathcal{R}) ~ H) \\\\\n  = & (2n - 2) ~ \\mathbb{F} + (\\frac{3(n - 1)n}{2} + 3(n - 1) \\log \\mathcal{R}) ~ H \\\\\n  = & (2n - 2) ~ \\mathbb{F} + (\\frac{3}{2} n^2 + (3\\log \\mathcal{R} - \\frac{3}{2}) n  - 3 \\log \\mathcal{R})  ~ H\n\\end{aligned}\n\nFor \\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)}), still using mmcs structure, \\pi_{\\hat{q}_{0}}(t^{(0)}) needs to send 2 (\\log(2^{0} \\cdot \\mathcal{R})) =2 \\log \\mathcal{R} hash values. Total complexity:\\mathbb{F} + 2 \\log \\mathcal{R} ~H\n\nTherefore, the size of \\pi_{q_{\\hat{q}_{n}}, \\ldots, q_{\\hat{q}_{0}}} is:\\begin{align}\n & n ~ H + \\mathbb{F} \\\\\n & + l \\cdot (2 ~ \\mathbb{F} + (n + \\log \\mathcal{R} - 1) ~ H)  \\\\\n & + l \\cdot ((2n - 2) ~ \\mathbb{F} + (\\frac{3}{2} n^2  + (3\\log \\mathcal{R} - \\frac{3}{2}) n  - 3 \\log \\mathcal{R})  ~ H )\\\\\n& + l \\cdot (\\mathbb{F} + 2 \\log \\mathcal{R} ~H) \\\\\n & = (2l \\cdot n + 3l - 1) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l)  ~ H\n\\end{align}\n\nTherefore, the total proof size is:\\begin{align}\n & H + (n + 1) ~ \\mathbb{F} + (2l \\cdot n + 3l - 1) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l)  ~ H \\\\\n=  & ((2l + 1) \\cdot n + 3l) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l + 1)  ~ H\n\\end{align}","type":"content","url":"/analysis/zeromorph-fri-analysis#proof-size","position":33},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#verification","position":34},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Verifier:","type":"content","url":"/analysis/zeromorph-fri-analysis#verification","position":35},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 1","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#step-1","position":36},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 1","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Verify the low degree test for q_{f_{\\zeta}}(X) and the n quotient polynomials \\{q_{\\hat{q}_k}\\}_{k = 0}^{n - 1} at once:\\mathsf{OPFRI.verify}( \\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n-1}}, \\ldots, q_{\\hat{q}_{0}}}, 2^{n}) \\stackrel{?}{=} 1\n\nSpecifically, Verifier repeats l times:\n\nVerify the correctness of \\hat{f}(t^{(n)}), \\hat{f}(-t^{(n)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X), \\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X), \\hat{f}(-t^{(n)}), \\pi_{\\hat{f}}(-t^{(n)})) \\stackrel{?}{=} 1\n\nThe height of \\hat{f}'s Merkle tree is 2^n \\cdot \\mathcal{R}. The Merkle Path sends n + \\log \\mathcal{R} - 1 hash values. Using bit reverse method to place two points differing by a sign on adjacent leaf nodes, the complexity is:\\mathsf{MTV}(2^n \\cdot \\mathcal{R}) = (n + \\log \\mathcal{R} - 1) ~ H\n\nVerifier calculates:q_{\\hat{q}_{n}}(t^{(n)}) = (1 + \\lambda \\cdot t^{(n)}) \\cdot \\frac{\\hat{f}(t^{(n)}) - \\hat{f}(\\zeta)}{t^{(n)} - \\zeta}q_{\\hat{q}_{n}}(-t^{(n)}) = (1 - \\lambda \\cdot t^{(n)}) \\cdot \\frac{\\hat{f}(-t^{(n)}) - \\hat{f}(\\zeta)}{-t^{(n)} - \\zeta}\n\nComplexity: 5 ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathbb{F}_{\\mathsf{inv}}.\n\nInitialize \\mathsf{fold} value to:\\mathsf{fold} = \\frac{q_{\\hat{q}_{n}}(t^{(n)}) + q_{\\hat{q}_{n}}(-t^{(n)})}{2} + \\beta^{(n - 1)} \\cdot \\frac{q_{\\hat{q}_{n}}(t^{(n)}) - q_{\\hat{q}_{n}}(-t^{(n)})}{2 \\cdot t^{(n)}}\n\nComplexity: 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 4 ~ \\mathbb{F}_{\\mathsf{mul}}\n\nFor i = n - 1, \\ldots , 1:\n\nVerifier calculates t^{(i)} = (t^{(i + 1)})^2\n\nVerifies correctness of \\hat{q}_{i}(t^{(i)}):\\mathsf{MMCS.verify}(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)})) \\stackrel{?}{=} 1\n\nVerifier will calculate hash values based on the proof sent by Prover. Prover sends 2i + 2 \\log \\mathcal{R} hash values, and since Verifier also calculates the hash of \\hat{q}_{i}(t^{(i)}, the complexity is (2i + 2 \\log \\mathcal{R}) ~ C + H.\n\nVerifier calculates:q_{\\hat{q}_{i}}(t^{(i)}) = (1 + \\lambda \\cdot t^{(i)}) \\cdot \\frac{\\hat{q}_{i}(t^{(i)}) - \\hat{q}_{i}(\\zeta)}{t^{(i)} - \\zeta}\n\nCalculation complexity: \\mathbb{F}_{\\mathsf{inv}} + 3~ \\mathbb{F}_{\\mathsf{mul}}.\n\nUpdates \\mathsf{fold} value to:\\mathsf{fold} = \\mathsf{fold} + q_{\\hat{q}_{i}}(t^{(i)})\n\nVerifier verifies correctness of \\mathsf{fold}^{(i)}(-t^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\mathsf{fold}^{(i)}(X)), \\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)})) \\stackrel{?}{=} 1\n\nVerifier’s calculation depends on the Merkle tree height, which is i + \\log \\mathcal{R}. The hash calculations to be performed is the tree height, i.e., (i + \\log \\mathcal{R}) ~ H.\n\nUpdates \\mathsf{fold} value:\\mathsf{fold} = \\frac{\\mathsf{fold}^{(i)}(-t^{(i)}) + \\mathsf{fold}}{2} + \\beta^{(i - 1)} \\cdot \\frac{\\mathsf{fold}^{(i)}(-t^{(i)}) - \\mathsf{fold}}{2 \\cdot t^{(i)}}\n\nThe verifier’s computation complexity here is 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 4 ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor i = 0:\n\nVerifier calculates t^{(0)} = (t^{(1)})^2\n\nVerifies correctness of \\hat{q}_0(t^{(0)}):\\mathsf{MMCS.verify}(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)})) \\stackrel{?}{=} 1\n\nVerifier’s computation complexity is H + C.\n\nVerifier calculates:q_{\\hat{q}_0}(t^{(0)}) = (1 + \\lambda \\cdot t^{(0)}) \\cdot \\frac{\\hat{q}_0(t^{(0)}) - \\hat{q}_0(\\zeta)}{t^{(0)} - \\zeta}\n\nCalculation complexity: \\mathbb{F}_{\\mathsf{inv}} + 3~ \\mathbb{F}_{\\mathsf{mul}}.\n\nVerifier checks correctness of:\\mathsf{fold}^{(0)}(y_0) \\stackrel{?}{=} \\mathsf{fold} + q_{\\hat{q}_0}(t^{(0)})\n\n📝 Notes\n\nFor example, in the previous Verifier query example, Verifier calculates the purple values through the values sent by Prover, as well as verifies the Merkle Tree proof sent by Prover about the orange part. Finally, Verifier checks if the last purple value it calculated equals the value previously sent by Prover.","type":"content","url":"/analysis/zeromorph-fri-analysis#step-1","position":37},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 1","lvl4":"Step 1","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl5","url":"/analysis/zeromorph-fri-analysis#verifier-cost-1","position":38},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 1","lvl4":"Step 1","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Summarizing the complexity above:\\begin{align}\n & (n + \\log \\mathcal{R} - 1) ~ H + 5 ~ \\mathbb{F}_{\\mathsf{mul}} + 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 4 ~ \\mathbb{F}_{\\mathsf{mul}}  \\\\\n & + \\sum_{i = 1}^{n - 1} \\left( (2i + 2 \\log \\mathcal{R}) ~ C + H + \\mathbb{F}_{\\mathsf{inv}} + 3~ \\mathbb{F}_{\\mathsf{mul}} + (i + \\log \\mathcal{R}) ~ H + 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 4 ~ \\mathbb{F}_{\\mathsf{mul}} \\right) \\\\\n& + H + C + \\mathbb{F}_{\\mathsf{inv}} + 3~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n=  & (7n + 5) ~ \\mathbb{F}_{\\mathsf{mul}} + (3n + 2) ~ \\mathbb{F}_{\\mathsf{inv}}  \\\\\n & + (\\frac{1}{2} n^2 + (\\frac{3}{2} + \\log \\mathcal{R})n - 1) ~H + (n^2 + (2 \\log \\mathcal{R} - 1) n + 1 - 2\\log \\mathcal{R}) ~C\n\\end{align}\n\nSince this is repeated l times, we multiply by l:(ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H + (7ln + 5l) ~ \\mathbb{F}_{\\mathsf{mul}} + (3ln + 2l) ~ \\mathbb{F}_{\\mathsf{inv}}","type":"content","url":"/analysis/zeromorph-fri-analysis#verifier-cost-1","position":39},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 2","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#step-2","position":40},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 2","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Calculate \\Phi_n(\\zeta) and \\Phi_{n - k}(\\zeta^{2^k})(0 \\le k < n), where:\\Phi_k(X^h) = 1 + X^h + X^{2h} + \\ldots + X^{(2^{k}-1)h}\n\nTherefore:\\Phi_n(\\zeta) = 1 + \\zeta + \\zeta^2 + \\ldots + \\zeta^{2^n-1}\\Phi_{n-k}(\\zeta^{2^k}) = 1 + \\zeta^{2^k} + \\zeta^{2\\cdot 2^k} + \\ldots + \\zeta^{(2^{n-k}-1)\\cdot 2^k}","type":"content","url":"/analysis/zeromorph-fri-analysis#step-2","position":41},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 2","lvl4":"Step 2","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl5","url":"/analysis/zeromorph-fri-analysis#verifier-cost-2","position":42},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 2","lvl4":"Step 2","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Verifier calculates:\\begin{align}\n\\Phi_k(X^h)  & = 1 + X^h + X^{2h} + \\ldots + X^{(2^{k}-1)h}  \\\\\n & = \\frac{1 - (X^h)^{2^k}}{1 - X^h}\n\\end{align}\n\nTherefore:\\Phi_n(\\zeta) = \\frac{1 - \\zeta^{2^n}}{1 - \\zeta}\\begin{align}\n\\Phi_{n - k}(\\zeta^{2^k})  & = \\frac{1 - (\\zeta^{2^k})^{2^{n - k}}}{1 - \\zeta^{2^k}} \\\\\n & = \\frac{1 - \\zeta^{2^n}}{1 - \\zeta^{2^k}}\n\\end{align}\n\nFor k = 0, 1, \\ldots, n - 1, first calculate \\zeta^{2^1}, \\zeta^{2^2}, \\ldots, \\zeta^{2^{n - 1}}, \\zeta^{2^n} with complexity n ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nFor k = 0, 1, \\ldots, n - 1, calculate the inverse of 1 - \\zeta^{2^k} then multiply by the numerator, with total complexity n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathbb{F}_{\\mathsf{inv}}.\n\nTherefore, the total complexity for this step is:2n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathbb{F}_{\\mathsf{inv}}","type":"content","url":"/analysis/zeromorph-fri-analysis#verifier-cost-2","position":43},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 3","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl4","url":"/analysis/zeromorph-fri-analysis#step-3","position":44},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl4":"Step 3","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Verify the correctness of the following equation:\\hat{f}(\\zeta) - v\\cdot\\Phi_n(\\zeta) = \\sum_{k = 0}^{n - 1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta)","type":"content","url":"/analysis/zeromorph-fri-analysis#step-3","position":45},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 3","lvl4":"Step 3","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"type":"lvl5","url":"/analysis/zeromorph-fri-analysis#verifier-cost-3","position":46},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl5":"Verifier Cost 3","lvl4":"Step 3","lvl3":"Verification","lvl2":"Evaluation Proof Protocol"},"content":"Computing v\\cdot\\Phi_n(\\zeta) has complexity \\mathbb{F}_{\\mathsf{mul}}.\n\nComputing \\sum_{k = 0}^{n - 1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta) has complexity:\\sum_{k = 0}^{n - 1} 3 ~ \\mathbb{F}_{\\mathsf{mul}} = 3n ~ \\mathbb{F}_{\\mathsf{mul}}\n\nTherefore, the total complexity is (3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}}.","type":"content","url":"/analysis/zeromorph-fri-analysis#verifier-cost-3","position":47},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Evaluation Proof Protocol"},"type":"lvl3","url":"/analysis/zeromorph-fri-analysis#verifier-cost","position":48},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl3":"Verifier Cost","lvl2":"Evaluation Proof Protocol"},"content":"Summarizing the Verifier’s complexity:\\begin{align}\n & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H + (7ln + 5l) ~ \\mathbb{F}_{\\mathsf{mul}} + (3ln + 2l) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n & + 2n ~ \\mathbb{F}_{\\mathsf{mul}} + n ~ \\mathbb{F}_{\\mathsf{inv}} + (3n + 1) ~ \\mathbb{F}_{\\mathsf{mul}} \\\\\n= & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H  \\\\\n & + ((7l + 5)n + 5l + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((3l + 1)n + 2l) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{align}","type":"content","url":"/analysis/zeromorph-fri-analysis#verifier-cost","position":49},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl2":"Complexity Summary"},"type":"lvl2","url":"/analysis/zeromorph-fri-analysis#complexity-summary","position":50},{"hierarchy":{"lvl1":"Zeromroph-fri Protocol Complexity Analysis","lvl2":"Complexity Summary"},"content":"Prover’s Cost:\\begin{align}\n& (2\\mathcal{R}\\cdot nN + (2\\mathcal{R} \\log \\mathcal{R} + 7 \\mathcal{R} + 3) \\cdot  N + n -  \\mathcal{R}\\log \\mathcal{R} - 4 \\mathcal{R} - 3) ~\\mathbb{F}_{\\mathsf{mul}} + (3 \\mathcal{R} \\cdot N - 2 \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) + \\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\n\\end{align}\n\nProof size:\\begin{align}\n((2l + 1) \\cdot n + 3l) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l + 1)  ~ H\n\\end{align}\n\nVerifier’s Cost:\\begin{aligned}\n  & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H  \\\\\n & + ((7l + 5)n + 5l + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((3l + 1)n + 2l) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{aligned}","type":"content","url":"/analysis/zeromorph-fri-analysis#complexity-summary","position":51},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes"},"type":"lvl1","url":"/basefold/basefold-01","position":0},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nJade Xie \n\njade@secbit.io\n\nBasefold can be regarded as an extension of FRI, thereby supporting Proximity Proofs and Evaluation Arguments for Multi-linear Polynomials. Compared to Libra-PCS, Hyrax-PCS, and Virgo-PCS, Basefold does not rely on the MLE Quotients Equation to prove the value of an MLE at the point \\mathbf{u}:\\tilde{f}(X_0, X_1, X_2, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{n-1}(X_i - u_i)\\cdot q_i(X_0, X_1, X_2, \\ldots, X_{i-1})\n\nInstead, Basefold leverages the Sumcheck protocol to reduce the value of \\tilde{f}(\\mathbf{X}) at a pre-specified point to its value at a random point \\vec{\\alpha}. The latter can then utilize an FRI-like approach, where the verifier provides a challenge vector \\vec{\\alpha} to recursively fold \\tilde{f}(\\mathbf{X}). This approach allows the prover to simultaneously demonstrate an upper bound on the degree of \\tilde{f}(\\mathbf{X}) (Proof of Proximity) and, based on the characteristics of MLE, prove the value of \\tilde{f}(\\vec{\\alpha}). By combining the Sumcheck protocol with an FRI-style folding protocol, we elegantly obtain an Evaluation Argument for an MLE.\n\nAnother significant insight of Basefold is how to apply an FRI-style Proof of Proximity over arbitrary finite fields. It is known that the FRI protocol fully utilizes the structure of Algebraic FFTs over finite fields and performs interactive folding, which allows the codeword length to be exponentially reduced while maintaining the minimum relative Hamming distance bound, thereby enabling the verifier to easily verify a folded short codeword. However, for general finite fields where constructing FFTs is challenging, the FRI protocol cannot be directly applied.\n\nBasefold introduces the concept of Random Foldable Codes. This is a framework for encoding using a recursive approach, where recursive encoding can be seen as the inverse process of recursive folding. In the Commit phase of the Basefold protocol, the prover first encodes each segment of the message using a base encoding scheme G_0, then encodes these codewords pairwise (a process similar to the butterfly operation in FFT computations), and finally obtains a single codeword. In the Commit-phase stage, the prover and verifier interactively perform half-folding on the committed single codeword and then commit to the half-folded codeword. It can be proven that the Relative Hamming Distance of this half-folded codeword remains above a clear lower bound. The parties then continue folding until the length is reduced to that of G_0-encoded length. In this way, both parties obtain a series of committed codewords. Note that since the encoding process is performed recursively, the codeword possesses folding capabilities similar to those of RS Code encoding. The advantage of this approach is evident: using Recursive Folding FRI protocols results in significantly smaller proof sizes compared to protocols like Tensor Codes. Additionally, Basefold’s technique can be applied to any finite field (|\\mathbb{F}|>2^{10}), including extension fields \\mathbb{F}_{p^k}. Naturally, similar to the FRI protocol, the commitment computation in the Basefold protocol requires O(N\\log(N)) time complexity, while the prover’s computational effort in the subsequent proof process remains at O(N). If a Merkle Tree is used as the commitment scheme for the codeword, the proof size complexity is O(\\log^2(N)).\n\nIn this article, we first explore the concept of Foldable Linear Codes.","type":"content","url":"/basefold/basefold-01","position":1},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"What are Foldable Linear Codes"},"type":"lvl2","url":"/basefold/basefold-01#what-are-foldable-linear-codes","position":2},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"What are Foldable Linear Codes"},"content":"Suppose there is a linear code C_0: \\mathbb{F}_p^{k_0}\\to\\mathbb{F}_p^{n_0} based on the finite field \\mathbb{F}_p, where the message length is k_0, and the codeword length is n_0. Assume that the encoded length is amplified by a factor of R compared to the message length, i.e., 1/R is the traditional code rate. According to the definition of linear codes, there must exist an encoding matrix G_0: \\mathbb{F}_p^{k_0\\times n_0} such that\\mathbf{m}{G_0} = \\mathbf{c}\n\nThen, we can construct a new encoding matrix G_1 based on the encoding matrix G_0:G_1 = \\begin{bmatrix}\nG_{0} &  G_{0} \\\\\nG_{0}\\cdot T_{0} & G_{0}\\cdot T'_{0}\n\\end{bmatrix}\n\nwhere T_0 and T'_0 are two diagonal matrices with diagonal elements set to the parameters of the encoding scheme, which we will explain later.\n\nIf we consider G_1 as the encoding matrix of another linear code C_1, then the parameters of C_1 are [R, 2k_0, 2n_0]. That is, compared to C_0, the code rate of C_1 remains unchanged, but the message and codeword lengths are both doubled.\n\nFor example, suppose \\mathbf{m}=(m_0, m_1, m_2, m_3), and the base encoding scheme C_0 is a simple Repetition Code with k_0=2 and n_0=4. Thus, the message length of \\mathbf{m} exactly meets the requirements of G_1, i.e., k_1=2\\cdot k_0 = 4 and the encoded length is n_1 = 2\\cdot n_0 = 8.\n\nThe encoding matrix G_0 of the base code C_0 is defined as:G_0 = \\begin{bmatrix}\n1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1\\\\\n\\end{bmatrix}\n\nAssume the diagonal elements of the system parameter T_0 are (t_0, t_1, t_2, t_3), and those of T'_0 are (t'_0, t'_1, t'_2, t'_3). We construct G_1 using the formula above:G_1 = \\left[\\begin{array}{cccc|cccc}\n1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\\\\n\\hline\nt_0 & 0 & t_2 & 0 & t'_0 & 0 & t'_2 & 0\\\\\n0 & t_1 & 0 & t_3 & 0 & t'_1 & 0 & t'_3\\\\\n\\end{array}\\right]\n\nSubstituting \\mathbf{m} directly, we obtain:\\mathbf{m}G_1 = \\begin{bmatrix}\nm_0 & m_1 & m_2 & m_3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\\\\nt_0 & 0 & t_2 & 0 & t'_0 & 0 & t'_2 & 0\\\\\n0 & t_1 & 0 & t_3 & 0 & t'_1 & 0 & t'_3\\\\\n\\end{bmatrix}\n\nWe list the encoded codeword vector separately:\\begin{array}{cccc}\nm_0 + t_0m_2 & m_1 + t_1m_3 & m_0 + t_2m_2 & m_1 + t_3m_3  \\\\\nm_0 + t'_0m_2 & m_1 + t'_1m_3 & m_0 + t'_2m_2 & m_1 + t'_3m_3 \n\\end{array}\n\nDirectly performing matrix operations, the structure of this encoding is not very clear. Let us approach it differently: the message \\mathbf{m} can be split into two equal-length parts, the left \\mathbf{m}_l=(m_0, m_1) and the right \\mathbf{m}_r=(m_2, m_3),\\mathbf{m}G_1 = (\\mathbf{m}_l\\parallel \\mathbf{m}_r)G_1=\n\\begin{bmatrix} 1 & 1 \\end{bmatrix} \n\\begin{bmatrix} \\mathbf{m}_l & 0 \\\\\n0 & \\mathbf{m}_r \\end{bmatrix}\n\\begin{bmatrix}\nG_{0} &  G_{0} \\\\\nG_{0}\\cdot T_{0} & G_{0}\\cdot T'_{0}\n\\end{bmatrix}=\n\\begin{bmatrix} 1 & 1 \\end{bmatrix} \n\\begin{bmatrix}\n\\mathbf{m}_lG_0 & \\mathbf{m}_lG_0\\\\\n\\mathbf{m}_rG_0T_0 & \\mathbf{m}_rG_0T'_0\\\\\n\\end{bmatrix}\n\nFor the 2\\times 2 matrix on the right side of the equation, we first compute the top-left and top-right submatrices, which are identical submatrices obtained by encoding \\mathbf{m}_l with G_0:\\mathbf{m}_lG_0 = \n\\begin{bmatrix}\nm_0 & m_1 \n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nm_0 & m_1 & m_0 & m_1 \\\\\n\\end{bmatrix}\n\nAnd the bottom-left submatrix is:\\begin{split}\n\\mathbf{m}_r(G_0T_0) = (\\mathbf{m}_rG_0)T_0 &=\\begin{bmatrix}\nm_2 & m_3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n\\end{bmatrix} T_0 \\\\\n& =  \\begin{bmatrix}\nm_2 & m_3 & m_2 & m_3\n\\end{bmatrix}\\begin{bmatrix}\nt_0 & 0 & 0 & 0\\\\\n0 & t_1 & 0 & 0\\\\\n0 & 0 & t_2 & 0\\\\\n0 & 0 & 0 & t_3\\\\\n\\end{bmatrix} \\\\\n& = (t_0m_2, t_1m_3, t_2m_2, t_3m_3)\n\\end{split}\n\nThe result for \\mathbf{m}_r(G_0T'_0) is almost the same, with t_i replaced by t'_i:\\mathbf{m}_r(G_0T'_0) = (t'_0m_2, t'_1m_3, t'_2m_2, t'_3m_3)\n\nIt is easy to verify that we obtain the same result for \\mathbf{m} encoded with G_1.\n\nWe can simplify this computation process with the following equation:\\mathbf{m}G_1 = \\mathbf{m}_lG_0 +  (t_0, t_1, t_2, t_3) \\circ\\mathbf{m}_rG_0 \\parallel  \\mathbf{m}_lG_0 + (t'_0, t'_1, t'_2, t'_3) \\circ \\mathbf{m}_rG_0\n\nIf we consider k_0 as a system parameter (constant), the encoding computation in the above equation has a complexity of O(n_0), including two G_0 encodings and two component-wise additions of length n_0. Additionally, this equation resembles the butterfly operation in (Multiplicative) FFT algorithms:a' = a + t \\cdot b, \\quad b' = a - t \\cdot b\n\nIn fact, as illustrated later, the encoding process of Foldable Codes is a generalized extension of RS-Code.\n\nWe can further recursively construct G_2, G_3, \\ldots, G_d, eventually obtaining a linear code\nC_d: \\mathbb{F}_p^{k}\\to\\mathbb{F}_p^{n}, where the message length is k=k_0\\cdot 2^d, and the encoding length is n=c\\cdot k_0\\cdot 2^{d}. The code rate is \\rho=\\frac{1}{R}, where k_0 is the message length of the base encoding, n_0 is the base encoding length, and the choice of the base encoding is highly flexible.\n\nWe use the symbol G_d to denote the encoding matrix (or generator matrix) of C_d, G_d\\in\\mathbb{F}_p^{k\\times n}. For i\\in\\{1,2,\\ldots,d\\}, we have the following recursive relation:G_i = \\begin{bmatrix}\nG_{i-1} &  G_{i-1} \\\\\nG_{i-1}\\cdot T_{i-1} & G_{i-1}\\cdot T'_{i-1}\n\\end{bmatrix}\n\nHere, T_{i-1} and T'_{i-1} are two diagonal matrices,T_{i-1} = \\begin{bmatrix}\nt_{i-1, 0} & 0 & \\cdots & 0 \\\\\n0 & t_{i-1,1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & t_{i-1, n_i-1}\n\\end{bmatrix}, \\quad\nT'_{i-1} = \\begin{bmatrix}\nt'_{i-1,0} & 0 & \\cdots & 0 \\\\\n0 & t'_{i-1,1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & t'_{i-1, n_i-1}\n\\end{bmatrix},\n\nand their diagonal elements at the same positions are distinct:\\forall j \\in [0, n_i-1], t_{i,j} \\neq t'_{i, j}\n\nAssuming we can compute G_d using the above recursive equation, then for a message \\mathbf{m}_d of length k, the encoding process can be directly calculated as follows:\\mathbf{w}_d = \\mathbf{m}_d G_d\n\nHowever, encoding directly using the generator matrix G_d is relatively inefficient, with a computational complexity of O(k\\cdot n). Instead, based on the above derivation, if we do not directly use the G_d matrix but instead employ a recursive approach to encode \\mathbf{m}_d, the basic idea is to split m into two parts m_{l} and m_{r}, then encode m_{l} and m_{r} separately using G_{d-1}, and finally concatenate the encoded C_l and C_r.\\mathbf{w}_d = \\Big(m_lG_{d-1} + \\mathsf{diag}(T_d)\\circ m_rG_{d-1} \\Big)\\parallel \\Big( m_lG_{d-1} + \\mathsf{diag}(T'_d)\\circ m_rG_{d-1} \\Big)\n\nIn this way, the encoding process using G_d is transformed into two encoding processes using G_{d-1}. We can continue to recursively compute m_lG_{d-1} and m_rG_{d-1} until the split message length meets k_0, after which we simply use the generator matrix G_0 for the base encoding. The encoding time complexity at this stage is O(k_0\\cdot n_0). With a total of d recursive rounds, the overall computational effort is reduced to O(n\\log(n)).","type":"content","url":"/basefold/basefold-01#what-are-foldable-linear-codes","position":3},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"RS code (foldable)"},"type":"lvl2","url":"/basefold/basefold-01#rs-code-foldable","position":4},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"RS code (foldable)"},"content":"The RS code used in the FRI protocol satisfies the aforementioned conditions, making RS code foldable.\n\nFirst, let us examine the generator matrix in the RS code:\\begin{bmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\n1 & \\omega & \\omega^2 & \\cdots & \\omega^{n-1} \\\\\n1 & \\omega^2 & \\omega^4 & \\cdots & \\omega^{2(n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & \\omega^{k-1} & \\omega^{2(k-1)} & \\cdots & \\omega^{(n-1)(k-1)}\n\\end{bmatrix}\n\nwhere \\omega is an n-th root of unity, satisfying \\omega^n=1, with n being the codeword length and k the message length. We can observe that the above matrix is a Vandermonde matrix. We will now explain how the generator matrix of the RS code satisfies the definition of Foldable Codes. For demonstration purposes, assume k_0=1, n_0=1, d=3, k_3=8, n_3=8, and \\omega^8=1,G^{(RS)} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\omega & \\omega^2 & \\omega^3 & \\omega^4 & \\omega^5 & \\omega^6 & \\omega^7 \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 & 1 & \\omega^{2} & \\omega^{4} & \\omega^{6} \\\\\n1 & \\omega^3 & \\omega^6 & \\omega^1 & \\omega^{4} & \\omega^{7} & \\omega^{2} & \\omega^{5} \\\\\n1 & \\omega^4 & 1 & \\omega^{4} & 1 & \\omega^{4} & 1 & \\omega^{4} \\\\\n1 & \\omega^5 & \\omega^{2} & \\omega^{7} & \\omega^{4} & \\omega & \\omega^{6} & \\omega^{3} \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} & 1 & \\omega^{6} & \\omega^{4} & \\omega^{2} \\\\\n1 & \\omega^7 & \\omega^{6} & \\omega^{5} & \\omega^{4} & \\omega^{3} & \\omega^{2} & \\omega \\\\\n\\end{bmatrix}\n\nFirst, we need to reorder the rows of G^{RS} according to the Reversed Bit Order (RBO) sequence. The reason for performing RBO reordering is related to the structure of Multiplicative FFTs, as explained in another article. The RBO sequence refers to reversing the binary representation of the indices and using these reversed binary numbers as the new indices. The RBO sequence is (0, 4, 2, 6, 1, 5, 3, 7). By reordering the rows of the above Vandermonde matrix according to the RBO sequence, we obtain the following matrix, denoted as G_2:G_2 = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} & 1 & \\omega^{4} & 1 & \\omega^{4} \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 & 1 & \\omega^{2} & \\omega^{4} & \\omega^{6} \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} & 1 & \\omega^{6} & \\omega^{4} & \\omega^{2} \\\\\n1 & \\omega & \\omega^2 & \\omega^3 & \\omega^4 & \\omega^5 & \\omega^6 & \\omega^7 \\\\\n1 & \\omega^5 & \\omega^{2} & \\omega^{7} & \\omega^{4} & \\omega & \\omega^{6} & \\omega^{3} \\\\\n1 & \\omega^3 & \\omega^6 & \\omega^1 & \\omega^{4} & \\omega^{7} & \\omega^{2} & \\omega^{5} \\\\\n1 & \\omega^7 & \\omega^{6} & \\omega^{5} & \\omega^{4} & \\omega^{3} & \\omega^{2} & \\omega \\\\\n\\end{bmatrix}\n\nObserving the reordered matrix, we can see that this matrix can be decomposed into submatrix operations involving G_1 and T_1, T'_1:G_2 = \n\\begin{bmatrix}\nG_1 & G_1 \\\\\nG_1T_1 & G_1T'_1\n\\end{bmatrix}\n=\n\\left[\n\\begin{array}{cccc|cccc}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} & 1 & \\omega^{4} & 1 & \\omega^{4} \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 & 1 & \\omega^{2} & \\omega^{4} & \\omega^{6} \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} & 1 & \\omega^{6} & \\omega^{4} & \\omega^{2} \\\\\n\\hline\n1 & \\omega & \\omega^2 & \\omega^3 & \\omega^4 & \\omega^5 & \\omega^6 & \\omega^7 \\\\\n1 & \\omega^5 & \\omega^{2} & \\omega^{7} & \\omega^{4} & \\omega & \\omega^{6} & \\omega^{3} \\\\\n1 & \\omega^3 & \\omega^6 & \\omega^1 & \\omega^{4} & \\omega^{7} & \\omega^{2} & \\omega^{5} \\\\\n1 & \\omega^7 & \\omega^{6} & \\omega^{5} & \\omega^{4} & \\omega^{3} & \\omega^{2} & \\omega \\\\\n\\end{array}\n\\right]\n\nwhere G_1 is:G_1 = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} \\\\\n\\end{bmatrix}\n\nWe can verify that the bottom-left submatrix can be expressed as G_1T_1:G_1T_1 = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &  &  &  \\\\\n & \\omega &  &  \\\\\n &  & \\omega^2 &  \\\\\n &  &  & \\omega^3 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & \\omega & \\omega^2 & \\omega^3 \\\\\n1 & \\omega^5 & \\omega^{2} & \\omega^{7} \\\\\n1 & \\omega^3 & \\omega^6 & \\omega^1 \\\\\n1 & \\omega^7 & \\omega^{6} & \\omega^{5} \\\\\n\\end{bmatrix}\n\nHere, the matrix T_1 is indeed a diagonal matrix satisfying the requirement that its diagonal elements are distinct. The bottom-right submatrix can be decomposed as follows:\\begin{bmatrix}\n\\omega^4 & \\omega^5 & \\omega^6 & \\omega^7 \\\\\n\\omega^4 & \\omega & \\omega^{6} & \\omega^{3} \\\\\n\\omega^4 & \\omega^7 & \\omega^2 & \\omega^5 \\\\\n\\omega^4 & \\omega^3 & \\omega^{2} & \\omega^{1} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} \\\\\n1 & \\omega^2 & \\omega^4 & \\omega^6 \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\omega^4 & & & \\\\\n    & \\omega^5 & & \\\\\n    & & \\omega^6 & \\\\\n    & & & \\omega^7 \\\\\n\\end{bmatrix}\n= G_1T'_1\n\nwhere T'_1 is exactly (-1)T_1, ensuring that the elements of T'_1 are distinct from those of T_1:T'_2 = \\begin{bmatrix}\n\\omega^4 & & & \\\\\n    & \\omega^5 & & \\\\\n    & & \\omega^6 & \\\\\n    & & & \\omega^7 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n-1 & & & \\\\\n    & -\\omega & & \\\\\n    & & -\\omega^2 & \\\\\n    & & & -\\omega^3 \\\\\n\\end{bmatrix}\n= -T_2\n\nNext, we can continue to recursively decompose G_1:G_1 = \\left[\\begin{array}{cc|cc}\n1 & 1 & 1 & 1 \\\\\n1 & \\omega^4 & 1 & \\omega^{4} \\\\\n\\hline\n1 & \\omega^2 & \\omega^4 & \\omega^6 \\\\\n1 & \\omega^6 & \\omega^{4} & \\omega^{2} \\\\\n\\end{array}\\right]\n= \\begin{bmatrix}\nG_0 & G_0 \\\\\nG_0\\cdot T_0 & G_1\\cdot T'_0\n\\end{bmatrix}\n\nHere, G_0, T_0, and T'_0 are:G_0=\\begin{bmatrix}\n1 & 1 \\\\\n1 & \\omega^4 \\\\\n\\end{bmatrix}, \\quad\nT_0=\\begin{bmatrix}\n1 & \\\\\n & \\omega^2 \\\\\n\\end{bmatrix}, \\quad\nT'_0=\\begin{bmatrix}\n\\omega^4 & \\\\\n & \\omega^6 \\\\\n\\end{bmatrix} = -T_0\n\nBy recursively decomposing, we reach the base encoding G_0, which remains an RS-Code, denoted as RS[k_0=1, n_0=1]. Thus, we have proven that RS-Code is a foldable code. However, it is evident that RS-Code imposes requirements on \\mathbb{F}_p, needing it to contain a multiplicative subgroup of order 2^s, where s is a positive integer, and ensuring that s is large enough to accommodate a codeword length of k_0\\cdot 2^{n_d}.","type":"content","url":"/basefold/basefold-01#rs-code-foldable","position":5},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-01#references","position":6},{"hierarchy":{"lvl1":"Notes on Basefold  (Part I): Foldable Linear Codes","lvl2":"References"},"content":"[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” In Annual International Cryptology Conference, pp. 138-169. Cham: Springer Nature Switzerland, 2024.\n\nRiad S. Wahby, Ioanna Tzialla, Abhi Shelat, Justin Thaler, and Michael Walfish. “Doubly-efficient zkSNARKs without trusted setup.” In 2018 IEEE Symposium on Security and Privacy (SP), pp. 926-943. IEEE, 2018.\n\nTiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song. “Libra: Succinct zero-knowledge proofs with optimal prover computation.” In Advances in Cryptology–CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18–22, 2019, Proceedings, Part III 39, pp. 733-764. Springer International Publishing, 2019.\n\nJiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent polynomial delegation and its applications to zero knowledge proof.” In 2020 IEEE Symposium on Security and Privacy (SP), pp. 859-876. IEEE, 2020.\n\nCharalampos Papamanthou, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” In Theory of Cryptography Conference, pp. 222-242. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013.","type":"content","url":"/basefold/basefold-01#references","position":7},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP"},"type":"lvl1","url":"/basefold/basefold-02","position":0},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nJade Xie \n\njade@secbit.io","type":"content","url":"/basefold/basefold-02","position":1},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Proof of Proximity"},"type":"lvl2","url":"/basefold/basefold-02#proof-of-proximity","position":2},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Proof of Proximity"},"content":"Below, we present a proof of implementing IOPP using Foldable codes.\n\nSuppose there is an MLE polynomial \\tilde{f}(\\mathbf{X}) represented as follows:\\tilde{f}(X_0, X_1, \\ldots, X_{d-1}) = f_0 + f_1X_0 + f_2X_1 + f_3X_0X_1 + \\cdots + f_{2^d-1}X_{d-1}\n\nSince \\tilde{f}(X) is a multivariate polynomial, there are d unknowns, making the length of its coefficient vector 2^d. Note that we choose the Lexicographic Order as the sorting method for the polynomial.\n\nWe encode the coefficient vector \\mathbf{f} of \\tilde{f}(\\mathbf{X}) to obtain the codeword c_\\mathbf{f} = \\mathsf{Enc}(\\mathbf{f}), which has a length of n_d. Then, we use a Hash-based Merkle Tree to generate the commitment:\\mathbf{cm}(\\mathbf{f}) = \\mathsf{Merklize}(\\mathsf{Enc}(f_0, f_1, f_2, \\ldots, f_{2^d-1}))\n\nSimilar to the FRI protocol, the Basefold-IOPP protocol is used to prove that a commitment \\pi_d=\\mathbf{cm}(\\mathbf{f}) is with high probability “close” to a vector encoded by C_d. Therefore, this protocol is called a Proof of Proximity. This protocol is one of the core protocols for constructing the Evaluation Argument.\n\nProof of Proximity leverages a remarkable property of linear codes: the “Proximity Gap.” Specifically, if two vectors \\pi, \\pi' are far from the legitimate codeword space, then their random linear combination \\pi+\\alpha\\cdot \\pi' will either have a very low probability of becoming legitimate or will remain far from the legitimate codeword space:\\begin{cases}\n\\Delta(\\pi_{i}, C_i) = 0 & \\text{(with negligible probability)} \\\\\n\\Delta(\\pi_{i}, C_i) \\leq \\Delta(\\pi_{i+1}, C_{i+1}) & \\text{(with non-negligible probability)} \\\\\n\\end{cases}\n\nThis indicates that the folding process of the codeword does not disrupt the distance between the vector and the legitimate codeword space. By folding the vector sufficiently, the Verifier can use a very short code to verify whether the final folded vector is a legitimate codeword, thereby determining whether the original vector is a legitimate codeword.\n\n📖 Notes on Proximity Gap\nProof of Proximity utilizes a remarkable property of linear codes: the “Proximity Gap.” Specifically, for two vectors \\pi, \\pi', folding them with a random scalar \\alpha \\in \\mathbb{F} yields a set A = \\{\\pi+\\alpha\\cdot \\pi': \\alpha \\in \\mathbb{F}\\}. Different \\alpha correspond to different elements in set A. The “Proximity Gap” theorem states that the elements in this set are either all close to the legitimate codeword space C_{i} or only a negligible fraction of the elements are close to C_{i}, while the majority are at a distance of \\delta from C_{i}. In probabilistic terms:\n\n> \\Pr_{a \\in A}[\\Delta(a, C_{i}) \\le \\delta] = \\begin{cases}\n>    \\epsilon \\quad \\text{(small enough)}\\\\\n>    1\n> \\end{cases}\n>\n\nThus, the Verifier can confidently use a random scalar \\alpha for folding, because even if only one of the two vectors \\pi, \\pi' provided by a cheating Prover is at a distance \\delta from C_i, the probability that the folded result is close to C_i is only \\epsilon, which is very small. In other words, a cheating Prover would need to be as lucky as winning the lottery to evade detection by the Verifier’s scrutiny. Therefore, if the Prover initially selects a \\pi_d that is far from the legitimate codeword space, the Verifier selects a series of random scalars to iteratively fold it until obtaining \\pi_0. During this process, there is a high probability that \\pi_0 does not become close to the legitimate codeword space, allowing the Verifier to detect cheating.\n\nThe “Proximity Gap” theorem provides a significant advantage to the Verifier: instead of verifying all elements in the set A = \\{\\pi+\\alpha\\cdot \\pi': \\alpha \\in \\mathbb{F}\\} to check their proximity to the legitimate codeword space, the Verifier only needs to randomly select one point for verification. This greatly reduces the Verifier’s computational load.\n\nThe Proof of Proximity protocol consists of two phases: the Commit-phase and the Query-phase. The former involves the subprotocol that performs multiple folding processes of the codeword and generates commitments (or oracles) for each folded codeword. The latter, the Query-phase, involves the Verifier performing random sampling to verify the legitimacy of each folding step.","type":"content","url":"/basefold/basefold-02#proof-of-proximity","position":3},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Commit-phase"},"type":"lvl2","url":"/basefold/basefold-02#commit-phase","position":4},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Commit-phase"},"content":"First, we explain the Commit-phase. The Prover performs multiple foldings of the encoded \\pi_d (with length n_d), obtaining codewords of lengths n_{d-1}, \\ldots, n_0, denoted as (\\pi_{d-1}, \\pi_{d-2}, \\ldots, \\pi_0), and then sends them to the Verifier. We should note that length of the all the codewords n_{d-1}, \\ldots, n_0 has to be a power of 2.\n\nRemember that this is an interactive protocol with a total of d rounds of interaction. In each round (assume the i-th round, 0 \\leq i < d), the Prover folds \\pi_{i+1} based on the random scalar \\alpha_{i} sent by the Verifier to obtain a new codeword, denoted as \\pi_{i}. After d rounds, the Prover obtains a codeword of length n_0, denoted as \\pi_0. The Prover then commits to each (\\pi_d, \\ldots, \\pi_0) and sends \\mathsf{cm}(\\pi_d), \\ldots, \\mathsf{cm}(\\pi_0) as the output of \\mathsf{IOPP.Commit}.\n\nNext, we analyze the technical details of a single folding \\pi_{i}. Suppose \\pi_{i}\\in C_i is a legitimate codeword (i.e., satisfying \\pi_{i} = \\mathbf{m}G_i), with length n_i:\\pi_i = (c_0, c_1, c_2, \\ldots, c_{n_{i}-1}) \\qquad                       0 \\leq i \\leq d\n\nWe split this vector into two parts and stack them:\\left(\n\\begin{array}{cccc}\nc_0, & c_1, & \\ldots, & c_{n_{i-1}-1} \\\\\nc_{n_{i-1}}, & c_{n_{i-1}+1}, & \\ldots, & c_{n_{i}-1} \\\\\n\\end{array}\n\\right)\n\nAt this point, the Verifier needs to provide a random scalar \\alpha^{(i)}. We perform a random linear combination of the two rows, or in other words, fold them:\\pi_{i-1} = \\big(\\mathsf{fold}_{\\alpha_i}(c_0, c_{n_{i-1}}), \\mathsf{fold}_{\\alpha_i}(c_1, c_{n_{i-1}+1}), \\ldots, \\mathsf{fold}_{\\alpha_i}(c_{n_{i-1}-1}, c_{n_{i}-1})\\big)\n\nThe above is the folded vector \\pi_{i-1}. Assuming the Prover is honest, the folded vector should be a legitimate C_{i-1} codeword. The function \\mathsf{fold}_{\\alpha_i} in the above equation is defined as follows:\\mathsf{fold}_{\\alpha}(c_j, c_{n_{i-1}+j}) = \\frac{t_j\\cdot c_{n_{i-1}+j} - t'_j\\cdot c_{j} }{t_j - t'_j} + \\alpha\\cdot \\frac{(c_{j} - c_{n_{i-1}+j})}{t_j - t'_j}\n\nHow should we understand the \\mathsf{fold}_{\\alpha}(\\cdot, \\cdot) function? It is essentially a polynomial interpolation process. We treat the two rows to be folded as sets of points on two separate domains, specifically the \\mathsf{diag}(T_i)=(t_0, t_1, \\ldots, t_{n_{i-1}-1}) and \\mathsf{diag}(T'_i)=(t'_0, t'_1, \\ldots, t'_{n_{i-1}-1}) used in the recursive encoding process:\\left(\n\\begin{array}{cccc}\n(t_0, c_0), & (t_1, c_1), & \\ldots, & (t_{i-1}, c_{n_{i-1}-1}) \\\\\n(t'_0, c_{n_{i-1}}), & (t'_1, c_{n_{i-1}+1}), & \\ldots, & (t'_{i-1}, c_{n_{i}-1}) \\\\\n\\end{array}\n\\right)\n\nWe then interpolate each column of the above matrix over the domain (t_j, t'_j) to produce a set of n_{i-1}={n_i}/{2} polynomials, denoted as p^{(i-1)}_j(X), where 0\\leq j < n_{i-1}. The Prover then evaluates each p^{(i-1)}_j(X) at X = \\alpha_{i}, resulting in n_{i-1} values at X=\\alpha_{i}. These values constitute the new codeword \\pi_{i-1}.\n\nThe definition of the folding function aligns with the linear polynomial interpolation process. We can manually derive the origin of the folding function definition. Since we are performing a half-folding of \\pi_{i}, the folded codeword will have n_{i-1} values corresponding to “linear polynomials.” Suppose the j-th polynomial describes a line passing through two points (x_0, y_0) and (x_1, y_1). The interpolating polynomial p(X) for these two points can be defined as:\\begin{split}\np(X) &= \\frac{y_0}{x_0-x_1}(X-x_1) + \\frac{y_1}{x_1-x_0}(X-x_0) \\\\[2ex]\n     &= \\frac{x_0y_1-x_1y_0}{x_0-x_1} + \\frac{y_0-y_1}{x_0-x_1}\\cdot X\n\\end{split}\n\nSubstituting  x_0=t_j, x_1=t'_j, and X=\\alpha yields the definition of the folding function \\mathsf{fold}_\\alpha(y_0, y_1) as above.\\begin{split}\n\\mathsf{fold}_\\alpha(y_0, y_1) = p(\\alpha) &= \\frac{t_j\\cdot y_1 - t'_j\\cdot y_0}{t_j - t'_j} + \\frac{y_0-y_1}{t_j - t'_j}\\cdot \\alpha\\\\\n\\end{split}\n\nIf x_0 and x_1 are negatives of each other, i.e., t_j=-t'_j, then \\mathsf{fold}_\\alpha(y_0, y_1) becomes the familiar definition from the FRI protocol:\\mathsf{fold}_\\alpha(y_0, y_1) = \\frac{1}{2}(y_0 + y_1) + \\alpha\\cdot \\frac{y_0-y_1}{2\\cdot t_j}\n\nSince we have defined the folded codeword \\mathbf{c}^{(i-1)}, the definition of the folding function needs to be consistent with the codeword space generated by the generator matrix G_{i-1}. Continuing with this intuition, assume that in the i-th round, if \\mathbf{c}^{(i)} is indeed the encoding of \\mathbf{m}, then by definition, it satisfies the properties of Foldable Codes:\\begin{split}\n\\pi_{i} &= \\mathbf{m} G_i \\\\\n& = (\\mathbf{m}_l \\parallel \\mathbf{m}_r)\\begin{bmatrix}\nG_{i-1} & G_{i-1} \\\\\nG_{i-1}\\cdot T_{i-1} & G_{i-1}\\cdot T'_{i-1}\n\\end{bmatrix}\\\\[4ex]\n&=\\Big(\\mathbf{m}_l G_{i-1} + \\mathbf{m}_r G_{i-1}\\circ \\mathbf{diag}(T_{i-1})\\Big) \\parallel \\Big(\\mathbf{m}_l G_{i-1} + \\mathbf{m}_r G_{i-1}\\circ \\mathbf{diag}(T'_{i-1})\\Big)\\\\\n\\end{split}\n\nThe Prover folds it in half to obtain the new codeword:\\begin{split}\n\\mathsf{fold}_\\alpha(\\pi_{i}) &= \\Big(\\mathsf{fold}_\\alpha(\\pi_{i}[0], \\pi_{i}[n_{i-1}]), \\mathsf{fold}_\\alpha(\\pi_{i}[1], \\pi_{i}[n_{i-1}+1]), \\ldots, \\mathsf{fold}_\\alpha(\\pi_{i}[n_{i-1}-1], \\pi_{i}[n_{i}-1])\\Big) \\\\\n\\end{split}\n\nWe now verify that each \\mathsf{fold}_\\alpha(\\pi_{i}[j], \\pi_{i}[n_{i-1}+j]) is a linear combination of \\mathbf{m}_lG_{i-1}[j] and \\mathbf{m}_rG_{i-1}[j] with respect to \\alpha:\\begin{split}\n\\mathsf{fold}_\\alpha(\\pi_{i}[j], \\pi_{i}[n_{i-1}+j]) & = \\frac{1}{t_j - t'_j}\\cdot\\Big(\nt_j\\cdot (\\mathbf{m}_l G_{i-1}[j] + t'_j\\cdot \\mathbf{m}_r G_{i-1}[j]) - t'_j\\cdot(\\mathbf{m}_l G_{i-1}[j] + t_j\\cdot \\mathbf{m}_r G_{i-1}[j]) \\Big)\\\\\n& + \\frac{\\alpha}{t_j - t'_j}\\cdot \\Big(\\mathbf{m}_l G_{i-1}[j] + t_j\\cdot \\mathbf{m}_r G_{i-1}[j] - \\mathbf{m}_l G_{i-1}[j] - t'_j\\cdot \\mathbf{m}_r G_{i-1}[j] \\Big) \\\\\n& = \\mathbf{m}_l G_{i-1}[j] + \\alpha\\cdot \\mathbf{m}_r G_{i-1}[j]\n\\end{split}\n\nThus, the entire folding of \\pi_{i} is equivalent to a linear combination of \\mathbf{m}_lG_{i-1} and \\mathbf{m}_rG_{i-1} with respect to \\alpha:\\mathsf{fold}_\\alpha(\\pi_{i}) = \\mathbf{m}_l G_{i-1} + \\alpha\\cdot \\mathbf{m}_r G_{i-1} = (\\mathbf{m}_l + \\alpha\\cdot \\mathbf{m}_r)G_{i-1}\n\nThe folded codeword is exactly the half-folded version of \\mathbf{m} with respect to \\alpha, denoted as \\mathbf{m}^{(i-1)}, and then encoded with G_{i-1} to obtain \\pi_{i-1}. This is not surprising because Foldable Codes and the recursive folding of codewords are inverse processes, so the parameters T_i and T'_i introduced by the encoding are eliminated after folding.\n\nBelow, we walk through a simple example to illustrate how the Commit-phase of the Basefold-IOPP protocol operates.\n\nPublic Input\n\nThe codeword of the MLE polynomial \\tilde{f}, \\pi_3=\\mathsf{Enc}_3(\\mathbf{f})=\\mathbf{f}G_3\n\nWitness\n\nThe coefficient vector of the MLE polynomial \\tilde{f}, \\mathbf{f} = (f_0, f_1, f_2, f_3, f_4, f_5, f_6, f_7)\n\nFirst Round: Verifier sends a random scalar \\alpha_2\n\nSecond Round: Prover computes \\pi_2 = \\mathsf{fold}_\\alpha(\\pi_3) and sends it to the Verifier\n\nThe process of computing \\pi_2 is as follows ( we have assumed the length of coefficient vector and encoded vector to be equal for simplifying the explanation):\\pi_2[j] = \\mathsf{fold}_\\alpha(\\pi_3[j], \\pi_3[j+4]), \\quad j\\in\\{0, 1, 2, 3\\}\n\nThe computed \\pi_2 = \\mathsf{Enc}_2(f^{(2)}), meaning \\pi_2 is the codeword of f^{(2)}, where f^{(2)} is the folding of f with respect to \\alpha_2:f^{(2)}(X_0, X_1) = f(X_0, X_1, \\alpha_2) = (f_0 + f_4\\alpha_2) + (f_1 + f_5\\alpha_2)X_0 + (f_2 + f_6\\alpha_2)X_1 + (f_3 + f_7\\alpha_2)X_0X_1\n\nThird Round: Verifier sends a random scalar \\alpha_1\n\nFourth Round: Prover computes \\pi_1 = \\mathsf{fold}_\\alpha(\\pi_2) and sends it to the Verifier\n\nThe process of computing \\pi_1 is as follows:\\pi_1[j] = \\mathsf{fold}_\\alpha(\\pi_2[j], \\pi_2[j+2]), \\quad j\\in\\{0, 1\\}\n\nThe computed \\pi_1 = \\mathsf{Enc}_1(f^{(1)}), meaning \\pi_1 is the codeword of f^{(1)}, where f^{(1)} is the folding of f^{(2)} with respect to \\alpha_1:f^{(1)}(X_0) = f(X_0, \\alpha_1, \\alpha_2)  = (f_0 + f_4 \\alpha_2 +  \\alpha_1 (f_2 +  + f_6 \\alpha_2)) + (f_1 + f_5 \\alpha_2 +  \\alpha_1 (f_3 +  + f_7 \\alpha_2))X_0\n\nFifth Round: Verifier sends a random scalar \\alpha_0\n\nSixth Round: Prover computes \\pi_0 = \\mathsf{fold}_\\alpha(\\pi_1) and sends it to the Verifier\n\nThe process of computing \\pi_0 is as follows:\\pi_0[j] = \\mathsf{fold}_\\alpha(\\pi_1[j], \\pi_1[j+1]), \\quad j=0\n\nSimilarly, \\pi_0 = \\mathsf{Enc}_0(f^{(0)}), meaning \\pi_0 is the codeword of f^{(0)}, where f^{(0)} is the folding of f with respect to (\\alpha_0, \\alpha_1, \\alpha_2):f^{(0)} = f(\\alpha_0, \\alpha_1, \\alpha_2) = f_0 + f_1\\alpha_0 + f_2\\alpha_1 + f_3\\alpha_0 \\alpha_1 + f_4\\alpha_2 + f_5\\alpha_0\\alpha_2 + f_6\\alpha_1\\alpha_2 + f_7\\alpha_0\\alpha_1\\alpha_2\n\nAt this point, the Commit-phase ends, and the Prover has sent (\\pi_{2}, \\pi_{1}, \\pi_0) to the Verifier. Upon receiving them, the Verifier first checks whether \\pi_0 is a constant polynomial. However, this alone is insufficient; the Verifier also needs to validate that the Prover’s folding operations were honest. If all foldings \\pi_i were to be verified, the Verifier would lose succinctness and, consequently, verification efficiency. Due to the Proximity Gap property, the Verifier only needs to perform a limited number of validations to ensure that \\pi_i is a legitimate codeword.","type":"content","url":"/basefold/basefold-02#commit-phase","position":5},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Query-phase"},"type":"lvl2","url":"/basefold/basefold-02#query-phase","position":6},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Query-phase"},"content":"Similar to the FRI protocol, in the Query-phase, the Verifier conducts multiple rounds of random sampling on the (\\pi_{d}, \\pi_{d-1}, \\ldots, \\pi_0) sent by the Prover to verify the honesty of the folding process. We now discuss each round of the sampling process.\n\nThe Verifier will randomly select a position \\mu within \\pi_{d} and send it to the Prover, noting that 0\\leq \\mu < n_{d-1}, where n_{d-1} pertains only to \\pi_{d}. The Prover opens the points \\pi_{d}[\\mu] and \\pi_{d}[\\mu+n_{d-1}] and also sends the value at position \\mu in the folded codeword \\pi_{d-1}, i.e., \\pi_{d-1}[\\mu], along with the Merkle Path for these three points.\n\nUpon receiving these, the Verifier first verifies that these three points correspond correctly to the codewords \\pi_{d} and \\pi_{d-1}. Then, the Verifier checks whether they satisfy the folding relationship:\\pi_{d-1}[\\mu] \\overset{?}{=} \\mathsf{fold}_{\\alpha_{d-1}}(\\pi_{d}[\\mu], \\pi_{d}[\\mu+n_{d-1}])\n\nMerely verifying the folding relationship from \\pi_d to \\pi_{d-1} is insufficient. The Verifier must also validate the folding relationships from \\pi_{d-1} to \\pi_{0}. The Prover must additionally provide the points from \\pi_{d-1} to \\pi_{d-2}. Here, the Verifier does not need to select new random scalars but continues to use \\mu, because in the next round of folding, the position \\pi_{d-1}[\\mu] will be folded with another symmetrical point regarding \\alpha_{d-2}. The specific symmetrical position depends on the situation: if \\mu < n_{d-2}, then \\pi_{d-1}[\\mu+n_{d-2}] is the symmetrical point; if \\mu \\geq n_{d-2}, then \\pi_{d-1}[\\mu-n_{d-2}] is the symmetrical point. Assume \\mu \\geq n_{d-2}; then the Prover sends \\pi_{d-1}[\\mu-n_{d-2}] and its Merkle Path to the Verifier to allow the Verifier to check the folding relationship from \\pi_{d-1} to \\pi_{d-2}.\n\nIn this way, by providing a single random scalar \\mu, the Verifier can verify all the folding relationships from \\pi_{d} to \\pi_{0}. This verification process constitutes one round.\n\nTo elevate reliability to a sufficient level, the Verifier must perform multiple rounds to ensure that the Prover has no room to cheat. The Query-phase leverages the Proximity Gap property. A cheating Prover who alters the codeword is likely to be far from the legitimate encoding space, enabling the Verifier to detect cheating with only a small number of sampling attempts.","type":"content","url":"/basefold/basefold-02#query-phase","position":7},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Summary"},"type":"lvl2","url":"/basefold/basefold-02#summary","position":8},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"Summary"},"content":"This article described the framework of the Commit-phase and Query-phase of the Basefold-IOPP protocol. This framework generalizes and extends the FRI protocol, expanding from RS-Codes to any Foldable Linear Codes. However, it is important to note that Basefold does not support codeword folding of degree greater than 2. This is because the Basefold-IOPP protocol must not only perform Proximity Testing but also provide an operational result of an MLE polynomial. This will be the topic of the next article in this series.","type":"content","url":"/basefold/basefold-02#summary","position":9},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-02#references","position":10},{"hierarchy":{"lvl1":"Notes on Basefold (Part II): IOPP","lvl2":"References"},"content":"[ZCF23] Zeilberger, H., Chen, B., Fisch, B. (2024). BaseFold: Efficient Field-Agnostic Polynomial Commitment Schemes from Foldable Codes. In: Reyzin, L., Stebila, D. (eds) Advances in Cryptology – CRYPTO 2024. CRYPTO 2024. Lecture Notes in Computer Science, vol 14929. Springer, Cham.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.","type":"content","url":"/basefold/basefold-02#references","position":11},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument"},"type":"lvl1","url":"/basefold/basefold-03","position":0},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nJade Xie \n\njade@secbit.io\n\nAssume we have an MLE polynomial \\tilde{f}(\\vec{X})\\in\\mathbb{F}[\\vec{X}]^{\\leq1}, an evaluation point \\mathbf{u}\\in\\mathbb{F}^d, and the result of the polynomial’s operation at the evaluation point v=\\tilde{f}(\\mathbf{u}). We aim to construct a Polynomial Evaluation Argument based on the Basefold-IOPP protocol.\n\nBased on FRI, we can utilize the DEEP Method to construct a PCS protocol, which verifies the existence of a Low Degree Polynomial q(X) = (f(X)-f(u))/(X-u). However, the FRI protocol can only handle the case of Univariate Polynomials. For MLE Polynomials, we generally need to use the following extended polynomial remainder theorem to reduce it to a quotient polynomial existence problem:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{i=0}^{n-1} (X_k-u_k) \\cdot q_i(X_0,X_1,\\ldots, X_{k-1})\n\nFor example, Virgo adopts this scheme for the MLE PCS protocol. However, Basefold [ZCF23] presents a refreshing approach to implementing the MLE Evaluation Argument by combining the Basefold-IOPP protocol and the Sumcheck protocol. We can use the Sumcheck protocol’s summation and proof as the main framework of the protocol, then use Basefold-IOPP to ensure the Low degree of the MLE polynomial. Additionally, the evaluation of the MLE polynomial at random points generated after multiple rounds of folding in the Basefold-IOPP protocol compensates for the correctness proof of the evaluation required in the last round of the Sumcheck protocol.","type":"content","url":"/basefold/basefold-03","position":1},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Final Output of the Basefold-IOPP Protocol"},"type":"lvl2","url":"/basefold/basefold-03#final-output-of-the-basefold-iopp-protocol","position":2},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Final Output of the Basefold-IOPP Protocol"},"content":"For demonstration purposes, we use an MLE polynomial \\tilde{f}(X_0, X_1, X_2) with d=3, defined as follows:\\tilde{f}(\\vec{X}) = f_0 + f_1\\cdot X_0 + f_2\\cdot X_1 + f_3\\cdot X_0X_1 + f_4\\cdot X_2 + f_5\\cdot X_0X_2 + f_6\\cdot X_1X_2 + f_7\\cdot X_0X_1X_2\n\nHere, \\mathbf{f} is the coefficient vector of \\tilde{f}, defined above as the Coefficients Form of the MLE polynomial.\n\nLet’s revisit the folding process of the Basefold-IOPP protocol. First, let f^{(3)}(X_0, X_1, X_2) correspond to the codeword after encoding \\tilde{f} with C_3. In each folding round, the Verifier sends a random challenge \\alpha_i to the Prover, who then generates f^{(i)} by folding f^{(i+1)}, and sends the encoded codeword (as an Oracle) to the Verifier.\n\nAfter one folding round of the Basefold-IOPP protocol, the polynomial corresponding to the codeword obtained by both parties, f^{(2)}(X_0,X_1), is:\\begin{split}\nf^{(2)}(X_0, X_1) & = f^{(3)}(X_0, X_1, \\alpha_2) \\\\\n& = (f_0 + \\alpha_2\\cdot f_4) + (f_1+\\alpha_2\\cdot f_5)\\cdot X_0 + (f_2+\\alpha_2\\cdot f_6)\\cdot X_1 + (f_3+\\alpha_2\\cdot f_7)\\cdot X_0X_1 \\\\\n\\end{split}\n\nAfter another folding round, the polynomial corresponding to the codeword, f^{(1)}(X_0), is:\\begin{split}\nf^{(1)}(X_0) & = f^{(2)}(X_0, \\alpha_1) \\\\\n& = (f_0 + \\alpha_2\\cdot f_4 + \\alpha_1\\cdot f_2 + \\alpha_2\\alpha_1\\cdot f_6) + (f_1+\\alpha_2\\cdot f_5 + \\alpha_1\\cdot f_3 + \\alpha_2\\alpha_1\\cdot f_7)\\cdot X_0 \n\\end{split}\n\nFinally, the polynomial corresponding to the codeword, the constant polynomial f^{(0)}, is as follows:\\begin{split}\nf^{(0)} & = f^{(1)}(\\alpha_0) \\\\\n& = (f_0 + \\alpha_2\\cdot f_4 + \\alpha_1\\cdot f_2 + \\alpha_2\\alpha_1\\cdot f_6) + (f_1+\\alpha_2\\cdot f_5 + \\alpha_1\\cdot f_3 + \\alpha_2\\alpha_1\\cdot f_7)\\cdot \\alpha_0 \\\\\n& = f_0 + f_1\\alpha_0 + f_2\\alpha_1 + f_3\\alpha_0\\alpha_1 + f_4\\alpha_2 + f_5\\alpha_0\\alpha_2 + f_6\\alpha_1\\alpha_2 + f_7\\alpha_0\\alpha_1\\alpha_2 \\\\\n\\end{split}\n\nUpon closer examination, f^{(0)} is exactly the evaluation of \\tilde{f}(X_0, X_1, X_2) at (\\alpha_0, \\alpha_1, \\alpha_2).\n\nThus, in the final round of the Basefold-IOPP protocol, the Prover sends the folded constant polynomial to the Verifier. The Verifier then uses the Query-phase to verify the correctness of each folding step. Meanwhile, the Verifier also obtains the evaluation result v = \\tilde{f}(\\vec{X}) at the random point \\vec{X} = (\\alpha_0, \\alpha_1, \\alpha_2). From another perspective, the Basefold-IOPP protocol not only completes the Proximity proof but also provides an additional evaluation of \\tilde{f}(X_0, X_1, X_2) at a random point. More precisely, this is a proof of a vector inner product:\\langle \\mathbf{f},(1,\\alpha_0)\\otimes(1,\\alpha_1)\\otimes(1,\\alpha_2) \\rangle = v\n\nHowever, this MLE evaluation point is not a pre-negotiated public input but is instead composed of random challenges generated during the execution of the Basefold-IOPP protocol.\n\nIn summary, in the final round of the IOPP protocol, the Prover folds to produce a constant polynomial, denoted as f^{(0)}, which is precisely \\tilde{f} evaluated at the random point (\\alpha_0, \\alpha_1, \\alpha_2). What we need is to prove the correctness of \\tilde{f} at a public point, such as (u_0, u_1, u_2).\\tilde{f}(u_0, u_1, u_2) = f_0 + f_1\\cdot u_0 + f_2\\cdot u_1 + f_3\\cdot u_0u_1 + f_4\\cdot u_2 + f_5\\cdot u_0u_2 + f_6\\cdot u_1u_2 + f_7\\cdot u_0u_1u_2\n\nThe subsequent question becomes: How can we use the evaluation of \\tilde{f} at a random point to prove the correctness of its evaluation at another public point?","type":"content","url":"/basefold/basefold-03#final-output-of-the-basefold-iopp-protocol","position":3},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Leveraging Sumcheck"},"type":"lvl2","url":"/basefold/basefold-03#leveraging-sumcheck","position":4},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Leveraging Sumcheck"},"content":"We can revisit \\tilde{f}(u_0, u_1, u_2) by utilizing the properties of MLE. According to the definition of MLE, we have the following equation:\\mathsf{EQ}_1: \\qquad \\tilde{f}(X_0, X_1, X_2) = \\sum_{\\mathbf{b}\\in\\{0,1\\}^3}\\tilde{f}(b_0, b_1, b_2)\\cdot \\tilde{eq}_{(b_0, b_1, b_2)}(X_0, X_1, X_2)\n\nHere, \\tilde{eq} is the Lagrange Polynomial of MLE, defined as:\\tilde{eq}_{(b_0,b_1,\\ldots,b_{n-1})}(X_0,X_1,\\ldots, X_{n-1})\\big) = \\prod_{i=0}^{n-1}\\big((1-b_i)(1-X_i) + b_i\\cdot X_i\\big)\n\nIt is easily verified that when (X_0, X_1, X_2)=\\mathbf{b}', \\mathbf{b}'\\in\\{0,1\\}^3, the left side of equation \\mathsf{EQ}_1 equals \\tilde{f}(\\mathbf{b}'). Observing each summand \\tilde{eq}_{\\mathbf{b}}(\\mathbf{b}'), only when the vectors \\mathbf{b}=\\mathbf{b}' are completely identical does \\tilde{eq}_{\\mathbf{b}}(\\mathbf{b}') = 1; otherwise, they all equal 0. Therefore, the right side of the equation reduces to \\tilde{f}(\\mathbf{b}')\\cdot \\tilde{eq}_{\\mathbf{b}}(\\mathbf{b}') = \\tilde{f}(\\mathbf{b}'), ensuring the equality holds.\n\nThis means that both sides of equation \\mathsf{EQ}_1 agree on their values over the 3-dimensional Boolean HyperCube. Based on the uniqueness property of MLE, we can conclude that \\mathsf{EQ}_1 holds for any \\mathbf{X}\\in F^3.\n\nAlthough the above is fundamental knowledge of MLE, note that equation \\mathsf{EQ}_1 introduces a new perspective: we can transform the evaluation problem of \\tilde{f}(u_0, u_1, u_2) into a Sumcheck problem:\\tilde{f}(u_0, u_1, u_2) = v = \\sum_{\\mathbf{b}\\in\\{0,1\\}^3}\\tilde{f}(\\mathbf{b})\\cdot \\tilde{eq}_{\\mathbf{b}}(u_0, u_1, u_2)\n\nIt is immediately apparent that we can use the Sumcheck protocol to perform a “sum proof” on the above equation. An important function of the Sumcheck protocol is to transform a “summation problem” into two MLE polynomial evaluation problems (specifically for \\tilde{f} and \\tilde{eq}) at a random point. However, in general, this approach is meaningless because, in the last round of Sumcheck, we need to prove the evaluation of \\tilde{f} at a new point, leading to a circular proof situation: we originally prove the evaluation of a polynomial at one point, then use the Sumcheck protocol to transform this proof into an evaluation at another point. This makes the Sumcheck protocol process redundant and useless. Typically, the protocol would rely on another MLE Evaluation Argument protocol, where the Prover sends the final evaluation and its correctness proof, thereby concluding the Sumcheck protocol. But here, we cannot rely on another MLE PCS protocol because our goal is to construct an MLE Evaluation Argument protocol, not to depend on an existing MLE PCS.\n\nHowever, this problem is not difficult to solve because the previously introduced Basefold-IOPP protocol provides a byproduct: a proof of \\tilde{f}'s evaluation at a random point. If the Sumcheck protocol and the Basefold-IOPP protocol share random challenges, the value provided by Basefold-IOPP in the final step can compensate for the correctness proof of the evaluation required in the last round of Sumcheck. Another remaining issue is how to handle the evaluation proof of \\tilde{eq}. This is simpler because \\tilde{eq} is a public polynomial, allowing the Verifier to compute it independently without requiring the Prover to send an evaluation proof. Moreover, computing the evaluation of \\tilde{eq} only has a complexity of O(\\log{N}), which does not increase the Verifier’s burden or affect the Verifier’s succinctness. At this point, the Sumcheck protocol’s role is no longer redundant but becomes crucial: it transforms the problem of proving a polynomial’s evaluation at a public point into a proof of its evaluation at a random point.\n\nNext, we need to synchronize the execution of the Basefold-IOPP protocol and the Sumcheck protocol, allowing both protocols to share a set of random challenges. This way, in the final step of both protocols, we can complete the collaboration, thereby ultimately proving the correctness of \\tilde{f}(u_0, u_1, u_2) = v.\n\nFollowing this approach, we attempt to outline the protocol flow when s=3.\n\nPublic Inputs\n\nCommitment of \\tilde{f}, \\pi_3=[\\tilde{f}]=\\mathsf{Enc}_3(\\mathbf{f})\n\nEvaluation point \\mathbf{u}=(u_0, u_1, u_2)\n\nOperation value v\n\nWitness\n\nCoefficient vector of MLE \\tilde{f}, \\mathbf{f} = (f_0, f_1, f_2, f_3, f_4, f_5, f_6, f_7)\n\nEvaluation vector of \\tilde{f}, \\mathbf{a} = (a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7)\n\nFirst Round: Prover sends the univariate polynomial h^{(2)}(X)h^{(2)}(X) = \\sum_{b_0,b_1\\in\\{0,1\\}^2}f(b_0, b_1, X)\\cdot \\tilde{eq}\\big((b_0, b_1, X), (u_0, u_1, u_2)\\big)\n\nSince the right side is a degree 2 Univariate Polynomial, the Prover can compute the coefficients of h^{(2)}(X):\\begin{split}\nh^{(2)}_0 &= \\sum_{i=0}^{3}{a_i \\cdot e_i} \\\\\nh^{(2)}_1 &= \\sum_{i=0}^{3}{a_{i+4} \\cdot e_i + a_i\\cdot e_{i+4}} \\\\\nh^{(2)}_2 &= \\sum_{i=0}^{3}{a_{i+4} \\cdot e_{i+4}} \\\\\n\\end{split}\n\nHere, \\mathbf{e} is the evaluation vector of \\tilde{eq}(\\mathbf{X}, \\mathbf{u}). It is easy to verify that h^{(2)}(X)=h^{(2)}_0 + h^{(2)}_1\\cdot X + h^{(2)}_2 \\cdot X^2.\n\nSecond Round: Verifier sends challenge \\alpha_2 \\leftarrow \\mathbb{F}_p\n\nThird Round: Prover and Verifier simultaneously execute the Basefold-IOPP protocol and the Sumcheck protocol:\n\nProver computes \\tilde{f}^{(2)}(X_1, X_2)=f(\\alpha_0, X_1, X_2)\\tilde{f}^{(2)}(X_0, X_1) = (f_0 + f_4\\alpha_2) + (f_1 + f_5\\alpha_2)\\cdot X_0 + (f_2 + f_6\\alpha_2)\\cdot X_1 + (f_3 + f_7\\alpha_2)\\cdot X_0X_1\n\nProver sends the folded vector encoding: \\pi_2 = \\mathrm{Enc}_{2}[\\tilde{f}^{(2)}]\n\nProver computes h^{(2)}(\\alpha_2) as the summation value for the next round of the Sumcheck protocol\n\nProver computes and sends h^{(1)}(X)h^{(1)}(X) = \\sum_{b_0\\in\\{0,1\\}}f(b_0, X, \\alpha_2)\\cdot \\tilde{eq}((b_0, X, \\alpha_2), (u_0, u_1, u_2))\n\nSince the right side is also a degree 2 Univariate Polynomial in X, the Prover can calculate the coefficients of h^{(1)}(X): (h^{(1)}_0, h^{(1)}_1, h^{(1)}_2).\n\nFourth Round: Verifier sends challenge \\alpha_1\\leftarrow \\mathbb{F}_p\n\nFifth Round: Prover continues executing the Basefold-IOPP protocol and the Sumcheck protocol:\n\nProver computes \\tilde{f}^{(1)}(X_2)\\tilde{f}^{(1)}(X_2) =f(X_0, \\alpha_1, \\alpha_2) = (f_0 + f_4\\alpha_2 + f_2\\alpha_1 + f_6\\alpha_1\\alpha_2) + (f_1 + f_5\\alpha_2 + f_3\\alpha_1 + f_7\\alpha_1\\alpha_2)\\cdot X_0\n\nProver sends the folded vector encoding \\pi_1 = \\mathrm{Enc}_{1}[\\tilde{f}^{(1)}]\n\nProver computes the summation value for the next round of the Sumcheck protocol: h^{(1)}(\\alpha_1)\n\nProver computes and sends h^{(0)}(X)h^{(0)}(X) = \\sum_{b_0\\in\\{0,1\\}}f(X, \\alpha_1, \\alpha_2)\\cdot \\tilde{eq}((b_0, \\alpha_1, \\alpha_2), (u_0, u_1, u_2))\n\nAssume the coefficients of h^{(0)}(X) are represented as (h^{(0)}_0, h^{(0)}_1, h^{(0)}_2).\n\nSixth Round: Verifier sends challenge \\alpha_0\\leftarrow \\mathbb{F}_p\n\nSeventh Round: Prover continues executing the Basefold-IOPP protocol:\n\nProver computes \\tilde{f}^{(0)}, a constant polynomial\\tilde{f}^{(0)} =f(\\alpha_0, \\alpha_1, \\alpha_2) = f_0 + f_1\\alpha_0 + f_2\\alpha_1 + f_3\\alpha_0\\alpha_1 + f_4\\alpha_2 + f_5\\alpha_0\\alpha_2 + f_6\\alpha_1\\alpha_2 + f_7\\alpha_0\\alpha_1\\alpha_2\n\nProver sends the folded vector encoding \\pi_0 = \\mathsf{Enc}_0[\\tilde{f}^{(0)}]\n\nEighth Round: Verifier validates the following equations:\n\nVerifier verifies several rounds of Basefold Queries, Q=\\{q_i\\}\n\nVerifier checks the correctness of each folding step in Sumcheck:\\begin{split}\nh^{(2)}(0) + h^{(2)}(1) &\\overset{?}{=} v \\\\\nh^{(1)}(0) + h^{(1)}(1) &\\overset{?}{=} h^{(2)}(\\alpha_2) \\\\\nh^{(0)}(0) + h^{(0)}(1) &\\overset{?}{=} h^{(1)}(\\alpha_1) \\\\\n\\end{split}\n\nVerifier checks whether the final encoding \\pi_0 is correct\\pi_0 \\overset{?}{=} \\mathsf{Enc}_0\\left(\\frac{h^{(0)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\alpha_1,\\alpha_2), (u_0, u_1, u_2))}\\right)","type":"content","url":"/basefold/basefold-03#leveraging-sumcheck","position":5},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"An Alternative Folding Method"},"type":"lvl2","url":"/basefold/basefold-03#an-alternative-folding-method","position":6},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"An Alternative Folding Method"},"content":"In the Basefold paper, the PCS protocol requires that the MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{d-1}) be converted to the Coefficients Form beforehand, and then follow the protocol described in the previous section for the proof. However, in many Sumcheck-based SNARK systems, the Sumcheck protocol finally produces the Evaluation Form of the MLE polynomial, that is, the evaluation of the MLE polynomial on a Boolean HyperCube. Therefore, if we directly connect the SNARK protocol to Basefold-PCS, we would still need to convert the MLE polynomial from the Evaluation Form to the Coefficients Form. This conversion algorithm has a complexity of O(N\\log(N)), where N=2^d, and d is the number of variables in the MLE polynomial. Below is the definition of the Evaluation Form of \\tilde{f}(X_0, X_1, \\ldots, X_{d-1}):\\tilde{f}(X_0, X_1, \\ldots, X_{d-1}) = \\sum_{\\mathbf{b}\\in\\{0,1\\}^d}a_{\\mathbf{b}}\\cdot eq_{\\mathbf{b}}(X_0, X_1, \\ldots, X_{d-1})\n\nAs noted in the FRI-Binius paper, we do not need to convert the MLE polynomial from the Evaluation Form to the Coefficients Form. Instead, we can directly construct the PCS protocol in the Evaluation Form. In the previous section, the reason we needed the Coefficients Form of \\tilde{f}(X_0, X_1, \\ldots, X_{d-1}) was that the Basefold-IOPP protocol’s Commit method required the coefficients of \\tilde{f}(X_0, X_1, \\ldots, X_{d-1}). If we follow the folding method of the Basefold-IOPP protocol described below, we can directly construct the PCS protocol in the Evaluation Form.\n\nNow, let’s consider the folding method of the Evaluation Form of \\tilde{f}(X_0, X_1, X_2). First, expand the definition of the Evaluation Form of \\tilde{f}(X_0, X_1, X_2):\\begin{split}\n\\tilde{f}^{(3)}(X_0, X_1, X_2) & = \\tilde{f}(X_0, X_1, X_2) \\\\\n& = a_0\\cdot eq_{(0,0,0)}(X_0, X_1, X_2) + a_1\\cdot eq_{(1,0,0)}(X_0, X_1, X_2) + a_2\\cdot eq_{(0,1,0)}(X_0, X_1, X_2) + a_3\\cdot eq_{(1,1,0)}(X_0, X_1, X_2) \\\\\n& + a_4\\cdot eq_{(0,0,1)}(X_0, X_1, X_2) + a_5\\cdot eq_{(1,0,1)}(X_0, X_1, X_2) + a_6\\cdot eq_{(0,1,1)}(X_0, X_1, X_2) + a_7\\cdot eq_{(1,1,1)}(X_0, X_1, X_2)\n\\end{split}\n\nWe can fold the above Evaluation Form, ultimately obtaining the evaluation \\tilde{f}^{(3)}(\\alpha_0, \\alpha_1, \\alpha_2). For example, we can fold the Evaluation Form of \\tilde{f}(X_0, X_1, X_2) with respect to \\alpha_2 to obtain:\\begin{split}\n\\tilde{f}^{(2)}(X_0, X_1) & = \\tilde{f}^{(3)}(X_0, X_1, \\alpha_2) \\\\\n& = a_0\\cdot eq_{(0,0,0)}(X_0, X_1, \\alpha_2) + a_1\\cdot eq_{(1,0,0)}(X_0, X_1, \\alpha_2) + a_2\\cdot eq_{(0,1,0)}(X_0, X_1, \\alpha_2) + a_3\\cdot eq_{(1,1,0)}(X_0, X_1, \\alpha_2) \\\\\n& + a_4\\cdot eq_{(0,0,1)}(X_0, X_1, \\alpha_2) + a_5\\cdot eq_{(1,0,1)}(X_0, X_1, \\alpha_2) + a_6\\cdot eq_{(0,1,1)}(X_0, X_1, \\alpha_2) + a_7\\cdot eq_{(1,1,1)}(X_0, X_1, \\alpha_2)\n\\end{split}\n\nNext, we need to decompose eq_{\\mathbf{b}}(X_0, X_1\\alpha_2):\\begin{split}\neq_{(b_0,b_1,b_2)}(X_0, X_1, \\alpha_2) & = eq_{(b_0, b_1)}(X_0, X_1)\\cdot \\Big((1-b_2)\\cdot(1-\\alpha_2) + b_2\\cdot \\alpha_2 \\Big)\\\\\n\\end{split}\n\nWhen b_2=0, the right side simplifies to (1-\\alpha_2)\\cdot eq_{(b_0,b_1)}(X_0, X_1); when b_2=1, it simplifies to \\alpha_2\\cdot eq_{(b_0,b_1)}(X_0, X_1). Then, continue simplifying \\tilde{f}^{(2)}(X_0, X_1):\\begin{split}\n\\tilde{f}^{(2)}(X_0, X_1) & = ((1-\\alpha_2)\\cdot a_0+\\alpha_2\\cdot a_4)\\cdot eq_{(0,0)}(X_0, X_1) \n+ ((1-\\alpha_2)\\cdot a_1 + \\alpha_2\\cdot a_5) \\cdot eq_{(0,1)}(X_0, X_1) \\\\\n& + ((1-\\alpha_2)\\cdot a_2 + \\alpha_2\\cdot a_6) \\cdot eq_{(1,0)}(X_0, X_1) + ((1-\\alpha_2)\\cdot a_3 + \\alpha_2\\cdot a_7) \\cdot eq_{(1,1)}(X_0, X_1)\n\\end{split}\n\nThis is equivalent to folding (linear combination) the vectors (a_0, a_1, a_2, a_3) and (a_4, a_5, a_6, a_7) with (1-\\alpha_2, \\alpha_2). Compared with the folding method in the Basefold-IOPP protocol, it uses (1, \\alpha_2) to fold (linear combination) the vectors (f_0, f_1, f_2, f_3) and (f_4, f_5, f_6, f_7). If we continue to fold in the new way, we can eventually get the same folding result as the Basefold-IOPP protocol.\\begin{split}\n\\tilde{f}^{(1)}(X_0) & = \\tilde{f}^{(2)}(X_0, \\alpha_1) \\\\\n& = ((1-\\alpha_2)\\cdot a_0 + \\alpha_2\\cdot a_4)\\cdot eq_{(0,0)}(X_0, \\alpha_1) + ((1-\\alpha_2)\\cdot a_1 + \\alpha_2\\cdot a_5) \\cdot eq_{(1,0)}(X_0, \\alpha_1) \\\\\n& + ((1-\\alpha_2)\\cdot a_2 + \\alpha_2\\cdot a_6) \\cdot eq_{(0,1)}(X_0, \\alpha_1) + ((1-\\alpha_2)\\cdot a_3 + \\alpha_2\\cdot a_7) \\cdot eq_{(1,1)}(X_0, \\alpha_1) \\\\\n& = ((1-\\alpha_2)\\cdot a_0 + \\alpha_2\\cdot a_4)\\cdot (1-\\alpha_1)\\cdot eq_{(0)}(X_0) + ((1-\\alpha_2)\\cdot a_1 + \\alpha_2\\cdot a_5) \\cdot (1-\\alpha_1)\\cdot eq_{(1)}(X_0) \\\\\n& + ((1-\\alpha_2)\\cdot a_2 + \\alpha_2\\cdot a_6) \\cdot \\alpha_1\\cdot eq_{(0)}(X_0) + ((1-\\alpha_2)\\cdot a_3 + \\alpha_2\\cdot a_7) \\cdot \\alpha_1\\cdot eq_{(1)}(X_0) \\\\\n& = \\Big((1-\\alpha_1)\\cdot ((1-\\alpha_2)\\cdot a_0 + \\alpha_2\\cdot a_4) + \\alpha_1\\cdot ((1-\\alpha_2)\\cdot a_2 + \\alpha_2\\cdot a_6) \\Big)\\cdot eq_{(0)}(X_0) \\\\\n& + \\Big((1-\\alpha_1)\\cdot ((1-\\alpha_2)\\cdot a_1 + \\alpha_2\\cdot a_5) + \\alpha_1\\cdot ((1-\\alpha_2)\\cdot a_3 + \\alpha_2\\cdot a_7) \\Big)\\cdot eq_{(1)}(X_0) \\\\\n\\end{split}\n\nContinuing the folding process, we eventually obtain:\\begin{split}\n\\tilde{f}^{(0)} & = \\tilde{f}^{(1)}(\\alpha_0) \\\\\n& = \\Big((1-\\alpha_0)\\cdot ((1-\\alpha_1)\\cdot ((1-\\alpha_2)\\cdot a_0 + \\alpha_2\\cdot a_4) + \\alpha_1\\cdot ((1-\\alpha_2)\\cdot a_2 + \\alpha_2\\cdot a_6)) \\\\\n& \\quad + \\alpha_0\\cdot ((1-\\alpha_1)\\cdot ((1-\\alpha_2)\\cdot a_1 + \\alpha_2\\cdot a_5) + \\alpha_1\\cdot ((1-\\alpha_2)\\cdot a_3 + \\alpha_2\\cdot a_7)) \\Big) \\\\\n& = \\tilde{f}^{(3)}(\\alpha_0, \\alpha_1, \\alpha_2)\n\\end{split}\n\nCan we improve the folding method of the Basefold-IOPP protocol so that it can directly fold the Evaluation Form of the MLE polynomial?","type":"content","url":"/basefold/basefold-03#an-alternative-folding-method","position":7},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Basefold-IOPP Protocol Based on the Evaluation Form"},"type":"lvl2","url":"/basefold/basefold-03#basefold-iopp-protocol-based-on-the-evaluation-form","position":8},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Basefold-IOPP Protocol Based on the Evaluation Form"},"content":"Next, we attempt to outline the Basefold-IOPP protocol for the Evaluation Form of \\tilde{f}(X_0, X_1, X_2) when d=3. First, we rewrite the folding function from the previous article:\\begin{split}\n\\mathsf{fold}^{*}_\\alpha(y_0, y_1) &= (1-\\alpha)\\cdot \\frac{t_j\\cdot y_1 - t'_j\\cdot y_0}{t_j - t'_j} + \\alpha\\cdot \\frac{y_0-y_1}{t_j - t'_j} \\\\\n\\end{split}\n\nFrom earlier discussions, we know that the folding function has the following homomorphic property: after folding, the codeword is equivalent to the original message being folded and then encoded, expressed as:\\mathsf{fold}_\\alpha(\\mathsf{Enc}_i(\\mathbf{m}))=\\mathsf{Enc}_i(\\mathsf{fold}_\\alpha(\\mathbf{m}))\n\nSimilarly, we can prove that the new folding function satisfies the above property. We can attempt to fold an element in the codeword \\pi_i using the new folding function:\\begin{split}\n\\mathsf{fold}^*_\\alpha(\\pi_{i}[j], \\pi_{i}[n_{i-1}+j]) & = \\frac{1-\\alpha}{t_j - t'_j}\\cdot\\Big(\nt_j\\cdot (\\mathbf{m}_l G_{i-1}[j] + t'_j\\cdot \\mathbf{m}_r G_{i-1}[j]) - t'_j\\cdot(\\mathbf{m}_l G_{i-1}[j] + t_j\\cdot \\mathbf{m}_r G_{i-1}[j]) \\Big)\\\\\n& + \\frac{\\alpha}{t_j - t'_j}\\cdot \\Big(\\mathbf{m}_l G_{i-1}[j] + t_j\\cdot \\mathbf{m}_r G_{i-1}[j] - \\mathbf{m}_l G_{i-1}[j] - t'_j\\cdot \\mathbf{m}_r G_{i-1}[j] \\Big) \\\\\n& = (1-\\alpha)\\cdot \\mathbf{m}_l G_{i-1}[j] + \\alpha\\cdot \\mathbf{m}_r G_{i-1}[j]\n\\end{split}\n\nThe above derivation demonstrates that the new folding function also satisfies the homomorphic property concerning the encoding function:\\mathsf{fold}^*_\\alpha(\\mathsf{Enc}_i(\\mathbf{m}))=\\mathsf{Enc}_i(\\mathsf{fold}^*_\\alpha(\\mathbf{m}))\n\nAdditionally, we can prove that the new folding process does not affect the reliability of the Basefold-IOPP protocol, i.e., the Minimum Hamming Weight of the folded codeword remains above a lower bound.\n\nThus, we have an important conclusion: whether we use \\mathsf{fold}_\\alpha or \\mathsf{fold}^*_\\alpha, both can be used to construct a Proximity-Proof protocol without significant differences.\n\nNext, we outline the Commit-phase protocol flow for better understanding:\n\nPublic Inputs\n\nCodeword of the MLE polynomial \\tilde{f}, \\pi_3=\\mathsf{Enc}_3(\\mathbf{a})=\\mathbf{a}G_3\n\nWitness\n\nEvaluation vector of the MLE polynomial \\tilde{f}, \\mathbf{a} = (a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7)\n\nFirst Round: Verifier sends a random number \\alpha_2\n\nSecond Round: Prover computes \\pi_2 = \\mathsf{fold}^*_\\alpha(\\pi_3) and sends it to the Verifier\n\nThe process of computing \\pi_2 is as follows:\\pi_2[j] = \\mathsf{fold}^*_\\alpha(\\pi_3[j], \\pi_3[j+4]), \\quad j\\in\\{0, 1, 2, 3\\}\n\nThe computed \\pi_2 = \\mathsf{Enc}_2(f^{(2)}), meaning that \\pi_2 is the codeword of f^{(2)}, where f^{(2)} is the folded result of f with respect to \\alpha_2:\\begin{split}\nf^{(2)}(X_0, X_1) &= ((1-\\alpha_2)a_0 + \\alpha_2a_4)\\cdot eq_{(0,0)}(X_0, X_1)+ ((1-\\alpha_2)a_1 + \\alpha_2a_5)\\cdot eq_{(1,0)}(X_0, X_1) \\\\\n& + ((1-\\alpha_2)a_2 + \\alpha_2a_6)\\cdot eq_{(0,1)}(X_0, X_1) + ((1-\\alpha_2)a_3 + \\alpha_2a_7)\\cdot eq_{(1,1)}(X_0, X_1) \\\\\n& = f(X_0, X_1, \\alpha_2)\n\\end{split}\n\nThird Round: Verifier sends a random number \\alpha_1\n\nFourth Round: Prover computes \\pi_1 = \\mathsf{fold}^*_\\alpha(\\pi_2) and sends it to the Verifier\n\nThe process of computing \\pi_1 is as follows:\\pi_1[j] = \\mathsf{fold}^*_\\alpha(\\pi_2[j], \\pi_2[j+2]), \\quad j\\in\\{0, 1\\}\n\nThe computed \\pi_1 = \\mathsf{Enc}_1(f^{(1)}), where f^{(1)} is the folded result of f^{(2)} with respect to \\alpha_1:\\begin{split}\nf^{(1)}(X_0)  & = \\Big((1-\\alpha_1) ((1-\\alpha_2)a_0 + \\alpha_2a_4) + \\alpha_1((1-\\alpha_2)a_1 + \\alpha_2a_5)\\Big)\\cdot eq_{(0)}(X_0) \\\\\n& + \\Big((1-\\alpha_1)((1-\\alpha_2)a_2 + \\alpha_2a_6) + \\alpha_1((1-\\alpha_2)a_3 + \\alpha_2a_7)\\Big)\\cdot eq_{(1)}(X_0) \\\\\n& = f(X_0, \\alpha_1, \\alpha_2) \\\\\n\\end{split}\n\nFifth Round: Verifier sends a random number \\alpha_0\n\nSixth Round: Prover computes \\pi_0 = \\mathsf{fold}_\\alpha(\\pi_1) and sends it to the Verifier\n\nThe process of computing \\pi_0 is as follows:\\pi_0[j] = \\mathsf{fold}^*_\\alpha(\\pi_1[j], \\pi_1[j+1]), \\quad j=0\n\nSimilarly, \\pi_0 = \\mathsf{Enc}_0(f^{(0)}), meaning \\pi_0 is the codeword of f^{(0)}, where f^{(0)} is the folded result of f with respect to (\\alpha_0, \\alpha_1, \\alpha_2):f^{(0)} = f(\\alpha_0, \\alpha_1, \\alpha_2)\n\nThen, we can improve the Evaluation Argument protocol.","type":"content","url":"/basefold/basefold-03#basefold-iopp-protocol-based-on-the-evaluation-form","position":9},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Basefold Evaluation Argument Protocol Based on the Evaluation Form"},"type":"lvl2","url":"/basefold/basefold-03#basefold-evaluation-argument-protocol-based-on-the-evaluation-form","position":10},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"Basefold Evaluation Argument Protocol Based on the Evaluation Form"},"content":"Public Inputs\n\nCommitment of \\tilde{f}, \\pi_3=\\mathsf{Enc}_3(\\mathbf{a})\n\nEvaluation point \\mathbf{u}\n\nOperation value v=\\tilde{f}(\\mathbf{u})\n\nWitness\n\nEvaluation vector of MLE \\tilde{f}, \\mathbf{a}=(a_0, a_1, \\ldots, a_7)\n\nSuch that\\tilde{f}(X_0, X_1, X_2) = \\sum_{\\mathbf{b}\\in\\{0,1\\}^3}a_{\\mathbf{b}}\\cdot eq_{\\mathbf{b}}(X_0, X_1, X_2)\n\nFirst Round: Prover sends the evaluations of h^{(2)}(X) at points (0, 1, 2)h^{(2)}(X) = \\sum_{b_1,b_2\\in\\{0,1\\}^2}f(X, b_1, b_2)\\cdot \\tilde{eq}((X, b_1, b_2), \\mathbf{u})\n\nSince the right side is a degree 2 Univariate Polynomial, the Prover can compute the evaluations of h^{(2)}(X) at X=0, 1, 2:\\begin{split}\nh^{(2)}(0) &= a_0\\cdot e_0 + a_1\\cdot e_1 + a_2\\cdot e_2 + a_3\\cdot e_3 \\\\\nh^{(2)}(1) &= a_4\\cdot e_4 + a_5\\cdot e_5 + a_6\\cdot e_6 + a_7\\cdot e_7 \\\\\nh^{(2)}(2) &= \\sum_{i=0}^{3} (2\\cdot a_{i+4} - a_i)\\cdot (2\\cdot e_{i+4} - e_i) \\\\\n\\end{split}\n\nHere, \\mathbf{e} is the evaluation vector of \\tilde{eq}(\\mathbf{X}, \\mathbf{u}).\n\nSecond Round: Verifier sends challenge \\alpha_2 \\leftarrow \\mathbb{F}_p\n\nThird Round: Prover simultaneously executes the Basefold-IOPP protocol and the Sumcheck protocol:\n\nProver sends the folded vector encoding: \\pi_2 = \\mathsf{fold}^*_{\\alpha_2}(\\pi_3)\n\nProver computes h^{(2)}(\\alpha_2) as the summation value for the next round of the Sumcheck protocol\n\nProver computes the evaluations vector of f^{(2)}(X_0, X_1): \\mathbf{a}^{(2)} = \\mathsf{fold}^{*}_{\\alpha_2}(\\mathbf{a})\n\nProver computes and sends h^{(1)}(X)h^{(1)}(X) = \\sum_{b_0\\in\\{0,1\\}}f(b_0, X, \\alpha_2)\\cdot \\tilde{eq}((b_0, X, \\alpha_2), (u_2, u_1, u_0))\n\nSince the right side is also a degree 2 Univariate Polynomial in X, the Prover can compute the evaluations of h^{(1)}(X) at X=0,1,2: (h^{(1)}(0), h^{(1)}(1), h^{(1)}(2)).\n\nFourth Round: Verifier sends challenge \\alpha_1\\leftarrow \\mathbb{F}_p\n\nFifth Round: Prover continues executing the Basefold-IOPP protocol and the Sumcheck protocol:\n\nProver sends the folded vector encoding \\pi_1 = \\mathsf{fold}^*_{\\alpha_1}(\\pi_2)\n\nProver computes the summation value for the next round of the Sumcheck protocol: h^{(1)}(\\alpha_1)\n\nProver computes the evaluations vector of \\mathbf{a}^{(1)}=\\mathsf{fold}^*_{\\alpha_1}(\\mathbf{a}^{(2)})\n\nProver computes and sends the evaluations of h^{(0)}(X) at X=0, 1, 2: (h^{(0)}(0), h^{(0)}(1), h^{(0)}(2))h^{(0)}(X) = f(X_0, \\alpha_1, \\alpha_2)\\cdot \\tilde{eq}((X_0, \\alpha_1, \\alpha_2), (u_0, u_1, u_2))\n\nSixth Round: Verifier sends challenge \\alpha_0\\leftarrow F\n\nSeventh Round: Prover continues executing the Basefold-IOPP protocol:\n\nProver sends the folded vector encoding \\pi_0 = \\mathsf{fold}^*_{\\alpha_0}(\\pi_1)\n\nEighth Round: Verifier validates the following equations:\n\nVerifier sends several rounds of Query, Q=\\{q_i\\}\n\nVerifier checks the correctness of each folding step in Sumcheck:\\begin{split}\nh^{(2)}(0) + h^{(2)}(1) &\\overset{?}{=} v \\\\\nh^{(1)}(0) + h^{(1)}(1) &\\overset{?}{=} h^{(2)}(\\alpha_2) \\\\\nh^{(0)}(0) + h^{(0)}(1) &\\overset{?}{=} h^{(1)}(\\alpha_1) \\\\\n\\end{split}\n\nVerifier checks whether the final encoding \\pi_0 is correct\\pi_0 \\overset{?}{=} \\mathsf{enc}_0\\left(\\frac{h^{(0)}(\\alpha_0)}{\\tilde{eq}((\\alpha_0,\\alpha_1,\\alpha_2), \\mathbf{u})}\\right)\n\nAt this point, we have obtained a Basefold Evaluation Argument protocol based on the Evaluation Form. It can be directly connected to Sumcheck-based zkSNARKs or similar Jolt-style zkVMs.","type":"content","url":"/basefold/basefold-03#basefold-evaluation-argument-protocol-based-on-the-evaluation-form","position":11},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-03#references","position":12},{"hierarchy":{"lvl1":"Notes on Basefold (Part III): MLE Evaluation Argument","lvl2":"References"},"content":"[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\nJiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent polynomial delegation and its applications to zero knowledge proof.” In 2020 IEEE Symposium on Security and Privacy (SP), pp. 859-876. IEEE, 2020.","type":"content","url":"/basefold/basefold-03#references","position":13},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes"},"type":"lvl1","url":"/basefold/basefold-04","position":0},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nPrevious articles have mentioned that BaseFold extends the FRI IOPP by introducing the concept of foldable codes. Additionally, by combining the Sumcheck protocol, it can support PCS for multi-linear polynomials. The next crucial question is how to explicitly construct such foldable codes. We aim for these foldable codes to possess the following properties:\n\nEfficient Encoding\n\nField Agnostic, i.e., applicable even for small fields\n\nCompatible with PCS for Multi-linear Polynomials\n\nAnother important aspect of encoding is the consideration of the Minimum Relative Hamming Distance. If readers are familiar with the FRI protocol, the Reed-Solomon codes used are likely not unfamiliar. They have a desirable property: their distance meets the Singleton bound, i.e., d = n - k + 1, and are thus known as Maximum Distance Separable (MDS) codes. These codes balance code length and error-correcting capability effectively, providing strong error detection and correction with minimal redundancy, thereby saving encoding space. In PCS protocols, this allows verifiers to perform checks more efficiently. Therefore, from a practical perspective, we also desire that such foldable codes satisfy the fourth property:\n\nGood Relative Minimum Distance\n\nThe BaseFold paper [ZCF23] constructs a type of code called Random Foldable Code (RFCs), which satisfies the aforementioned properties. Next, we will explore how it achieves these points.","type":"content","url":"/basefold/basefold-04","position":1},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Efficient Encoding Algorithms"},"type":"lvl2","url":"/basefold/basefold-04#efficient-encoding-algorithms","position":2},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Efficient Encoding Algorithms"},"content":"The first article in this series has already introduced the concept of foldable linear codes and the BaseFold encoding algorithm. Here is a brief review.\n\nDefinition 1 [ZCF23, Definition 5] ((c, k_{0}, d) - Foldable Linear Codes). Let c, k_{0}, d \\in \\mathbb{N} and \\mathbb{F} denote a finite field. A linear code C_d : \\mathbb{F}^{k_0 \\cdot 2^d} \\rightarrow \\mathbb{F}^{c k_0 \\cdot 2^d} with generator matrix \\mathbf{G}_{d} is called foldable if there exists a sequence of generator matrices (\\mathbf{G}_0, \\ldots, \\mathbf{G}_{d-1}) and diagonal matrices (T_0, \\ldots, T_{d-1}) and (T_0',\\ldots,T_{d-1}') such that for any i \\in [1,d], the following holds:\n\nThe diagonal matrices T_{i-1},T_{i-1}' \\in F^{c k_0 \\cdot 2^{i-1} \\times c k_0 \\cdot 2^{i-1}} satisfy \\mathrm{diag}(T_{i-1})[j] \\neq \\mathrm{diag}(T_{i-1}')[j] for all j \\in [c k_0 \\cdot 2^{i-1}];\n\nThe matrix \\mathbf{G}_i \\in F^{k_0 \\cdot 2^i \\times c k_0 \\cdot 2^i} (arranged row-wise) is equal to\\mathbf{G}_i = \\begin{bmatrix}\n    \\mathbf{G}_{i-1} & \\mathbf{G}_{i-1} \\\\\n    \\mathbf{G}_{i-1}  \\cdot T_{i-1} & \\mathbf{G}_{i-1} \\cdot T'_{i-1} \\\\\n\\end{bmatrix}.\n\nTo efficiently construct a foldable linear code, a uniform sampling method is employed by first defining a set of random foldable distributions.\n\nDefinition 2 [ZCF23, Definition 9] ((c, k_{0}) - Foldable Distributions). Fix a finite field \\mathbb{F} and c, k_{0} \\in \\mathbb{N}. Let \\mathbf{G}_0 \\in \\mathbb{F}^{k_0 \\times c k_0} be the generator matrix of an [c k_0, k_0] linear code that satisfies maximum distance separability, and let D_0 be the distribution that outputs \\mathbf{G}_0 with probability 1. For each i > 0, we recursively define the distribution D_i, which samples the generator matrices (\\mathbf{G}_0, \\mathbf{G}_1, \\ldots, \\mathbf{G}_i) where \\mathbf{G}_i \\in F^{k_i \\times n_i} with k_i := k_0 \\cdot 2^i, n_i := c k_i:\n\nSample (\\mathbf{G}_0, \\ldots, \\mathbf{G}_{i-1}) \\leftarrow D_{i-1};\n\nSample \\mathrm{diag}(T_{i-1}) \\leftarrow \\$ (\\mathbb{F}^{\\times})^{n_{i-1}} and define \\mathbf{G}_i as\\mathbf{G}_i = \\begin{bmatrix}\n    \\mathbf{G}_{i-1} & \\mathbf{G}_{i-1} \\\\\n    \\mathbf{G}_{i-1} \\cdot T_{i-1} & \\mathbf{G}_{i-1} \\cdot -T_{i-1} \\\\\n\\end{bmatrix}.\n\nOnce the initial generator matrix \\mathbf{G}_0 is determined, uniformly sample n_{0} random elements from \\mathbb{F}^{\\times} (i.e., excluding the zero element) to generate the diagonal elements of T_{0}, thereby obtaining the next generator matrix \\mathbf{G}_1. This process is then recursively applied to generate (\\mathbf{G}_2, \\ldots, \\mathbf{G}_i). In PCS, generating foldable codes via uniform sampling aids in achieving an efficient prover.\n\nNote that the above definition requires the initial \\mathbf{G}_0 to be the generator matrix of a linear code satisfying the MDS property. However, as mentioned in a footnote in [ZCF23], this requirement is not strictly necessary. Including this property is merely for simplifying the analysis of the code distance later on. In fact, the distance analysis holds for any linear code.\n\nProtocol 1 \\mathrm{Enc}_{d} [ZCF23, Protocol 1]: BaseFold Encoding Algorithm\n\nInput: Original message \\mathbf{m} \\in \\mathbb{F}^{k_d}\n\nOutput: \\mathbf{w} \\in \\mathbb{F}^{n_d} such that \\mathbf{w} = \\mathbf{m} \\cdot \\mathbf{G}_d\n\nParameters: \\mathbf{G}_0 and diagonal matrices (T_0, T_1, \\ldots, T_{d-1})\n\nIf d = 0 (i.e., \\mathbf{m} \\in \\mathbb{F}^{k_0}):\n\n(a) Return \\mathrm{Enc}_0(\\mathbf{m})\n\nElse:\n\n(a) Split \\mathbf{m} := (\\mathbf{m}_l, \\mathbf{m}_r)\n\n(b) Let \\mathbf{l} := \\mathrm{Enc}_{d-1}(\\mathbf{m}_l), \\mathbf{r} := \\mathrm{Enc}_{d-1}(\\mathbf{m}_r), and \\mathbf{t} = \\mathrm{diag}(T_{d-1})\n\n(c) Return (\\mathbf{l} + \\mathbf{t} \\circ \\mathbf{r}, \\mathbf{l} - \\mathbf{t} \\circ \\mathbf{r})\n\nBy analyzing Protocol 1, we can see that encoding to obtain C_{d} requires only \\frac{dn_{d}}{2} field multiplications and d n_{d} field additions, i.e., 0.5 n \\log n field multiplications and n \\log n field additions. Overall, the encoding complexity is O(n \\log n). Thus, we have introduced the explicit construction of Random Linear Foldable Codes provided by BaseFold and verified that they indeed support efficient encoding.","type":"content","url":"/basefold/basefold-04#efficient-encoding-algorithms","position":3},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Polynomials Save the World: Polynomial-Based Encoding"},"type":"lvl2","url":"/basefold/basefold-04#polynomials-save-the-world-polynomial-based-encoding","position":4},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Polynomials Save the World: Polynomial-Based Encoding"},"content":"Next, we examine the second and third properties of Random Foldable Codes:\n\nField Agnostic, i.e., applicable even for small fields\n\nCompatible with PCS for Multi-linear Polynomials\n\nAs mentioned earlier, Reed-Solomon codes can achieve the Singleton bound, but only when the alphabet size is relatively large (i.e., q \\gg n). Fortunately, we can extend Reed-Solomon codes to Reed-Muller codes, transitioning from univariate polynomial codes to multivariate polynomial codes. This allows applicability over small fields (q \\ll n), although there is a slight trade-off in the balance between distance and error-correcting capability. However, this is worthwhile.\n\nAppendix D of [ZCF23] informs us that Random Foldable Codes are a special case of truncated Reed-Muller codes (Punctured Reed-Muller Codes). Thus, Random Foldable Codes are field agnostic and, by venturing into the realm of multivariate polynomials, are suitable for PCS of multi-linear polynomials.\n\nWe know that the encoding space of Reed-Solomon codes consists of univariate polynomials of degree at most d. Reed-Muller codes extend this to multivariate polynomials, with the encoding space comprising multivariate polynomials of total degree at most d.\n\nFor n, d, q with d < q, define Reed-Muller encoding as ([GJX15])\\mathrm{RM}_q(n,d) := \\{(F(\\mathbf{u}))_{\\mathbf{u} \\in \\mathbb{F}_q^n} : F \\in \\mathbb{F}_q[X_1, \\cdots, X_n], \\deg(F) \\le d \\}.\n\nReed-Muller codes represent the set of evaluations of n-variate polynomials of total degree at most d over \\mathbb{F}_q^n. The length of the encoding \\mathrm{RM}_q(n,d) is q^n, and the dimension is \\binom{n+d}{n}.\n\nIntuitively, Punctured Reed-Muller Codes are simply Reed-Muller codes with truncation. Specifically, the evaluation points \\mathbf{u} are not taken from all of \\mathbb{F}_q^n but only a subset, denoted as \\mathcal{T} = \\{ \\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_N \\}. Typically, Punctured Reed-Muller Codes allow this set \\mathcal{T} to be a multiset (i.e., permitting duplicate elements), but we do not impose this requirement here. Let \\mathrm{RM}_q(n,d)|_{\\mathcal{T}} denote the \\mathbb{F}_q-linear code:\\mathrm{RM}_q(n,d)|_{\\mathcal{T}} := \\{(F(\\mathbf{u}_1),F(\\mathbf{u}_2), \\cdots, F(\\mathbf{u}_N)): F \\in \\mathbb{F}_q[X_1, \\cdots, X_n], \\deg(F) \\le d \\}.\n\nThis is called a punctured Reed-Muller code ([GJX15]). From the definition, it is evident that this is merely a subset of Reed-Muller codes, selecting only N points, which aligns with the literal meaning of “truncated” or “punctured.”\n\nWith the concept of Punctured Reed-Muller Codes established, let’s examine the following lemma provided in Appendix D of [ZCF23], which states that foldable linear codes are a special case of punctured Reed-Muller codes.\n\nLemma 1 [ZCF23, Lemma 11] (Foldable Punctured Reed-Muller Codes). Let C_d be a foldable linear code with generator matrices (\\mathbf{G}_0, \\ldots, \\mathbf{G}_{d-1}) and diagonal matrices (T_0, \\ldots, T_{d-1}), (T_0', \\ldots, T_{d-1}'). Then there exists a subset D \\subset \\mathbb{F}^d such that C_d = \\{(P(\\mathbf{x}) : \\mathbf{x} \\in D) : P \\in \\mathbb{F}[X_1, \\ldots, X_d]\\}, i.e., each codeword in C_d is a vector obtained by evaluating a multilinear polynomial P at each point in D.\n\nProof: By induction. For simplicity, consider C_0 as a repetition code. In the base case, \\mathrm{Enc}_0(m) = m \\| \\ldots \\| m is a constant polynomial P \\equiv m evaluated at c distinct points. Assume that for i < d, there exists a set D_i such that C_i = \\{(P(\\mathbf{x}): \\mathbf{x} \\in D_i): P \\in \\mathbb{F}[X_1, \\ldots, X_i]\\}. Without loss of generality, assign an integer j \\in [1, c \\cdot 2^i] to index each element in D_i sequentially, representing x_j as the j-th element in D_i.\n\nLet t = \\mathrm{diag}(T_i), t' = \\mathrm{diag}(T'_i), n_i = c \\cdot 2^i, \\mathbf{v} \\in \\mathbb{F}^{2^{i+1}}, and let P \\in \\mathbb{F}[X_1, \\ldots, X_{i+1}] be a polynomial with coefficients from \\mathbf{v}. Finally, let P_l, P_r \\in \\mathbb{F}[X_1, \\ldots, X_{i}] such that P(X_1, \\ldots, X_{i+1}) = P_l(X_1, \\ldots, X_{i}) + X_{i+1} P_r(X_1, \\ldots, X_{i}). Then,\\begin{aligned}\n    \\mathrm{Enc}_{i+1}(\\mathbf{v}) \n    & = \\mathrm{Enc}_{i}(\\mathbf{v}_l) + \\mathrm{diag}(T_i) \\circ \\mathrm{Enc}_{i}(\\mathbf{v}_r) \\quad \\| \\quad \\mathrm{Enc}_{i}(\\mathbf{v}_l) + \\mathrm{diag}(T_i) \\circ \\mathrm{Enc}_{i}(\\mathbf{v}_r) \\\\\n    & \\quad \\text{\\color{blue}(by the encoding algorithm in Protocol 1)} \\\\\n    & = (P_l(\\mathbf{x}_1), \\ldots, P_l(\\mathbf{x}_n)) + \\mathrm{diag}(T_i) \\circ (P_r(\\mathbf{x}_1), \\ldots, P_r(\\mathbf{x}_n)) \\\\\n    & \\| \\quad (P_l(\\mathbf{x}_1), \\ldots, P_l(\\mathbf{x}_n)) + \\mathrm{diag}(T'_i) \\circ (P_r(\\mathbf{x}_1), \\ldots, P_r(\\mathbf{x}_n)) \\\\\n    & \\quad\\text{\\color{blue}(by the induction hypothesis)} \\\\\n    & = (P_l(\\mathbf{x}_1) + t_1 P_r(\\mathbf{x}_1), \\ldots, P_l(\\mathbf{x}_n) + t_n P_r(\\mathbf{x}_n), P_l(\\mathbf{x}_1) + t'_1 P_r(\\mathbf{x}_1), \\ldots, P_l(\\mathbf{x}_n) + t'_n P_r(\\mathbf{x}_n)) \\\\\n    & \\quad\\text{\\color{blue}(by the definition of the Hadmard product)} \\\\\n    & = (P(\\mathbf{x}_1,t_1), \\ldots, P(\\mathbf{x}_n, t_n), P(\\mathbf{x}_1,t'_1), \\ldots, P(\\mathbf{x}_n, t'_n))\\\\\n    & \\quad\\text{\\color{blue}(by the definition of $P$)}\n\\end{aligned}\n\nTherefore, let D_{i+1} = \\{(\\mathbf{x}_1,t_1), \\ldots, (\\mathbf{x}_n, t_n), (\\mathbf{x}_1,t'_1), \\ldots, (\\mathbf{x}_n, t'_n)\\}, and the lemma holds for i + 1. Thus, by induction, the proof is complete. \n\n\\Box","type":"content","url":"/basefold/basefold-04#polynomials-save-the-world-polynomial-based-encoding","position":5},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Good Relative Minimum Distance"},"type":"lvl2","url":"/basefold/basefold-04#good-relative-minimum-distance","position":6},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"Good Relative Minimum Distance"},"content":"Finally, we focus on the fourth property satisfied by Random Foldable Codes (RFCs):\n\nGood Relative Minimum Distance\n\nIn [ZCF23], it is proven that RFCs have tight bounds on their minimum Hamming distance (a “tight” bound means that the actual bounds are achievable). For example, an RFC over a 256-element finite field with a message length of \n\n225 and a code rate of \\frac{1}{8} has a relative minimum distance of 0.728 with overwhelming probability. For a code with a rate of \\frac{1}{8}, the maximum achievable relative minimum Hamming distance is approximately 1 - \\frac{1}{8} = 0.875. Clearly, 0.728 is fairly close to 0.875. This is practically useful and implies that foldable codes generated via overwhelming probability (which enable efficient PCS provers) also have good relative minimum distances (enabling efficient PCS verifiers).\n\nThe term “overwhelming probability” arises from the distribution (\\mathbf{G}_0, \\ldots, \\mathbf{G}_{d}) \\leftarrow D_{d} introduced during encoding. When uniformly sampling diagonal matrices T_{i} from \\mathbb{F}^{\\times} and setting T'_{i} = -T_{i}, the relative minimum distance of C_{d} achieved with overwhelming probability is equal to1 - \\left( \\frac{\\epsilon_{\\mathbb{F}}^d}{c} + \\frac{\\epsilon_{\\mathbb{F}}}{\\log |\\mathbb{F}|} \\sum_{i = 0}^{d} (\\epsilon_{\\mathbb{F}})^{d-i} \\left( 0.6 + \\frac{2 \\log(n_i / 2) + \\lambda}{n_i} \\right)  \\right) \\tag{1}\n\nwhere c is the reciprocal of the code rate, \\epsilon_{\\mathbb{F}} = \\frac{\\log|\\mathbb{F}|}{\\log|\\mathbb{F}| - 1.001}, n_{i} is the encoding length, d is the logarithm of the message length, and \\lambda is the security parameter. By setting \\lambda = 128, it ensures that (c,k_{0},d)-random foldable linear codes achieve the above relative minimum distance with a probability of at least 1 - 2^{-128}.\n\nNext, we examine how the result in equation (1) is derived. Our goal is to analyze the relative minimum distance of foldable random codes C_{d}. For a linear code, the minimum distance equals the minimum Hamming weight among all non-zero codewords becaused = \\min_{\\substack{\\vec{c_1} \\neq \\vec{c_2} \\\\ \\vec{c_1}, \\vec{c_2} \\in C_d}} \\Delta(\\vec{c_1}, \\vec{c_2}) = \\min_{\\substack{\\vec{c_1} \\neq \\vec{c_2} \\\\ \\vec{c_1}, \\vec{c_2} \\in C_d}} wt(\\vec{c_1} - \\vec{c_2}) = \\min_{\\substack{\\vec{c} \\neq \\vec{0}, \\vec{c} \\in C_d}} wt(\\vec{c} )\n\nSince it is a linear code, \\vec{c_1} - \\vec{c_2} is also a codeword in C_d, hence the last equality holds. Therefore, we want to show that for any non-zero message, i.e., \\forall \\vec{m} \\neq \\vec{0}, the encoded codeword \\mathrm{Enc}_d(\\vec{m}) does not have too many zero components. Suppose it has at most t_d zero components, letting \\mathrm{nzero}(\\cdot) denote the number of zero components in a vector, we aim to show\\forall \\vec{m} \\neq \\vec{0}, \\quad \\mathrm{nzero}(\\mathrm{Enc}_d(\\vec{m})) \\le t_d \\tag{2}\n\nLet n_d denote the length of the codeword \\mathrm{Enc}_d(\\vec{m}). Then from (2), we have\\forall \\vec{m} \\neq \\vec{0}, \\quad wt(\\mathrm{Enc}_d(\\vec{m})) \\ge n_d - t_d\n\nThus, the relative minimum distance that C_d can achieve is\\Delta(C_d) = \\frac{\\min_{\\substack{\\vec{c} \\neq \\vec{0}, \\vec{c} \\in C_d}} wt(\\vec{c} )}{n_d} = \\frac{n_d - t_d}{n_d} = 1 - \\frac{t_d}{n_d} \\tag{3}\n\nThe result in equation (1) is derived from equation (3). The remaining task is to analyze what t_d equals, that is, how many zero components a codeword can have after encoding any non-zero message.","type":"content","url":"/basefold/basefold-04#good-relative-minimum-distance","position":7},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl3":"Utilizing Induction","lvl2":"Good Relative Minimum Distance"},"type":"lvl3","url":"/basefold/basefold-04#utilizing-induction","position":8},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl3":"Utilizing Induction","lvl2":"Good Relative Minimum Distance"},"content":"Using the powerful tool of induction, we analyze t_d. Assume that with overwhelming probability (based on the choice of diagonal matrices T_0, \\ldots, T_{i-1}), for any non-zero message \\vec{m} \\in \\mathbb{F}^{k_i} \\ \\{0^{k_i}\\}, the encoded \\mathrm{Enc}_i(\\vec{m}) has at most t_i zero components. We analyze the case for i + 1. For any non-zero message \\vec{m} = (\\vec{m_l}, \\vec{m_r}) \\in \\mathbb{F}^{2k_i},\\begin{aligned}\n    \\mathrm{Enc}_{i+1}(\\vec{m}) & = (\\vec{m_l}, \\vec{m_r}) \\begin{bmatrix}\n    \\mathbf{G}_i & \\mathbf{G}_i \\\\\n    \\mathbf{G}_i \\cdot T_i & \\mathbf{G}_i \\cdot -T_i\n    \\end{bmatrix} \\\\\n    & = (\\vec{m_l}\\mathbf{G}_i + \\vec{m_l}\\mathbf{G}_i\\cdot T_i, \\vec{m_l}\\mathbf{G}_i - \\vec{m_l}\\mathbf{G}_i\\cdot T_i) \\\\\n    & = (\\mathrm{Enc}_i(\\vec{m}) + \\mathrm{Enc}_i(\\vec{m}) \\circ \\mathrm{diag}(T_i), \\mathrm{Enc}_i(\\vec{m}) - \\mathrm{Enc}_i(\\vec{m}) \\circ \\mathrm{diag}(T_i)) \\\\\n    & := (\\mathbf{M}_l \\|  \\mathbf{M}_r)\n\\end{aligned}\n\nThis means examining the number of zero components in the vector (\\mathbf{M}_l \\|  \\mathbf{M}_r). Separating \\mathbf{M}_l and \\mathbf{M}_r:\\begin{aligned}\n    \\mathbf{M}_l = \\mathrm{Enc}_i(\\vec{m_l}) + \\mathrm{Enc}_i(\\vec{m_r}) \\circ \\mathrm{diag}(T_i) \\\\\n    \\mathbf{M}_r = \\mathrm{Enc}_i(\\vec{m_l}) - \\mathrm{Enc}_i(\\vec{m_r}) \\circ \\mathrm{diag}(T_i)\n\\end{aligned}\n\nLet \\mathbf{t} = \\mathrm{diag}(T_i). For each j \\in [1, n_i], define A_j =  \\mathrm{Enc}_i(\\vec{m_l})[j] ，B_j = \\mathrm{Enc}_i(\\vec{m_r})[j], and define a function:f_j(x) = A_j + x B_j \\tag{4}\n\nIf f_j(\\mathbf{t}[j]) = 0 or f_j(-\\mathbf{t}[j]) = 0, then \\mathbf{M}_l[j] = 0 or \\mathbf{M}_r[j] = 0, indicating a zero component in the encoded vector. Let’s analyze whether f_j(x) can be zero based on the values of A_j and B_j, considering the following cases:\n\n\n\nA_{j} = 0\n\nA_{j} \\neq 0\n\nB_{j} = 0\n\nf_j(x) \\equiv 0\n\nf_{j}(x) = A_{j} \\neq 0\n\nB_{j} \\neq 0\n\nf_{j}(x) = x B_{j}\n\nf_{j}(x) = A_{j} + x B_{j}\n\nFirst, consider the case where A_j = B_j = 0. Here, f_j(x) \\equiv 0 for any x, meaning \\mathbf{M}_l[j] = 0 and \\mathbf{M}_r[j] = 0. Let S \\subseteq [n_i] denote such indices, and by the induction hypothesis, |S| \\le t_i. Define m_{i+1}(S) as those non-zero messages that satisfy A_j = B_j = 0, i.e.,S = \\{ j \\in [1, n_i]: A_j = B_j = 0 \\}.\n\nIn this case, \\mathbf{M}_l[j] = \\mathbf{M}_r[j] = 0, resulting in 2 |S| zero components in (\\mathbf{M}_l \\|  \\mathbf{M}_r).\n\nConsider the second case, where A_j \\neq 0 and B_j = 0. Here, f_{j}(x) = A_{j} \\neq 0, so no zero components are found.\n\nNext, consider the last row of the table where B_j \\neq 0. In this case, the index j is certainly not in S. Define a subset \\urcorner S^* \\subseteq  \\urcorner S such that\\urcorner S^* = \\{j \\in [1, n_i] \\backslash  S, B_j \\neq 0\\}\n\nFor each j \\in \\urcorner S^*, define a random variableX_j = 1 \\{f_j(\\mathbf{t}[j]) = 0\\} + 1 \\{f_j(-\\mathbf{t}[j]) = 0\\}\n\nwhere 1(\\cdot) is an indicator function that equals 1 if the condition inside holds, and 0 otherwise. Thus, X_j indicates how many zero components exist at position j in \\mathbf{M}_l and \\mathbf{M}_r, with possible values \\{0,1,2\\}. Notice that X_j is an independent Bernoulli trial because \\mathbf{t}[j] is uniformly sampled from \\mathbb{F}^{\\times}. Let z_j \\in \\mathbb{F}^{\\times} satisfy f_j(z_j) = 0. Then, when \\mathbf{t}[j] = z_j, 1 \\{f_j(\\mathbf{t}[j]) = 0\\} = 1 and when \\mathbf{t}[j] = -z_j, 1 \\{f_j(-\\mathbf{t}[j]) = 0\\} = 1. Analyzing the possible values of X_j:\n\nX_j = 2: This implies f_j(\\mathbf{t}[j]) = 0 and f_j(-\\mathbf{t}[j]) = 0, which leads to \\mathbf{t}[j] = z_j = -z_j. This would mean z_j = 0, which is impossible since z_j \\in \\mathbb{F}^{\\times}.\n\nX_j = 1: This implies either f_j(\\mathbf{t}[j]) = 0 or f_j(-\\mathbf{t}[j]) = 0, meaning \\mathbf{t}[j] = z_j or \\mathbf{t}[j] = -z_j. The probability of this occurring is \\frac{2}{|\\mathbb{F}| - 1}.\n\nX_j = 0: This occurs when \\mathbf{t}[j] \\neq z_j and \\mathbf{t}[j] \\neq -z_j, with probability 1 - \\frac{2}{|\\mathbb{F}| - 1}.\n\nFor all j \\in \\urcorner S^*, summing up all X_j gives the total number of zero components in (\\mathbf{M}_l \\|  \\mathbf{M}_r), denoted as X = \\sum_{j \\in \\urcorner S^*}X_j.\n\nHaving analyzed all cases in the table, we obtain\\mathrm{nzero}(\\mathrm{Enc}_{i+1}(\\vec{m})) = 2|S| + X\n\nNext, we analyze |S| and X to show that for any non-zero message \\vec{m} \\in \\mathbb{F}^{2k_i} \\backslash \\{0^{2k_i}\\}, \\mathrm{Enc}_{i+1}(\\vec{m}) has at most t_{i+1} zero components with overwhelming probability. We analyze the probability that \\mathrm{Enc}_{i+1}(\\vec{m}) has at least 2 t_i + l_{i} zero components:\\begin{aligned}\n    \\Pr [ {\\mathrm{nzero}(\\mathrm{Enc}_{i+1}(\\vec{m})) \\ge 2 t_i + l_{i}} ] & = \\Pr [ 2|S|  + X \\ge 2 t_i + l_{i} ] \\\\\n    & = \\Pr [ X \\ge 2 t_i + l_{i} - 2|S|] \\\\\n    & = \\Pr [\\sum_{j \\in \\urcorner S^*}X_j \\ge 2 t_i + l_{i} - 2|S|] \\\\\n    & \\le \\sum_{j = 2 t_i + l_{i} - 2|S|}^{|\\urcorner S^*|} \\binom{|\\urcorner S^*|}{i} \\cdot (\\frac{2}{|\\mathbb{F}| - 1})^{i} \\cdot (1 - \\frac{2}{|\\mathbb{F}| - 1})^{|\\urcorner S^*| - i}\\\\\n    & \\quad \\text{\\color{blue}{(By the binomial theorem,  $\\binom{|\\urcorner S^*|}{i} \\le 2^{|\\urcorner S^*|})$}} \\\\\n    & \\le |\\urcorner S^*| \\cdot 2^{|\\urcorner S^*|}  (\\frac{2}{|\\mathbb{F}| - 1})^{2 t_i + l_{i} - 2|S|} \\\\\n    & \\le |\\urcorner S| \\cdot 2^{|\\urcorner S|} \\cdot (\\frac{2}{|\\mathbb{F}| - 1})^{2 t_i + l_{i} - 2|S|} \\quad (\\urcorner S^* \\subseteq \\urcorner S)\\\\\n    & = |[1, n_i] \\backslash S| \\cdot 2^{|[1, n_i] \\backslash S|} \\cdot (\\frac{2}{|\\mathbb{F}| - 1})^{2 t_i + l_{i} - 2|S|}\\\\\n    & = (n_i - |S|) \\cdot 2^{n_i - |S|} \\cdot (\\frac{2}{|\\mathbb{F}| - 1})^{2 t_i + l_{i} - 2|S|}\\\\\n    & \\quad \\color{blue}{\\text{(Assume } |\\mathbb{F}| \\ge 2^{10} , \\text{ then } \\frac{2}{|\\mathbb{F}| - 1} \\le \\frac{2.002}{|\\mathbb{F}|})}\\\\\n    & \\le n_i \\cdot 2^{n_i - |S|} \\left(\\frac{2.002}{|\\mathbb{F}|}\\right)^{2 t_i + l_{i} - 2|S|}\n\\end{aligned}\n\nWe observe that for an index set S \\subseteq [1, n_i], if any set S is selected, each index i \\in [1, n_{i}] has two possibilities: to include it in S or not. Therefore, there are a total of 2^{n_{i}} possible selections for the set S. When we enumerate all possible sets S, the union of the resulting m_{i+1}(S), denoted by \\cup_{S \\subseteq [1, n_{i}]}m_{i+1}(S), can cover all messages in \\mathbb{F}^{k_{i+1}} = \\mathbb{F}^{2k_{i}}. Lemma 2 in paper [ZCF23] tells us that the size of the set m_{i+1}(S) is at most \\mathbb{F}^{t_i - |S|}. Thus, by iterating through all 2^{n_{i}} possible sets S, each set S contains at most \\mathbb{F}^{t_i - |S|} messages \\vec{m} \\in \\mathbb{F}^{2k_i}. By combining all S and considering the bounds on the size of each S, we can conclude that when l_i is sufficiently large, that is, when |\\mathbb{F}|^{l_i} \\gg  2^{n_i}, the expression n_i \\cdot 2^{n_i - |S|} \\left(\\frac{2.002}{|\\mathbb{F}|}\\right)^{2 t_i + l_{i} - 2|S|} becomes sufficiently small. In this case, for any non-zero vector \\vec{m} \\in \\mathbb{F}^{2k_{i}}, we have \\mathrm{nzero}(\\mathrm{Enc}_{i+1}(\\vec{m})) \\le 2 t_i + l_{i}, that is, \\mathrm{Enc}_{i+1}(\\vec{m}) contains at most 2t_i + l_i zero components.\n\nThe BaseFold paper [ZCF23] presents a more specific statement in the form of a theorem.\n\nTheorem 1 [ZCF23, Theorem 2] Fix any finite field \\mathbb{F} with |\\mathbb{F}| \\ge 2^{10}, and let \\lambda \\in \\mathbb{N} be the security parameter. For a vector \\mathbf{v} with components in \\mathbb{F}, let {\\mathrm{nzero}}(\\mathbf{v}) denote the number of zero components in \\mathbf{v}. For any d \\in \\mathbb{N}, let D_d be a (c, k_0)-foldable distribution, and for each i \\le d, set k_i = k_0 2^i, n_i = c k_i. Then,\\Pr_{(\\mathbf{G}_0, \\ldots, \\mathbf{G}_d) \\leftarrow D_d}\\left[ \\exists \\mathbf{m} \\in \\mathbb{F}^{k_d} \\backslash \\{ \\mathbf{0} \\}, \\mathrm{nzero}(\\mathrm{Enc}_d(\\mathbf{m})) \\ge t_d \\right] \\le d \\cdot 2^{- \\lambda}  \\tag{5}\n\nwhere t_0 = k_0 and for each i \\in [d], t_i = 2 t_{i-1} + l_i, withl_i := \\frac{2(d - 1) \\log n_0 + \\lambda + 2.002 t_{d-1} + 0.6 n_d}{\\log |\\mathbb{F}| - 1.001}.\n\nEquation (5) indicates that the number of zero components in \\mathrm{Enc}_d(\\mathbf{m}) is bounded by t_d with negligible probability if exceeded. Given the iterative formula t_i = 2 t_{i-1} + l_i, we can compute t_d through iterative summation. Consequently, the maximum relative number of zero components in C_d is Z_{C_d} = \\frac{t_d}{n_d}, and calculating 1 - Z_{C_d} yields the minimum relative Hamming distance \\Delta_{C_d} of C_d, resulting in equation (1):1 - \\left( \\frac{\\epsilon_{\\mathbb{F}}^d}{c} + \\frac{\\epsilon_{\\mathbb{F}}}{\\log |\\mathbb{F}|} \\sum_{i = 0}^{d} (\\epsilon_{\\mathbb{F}})^{d-i} \\left( 0.6 + \\frac{2 \\log(n_i / 2) + \\lambda}{n_i} \\right)  \\right) .\n\nFrom the iterative formula t_i = 2 t_{i-1} + l_i, we observe that as i increases, t_i grows by more than 2 t_{i-1}. Since the encoding length doubles each iteration, the relative maximum number of zero components increases, and therefore the minimum relative Hamming distance decreases. If \\Delta_{C_d} is sufficiently large, then through this iterative method, we obtain with overwhelming probability that \\Delta_{C_0} \\ge \\Delta_{C_1} \\ge \\ldots \\ge \\Delta_{C_d}. From i = d to i = 0, this encoding method does not decrease \\Delta_{C_d}. In the IOPP protocol, if the initial minimum relative Hamming distance is large, then \\Delta_{C_{0}} remains large with overwhelming probability, which plays a significant role in analyzing the soundness of the IOPP.","type":"content","url":"/basefold/basefold-04#utilizing-induction","position":9},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-04#references","position":10},{"hierarchy":{"lvl1":"Notes on BaseFold (Part IV): Random Foldable Codes","lvl2":"References"},"content":"[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\n[GJX15] Venkatesan Guruswami, Lingfei Jin, and Chaoping Xing. “Efficiently List-Decodable Punctured Reed-Muller Codes”. In: IEEE Transactions on Information Theory 63 (2015), pp. 4317–4324. url: \n\nhttps://​api​.semanticscholar​.org​/CorpusID: 14176561.","type":"content","url":"/basefold/basefold-04#references","position":11},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness"},"type":"lvl1","url":"/basefold/basefold-05","position":0},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nIn this article, we will outline the proof approach for IOPP soundness presented in the [ZCF23] paper, which is similar to the soundness proof for the FRI protocol in [BKS18]. It employs a binary tree method to analyze points where the Prover might cheat, a concept also appearing in the soundness proof of the DEEP-FRI protocol in [BGKS20].","type":"content","url":"/basefold/basefold-05","position":1},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"IOPP Protocol"},"type":"lvl2","url":"/basefold/basefold-05#iopp-protocol","position":2},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"IOPP Protocol"},"content":"The IOPP protocol has been thoroughly introduced in the second article above. For the analysis in the later sections, we will briefly outline the IOPP protocol here. It is an extension of the FRI protocol, and the process of understanding the protocol can fully leverage the understanding of the FRI protocol, as both the commit and query phases are consistent.\n\nProtocol 1 [ZCF23, Protocol 2] IOPP.commit\n\nInput oracle: \\pi_d \\in \\mathbb{F}^{n_d}Output oracles: (\\pi_{d-1}, \\ldots, \\pi_0) \\in \\mathbb{F}^{n_{d-1}} \\times \\cdots \\times \\mathbb{F}^{n_0}\n\nFor i from d-1 down to 0:\n\nVerifier samples and sends \\alpha_i \\leftarrow \\$ \\mathbb{F} from \\mathbb{F} to Prover\n\nFor each index j \\in [1, n_i], Prover:\na. Sets f(X) := \\mathrm{interpolate}((\\mathrm{diag}(T_i)[j], \\pi_{i+1}[j]), (\\mathrm{diag}(T'_i)[j], \\pi_{i+1}[j+n_i]))\nb. Sets \\pi_i[j] = f(\\alpha_i)\n\nProver outputs oracle \\pi_i \\in \\mathbb{F}^{n_i}.\n\nProtocol 2 [ZCF23, Protocol 3] IOPP.query\n\nInput oracles: (\\pi_{d-1}, \\ldots, \\pi_0) \\in \\mathbb{F}^{n_{d-1}} \\times \\ldots \\times \\mathbb{F}^{n_0}Output: accept or reject\n\nVerifier samples \\mu \\leftarrow \\$ [1, n_{d-1}]\n\nFor i from d-1 down to 0, Verifier:\n\nQueries oracle \\pi_{i+1}[\\mu], \\pi_{i+1}[\\mu + n_i]\n\nComputes p(X) := \\mathrm{interpolate}((\\mathrm{diag}(T_i)[\\mu], \\pi_{i+1}[\\mu]), (\\mathrm{diag}(T'_i)[\\mu], \\pi_{i+1}[\\mu + n_i]))\n\nChecks p(\\alpha_i) = \\pi_i[\\mu]\n\nIf i > 0 and \\mu > n_i - 1, then updates \\mu \\leftarrow \\mu - n_{i - 1}\n\nIf \\pi_0 is a valid codeword with respect to the generator matrix \\mathbf{G}_0, output accept; otherwise, output reject.","type":"content","url":"/basefold/basefold-05#iopp-protocol","position":3},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"Analysis Approach"},"type":"lvl2","url":"/basefold/basefold-05#analysis-approach","position":4},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"Analysis Approach"},"content":"The analysis of IOPP soundness examines, for any Prover who might cheat, what is the maximum probability that the Verifier outputs accept in such a scenario. We aim for this probability to be sufficiently small to ensure the protocol’s security. Naturally, this probability depends on certain parameters of the protocol, and in practice, we desire it to be below a predetermined security parameter \\lambda (for example, \\lambda could be set to 128 or 256), meaning that this probability should be less than 2^{-\\lambda}.\n\nLet us now examine areas within the IOPP protocol where a cheating Prover might exploit to cause the Verifier to output accept. We note that there are two points where the Verifier introduces randomness:\n\nIn the IOPP.commit phase, step 1 of the protocol, the Verifier selects a random number \\alpha_{i} from \\mathbb{F} and sends it to the Prover, who uses it to fold the original \\pi_{i+1} to obtain \\pi_i.\n\nIn the IOPP.query phase, step 1 of the protocol, the Verifier samples \\mu \\leftarrow \\$ [1, n_{d-1}] and then checks whether the Prover’s folding was correct.\n\nSuppose the initial cheating Prover provides a \\pi_{d} that is \\delta away from C_{d}. We aim for the Verifier to ultimately check that \\pi_{0} is also at least \\delta away from C_0, meaning that the \\delta distance is preserved throughout the folding process. Another scenario is detecting that the Prover did not fold correctly. We consider two cases:\n\nThe Prover is extremely lucky, and the random number \\alpha_{i} chosen by the Verifier causes the folded \\pi_i to be less than \\delta away from the corresponding C_i, leading the Verifier to output accept. We consider this situation very lucky for the Prover because, according to the Proximity Gaps theorem, the probability of such an event is exceedingly small (assuming this probability is \\epsilon), such that its occurrence is akin to the Prover winning the lottery.\n\nThe Prover is not as lucky as in Case 1. In this scenario, after folding with the random number, the message \\pi_i still remains at least \\delta away from the corresponding C_i. Since the Verifier randomly selects \\mu \\leftarrow \\$ [1,n_{d-1}] in the IOPP.query phase and only checks a subset of the Prover’s foldings, this provides an opportunity for the Prover to potentially evade Verifier checks. For example, if after folding, the message a satisfies \\Delta(a, C) > \\delta in relative Hamming distance, i.e., \\Delta(a, C) > \\delta. The Verifier will randomly check a[i] against c[i], and if they are unequal, the Verifier will reject.\n\nSince \\Delta(a, C) > \\delta, more than a \\delta proportion of the components in a differ from the codewords in the encoding space. When the Verifier selects one of these differing positions, it will reject, hence the probability that the Verifier catches the Prover cheating exceeds \\delta.\n\nIf the Verifier queries l times, the probability that the Prover can pass all Verifier checks is at most (1 - \\delta)^{l}.\n\nCombining the two cases, the probability that the cheating Prover succeeds is bounded above by\\epsilon + (1 - \\delta)^{l} \\tag{1}\n\nThis represents an overall analysis approach. The specific expression may vary, but the following IOPP soundness theorem will elaborate further.","type":"content","url":"/basefold/basefold-05#analysis-approach","position":5},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"IOPP Soundness Theorem"},"type":"lvl2","url":"/basefold/basefold-05#iopp-soundness-theorem","position":6},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"IOPP Soundness Theorem"},"content":"Theorem 1 [ZCF23, Theorem 3] (IOPP Soundness for Foldable Linear Codes) Let C_d be a (c, k_0, d) foldable linear code with generator matrices (G_0, \\ldots, G_d). Let C_i (0 \\le i < d) denote the code generated by G_i, and assume that for all i \\in [0, d-1], the relative minimum distance \\Delta C_i \\ge \\Delta C_{i+1}. Let \\gamma > 0, and set \\delta := \\min(\\Delta^*(\\pi_d, C_d), J_{\\gamma}(J_{\\gamma}(\\Delta_{C_d}))), where \\Delta^*(\\pi_d, C_d) is the relative coset minimum distance between \\mathbf{v} and C_d. Then, for any (adaptively chosen) Prover oracles \\pi_{d-1}, \\ldots, \\pi_0, the Verifier in the IOPP.query phase repeated \\ell times outputs accept with probability at most (1 - \\delta + \\gamma d)^{\\ell}.\n\nThe term J_{\\gamma}(J_{\\gamma}(\\Delta_{C_d})) mentioned in the theorem refers to the composition of two Johnson functions, defined as follows.\n\nDefinition 1 [ZCF23, Definition 4] (Johnson Bound) For any \\gamma \\in (0,1], define J_{\\gamma}: [0,1] \\rightarrow [0,1] asJ_{\\gamma}(\\lambda) := 1 - \\sqrt{1 - \\lambda(1 - \\gamma)} .\n\nThe definition of relative coset minimum distance is as follows.\n\nDefinition 2 [ZCF23, Definition 5] (Relative Coset Minimum Distance) Let n be an even number, and let C be a [n,k,d] error-correcting code. For a vector \\mathbf{v} \\in \\mathbb{F}^{n} and a codeword c \\in C, the relative distance \\Delta^*(\\mathbf{v},c) between \\mathbf{v} and c is defined as\\Delta^*(\\mathbf{v},c) = \\frac{2 |\\{ j \\in [1, n/2]: \\mathbf{v}[j]  \\neq c[j] \\vee \\mathbf{v}[j + n/2]  \\neq c[j + n/2] \\}  |}{n}.\n\nThis definition is similar to the block-wise distance definition used in [BBHR18] to prove soundness ([BBHR18, Definition 3.2]). It is an alternative version of the relative minimum Hamming distance. Pairs \\{j,j+n/2\\} are considered as a pair, corresponding to a coset, analogous to the FRI protocol. For example, for n = 8, let the generator be \\omega with \\omega^8 = 1, and select the mapping x \\mapsto x^2, then it can be seen that \\{1,5\\}, \\{2,6\\}, \\{3,7\\}, \\{4,8\\} correspond to elements that form a coset, resulting in a total of 4 cosets.\n\n\\Delta^*(\\mathbf{v},c) measures the proportion of cosets in which \\mathbf{v} and c are not entirely consistent.\n\nLet \\Delta^*(\\mathbf{v},C) := \\min_{c \\in C}\\Delta^*(\\mathbf{v},c). Then, it relates to the relative minimum Hamming distance as follows: \\Delta(\\mathbf{v},C) \\le \\Delta^*(\\mathbf{v},C).\n\nDespite introducing these different definitions and Johnson functions, the proof approach for IOPP soundness remains consistent with the earlier outlined analysis, discussing two cases. Our aim is to analyze the probability that a cheating Prover can pass all Verifier checks and ultimately output accept. The proof approach is as follows:\n\nCase 1: The Prover is extremely lucky. Due to the Verifier selecting random numbers \\alpha_i, the folded messages are sufficiently close to the encoding space, allowing the Prover to pass all subsequent Verifier checks. For the Verifier, this corresponds to some “bad” events occurring, where there exists an i \\in [0, d-1] such that\\Delta(\\mathrm{fold}_{\\alpha_i}(\\pi_{i+1}), C_i) \\le \\min(\\Delta^*(\\pi_{i+1}, C_{i+1}), J_{\\gamma}(J_{\\gamma}(\\Delta_{C_d}))) - \\gamma\n\nUsing proof by contradiction through the Correlated Agreement theorem (which can derive the corresponding Proximity Gaps theorem), it can be shown that the probability of such “bad” events is small, proven to be at most \\frac{2d}{\\gamma^3 |\\mathbb{F}|}.\n\nCase 2: Suppose the Prover is not as lucky, meaning that the “bad” events described in Case 1 do not occur. Then, in the IOPP.query phase, the Verifier selects \\mu \\leftarrow \\$ [1, n_{d-1}], and in this scenario, the Prover might evade the Verifier’s checks by having the Verifier select points where the Prover has not cheated. Repeating the IOPP.query phase l times, the probability that the Prover manages to pass each check is at most (1 - \\delta + \\gamma d)^{l}.\n\nCombining Cases 1 and 2, for foldable linear codes, the IOPP Soundness is at least\\mathbf{s}^-(\\delta) = 1 - \\left(\\frac{2d}{\\gamma^3 |\\mathbb{F}|} + \\left(1 - \\delta + \\gamma d \\right)^l \\right) .\n\nThus, Theorem 1 is proven.","type":"content","url":"/basefold/basefold-05#iopp-soundness-theorem","position":7},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl3":"Proof of Case 1","lvl2":"IOPP Soundness Theorem"},"type":"lvl3","url":"/basefold/basefold-05#proof-of-case-1","position":8},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl3":"Proof of Case 1","lvl2":"IOPP Soundness Theorem"},"content":"The following Corollary 1 demonstrates that for any specific i, the probability that after folding, the result is within a relative Hamming distance \\delta - \\gamma of C_i, denoted as event B^{(i)}, is at most \\frac{2}{\\gamma^3 |\\mathbb{F}|}. Therefore, if certain events B_i occur, their probability does not exceed the sum of the probabilities of these B_i events, i.e.,\\Pr\\left[\\bigcup_{i = 0}^{d-1} B^{(i)} \\right] \\le \\sum_{i = 0}^{d-1} \\Pr[B^{(i)}] \\le \\frac{2d}{\\gamma^3 |\\mathbb{F}|}.\n\nLet us examine Corollary 1 in detail.\n\nCorollary 1 [ZCF23, Corollary 1] For any fixed i \\in [0, d-1] and \\gamma, \\delta > 0, such that \\delta \\le J_{\\gamma}(J_{\\gamma}(\\Delta_{C_d})), if \\Delta^*(\\mathbf{v}, C_{i+1}) > \\delta, then\\Pr_{\\alpha_i \\leftarrow \\$ \\mathbb{F}}[\\Delta(\\mathrm{fold}_{\\alpha_i}(\\mathbf{v}), C_i) \\le \\delta - \\gamma] \\le \\frac{2}{\\gamma^3 |\\mathbb{F}|}. \\tag{2}\n\nThe function \\mathrm{fold}_{\\alpha_i}(\\cdot) is defined as follows. Let \\mathbf{u},\\mathbf{u'} \\in \\mathbf{F}^{n_i} be the unique interpolated vectors such that\\pi_{i+1} = (\\mathbf{u} + \\mathrm{diag}(T_i) \\circ \\mathbf{u}', \\mathbf{u} + \\mathrm{diag}(T_i') \\circ \\mathbf{u}')\n\nThen, \\mathrm{fold}_{\\alpha_i}(\\pi_{i+1}) is defined as\\mathrm{fold}_{\\alpha_i}(\\pi_{i+1}) := \\mathbf{u}' + \\alpha_i \\mathbf{u}.\n\nThis essentially represents the process of folding \\pi_{i+1} with the random number \\alpha_i.\n\nCorollary 1 generalizes [BKS18] Corollary 7.3 to general foldable linear codes.\n\nProof Idea of Corollary 1: To prove that the relative Hamming distance after folding with the random number \\alpha_i is smaller than the original distance is a rare event, specifically not exceeding \\frac{2}{\\gamma^3 |\\mathbb{F}|}. Suppose, for contradiction, that this event occurs with a significantly higher probability. Then, by directly applying the Correlated Agreement theorem (from [BKS18] Theorem 4.4), it can be shown that for the affine space U = \\{\\mathbf{u} + x \\mathbf{u'} : x \\in \\mathbb{F}\\}, there exists a sufficiently large Correlated Agree subset T within C_i such that there exist \\mathbf{w}, \\mathbf{w'} \\in C_i agreeing with \\mathbf{u} and \\mathbf{u'} on T, respectively. Encoding \\mathbf{w}, \\mathbf{w'} yields a codeword c_{w} in C_{i+1}, thereby estimating \\Delta^{*}(\\mathbf{v}, C_{i+1}) \\le \\delta, which contradicts our assumption. Therefore, the corollary holds.","type":"content","url":"/basefold/basefold-05#proof-of-case-1","position":9},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl3":"Proof of Case 2","lvl2":"IOPP Soundness Theorem"},"type":"lvl3","url":"/basefold/basefold-05#proof-of-case-2","position":10},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl3":"Proof of Case 2","lvl2":"IOPP Soundness Theorem"},"content":"To prove that repeating the IOPP.query phase l times results in the Verifier outputting accept with probability at most (1 - \\delta + \\gamma d)^l, we merely need to demonstrate that, in one execution of IOPP.query, the probability that the Verifier outputs reject is at least \\delta - \\gamma d.\n\nUsing the binary tree concept for the proof, we first define a “bad” node (i,\\mu), as shown in the figure below. These are the points where the Prover fails to pass step 3 of the IOPP.query protocol, meaning that after the Verifier selects a random number \\mu, for any i \\in [0, d-1] and \\mu \\in [n_i], the Verifier computes in step 2 of IOPP.query:p(X) := \\mathrm{interpolate}((\\mathrm{diag}(T_i)[\\mu], \\pi_{i+1}[\\mu]), (\\mathrm{diag}(T'_i)[\\mu], \\pi_{i+1}[\\mu + n_i]))\n\nSubsequently, in step 3 of the IOPP.query protocol, the Verifier checks whetherp(\\alpha_i) \\neq \\pi_i[\\mu]\n\nAt this point, we say that the node (i,\\mu) is “bad”.\n\nNext, consider i from d-1 down to 0. For any \\mu \\in [1, n_{d-1}], \\mu can generate a binary tree, forming n_{0} such binary trees as depicted below.\n\nIf there exists at least one “bad” node (i,\\mu) within any of these trees—assuming that all nodes from layers d-1 down to i +1 and their children are “good”, i.e., they pass step 3 of the IOPP.query protocol—then when a “bad” node occurs at level i, the Verifier will reject. As shown in the figure, nodes from levels i +1 to d-1 are all “good”. This implies that as long as there is at least one bad node in the entire tree, the Verifier will reject. If we let \\beta_i denote the ratio of “bad” nodes at layer i, then the probability that the Verifier rejects at layer i is \\beta_i. Considering the entire IOPP.query phase, the Verifier’s probability of rejection is thus \\sum_{i=0}^{d-1}\\beta_i, where \\beta_i := \\Delta(\\pi_i, \\mathrm{fold}_{\\alpha_i}(\\pi_{i+1})), representing those “bad” points where the folded \\pi_{i+1} does not agree with \\pi_i.\n\nThus, the remaining task is to estimate \\sum_{i=0}^{d-1}\\beta_i. [ZCF23, Claim 2] provides inequalities for each \\beta_i.\n\nClaim 1 [ZCF23, Claim 2] For any i \\in [0, d], define \\delta^{(i)} := \\min(\\Delta^*(\\pi_i, C_i), J_{\\gamma}(J_{\\gamma}(\\Delta_{C_d}))). For all i \\in [0, d-1],\\beta_i \\ge \\delta^{(i+1)} - \\delta^{(i)} - \\gamma .\n\nUnder the soundness conditions, \\delta = \\delta^{(d)}. Also, since \\Delta^{*}(\\pi_0, C_0) = \\Delta(\\pi_0, C_0) = 0, we have \\delta^{(0)} = 0. Thus, according to the claim:\\delta = \\delta^{(d)} - \\delta^{(0)} = \\sum_{i = 0}^{d-1} \\delta^{(i+1)} - \\delta^{(i)} \\le \\sum_{i = 0}^{d-1} \\beta_i + \\gamma d,\n\nhence,\\sum_{i = 0}^{d-1} \\beta_i \\ge \\delta - \\gamma d.\n\nTherefore, if no bad event B occurs, executing the IOPP.query phase once, the probability that the Verifier rejects is at least \\delta - \\gamma d. This concludes the proof of Case 2.","type":"content","url":"/basefold/basefold-05#proof-of-case-2","position":11},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-05#references","position":12},{"hierarchy":{"lvl1":"Notes on Basefold (Part V): IOPP Soundness","lvl2":"References"},"content":"[BBHR18] Eli Ben-Sasson, Iddo Bentov, Ynon Horesh, and Michael Riabzev. Fast Reed-Solomon Interactive Oracle Proofs of Proximity. In Proceedings of the 45th International Colloquium on Automata, Languages, and Programming (ICALP), 2018. Available online as Report 134-17 on Electronic Colloquium on Computational Complexity.\n\n[BGKS20] Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. DEEP-FRI: sampling outside the box improves soundness. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA, volume 151 of LIPIcs, pages 5:1–5:32. Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2020.\n\n[BKS18] Eli Ben-Sasson, Swastik Kopparty, and Shubhangi Saraf. “Worst-Case to Average Case Reductions for the Distance to a Code”. In: Proceedings of the 33rd Computational Complexity Conference. CCC ’18. San Diego, California: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018. ISBN: 9783959770699.\n\n[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.","type":"content","url":"/basefold/basefold-05#references","position":13},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding"},"type":"lvl1","url":"/basefold/basefold-habock-overview","position":0},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article mainly outlines the security proof for the Basefold [ZCF23] multilinear PCS under list decoding, as presented in Ulrich Haböck’s paper [H24]. In [ZCF23], the soundness proof was given under unique decoding for foldable linear codes, while in [H24], the proof is for Reed-Solomon codes under list decoding, raising the bound to the Johnson bound, i.e., 1 - \\sqrt{\\rho}. To prove security, the paper presents two correlated agreement theorems stronger than the one given in [BCIKS20]:\n\n[H24, Theorem 3] Correlated agreement for subcodes.\n\n[H24, Theorem 4] Weighted correlated agreement for subcodes.\n\nWhen considering the Basefold protocol applied to Reed-Solomon codes, the protocol combines FRI and sumcheck. To prove its security, [H24] proposes subcodes that incorporate sumcheck-like constraints on top of Reed-Solomon codes. By combining this with the corresponding correlated agreement theorems, the security of the protocol can be proven.","type":"content","url":"/basefold/basefold-habock-overview","position":1},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Basefold Protocol"},"type":"lvl2","url":"/basefold/basefold-habock-overview#basefold-protocol","position":2},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Basefold Protocol"},"content":"For a multilinear polynomial P(X_1, X_2, \\ldots, X_n) \\in F[X_1, \\ldots, X_n], we want to prove that for any query \\vec{\\omega} = (\\omega_1, \\ldots, \\omega_n) from F^n, we have v = P(\\omega_1, \\ldots, \\omega_n). To implement PCS for the multilinear polynomial P(X_1, X_2, \\ldots, X_n), the Basefold protocol combines the Sumcheck and FRI protocols. The following introduction is based on the description in [H24].","type":"content","url":"/basefold/basefold-habock-overview#basefold-protocol","position":3},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Combining with Sumcheck Protocol","lvl2":"Basefold Protocol"},"type":"lvl3","url":"/basefold/basefold-habock-overview#combining-with-sumcheck-protocol","position":4},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Combining with Sumcheck Protocol","lvl2":"Basefold Protocol"},"content":"To prove v = P(\\omega_1, \\ldots, \\omega_n), we first convert the query value P(\\omega_1, \\ldots, \\omega_n) into a Sumcheck sum form:P(\\omega_1, \\ldots, \\omega_n) = \\sum_{\\vec{x} = (x_1, \\ldots, x_n) \\in H_n} L(\\vec{x}, \\vec{\\omega}) \\cdot P(\\vec{x})\n\nwhere H_n = \\{0,1\\}^n, and L(\\vec{x}, \\vec{\\omega}) is actually the eq(\\cdot, \\cdot) function:L(\\vec{x}, \\vec{\\omega}) = \\prod_{i = 1}^n \\left ((1 - x_i)(1 - \\omega_i) + x_i\\omega_i \\right)\n\nTherefore, the proof of v = P(\\omega_1, \\ldots, \\omega_n) is transformed into proving the sum over H_n:\\sum_{\\vec{x} = (x_1, \\ldots, x_n) \\in H_n} L(\\vec{x}, \\vec{\\omega}) \\cdot P(\\vec{x}) = v\n\nThe Sumcheck protocol can then be used to prove this sum is correct.\n\nFor i = 1, \\ldots, n - 1, the Prover needs to construct a univariate polynomial based on the challenge random numbers \\lambda_1, \\ldots,\\lambda_i:q_i(X) = \\sum_{\\vec{x} = (x_{i + 2}, \\ldots, x_n) \\in H_{n - (i + 1)}} L(\\lambda_1,\\ldots, \\lambda_i,X,\\vec{x}, \\vec{\\omega}) \\cdot P(\\lambda_1, \\ldots, \\lambda_i,X,\\vec{x})\n\nThis corresponds to the polynomial P(\\lambda_1, \\ldots, \\lambda_i, X, \\omega_{i+2}, \\ldots, \\omega_{n}).\n\nWe can see that in q_i(X), L(\\lambda_1,\\ldots, \\lambda_i,X,\\vec{x}, \\vec{\\omega}) is linear in X, and P(\\lambda_1, \\ldots, \\lambda_i,X,\\vec{x}) is also linear in X. Their product becomes quadratic in X. To correspond with the correlated agreement theorem for linear subcodes later, we extract the linear term in X. The Prover needs to send the linear polynomial:\\Lambda_i(X) = \\sum_{\\vec{x} = (x_{i + 2}, \\ldots, x_n) \\in H_{n - (i + 1)}} L(\\vec{x}, (\\omega_{i+2}, \\ldots, \\omega_n)) \\cdot P(\\lambda_1, \\ldots, \\lambda_i,X,\\vec{x})\n\nSince\\begin{aligned}\n\tL((\\lambda_1, \\ldots, \\lambda_i, X, \\vec{x}), \\vec{\\omega}) & = L((\\lambda_1, \\ldots, \\lambda_i, X, \\vec{x}), (\\omega_1, \\ldots, \\omega_{i}, \\omega_{i+1}, \\omega_{i+2}, \\ldots, \\omega_n)) \\\\\n\t& = \\left( \\prod_{j = 1}^{i}[(1 - \\lambda_j)(1 - \\omega_j) + \\lambda_j \\omega_j] \\right) \\\\\n\t& \\quad \\cdot \\left( (1 - \\lambda_{i+1})(1 - \\omega_{i+1}) + X \\cdot \\omega_{i+1} \\right) \\\\\n\t& \\quad \\cdot \\left( \\prod_{j = i+2}^{n}[(1 - \\lambda_j)(1 - \\omega_j) + \\lambda_j \\omega_j] \\right) \\\\\n\t& = L(\\lambda_1, \\ldots, \\lambda_i, \\omega_1, \\ldots, \\omega_i) \\cdot L(X, \\omega_{i+1}) \\cdot L(\\vec{x}, (\\omega_{i+2}, \\ldots, \\omega_n))\n\\end{aligned}\n\nTherefore,q_i(X) = L(\\lambda_1, \\ldots, \\lambda_i, \\omega_1, \\ldots, \\omega_i) \\cdot L(X, \\omega_{i+1}) \\cdot \\Lambda_i(X).\n\nThe Prover only needs to provide \\Lambda_i(X), and the Verifier can calculate q_i(X) using the above equation.\n\nIn the Sumcheck protocol, the Prover first sends a univariate polynomial \\Lambda_0(X) = \\sum_{\\vec{x} = (x_2, \\ldots, x_n) \\in H_{n - 1}}L(\\vec{x}, (\\omega_2, \\ldots, \\omega_n)) \\cdot P(X, \\vec{x}) and s_0 = v. Then in round 1 \\le i \\le n - 1:\n\nThe Verifier can calculate q_{i-1}(X) based on \\Lambda_{i-1}(X) and check if s_{i-1} = q_{i-1}(0) + q_{i-1}(1). Then choose a random number \\lambda_i \\leftarrow \\$ F and send it to the Prover.\n\nThe Prover calculates P(\\lambda_1, \\ldots, \\lambda_i, X_{i+1}, \\ldots, X_n) based on \\lambda_i, computes \\Lambda_i(X) and sends it to the Verifier. Both the Prover and Verifier set s_i = q_{i-1}(\\lambda_i).\n\nIn the last step of Sumcheck, we need to obtain the value of P(X_1, \\ldots, X_n) at a random point (\\lambda_1, \\lambda_2, \\ldots, \\lambda_n), i.e.,P(\\lambda_1,  \\ldots, \\lambda_n),\n\nThis value can be obtained by folding a univariate polynomial f_{0}(X) of degree not exceeding 2^n - 1 corresponding to the multilinear polynomial P(X_1, X_2, \\ldots, X_n) using the same random numbers \\lambda_1,  \\ldots, \\lambda_n in the FRI protocol. After folding f_{0}(X) n times, we will get a constant c, and we want its value to be P(\\lambda_1,  \\ldots, \\lambda_n) = c.","type":"content","url":"/basefold/basefold-habock-overview#combining-with-sumcheck-protocol","position":5},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Combining with FRI Protocol","lvl2":"Basefold Protocol"},"type":"lvl3","url":"/basefold/basefold-habock-overview#combining-with-fri-protocol","position":6},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Combining with FRI Protocol","lvl2":"Basefold Protocol"},"content":"For the multilinear polynomial P(X_1, X_2, \\ldots, X_n), there is a univariate polynomial f_{0}(X) (called univariate representation in [H24]) corresponding to it:f_0(X) = \\sum_{i = 0}^{2^n - 1} P(i_1, \\ldots, i_n) \\cdot X^i\n\nwhere i_1, \\ldots, i_n is the binary representation of i, with i_1 being the least significant bit and i_n being the most significant bit.\n\nFor example, when n = 3, suppose the multilinear polynomial is:P(X_1, X_2, X_3) = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1X_2 + a_4 X_3 + a_5 X_1 X_3 + a_6 X_2 X_3 + a_7 X_1 X_2 X_3\n\nThen the univariate polynomial f_0(X) corresponding to P(X_1, X_2, X_3) is:\\begin{aligned}\n\tf_0(X) & = \\sum_{i = 0}^{7}P(i_1,i_{2},i_3) \\cdot X^i \\\\\n\t& = P(0, 0, 0) + P(1, 0, 0) X + P(0, 1, 0) X^2  + P(1,1,0) X^3  \\\\\n    & \\quad + P(0,0,1) X^4 + P(1,0,1) X^5 + P(0,1,1) X^6 +  P(1, 1, 1) X^{7} \\\\\n    & = (P(0,0,0) + P(0, 1, 0) X^2 + P(0,0,1) X^4) \\\\\n    & \\quad + X \\cdot (P(1, 0, 0) + P(1,1,0) X^2 + P(1,0,1) X^4 + P(1, 1, 1) X^{6}) \\\\\n    & = f_{0,0}(X^2) + X \\cdot f_{0,1}(X^2)\n\\end{aligned}\n\nHere, f_{0,0}(X^2) corresponds to the even terms of f_0(X), while f_{0,1}(X^2) corresponds to the odd terms. We can see that the coefficients in the even terms P(0,0,0) + P(0, 1, 0) X^2 + P(0,0,1) X^4 correspond to P(0, \\cdot, \\cdot) in the multilinear polynomial, while the coefficients in the odd terms correspond to P(1, \\cdot, \\cdot). In other words, f_{0,0}(X) is the univariate representation of P(0, X_2, X_3), and f_{0,1}(X) is the univariate representation of P(1, X_2, X_3), because:\\begin{aligned}\n    f_{0,0}(X) = \\sum_{i = 0}^{3}P(0,i_1,i_2) \\cdot X^i \\\\\n     f_{0,1}(X) = \\sum_{i = 0}^{3}P(1,i_1,i_2) \\cdot X^i \n\\end{aligned}\n\nUsing \\lambda_1 to fold f_{0,0}(X) and f_{0,1}(X), we get:f_1(X) = (1 - \\lambda_1) \\cdot f_{0,0}(X) + \\lambda_1 \\cdot f_{0,1}(X)\n\nAnd f_1(X) is precisely the univariate representation of P(\\lambda_1, X_2, X_3). Note that the folding method here is not the common one in the FRI protocol:f_1(X) = f_{0,0}(X) + \\lambda_1 \\cdot f_{0,1}(X)\n\nThis is because in this case, the resulting f_1(X) does not correspond to P(\\lambda_1, X_2, \\ldots, X_n). This is tied to the correspondence relationship between univariate polynomials and multilinear polynomials. Under this folding method, their correspondence relationship should change to (the WHIR paper [ACFY24] adopts this correspondence method):f_0(X) = P(X^{2^0}, X^{2^1}, \\ldots, X^{2^{n-1}})\n\nWe won’t elaborate here on how f_1(X) can correspond to P(\\lambda_1, X_2, \\ldots, X_n) under this correspondence relationship.\n\nReturning to the correspondence relationship between univariate polynomials and multilinear polynomials given in [H24], let’s now derive that f_1(X) obtained by folding f_0(X) with 1 - \\lambda_1 and \\lambda_1 indeed corresponds to P(\\lambda_1, X_2, X_3). For general P(X_1, \\ldots, X_n), we have:P(\\vec{X}) = \\sum_{\\vec{x} \\in H_n} P(\\vec{x}) \\cdot L(\\vec{x}, \\vec{X})\n\nTherefore,\\begin{aligned}\n\tP(\\lambda_1, X_2, \\ldots, X_n) & = \\sum_{\\vec{b} \\in H_n} P(\\vec{b}) \\cdot L(\\vec{b}, (\\lambda_1, X_2, \\ldots, X_n)) \\\\\n\t& = \\sum_{\\vec{b} \\in H_n} \\left(P(\\vec{b}) \\cdot \\left((1- b_1)(1 - \\lambda_1) + b_1 \\lambda_1\\right)\\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& = \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(0, \\vec{b}) \\cdot \\left((1- 0)(1 - \\lambda_1) + 0 \\cdot  \\lambda_1\\right)\\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& \\quad + \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(1, \\vec{b}) \\cdot \\left((1- 1)(1 - \\lambda_1) + 1 \\cdot \\lambda_1\\right)\\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& = \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(0, \\vec{b}) \\cdot (1 - \\lambda_1)\\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& \\quad + \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(1, \\vec{b}) \\cdot \\lambda_1 \\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& =  (1 - \\lambda_1) \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(0, \\vec{b}) \\cdot\\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& \\quad + \\lambda_1 \\sum_{\\vec{b} \\in H_{n-1}} \\left(P(1, \\vec{b}) \\cdot  \\prod_{i = 2}^n \\left[(1- b_i)(1 - X_i) + b_i X_i\\right] \\right) \\\\\n\t& = (1 - \\lambda_1) P(0, X_2, \\ldots, X_n) + \\lambda_1 P(1, X_2, \\ldots, X_n)\n\\end{aligned}\n\nThus, we have:\\begin{aligned}\n    f_1(X) & = (1 - \\lambda_1) \\cdot f_{0,0}(X) + \\lambda_1 \\cdot f_{0,1}(X) \\\\\n    & = (1 - \\lambda_1) \\cdot \\sum_{i = 0}^{3}P(0,i_1,i_2) \\cdot X^i + \\lambda_1 \\cdot \\sum_{i = 0}^{3}P(1,i_1,i_2) \\cdot X^i \\\\\n    & = \\sum_{i = 0}^{3}((1 - \\lambda_1)P(0,i_1,i_2) + \\lambda_1 P(1,i_1,i_2)) \\cdot X^i \\\\\n    & = \\sum_{i = 0}^{3}P(\\lambda_1, i_1, i_2) \\cdot X^i\n\\end{aligned}\n\nThis also shows that f_1(X) is the univariate representation of P(\\lambda_1, X_2, X_3). Then, by folding f_1(X) in this way using random numbers \\lambda_2, \\lambda_3, we finally get a constant polynomial whose value corresponds exactly to P(\\lambda_1, \\lambda_2, \\lambda_3). To summarize, the Basefold protocol performs the Sumcheck protocol on the multilinear polynomial using random numbers on one hand, and the FRI protocol on the corresponding univariate polynomial using the same random numbers on the other hand, thus achieving PCS for multilinear polynomials. This corresponds to [H24, Protocol 1], which is the Basefold protocol for Reed-Solomon codes. The overall protocol idea is as such, so we won’t repeat the specific protocol process here. See [H24, Protocol 1] for details.\n\n🐞 typo\nIn [H24, Protocol 1], during the Query phase, the paper states that the folding relationship to be checked is:>  f_{i+1}(x_{i+1}) = \\frac{f_0(x_i) + f_0(-x_i)}{2} + \\lambda_i \\cdot \\frac{f_0(x_i) + f_0(-x_i)}{2 \\cdot x_i}\n>\n\nHowever, based on the folding relationship given earlier, I believe it should be changed to:>  f_{i+1}(x_{i+1}) = (1 - \\lambda_i) \\cdot \\frac{f_0(x_i) + f_0(-x_i)}{2} + \\lambda_i \\cdot \\frac{f_0(x_i) - f_0(-x_i)}{2 \\cdot x_i}\n>\n\nThe soundness proof in [H24] is for a more general protocol, namely the batch version of the Basefold protocol.\n\nProtocol 2 [H24, Protocol 2] (Batch Reed-Solomon code Basefold). The prover shares the Reed-Solomon codewords g_0, \\ldots, g_M \\in \\mathcal{C}_0 = \\mathrm{RS}_{2^n}[F,D_0] = \\{q(x)|_{x \\in D_0}: q(x) \\in F[X]^{<2^n} \\} of the multilinears G_0, \\ldots, G_M , together with their evaluation claims v_0, \\ldots, v_M at \\vec{\\omega} \\in F^n with the verifier. Then they engage in the following extension of Protocol 1:\n\nIn a preceding round i = 0 , the verifier sends a random \\lambda_0 \\leftarrow \\$ F , and the prover answers with the oracle for> f_0 = \\sum_{k = 0}^{M} \\lambda_0^k \\cdot g_k \\tag{1} \n>\n\nThen both prover and verifier engage in Protocol 1 on f_0 and the claim v_0 = \\sum_{k = 0}^M \\lambda_0^k \\cdot v_k . In addition to the checks in Protocol 1, the verifier also checks that equation (1) holds at every sample x from D_0 .\n\nThe batch version of the Basefold protocol essentially uses a random number \\lambda_{0} to linearly combine g_{0}, \\ldots, g_{M} through its powers, transforming them into a function f_{0}, and then applies Protocol 1 to it.","type":"content","url":"/basefold/basefold-habock-overview#combining-with-fri-protocol","position":7},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Soundness Overview"},"type":"lvl2","url":"/basefold/basefold-habock-overview#soundness-overview","position":8},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Soundness Overview"},"content":"This section mainly analyzes the proof approach for the soundness error of Protocol 2. First, let’s explain the meaning of soundness error. For any potentially malicious Prover P^*, if there exists a g_{k} among the provided g_{0}, \\ldots, g_{M} that is more than \\theta distant from the Reed-Solomon encoding space \\mathcal{C}_0 ([H24] studies the proof under list decoding, so it considers the parameter \\theta \\in \\left( \\frac{1 - \\rho}{2}, 1 - \\sqrt{\\rho} \\right)), or if the multilinear representation P_k corresponding to g_k does not satisfy the evaluation claim P_{k}(\\vec{\\omega}) = v_{k}, under this condition, the probability that P^* passes the Verifier’s checks does not exceed \\varepsilon. This probability \\varepsilon is called the soundness error. In other words, the soundness error analyzes the probability that a malicious Prover P^* can luckily pass the Verifier’s checks. P^* might luckily pass the checks at places where randomness is introduced. Analyzing the protocol, we find three such places:\n\nCommit phase\n\nUsing the random number \\lambda_{0} to batch g_{0},\\ldots, g_{M}, let this probability be \\varepsilon_{C_1}.\n\nUsing random numbers \\lambda_1, \\ldots, \\lambda_n for the sumcheck protocol and FRI-like folding process, let this probability be \\varepsilon_{C_2}.\n\nQuery phase\n\nThe Verifier randomly selects x_0 \\leftarrow D_0 to check if the folding is correct, let this probability be \\varepsilon_{\\mathrm{query}}\n\nTherefore, the soundness error of the entire protocol is:\\varepsilon < \\varepsilon_{C_1} + \\varepsilon_{C_2} + \\varepsilon_{\\mathrm{query}}.","type":"content","url":"/basefold/basefold-habock-overview#soundness-overview","position":9},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Commit Phase","lvl2":"Soundness Overview"},"type":"lvl3","url":"/basefold/basefold-habock-overview#commit-phase","position":10},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Commit Phase","lvl2":"Soundness Overview"},"content":"Now let’s consider the case of folding f_0 into f_{\\lambda_1} using \\lambda_1 \\leftarrow \\$ F, i.e.,f_{\\lambda_1} = (1 - \\lambda_1) \\cdot f_{0,0} + \\lambda_1 \\cdot f_{0,1}\n\nAssume the given parameter \\theta = \\frac{1}{2}. Since \\lambda_1 is a random number selected from F, the following situation might occur:\n\nIn the figure, p_0, p_{0,0}, p_{0,1}, p_{\\lambda_1} are the closest codewords in the corresponding Reed-Solomon encoding space to f_0, f_{0,0}, f_{0,1}, f_{\\lambda_1} respectively. The same green color indicates that they have the same value at that point, while different colors indicate different values at that point. We can see that for the f_0 provided by the malicious Prover, its distance from the Reed-Solomon space \\mathcal{C}_0 is greater than \\theta = \\frac{1}{2}, but after folding with \\lambda_1, the resulting f_{\\lambda_1} might end up less than \\theta distant from the Reed-Solomon space \\mathcal{C}_1. This way, f_1 would pass the Verifier’s folding verification in the subsequent protocol, and P^* would successfully deceive the Verifier.\n\nSo what’s the probability of this situation occurring? It’s given by the Correlated Agreement theorem from [BCIKS20]. This theorem states that if\\Pr_{\\lambda_1 \\in F}[\\Delta((1 - \\lambda_1) f_{0,0} + \\lambda_1 f_{0,1}, \\mathcal{C}_1) \\le \\theta] > \\epsilon\n\nwhere \\epsilon is an expression related to \\theta, \\rho, |F|, |D_1|, which can also be written as \\epsilon(\\theta, \\rho, |F|, |D_1|), and its form differs under unique decoding and list decoding (this part will be explained in detail in the next section). In other words, if we take all possible \\lambda_1 in F to get f_{\\lambda_1}, and the proportion of those not exceeding \\theta in distance from \\mathcal{C}_1 is greater than \\epsilon, then there must exist a subset D' \\subset D_1 and codewords p_{0,0}, p_{0,1} in \\mathcal{C}_1 such that:\n\n|D'|/|D_1| \\ge 1 - \\theta,\n\nf_{0,0}|_{D'} = p_{0,0}|_{D'} and f_{0,1}|_{D'} = p_{0,1}|_{D'}.\n\nNow we can see that not only are f_{0,0} and f_{0,1} close to the encoding space \\mathcal{C}_1, but they also share the same set D' where they match the corresponding codewords. This is a good conclusion that can help us derive the distance of the original f_0 to \\mathcal{C}_0.\n\nThrough the mapping \\pi: x \\mapsto x^2, points in D_0 can be mapped to D_1. Now we can use \\pi^{-1} to map the points in D' \\subseteq D_1 back to D_0. For example, let \\omega^8 = 1 andD_0 = \\{\\omega^0, \\omega^1, \\omega^2, \\omega^3, \\omega^4, \\omega^5, \\omega^6, \\omega^7\\}\n\nThen through the mapping \\pi: x \\mapsto x^2, we can getD_1 = \\{\\omega^0, \\omega^2, \\omega^4, \\omega^6\\}\n\nSuppose D' = \\{\\omega^0, \\omega^2, \\omega^4 \\}, then we can get\\pi^{-1}(D') = \\{\\omega^0, \\omega^1, \\omega^2, \\omega^4, \\omega^5, \\omega^6 \\}\n\nAs shown in the following figure:\n\nNow, according to the correlated agreement theorem, we have f_{0,0}|_{D'} = p_{0,0}|_{D'} and f_{0,1}|_{D'} = p_{0,1}|_{D'}. Therefore, we can obtain the polynomial before folding based on p_{0,0} and p_{0,1}:p_0(X) = p_{0,0}(X^2) + X \\cdot p_{0,1}(X^2)\n\nWe can conclude that f_0(X) and p_0(X) have consistent values on \\pi^{-1}(D'), thus obtaining the distance of f_0(X) to the encoding space \\mathcal{C}_0:\\Delta(f_0, \\mathcal{C}_0) \\le \\frac{|\\pi^{-1}(D')|}{|D_0|} \\le \\theta\n\nThis also indicates that the Prover did not cheat, and the function f_0 is not more than \\theta distant from the corresponding encoding space. Returning to the initial question, we wanted to analyze the probability of a cheating Prover successfully deceiving the Verifier. Now the correlated agreement theorem tells us that except for a probability \\epsilon, we can ensure the Prover did not cheat, which also means that if the Prover cheats, the probability of successfully deceiving the Verifier will not exceed this probability \\epsilon.\n\nHave we finished analyzing the soundness error of the Commit phase? Reviewing the above analysis, we used the correlated agreement theorem to obtain the probability that the folded polynomial could deceive the Verifier due to the introduction of the folding random number \\lambda_1. However, one thing to remember is that the Basefold protocol not only checks if the FRI-like folding is correct but also simultaneously checks the sumcheck constraint. Therefore, the above analysis is not sufficient. Following the idea of the correlated agreement theorem, we add the sumcheck constraint on top of it. If there exists a polynomial p_{\\lambda_1} corresponding to P_{\\lambda_1} = P(\\lambda_1, X_2, \\ldots, X_n) that satisfies the sumcheck constraint, we want to obtain P_0 = P(0, X_2, \\ldots, X_n) and P_1 = P(1, X_2, \\ldots, X_n) corresponding to p_{0,0}(X) and p_{0,1}(X) that also satisfy the sumcheck constraint. This way, we can infer whether the sumcheck constraint is satisfied before folding.\n\nNow consider the sumcheck constraint. We know:\\langle L((\\lambda_1, \\cdot), \\vec{\\omega}), P_{\\lambda_1} \\rangle_{H_{n-1}} = q_0(\\lambda_1)\n\nWe want to obtain:\\langle L((0, \\cdot), \\vec{\\omega}), P_{0} \\rangle_{H_{n-1}} = q_0(0) \\tag{2}\\langle L((1, \\cdot), \\vec{\\omega}), P_{1} \\rangle_{H_{n-1}} = q_0(1) \\tag{3}\n\nIf equations (2) and (3) hold, since s_0 = q_0(0) + q_0(1), we can conclude that the multilinear polynomial P(X) corresponding to p_0(X) obtained from p_{0,0}(X) and q_{0,1}(X) satisfies the sumcheck constraint.\n\nFollowing the approach in Section 3.2 of [H24], we derive that equations (2) and (3) hold. Based on the relationship between q_i(X) and \\Lambda_i(X), we get:q_0(\\lambda_1) = L(\\lambda_1, \\omega_1)  \\cdot \\Lambda_0(\\lambda_1)\n\nAnd:\\langle L((\\lambda_1, \\cdot), \\vec{\\omega}), P_{\\lambda_1} \\rangle_{H_{n-1}}  = L(\\lambda_1, \\omega_1) \\cdot \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} \\rangle_{H_{n-1}}\n\nTherefore:L(\\lambda_1, \\omega_1) \\cdot \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} \\rangle_{H_{n-1}}  = L(\\lambda_1, \\omega_1)  \\cdot \\Lambda_0(\\lambda_1)\n\nSince L(X, \\omega_1) = (1 - X)(1 - \\omega_1) + X \\cdot \\omega_1 is a linear polynomial, L(X) has only one zero point in F. When \\lambda_1 takes this point, we have L(\\lambda_1, \\omega_1) = 0, and the above equation naturally holds. The probability of this happening is 1 / |F|. If L(\\lambda_1, \\omega_1) \\neq 0, then:\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} \\rangle_{H_{n-1}} = \\Lambda_0(\\lambda_1)\n\n[H24] gives:\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} - \\Lambda_0(\\lambda_1)  \\rangle_{H_{n-1}} = 0 \\tag{4}\n\nHere’s a detailed derivation: Since P_{\\lambda_1} = P(\\lambda_1, X_2, \\ldots, X_n), we have:\\begin{aligned}\n\t\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1}  \\rangle_{H_{n-1}} & = \\sum_{\\vec{x} \\in H_{n-1}} L(\\vec{x}, (\\omega_2, \\ldots, \\omega_n)) \\cdot P(\\lambda_1, \\vec{x}) \\\\\n\t& = P(\\lambda_1, \\omega_2, \\ldots, \\omega_n)\n\\end{aligned}\n\nSo the above equation becomes:P(\\lambda_1, \\omega_2, \\ldots, \\omega_n) = \\Lambda_0(\\lambda_1)\n\nLet a function be P'(X_2,\\ldots, X_n) = P(\\lambda_1, X_2, \\ldots, X_n) - \\Lambda_0(\\lambda_1), its evaluation at point (\\omega_2, \\ldots, \\omega_n) is P'(\\omega_2, \\ldots, \\omega_n) = 0, and P'(\\omega_2, \\ldots, \\omega_n) can be expressed as:\\begin{aligned}\n\tP'(\\omega_2, \\ldots, \\omega_n) & = \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P'(\\cdot)  \\rangle_{H_{n-1}} \\\\\n\t& = \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P(\\lambda_1, \\cdot) - \\Lambda_0(\\lambda_1)  \\rangle_{H_{n-1}} \\\\\n\t& = \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} - \\Lambda_0(\\lambda_1)  \\rangle_{H_{n-1}} \\\\\n\t& = 0\n\\end{aligned}\n\nTherefore:\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} - \\Lambda_0(\\lambda_1)  \\rangle_{H_{n-1}} = 0\n\nBy linearity, we know:\\Lambda_{0}(\\lambda_1) = (1 - \\lambda_1) \\cdot \\Lambda_{0}(0) + \\lambda_1 \\cdot \\Lambda_{0}(1) \\tag{5}\n\nAt the same time:f_{\\lambda_1} = (1 - \\lambda_1) \\cdot f_{0,0} + \\lambda_1 \\cdot f_{0,1} \\tag{6}\n\nSubtracting (5) from (6), we get:f_{\\lambda_1} - \\Lambda_{0}(\\lambda_1) = (1 - \\lambda_1) \\cdot (f_{0,0} - \\Lambda_{0}(0)) + \\lambda_1 \\cdot (f_{0,1} - \\Lambda_{0}(1))\n\nLet the new polynomial be:f_{\\lambda_1}' =  (1 - \\lambda_1) \\cdot (f_{0,0} - \\Lambda_{0}(0)) + \\lambda_1 \\cdot (f_{0,1} - \\Lambda_{0}(1))\n\nFollowing the idea of the correlated agreement theorem, according to the condition, if:\\Pr_{\\lambda_1 \\in F}[\\Delta((1 - \\lambda_1) f_{0,0} + \\lambda_1 f_{0,1}, \\mathcal{C}_1) \\le \\theta] > \\epsilon\n\nThat is, if f_{\\lambda_1} is not more than \\theta distant from p_{\\lambda_1} with a probability greater than a bound \\epsilon, then subtracting a number \\Lambda_0(\\lambda_1) from both of them does not affect the distance between them. Therefore, f_{\\lambda_1}' is not more than \\theta distant from p_{\\lambda_1}' = p_{\\lambda_1} - \\Lambda_0(\\lambda_1).\n\nWhich encoding space does p_{\\lambda_1}' = p_{\\lambda_1} - \\Lambda_0(\\lambda_1) belong to? We know that p_{\\lambda_1} \\in \\mathcal{P}_{n - 1} = F[X]^{<2^{n-1}}, and \\Lambda_0(\\lambda_1) is essentially a number, so p_{\\lambda_1}' is still in the \\mathcal{P}_{n - 1} space. At the same time, we have already derived:\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{\\lambda_1} - \\Lambda_0(\\lambda_1)  \\rangle_{H_{n-1}} = 0\n\nThis indicates that the multilinear polynomial corresponding to p_{\\lambda_1}' also satisfies such an inner product constraint. Therefore, we can say that p_{\\lambda_1}' is in a subspace of \\mathcal{P}_{n - 1}, namely:\\mathcal{P}_{n - 1}' = \\{u(X) \\in \\mathcal{P}_{n - 1}: \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), U \\rangle_{H_{n-1}} = 0 \\}\n\nIn the above equation, U is the multilinear polynomial corresponding to u(X). Such a polynomial subspace can form a linear subcode \\mathcal{C}_1' of the encoding space \\mathcal{C}_1. We can see that by extracting the linear term \\Lambda_i(X) from q_i(X) and adding a sumcheck-like constraint, we find that the encoding space to be considered is a linear subspace of the original encoding space.\n\nNow let’s summarize the conclusions we’ve reached so far. Let f_{0,0}' := f_{0,0} - \\Lambda_0(0), f_{0,1}' := f_{0,1} - \\Lambda_0(1), we have:f_{\\lambda_1}' = (1 - \\lambda_1) \\cdot f_{0,0}' + \\lambda_1 \\cdot f_{0,1}'\n\nAt the same time, f_{\\lambda_1}' is not more than \\theta distant from p_{\\lambda_1}' = p_{\\lambda_1} - \\Lambda_0(\\lambda_1) with a probability greater than \\epsilon, and p_{\\lambda_1}' \\in \\mathcal{P}_{n-1}', i.e.,\\Pr_{\\lambda_1 \\in F}[\\Delta((1 - \\lambda_1) f_{0,0}' + \\lambda_1 f_{0,1}', \\mathcal{C}_1') \\le \\theta] > \\epsilon\n\n[H24, Theorem 3] gives the correlated agreement theorem for linear subcodes, whose strict description will be introduced in the next section. The conclusion of this theorem states that there exist polynomials p_{0,0}' and p_{0,1}' from \\mathcal{P}_{n-1}', and D' \\subseteq D_1, satisfying:\n\n|D'|/|D_1| \\ge 1 - \\theta,\n\nf_{0,0}'|_{D'} = p_{0,0}'|_{D'} and f_{0,1}'|_{D'} = p_{0,1}'|_{D'}.\n\nHere, \\mathcal{P}_{n-1}' includes the sumcheck constraint. According to the definitions of f_{0,0}' and f_{0,1}', we can get:\\begin{aligned}\n\tf_{0,0} = f_{0,0}' + \\Lambda_0(0) \\\\\n\tf_{0,1} = f_{0,1}' + \\Lambda_0(1)\n\\end{aligned}\n\nSince \\Lambda_0(0) and \\Lambda_0(1) essentially represent numbers, according to conclusion 2, we have:\\begin{aligned}\n\tf_{0,0}(X)|_{D'} = p_{0,0}'(X)|_{D'} + \\Lambda_0(0) = (p_{0,0}'(X) + \\Lambda_0(0))|_{D'} \\\\\n\tf_{0,1}(X)|_{D'} = p_{0,1}'(X)|_{D'} + \\Lambda_0(1) = (p_{0,1}'(X) + \\Lambda_0(1))|_{D'} \\\\\n\\end{aligned}\n\nLet:\\begin{aligned}\n\tp_{0,0}(X) = p_{0,0}'(X) + \\Lambda_0(0) \\\\\n\tp_{0,1}(X) = p_{0,1}'(X) + \\Lambda_0(1)\n\\end{aligned}\n\nTherefore, f_{0,0}(X) and f_{0,1}(X) are consistent with p_{0,0}(X) and p_{0,1}(X) on D' respectively. From p_{0,0}(X) and p_{0,1}(X), we can obtain their corresponding multivariate polynomials P_0, P_1 \\in F[X_2, \\ldots, X_n]. Since p_{0,0}'(X), p_{0,1}'(X) \\in \\mathcal{P}_{n-1}', their corresponding multilinear polynomials P_{0,0}', P_{0,1}' satisfy:\\begin{aligned}\n\t\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{0,0}' \\rangle_{H_{n-1}} = 0 \\\\\n\t\\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{0,1}' \\rangle_{H_{n-1}} = 0 \\\\\n\\end{aligned}\n\nTherefore:\\begin{aligned}\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{0,0}' + \\Lambda_0(0) - \\Lambda_0(0) \\rangle_{H_{n-1}} = 0 \\\\\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_{0,1}' + \\Lambda_0(1) - \\Lambda_0(1) \\rangle_{H_{n-1}} = 0 \\\\\n\t\\Rightarrow \\qquad \\\\\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_0 - \\Lambda_0(0) \\rangle_{H_{n-1}} = 0 \\\\\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_1 - \\Lambda_0(1) \\rangle_{H_{n-1}} = 0 \\\\\n\t\\Rightarrow \\qquad \\\\\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_0 \\rangle_{H_{n-1}} =  \\Lambda_0(0) \\\\\n\t& \\langle L(\\cdot, \\omega_2, \\ldots, \\omega_n), P_1 \\rangle_{H_{n-1}} = \\Lambda_0(1) \n\\end{aligned}\n\nMultiplying both sides by L(0, \\omega_1), L(1, \\omega_1) respectively, and using q_0(X) = L(X, \\omega_1) \\cdot \\Lambda_0(X), we get:\\begin{aligned}\n\t\\langle L((0, \\cdot), \\vec{\\omega}), P_{0} \\rangle_{H_{n-1}} = q_0(0)\\\\\n\t\\langle L((1, \\cdot), \\vec{\\omega}), P_{1} \\rangle_{H_{n-1}} = q_0(1)\n\\end{aligned}\n\nThis also shows that equations (2) and (3) hold, which implies that P(X) corresponding to p_0(X) satisfies the sumcheck constraint.\n\nIn summary, the soundness error in the commit phase can be analyzed following the above approach. The specific probability is given by the correlated agreement theorem. [H24, Theorem 1] gives the soundness error in the commit phase as:\n\nBatching phase: \\varepsilon_{C_1} = \\varepsilon(\\mathcal{C}_0, M, 1, \\theta).\n\nSumcheck and FRI-like folding phase: \\varepsilon_{C_2} = \\sum_{i = 1}^n \\left(\\frac{1}{|F|} + \\varepsilon(\\mathcal{C}_i, 1, B_i, \\theta) \\right), where \\frac{1}{|F|} is the additional probability introduced when simplifying the sumcheck constraint to make L(X, \\omega_i) = 0.\n\nThe above \\varepsilon(\\mathcal{C}_i, M_i, B_i, \\theta) is given by the weighted correlated agreement theorem [H24, Theorem 4].","type":"content","url":"/basefold/basefold-habock-overview#commit-phase","position":11},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Query Phase","lvl2":"Soundness Overview"},"type":"lvl3","url":"/basefold/basefold-habock-overview#query-phase","position":12},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl3":"Query Phase","lvl2":"Soundness Overview"},"content":"For a malicious Prover P^*, now excluding the case where it can luckily pass the Verifier’s check in the Commit phase, after one folding, f_{\\lambda_1} will be \\theta far from \\mathcal{C}_1, or the sumcheck constraint will be incorrect.\n\nFor \\Delta(f_0, \\mathcal{C}_0) > \\theta, since the Verifier will randomly select an x_0 from D_0 to check if the folding is correct, if it queries those points where f_{\\lambda_1} and p_{\\lambda_1} are consistent on D_1, it will pass the check. This proportion does not exceed 1 - \\theta. If the query is repeated s times, then the probability of P^* luckily passing the check does not exceed (1 - \\theta)^s.\n\nFor the case where the sumcheck constraint is incorrect, the verifier will use the sumcheck protocol to check if the constraint is correct. Here, P^* cannot successfully cheat and will definitely be caught.\n\nIn summary, the soundness error in the query phase is \\varepsilon_{\\mathrm{query}} = (1 - \\theta)^s.\n\nTherefore, we obtain the soundness error of the entire protocol given by [H24, Theorem 1]:\\begin{aligned}\n\t\\varepsilon & < \\varepsilon_{C_1} + \\varepsilon_{C_2} + \\varepsilon_{\\mathrm{query}} \\\\\n\t& =  \\varepsilon(\\mathcal{C}_0, M, 1, \\theta) + \\sum_{i = 1}^n \\left(\\frac{1}{|F|} + \\varepsilon(\\mathcal{C}_i, 1, B_i, \\theta) \\right) + (1 - \\theta)^s\n\\end{aligned}\n\n🐞 typo\nI believe the condition \\theta = (1 + \\frac{1}{2m}) \\cdot \\sqrt{\\rho} given in [H24, Theorem 1] is incorrect. Based on the condition given in [H24, Theorem 4] later, it should be changed to \\theta = 1 - (1 + \\frac{1}{2m}) \\cdot \\sqrt{\\rho}.","type":"content","url":"/basefold/basefold-habock-overview#query-phase","position":13},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Correlated Agreement Theorems"},"type":"lvl2","url":"/basefold/basefold-habock-overview#correlated-agreement-theorems","position":14},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"Correlated Agreement Theorems"},"content":"This section introduces the correlated agreement theorem given by [BCIKS20], as well as the correlated agreement theorem for subcodes given by [H24] based on this.\n\nFirst is the correlated agreement theorem given by [BCIKS20], which includes two theorems: one for the unique decoding bound and one for reaching the Johnson bound under list decoding. Some symbols have been changed. Let F denote a finite field, \\mathcal{C} = RS_k[F, D] denote a Reed-Solomon code over F, with evaluation domain D and rate \\rho = k /|D|.\n\nTheorem 3 [BCIKS20, Theorem 6.1] Suppose \\theta \\leq \\frac{1 - \\rho}{2}. Let f_0, f_1, \\ldots, f_M \\in F^D be functions from D to F. If\\frac{|\\{ z \\in F : \\Delta(f_0 + z \\cdot f_1 + \\ldots + z^M \\cdot f_M, \\mathcal{C}) \\le \\theta \\}|}{|F|} > \\varepsilon\n\nwhere\\varepsilon = M \\cdot \\frac{|D|}{|F|}\n\nthen for any z \\in F, we have\\Delta(f_0 + z \\cdot f_1 + \\ldots + z^M \\cdot f_M, \\mathcal{C}) \\leq \\theta,\n\nMoreover, there exist p_0, \\ldots, p_M \\in \\mathcal{C} such that for all z \\in F,\\Delta(u_0 + zu_1 + \\cdots + z_l u_l, v_0 + zv_1 + \\cdots + z_l v_l) \\leq \\theta\n\nIn fact,| \\{ x \\in D : (u_0(x), \\ldots, u_l(x)) \\neq (v_0(x), \\ldots, v_l(x)) \\} | \\leq \\theta |D|.\n\nTheorem 4 [BCIKS20, Theorem 6.2] Let f_0, f_1, \\ldots, f_M \\in F^D be functions from D to F. Let m \\ge 3, define \\theta_0(\\rho, m) := 1 - \\sqrt{\\rho} \\cdot (1 + \\frac{1}{2m}), and let \\theta \\le \\theta_0(\\rho, m). If\\frac{|\\{ z \\in F : \\Delta(f_0 + z \\cdot f_1 + \\ldots + z^M \\cdot f_M, \\mathcal{C}) \\le \\theta \\}|}{|F|} > \\varepsilon\n\nwhere\\varepsilon = M \\cdot \\frac{(m+\\frac{1}{2})^7}{3 \\cdot \\rho^{3/2}} \\cdot \\frac{|D|^2}{|F|}\n\nthen f_0, f_1, \\ldots, f_M are simultaneously \\theta-close to \\mathcal{C}_0, i.e., there exist p_0, \\ldots, p_M \\in \\mathcal{C} such that| \\{ x \\in D : \\forall 0 \\leq i \\leq M, f_i(x) = p_i(x) \\} | \\geq (1 - \\theta) |D|.\n\nTheorem 3 and Theorem 4 give the correlated agreement theorems under unique decoding and list decoding, respectively. Although their formulations are somewhat different from those given in the previous section, they express the same meaning. Here, the specific expressions for \\varepsilon are given.\n\nIn [H24], by analyzing the Guruswami-Sudan list decoder in the proof of the correlated agreement theorem in [BCIKS20], a correlated agreement theorem for subcodes under list decoding is obtained.\n\nTheorem 5 [H24, Theorem 3] (Correlated Agreement for Subcodes) Let F be a finite field of arbitrary characteristic, \\mathcal{C} = RS_k[F, D] be a Reed-Solomon code over F with evaluation domain D and rate \\rho = k /|D|. Let \\mathcal{C}' be a linear subcode of \\mathcal{C}, generated by a subspace \\mathcal{P}' of polynomials from F[X]^{<k}. Given a proximity parameter \\theta = 1 - \\sqrt{\\rho} \\cdot \\left(1 + \\frac{1}{2m}\\right), where m \\geq 3, and f_0, f_1, \\ldots, f_M \\in F^D satisfying\\frac{|\\{ z \\in F : \\Delta(f_0 + z \\cdot f_1 + \\ldots + z^M \\cdot f_M, \\mathcal{C}') < \\theta \\}|}{|F|} > \\varepsilon,\n\nwhere\\varepsilon = M \\cdot \\frac{(m+\\frac{1}{2})^7}{3 \\cdot \\rho^{3/2}} \\cdot \\frac{|D|^2}{|F|} ,\n\nthen there exist polynomials p_0, p_1, \\ldots, p_M \\in \\mathcal{P}', and a set D' \\subseteq D, satisfying\n\n|D'|/|D| \\ge 1 - \\theta\n\nf_0, f_1, \\ldots, f_M are consistent with p_0, p_1, \\ldots, p_M respectively on D'.\n\nComparing Theorem 5 and Theorem 4, in terms of the expression of \\varepsilon, their forms can be said to be consistent. The difference is that Theorem 5 is considered in a linear subcode of the Reed-Solomon encoding space. It’s natural to conjecture that for unique decoding, there is also a similar result to Theorem 4 for subcodes.\n\nConjecture 6 Let F be a finite field of arbitrary characteristic, \\mathcal{C} = RS_k[F, D] be a Reed-Solomon code over F with evaluation domain D and rate \\rho = k /|D|. Let \\mathcal{C}' be a linear subcode of \\mathcal{C}, generated by a subspace \\mathcal{P}' of polynomials from F[X]^{<k}. Let \\theta \\le \\frac{1 - \\rho}{2}, and f_0, f_1, \\ldots, f_M \\in F^D satisfying\\frac{|\\{ z \\in F : \\Delta(f_0 + z \\cdot f_1 + \\ldots + z^M \\cdot f_M, \\mathcal{C}') < \\theta \\}|}{|F|} > \\varepsilon,\n\nwhere\\varepsilon = M \\cdot \\frac{|D|}{|F|}\n\nthen there exist polynomials p_0, p_1, \\ldots, p_M \\in \\mathcal{P}', and a set D' \\subseteq D, satisfying\n\n|D'|/|D| \\ge 1 - \\theta\n\nf_0, f_1, \\ldots, f_M are consistent with p_0, p_1, \\ldots, p_M respectively on D'.\n\nSimilar to the soundness proof of the batch FRI protocol in [BCIKS20], which used a weighted version of the correlated agreement theorem, [H24] also gives a weighted correlated agreement theorem for the batch Basefold protocol. According to the description in [H24], let’s first explain the meaning of “weighted”. Given a sub-probability measure \\mu on D and f \\in F^D, we write\\mathrm{agree}_{\\mu}(f, \\mathcal{C}') \\ge 1 - \\theta\n\nto mean that there exists a polynomial p(X) in \\mathcal{P}' such that \\mu(\\{x \\in D: f(x) = p(x)\\}) \\ge 1 - \\theta. This means using the measure \\mu to calculate the set of x in D that satisfy f(x) = p(x). For completeness, here’s the weighted correlated agreement theorem for list decoding given in [H24].\n\nTheorem 7 [H24, Theorem 4] (Weighted Correlated Agreement for Subcodes) Let C' be a linear subcode of RS_k[F,D], and choose \\theta=1-\\sqrt{\\rho}\\cdot\\left(1+\\frac{1}{2m}\\right), for some integer m\\geq3, where \\rho=k/|D|. Assume a density function \\delta:D\\to[0,1]\\cap\\mathbb{Q} with common denominator B\\geq1, i.e. for all x in D,\\delta(x)=\\frac{m_x}{B},\n\nfor an integer value m_x\\in[0,B], and let \\mu be the sub-probability measure with density \\delta, defined by \\mu(\\{x\\})=\\delta(x)/ |D|. If for f_0,f_1,\\ldots,f_M\\in F^D,\\frac{\\{z\\in F:\\text{agree}_\\mu(f_0+z\\cdot f_1+\\ldots+z^M\\cdot f_M,\\mathcal{C}')\\ge 1-\\theta\\}}{|F|} > \\varepsilon(\\mathcal{C},M,B,\\theta)\n\nwhere\\varepsilon(\\mathcal{C},M,B,\\theta)=\\frac{M}{|F|} \\cdot\\frac{(m + \\frac{1}{2})}{\\sqrt{\\rho}}\\cdot\\max\\left(\\frac{(m + \\frac{1}{2})^6}{3\\cdot\\rho}\\cdot|D|^2, 2\\cdot(B\\cdot|D|+1)\\right),\n\nthen there exist polynomials p_0(X),p_1(X),\\ldots,p_M(X) belonging to the subcode \\mathcal{C}', and a set A with \\mu(A)\\ge 1-\\theta on which f_0,f_1,\\ldots,f_M coincide with p_0(X),p_1(X),\\ldots,p_M(X), respectively.\n\nThe advantage of the weighted correlated agreement theorem is that in the process of proving the protocol’s soundness, \\mu can be defined by oneself, increasing flexibility. The details of the soundness proof for the Basefold protocol will be introduced in the next article.","type":"content","url":"/basefold/basefold-habock-overview#correlated-agreement-theorems","position":15},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-habock-overview#references","position":16},{"hierarchy":{"lvl1":"Note on Basefold’s Soundness Proof under List Decoding","lvl2":"References"},"content":"[H24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).\n\n[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[ACFY24] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. \"WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.\"Cryptology ePrint Archive(2024).","type":"content","url":"/basefold/basefold-habock-overview#references","position":17},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding"},"type":"lvl1","url":"/basefold/basefold-habock-soundness","position":0},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nIn the previous article “Overview of Basefold’s Soundness Proof under List Decoding”, we outlined the approach to the soundness proof in the [H24] paper. This article will delve deeper into the proof details following this approach, focusing mainly on the proof of [H24, Lemma 1], which demonstrates the soundness error of the Basefold protocol in the commit phase.\n\nLemma 1 [H24, Lemma 1] (Soundness commit phase). Take a proximity parameter \\theta=1-\\left(1 + \\frac{1}{2 \\cdot m}\\right) \\cdot \\sqrt{\\rho}, with m\\geq3. Suppose that a (possibly computationally unbounded) algorithm P^* succeeds the commitment phase with r\\geq0 rounds with probability larger than\\varepsilon_C=\\varepsilon_0+\\varepsilon_1+\\ldots+\\varepsilon_r,\n\nwhere \\varepsilon_0=\\varepsilon(\\mathcal{C}_i,M,\\theta) is the soundness error from Theorem 3, and\\varepsilon_i:=\\varepsilon(\\mathcal{C}_i,1,B_i,\\theta)+\\frac{1}{|F|},\n\nwith \\varepsilon(\\mathcal{C}_i,1,B_i,\\theta) being the soundness error from Theorem 4, where B_i=\\frac{|D|}{|D_i|}=2^i. Then (g_0,\\ldots,g_M) belongs to \\mathcal{R}.\n\n[H24, Theorem 3] mentioned in the lemma is the correlated agreement theorem for subcodes under list decoding, while [H24, Theorem 4] is the weighted version of [H24, Theorem 3].\n\nThe relation \\mathcal{R} implies that P^* has not cheated, indicating that the committed polynomials (g_{0}, \\ldots, g_{M}) are both within distance \\theta from the corresponding encoding space and consistent with the committed values v_0, \\ldots, v_M at the query point \\vec{\\omega} = (\\omega_1, \\ldots, \\omega_n), i.e.,\\mathcal{R}=\\left\\{\\begin{array}{c}\n\\exists p_0, \\ldots, p_M \\in \\mathscr{F}[X]^{<2^n} \\text { s.t. } \\\\\n\\left(g_0, \\ldots, g_M\\right): d\\left(\\left(g_0, \\ldots, g_M\\right),\\left(p_0, \\ldots, p_M\\right)\\right)<\\theta \\\\\n\\wedge \\bigwedge_{k=0}^M P_k\\left(\\omega_1, \\ldots, \\omega_n\\right)=v_k\n\\end{array}\\right\\}.\n\nLemma 1 states that if P^*'s success probability in the commit phase exceeds \\varepsilon_C, we can trust that P^* has not cheated, and the claimed relation \\mathcal{R} holds.\n\nHere, we need to mathematically define what it means for P^* to succeed in the 0 \\le r \\le n round of the commit phase. This is the concept of \\alpha-good given in the [H24] paper. From the protocol itself, P^*'s success means that the verifier receives f_0, \\Lambda_0, f_1, \\Lambda_1,  f_2, \\Lambda_2, \\ldots, \\Lambda_{r-1}, f_r from P^*, then performs checks: one is the sumcheck, and the other is randomly selecting x in D_0 to verify that the FRI folding is correct. First, the parameter \\alpha = 1 - \\theta \\in (0,1), i.e.,\\alpha = \\left(1 + \\frac{1}{2 \\cdot m}\\right) \\cdot \\sqrt{\\rho}\n\nLet \\mathcal{F}_i represent the polynomial space corresponding to the Reed-Solomon code \\mathcal{C}_i = \\mathrm{RS}_{2^{n-i}}[F, D_i], where D_i is the result of applying the mapping \\pi to D i times, i.e., D_i = \\pi^i(D), i = 0, \\ldots, n. Therefore, the polynomial subspace corresponding to \\mathcal{C}_i' \\subseteq \\mathcal{C}_i is defined as\\mathcal{F}_i' = \\left\\{p(X) \\in \\mathcal{F}_i: P(\\omega_{i + 1}, \\ldots, \\omega_n) = 0 \\right\\}.\n\nThe sumcheck is correct. This means there exists p_r(X) \\in \\mathcal{F}_r, with corresponding multivariate polynomial P_r satisfyingL((\\omega_1, \\ldots, \\omega_r), (\\lambda_1, \\ldots, \\lambda_r)) \\cdot P_r(\\omega_1, \\ldots, \\omega_n) = q_{r-1}(\\lambda_r)\n\nBased on the relationship between q_i(X) and \\Lambda_i(X), we can deduce that P_r needs to satisfy\\begin{aligned}\n        L((\\omega_1, \\ldots, \\omega_r), (\\lambda_1, \\ldots, \\lambda_r)) \\cdot & P_r(\\omega_{r + 1}, \\ldots, \\omega_n) = q_{r-1}(\\lambda_r) \\\\\n        & =  L((\\omega_1, \\ldots, \\omega_r), (\\lambda_1, \\ldots, \\lambda_r)) \\cdot \\Lambda_{r - 1}(\\lambda_r)\n    \\end{aligned} \\tag{1}\n\nThe folding is correct. It needs to satisfy\\left| \\left\\{ x \\in D_0 : \\quad \\begin{array}{c}\n        (f_0, \\ldots, f_r) \\text{ satisfy all folding checks along } x \\\\\n        \\wedge f_r(\\pi^r(x)) = p_r(\\pi^r(x))\n    \\end{array}\\right\\}\\right| \\ge \\alpha \\cdot |D_0| \\tag{2}\n\nHere, only when the proportion of x in D_0 satisfying the folding check is greater than \\alpha, after mapping through \\pi^r, will the verifier pass in the end.\n\nWhen conditions 1 and 2 are met, we say that such (f_0, \\Lambda_0, f_1, \\Lambda_1,  f_2, \\Lambda_2, \\ldots, \\Lambda_{r-1}, f_r) is \\alpha-good for (\\lambda_0, \\ldots, \\lambda_r).","type":"content","url":"/basefold/basefold-habock-soundness","position":1},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding","lvl2":"Proof of Lemma 1"},"type":"lvl2","url":"/basefold/basefold-habock-soundness#proof-of-lemma-1","position":2},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding","lvl2":"Proof of Lemma 1"},"content":"The proof of Lemma 1 uses mathematical induction. First, it proves that the conclusion holds when r = 0, using [H24, Theorem 3]. Then, assuming Lemma 1 holds for 0 \\le r < n, it proves that the conclusion also holds for r + 1. This process uses the weighted [H24, Theorem 4], following a similar approach to the one introduced in the previous article. For example, in the r + 1 round, starting with the conditions satisfied by f_{r + 1} obtained after folding with the random number \\lambda_{r + 1}, which is close to the corresponding encoding space and satisfies the sumcheck constraint, we first deduce that the corresponding f_{r + 1}' satisfies some conditions. This allows us to use the correlated agreement theorem for subcodes. Applying the theorem’s conclusion, we can then derive the properties satisfied by f_{r,0} and f_{r,1} before folding, and from this, deduce the properties satisfied by f_r. At this point, applying the induction hypothesis, we can obtain that the conditions of the lemma are satisfied in the r-th round, thus proving that the conclusion holds in the r-th round, which in turn proves that the lemma holds in the (r + 1)-th round.\n\nProof: First, prove that the lemma holds when r = 0. The given condition is that P^*'s success probability in the commit phase is greater than \\varepsilon(\\mathcal{C}_0,M,\\theta), and we want to prove that (g_1, \\ldots, g_M) \\in \\mathcal{R}. According to the condition and the definition of \\alpha-good, we can deduce that with a probability greater than \\varepsilon(\\mathcal{C}_0,M,\\theta), the f_0 provided by P^* is \\alpha-good for \\lambda_0. Then, considering the polynomials g_k' = g_k - v_k before folding, the probability that they are within distance \\theta from the corresponding subcode \\mathcal{C}_0' \\subseteq \\mathcal{C}_0 (which means the consistent part is greater than \\alpha) is\\Pr \\left[ \\lambda_0: \\exists p_0' \\in \\mathcal{F}_0' \\text{  s.t.  } \\mathrm{agree} \\left( \\sum_{k = 0}^{M} g_k' \\cdot \\lambda_0^k, p_0'(X) \\right) \\ge \\alpha  \\right] > \\varepsilon(\\mathcal{C}_0,M,\\theta)\n\nThe purpose of considering polynomials g_k' = g_k - v_k instead of g_k is to allow our analysis to enter the scope of the linear subcode \\mathcal{C}_0', so we can use [H24, Theorem 3] to obtain polynomialsp_0'(X), \\ldots, p_M'(X) \\in \\mathcal{F}_0'\n\nand a set D_0' \\subseteq D, satisfying\n\n|D_0'|/|D| \\ge \\alpha\n\np_k'(X)|_{D_0'} = g_k'(X)|_{D_0'}\n\nNow that we have found polynomials p_0'(X), \\ldots, p_M'(X), for polynomialsp_0'(X) + v_0, \\ldots, p_M'(X) + v_M \\in \\mathcal{F}_0\n\nthey satisfy(p_k'(X) + v_k)|_{D_0'} = (g_k'(X) + v_k)|_{D_0'} = g_k(X)|_{D_0'} \\quad 0 \\le k \\le M\n\nThe multilinear polynomial P_k \\in F[X_1, \\ldots, X_n] corresponding to p_0'(X) + v_0 also satisfies P_k(\\vec{\\omega}) = v_k, therefore (g_1, \\ldots, g_M) \\in \\mathcal{R}.\n\nNow assume the lemma holds for 0 \\le r < n, and we want to prove that it still holds for r + 1. According to the conditions of the lemma, in the (r + 1)-th round, P^*'s success probability in the commit phase exceeds (\\varepsilon_0 + \\varepsilon_1 + \\ldots + \\varepsilon_r) + \\varepsilon_{r + 1}. Let \\mathfrak{T} be the set composed of \\mathrm{tr}_r = (\\lambda_0, f_0, \\Lambda_0, \\ldots, \\lambda_r, f_r, \\Lambda_r). Therefore, under the condition\\operatorname{Pr}[\\mathfrak{T}]>\\varepsilon_0+\\ldots+\\varepsilon_r\n\nP^*'s success probability is greater than \\varepsilon_{r + 1}, i.e.,\\Pr \\left[ \\lambda_{r+1}: \n\\begin{array}{c}\n    \\exists f_{r + 1} \\text{  s.t.  } (\\lambda_0, f_0, \\Lambda_0, \\ldots, \\lambda_r, f_r, \\Lambda_r, f_{r + 1}) \\\\\n    \\text{is $\\alpha$-good for } (\\lambda_0, \\ldots, \\lambda_{r + 1})\n\\end{array}\n\\right] > \\varepsilon_{r + 1}\n\nFrom the definition of \\alpha-good, we can deduce that for \\lambda_{r + 1} satisfying \\alpha-good, there exists a polynomial p_{r + 1} \\in \\mathcal{F}_{r + 1} satisfying the sumcheck constraint, such that\\mathrm{agree}_{\\nu_r}((1 - \\lambda_{r + 1}) \\cdot f_{r,0} + \\lambda_{r + 1} \\cdot f_{r,1}, p_{r + 1}) \\ge \\alpha \\tag{3}\n\nHere, \\nu_r is a sub-probability measure with density function defined as, for y \\in D_{r + 1}\\delta_r(y) : = \\frac{|\\{x \\in \\pi^{-(r + 1)}(y): (f_0, \\ldots, f_r) \\text{ satisfies all folding checks along } x \\}|}{|\\pi^{-(r+1)}(y)|}\n\nHere’s an explanation of what equation (3) essentially represents: it’s equivalent to equation (2) in the definition of \\alpha-good. According to the definition of the \\mathrm{agree} function, equation (3) is equivalent to\\frac{\\nu_r(\\{ y \\in D_{r + 1}: ((1 - \\lambda_{r + 1}) \\cdot f_{r,0} + \\lambda_{r + 1} \\cdot f_{r,1})(y) =  p_{r + 1}(y)\\})}{|D_{r + 1}|} \\ge \\alpha\n\nFirst, let’s form a set S_{r + 1} consisting of y in D_{r + 1} that satisfy the folding relation, then calculate this set using the \\nu_r function.\\begin{aligned}\n    \\nu_r (S_{r + 1}) & = \\sum_{y \\in S_{r + 1}} \\delta_r(y)  \\\\\n    & = \\sum_{y \\in S_{r + 1}} \\frac{|\\{x \\in \\pi^{-(r + 1)}(y): (f_0, \\ldots, f_r) \\text{ satisfies all folding checks along } x \\}|}{|\\pi^{-(r+1)}(y)|}  \\\\\n    & = \\sum_{y \\in S_{r + 1}} \\frac{|\\{x \\in \\pi^{-(r + 1)}(y): (f_0, \\ldots, f_r) \\text{ satisfies all folding checks along } x \\}|}{2^{r + 1}}  \\\\\n    & := \\sum_{y \\in S_{r + 1}} \\frac{|S_{y,0}|}{2^{r + 1}} \\\\\n    & = \\frac{\\sum_{y \\in S_{r + 1}} |S_{y,0}|}{2^{r + 1}}\n\\end{aligned}\n\nTherefore\\begin{aligned}\n    \\mathrm{agree}_{\\nu_r}((1 - \\lambda_{r + 1}) \\cdot f_{r,0} + \\lambda_{r + 1} \\cdot f_{r,1}, p_{r + 1}) & = \\frac{\\nu_r(S_{r + 1})}{|D_{r + 1}|} \\\\\n    & = \\frac{\\sum_{y \\in S_{r + 1}} |S_{y,0}|}{2^{r + 1} \\cdot |D_{r + 1}|} \\\\\n    & = \\frac{\\sum_{y \\in S_{r + 1}} |S_{y,0}|}{|D_{0}|}\n\\end{aligned}\n\nThe numerator \\sum_{y \\in S_{r + 1}} |S_{y,0}| in the above equation represents the number of points in D_0 that satisfy the (r + 1)-th folding correctly, and also pass the folding checks for (f_0, \\ldots, f_r). Equation (3) becomes\\sum_{y \\in S_{r + 1}} |S_{y,0}| \\ge \\alpha \\cdot |D_{0}|\n\nThis is completely consistent with equation (2) in the definition of \\alpha-good. Next, following the soundness proof approach introduced in the previous article, since the multilinear polynomial P_{r+1} corresponding to p_{r+1}(X) satisfies the sumcheck constraint, it satisfies\\begin{aligned}\n    L((\\omega_1, \\ldots, \\omega_{r+1}), (\\lambda_1, \\ldots, \\lambda_{r + 1})) \\cdot & P_{r+1}(\\omega_{r + 2}, \\ldots, \\omega_n) = q_{r}(\\lambda_{r + 1}) \\\\\n    & =  L((\\omega_1, \\ldots, \\omega_{r + 1}), (\\lambda_1, \\ldots, \\lambda_{r + 1})) \\cdot \\Lambda_{r}(\\lambda_{r + 1})\n\\end{aligned}\n\nThis leads to\\begin{aligned}\n    L((\\omega_1, \\ldots, \\omega_{r}), (\\lambda_1, \\ldots, \\lambda_{r})) \\cdot L(\\omega_{r+1}, \\lambda_{r + 1}) \\cdot & P_{r+1}(\\omega_{r + 2}, \\ldots, \\omega_n)  \\\\\n    & =   L((\\omega_1, \\ldots, \\omega_{r}), (\\lambda_1, \\ldots, \\lambda_{r})) \\cdot L(\\omega_{r+1}, \\lambda_{r + 1}) \\cdot \\Lambda_{r}(\\lambda_{r + 1})\n\\end{aligned}\n\nFor the choice of \\lambda_{r + 1}, there is a 1/|F| probability that L(\\omega_{r+1}, \\lambda_{r + 1}) = 0, making the above equation hold. Therefore, with a probability exceeding\\varepsilon_{r + 1} - \\frac{1}{|F|} = \\varepsilon(\\mathcal{C}_{i + 1}, 1, B_{r + 1}, \\theta)\n\npolynomials p_{r+1}' = p_{r + 1} - \\Lambda_r(\\lambda_{r + 1}) \\in \\mathcal{F}_{r + 1}', and f_{r,0}' = f_{r,0} - \\Lambda_r(0), f_{r,1}' = f_{r,1} - \\Lambda_r(1) satisfy\\mathrm{agree}_{\\nu_r}((1 - \\lambda_{r + 1}) \\cdot f_{r,0}' + \\lambda_{r + 1} \\cdot f_{r,1}', p_{r + 1}') \\ge \\alpha\n\nThe above satisfied condition can be written as\\begin{aligned}\n    \\Pr \\left[ \\lambda_{r+1}: \\quad\n    \\begin{array}{c}\n        \\exists p_{r + 1}' \\in \\mathcal{F}_{r+1}' \\text{  s.t.  }  \\\\\n       \\mathrm{agree}_{\\nu_r}((1 - \\lambda_{r + 1}) \\cdot f_{r,0}' + \\lambda_{r + 1} \\cdot f_{r,1}', p_{r + 1}') \\ge \\alpha\n    \\end{array}\n    \\right] > \\varepsilon(\\mathcal{C}_{i + 1}, 1, B_{r + 1}, \\theta)\n\\end{aligned}\n\nThis also satisfies the conditions of the weighted correlated agreement theorem [H24, Theorem 4], so we can obtain polynomials p_{r,0}'(X), p_{r,1}'(X) \\in \\mathcal{F}_{r+1}', and a set A_{r + 1} \\subseteq D_{r+1} satisfying:\n\n\\nu_r(A_{r+1}) \\ge 1 - \\theta\n\np_{r,0}'(X)|_{A_{r+1}} = f_{r,0}'(X)|_{A_{r+1}}, p_{r,1}'(X)|_{A_{r+1}} = f_{r,1}'(X)|_{A_{r+1}}\n\nNow that we have found polynomials p_{r,0}'(X), p_{r,1}'(X), there exist polynomialsp_{r,0}(X) = p_{r,0}'(X) + \\Lambda_r(0), \\quad p_{r,1}(X) = p_{r,1}'(X) + \\Lambda_r(1) \\in \\mathcal{F}_{r+1}\n\nandf_{r,0}(X) = f_{r,0}'(X) + \\Lambda_r(0), \\quad f_{r,1}(X) = f_{r,1}'(X) + \\Lambda_r(1)\n\nAccording to conclusion 2 given by correlated agreement, we can getp_{r,0}(X)|_{A_{r+1}} = f_{r,0}(X)|_{A_{r+1}}, \\quad p_{r,1}(X)|_{A_{r+1}} = f_{r,1}(X)|_{A_{r+1}}\n\nFor the multilinear polynomials P_{r,0} and P_{r,1} corresponding to p_{r,0}(X), p_{r,1}(X), according to the definition of \\mathcal{F}_{r}', we can get\\begin{aligned}\n    P_{r,0}(\\omega_{r+2}, \\ldots, \\omega_{n}) = \\Lambda_r(0) \\\\\n    P_{r,1}(\\omega_{r+2}, \\ldots, \\omega_{n}) = \\Lambda_r(1)\n\\end{aligned}\n\nObtain A_r = \\pi^{-1}(A_{r+1}) \\subseteq D_r by inverse mapping the points in set A_{r+1} through \\pi. At these points, f_r must be consistent withp_r(X) = p_{r,0}(X^2) + X \\cdot p_{r,1}(X^2) \\in \\mathcal{F}_{r}\n\nFor the multilinear polynomial P_r corresponding to p_r(X), it satisfies\\begin{aligned}\n    P_r(\\omega_{r + 1}, \\omega_{r + 2}, \\ldots, \\omega_n) & = (1 - \\omega_{r + 1}) \\cdot P_{r,0}(\\omega_{r+2}, \\ldots, \\omega_{n}) + \\omega_{r + 1} \\cdot P_{r,1}(\\omega_{r+2}, \\ldots, \\omega_{n}) \\\\\n    & = (1 - \\omega_{r + 1}) \\cdot \\Lambda_r(0) + \\omega_{r + 1} \\cdot \\Lambda_r(1) \\\\\n    & = L(\\omega_{r + 1}, 0) \\cdot \\Lambda_r(0) +  L(\\omega_{r + 1}, 1) \\cdot \\Lambda_r(1)\n\\end{aligned}\n\nFrom this, we can conclude that the sumcheck in the r-th round is satisfied:\\begin{aligned}\n    L(\\omega_{1}, \\ldots, \\omega_{r}, \\lambda_1, \\ldots, \\lambda_r) &\\cdot P_r(\\omega_{r + 1}, \\omega_{r + 2}, \\ldots, \\omega_n)  \\\\\n    & = L(\\omega_{1}, \\ldots, \\omega_{r}, \\lambda_1, \\ldots, \\lambda_r) \\cdot L(\\omega_{r + 1}, 0) \\cdot \\Lambda_r(0) \\\\\n    & \\quad +  L(\\omega_{1}, \\ldots, \\omega_{r}, \\lambda_1, \\ldots, \\lambda_r) \\cdot L(\\omega_{r + 1}, 1) \\cdot \\Lambda_r(1) \\\\\n    & = q_r(0) + q_r(1) \\\\\n    & = q_{r - 1}(\\lambda_r)\n\\end{aligned}\n\nNow that we have obtained that the sumcheck in the r-th round is satisfied, we need to consider whether the folding relation is satisfied. Consider x \\in \\pi^{-1}(A_r), we have\\begin{aligned}\n    & \\frac{|\\{x \\in \\pi^{-r}(A_r): \\text{all folding checks hold for } f_0, \\ldots, f_r \\}|}{|D_0|} \\\\\n    & = \\frac{1}{|D_0|} \\cdot \\sum_{y \\in A_{r+1}} \\delta(y) \\cdot |\\pi^{-(r+1)}(y)| \\\\\n    & = \\frac{2^{r + 1}}{|D_0|} \\cdot \\sum_{y \\in A_{r+1}} \\delta(y) \\\\\n    & =  \\frac{1}{|D_{r + 1}|} \\cdot \\sum_{y \\in A_{r+1}} \\delta(y) \\\\\n    & = \\nu_r(A_{r+1})\n\\end{aligned}\n\nWe have already obtained \\nu_r(A_{r+1}) \\ge \\alpha through the correlated agreement theorem, so the proportion of x in D_0 that can satisfy the folding check exceeds \\alpha. Combining the sumcheck constraint and folding relation in the r-th round, we get that (f_0, \\Lambda_0, \\ldots, f_r, \\Lambda_r) is \\alpha-good for (\\lambda_0, \\ldots, \\lambda_r). Since the probability of generating such a trace set is\\operatorname{Pr}[\\mathfrak{T}]>\\varepsilon_0+\\ldots+\\varepsilon_r\n\nit satisfies the conditions of the lemma. By the induction hypothesis, the lemma holds in the r-th round, so we can conclude that (g_0, \\ldots, g_M) \\in \\mathcal{R}. This proves that the lemma also holds in the (r + 1)-th round. Thus, the lemma is proved. \n\n\\Box","type":"content","url":"/basefold/basefold-habock-soundness#proof-of-lemma-1","position":3},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-habock-soundness#references","position":4},{"hierarchy":{"lvl1":"Note on Soundness Proof of Basefold under List Decoding","lvl2":"References"},"content":"[H24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).","type":"content","url":"/basefold/basefold-habock-soundness#references","position":5},{"hierarchy":{"lvl1":"What does Sumcheck prove?"},"type":"lvl1","url":"/basefold/basefold-notebook","position":0},{"hierarchy":{"lvl1":"What does Sumcheck prove?"},"content":"","type":"content","url":"/basefold/basefold-notebook","position":1},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"What does Sumcheck prove?"},"type":"lvl2","url":"/basefold/basefold-notebook#what-does-sumcheck-prove","position":2},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"What does Sumcheck prove?"},"content":"For a polynomial f : 𝔽² → 𝔽 (here n = 3) given as a black box,the Prover wants to convince the Verifier thatS = \\sum_{x \\in \\{0,1\\}^3} f(x) \\quad (= 36 \\text{ in our example})\n\nwithout sending all 2ⁿ evaluations.Sumcheck does it in n rounds:\n\nRound i Prover sends a univariate degree-≤ d polynomial gᵢ.\n\nVerifier checks gᵢ(0)+gᵢ(1) = claimed_sum.\n\nVerifier samples a random field element rᵢ and sends it back.\n\nBoth parties restrict f to that slice and continue with n − i − 1 variables.\n\nSoundness error ≤ d/|𝔽| per round.  (We use plain integers for clarity.)\n\n\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom mpl_toolkits.mplot3d import Axes3D   \n\n\ndef bits(i, n):\n    \"Return i as an n-bit *list* (big-endian).\"\n    return list(map(int, format(i, f'0{n}b')))\n\ndef bits_reverse(i, n):\n    \"Little-endian version (least-significant bit first).\"\n    return bits(i, n)[::-1]\n\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\ndef bits_of(i: int, n: int = 3):\n    \"\"\"Return the n-bit little-endian tuple of i (e.g. 5 → (1,0,1)).\"\"\"\n    return tuple((i >> j) & 1 for j in range(n))\n\nf_vec = [1, 2, 3, 4, 5, 6, 7, 8]               # f(0,0,0) … f(1,1,1)\nprint(\"evaluations= \", f_vec)\n\nn = 3\nX0, X1, X2 = sp.symbols(f\"x0:{n}\")   # good\nX = (X0, X1, X2)              # (X0, X1, X2)\n\nf_tilde = sum(coeff * eq_tilde(bits(i, n), X)\n              for i, coeff in enumerate(f_vec))\n\n\nN = len(f_vec)\n\nprint(\"MLE polynomial in evaluation form:\\n\")\nfor i in range(N):\n    term = eq_tilde(bits(i, n), X)\n    print(f\"a[{i}] = {f_vec[i]} => term = {f_vec[i]}*({term})\")\n\nprint(\"\\nExpanded polynomial:\")\nsp.pretty_print(sp.expand(f_tilde))\n\ncoords  = [bits_of(i, 3) for i in range(8)]   \n\n# 3-D scatter plot\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\nxs, ys, zs = zip(*coords)\n\nax.scatter(xs, ys, zs, s=60)\n\n# annotate each vertex with its f-value\nfor (x0, x1, x2), val in zip(coords, f_vec):\n    ax.text(x0, x1, x2, f\"{val}\", fontsize=10, ha='center')\n\nax.set_xlabel(\"X₀\")\nax.set_ylabel(\"X₁\")\nax.set_zlabel(\"X₂\")\nax.set_title(\"Evaluation vector  f  on {0,1}³\")\n\nplt.show()\n\n# first-round h₁(X) polynomial\n\n# Even half  (X₀ = 0)\ng1_0 = sum(f_vec[idx] for idx in range(8) if bits_of(idx)[0] == 0)\n# Odd  half  (X₀ = 1)\ng1_1 = sum(f_vec[idx] for idx in range(8) if bits_of(idx)[0] == 1)\n\n# h₁(X) = a + b·X\na = g1_0\nb = g1_1 - g1_0\n\nprint(\"\\n── First-round Sumcheck polynomial  h₁(X) ──\")\nprint(f\"h₁(X) = {a}  +  {b}·X\")\nprint(f\"h₁(0) = {a}        (even-half sum)\")\nprint(f\"h₁(1) = {a + b}    (odd-half  sum)\")\nprint(f\"h₁(2) = {a + 2*b}  (third evaluation sent to the verifier)\")\n\n\n\n\nimport random\nrandom.seed(int(12345))        # deterministic for the demo\n\n# verifier samples α₀\nalpha0 = random.randint(2, 9)\nprint(f\"Verifier samples  α₀ = {alpha0}\")\n\n# fold the vector:  f¹(x₁,x₂) = f(α₀, x₁, x₂) \n# even half  = indices where X₀ = 0   (0,1,2,3)\n# odd  half  = indices where X₀ = 1   (4,5,6,7)\nf_even = f_vec[0:4]\nf_odd  = f_vec[4:8]\n\nf_fold = [(1 - alpha0) * e + alpha0 * o     # (1-α)·even + α·odd\n          for e, o in zip(f_even, f_odd)]\n\nprint(\"\\nFolded 4-point vector  f¹  on the (X₁,X₂) plane:\")\nfor idx, val in enumerate(f_fold):\n    print(f\"  ({bits_of(idx,2)[0]}, {bits_of(idx,2)[1]})  →  {val}\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Arrange f_fold = [f(0,0), f(1,0), f(0,1), f(1,1)] into a 2-by-2 matrix\ngrid = np.array([[f_fold[0], f_fold[1]],\n                 [f_fold[2], f_fold[3]]])\n\nfig, ax = plt.subplots(figsize=(4, 4))\nim = ax.imshow(grid,\n               cmap=\"Blues\",\n               origin=\"lower\",            # (0,0) in the lower-left corner\n               extent=[-0.5, 1.5, -0.5, 1.5],\n               vmin=grid.min(), vmax=grid.max())\n\n# Annotate each cell with its numeric value\nfor (i, j), val in np.ndenumerate(grid):\n    ax.text(j, i, f\"{val:.0f}\",\n            ha=\"center\", va=\"center\", color=\"black\", fontsize=14)\n\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xlabel(\"X₁\")\nax.set_ylabel(\"X₂\")\nax.set_title(\"Folded function  $f^{(1)}$  after Round 1\")\n\nplt.colorbar(im, shrink=0.75, label=\"value\")\nplt.show()\n\n\n# partial sums over X₁\ng2_0 = f_fold[0] + f_fold[1]          # X₁ = 0  (indices 00, 01)\ng2_1 = f_fold[2] + f_fold[3]          # X₁ = 1  (indices 10, 11)\n\na2 = g2_0\nb2 = g2_1 - g2_0                    \n\nprint(\"\\n── Second-round polynomial  h₂(X)  ──\")\nprint(f\"h₂(X) = {a2:.1f}  +  {b2:.1f}·X\")\nprint(f\"h₂(0) = {a2:.1f}\")\nprint(f\"h₂(1) = {a2 + b2:.1f}\")\nprint(f\"h₂(2) = {a2 + 2*b2:.1f}\")\n\nprint(f\"a2        = {a2}\")\nprint(f\"b2        = {b2}\")\nprint(f\"h₂(0)+h₂(1)  = {a2 + (a2 + b2)}\")\nprint(f\"Σ f_fold     = {sum(f_fold)}\")\n\n\n# verifier checks the sum relation\nassert abs((a2) + (a2 + b2) - sum(f_fold)) < 1e-9\nprint(\"Verifier check  h₂(0)+h₂(1)  =  Σ f¹(x)   ✓\")\n\n\n# sample α₁ \nalpha1 = random.randint(2, 9)\nprint(f\"\\nVerifier samples  α₁ = {alpha1}\")\n\n# fold again over X₁ \nf1_even = f_fold[0:2]                 # X₁ = 0 slice\nf1_odd  = f_fold[2:4]                 # X₁ = 1 slice\nf_fold2 = [(1 - alpha1) * e + alpha1 * o\n           for e, o in zip(f1_even, f1_odd)]     # length 2\n\nprint(\"Folded vector  f²  (depends only on X₂ now):\", f_fold2)\n\n# build h₃(X) over the last variable X₂ \ng3_0, g3_1 = f_fold2                  \na3 = g3_0\nb3 = g3_1 - g3_0\n\nprint(\"\\n── Third-round polynomial  h₃(X)  ──\")\nprint(f\"h₃(X) = {a3:.1f}  +  {b3:.1f}·X\")\nprint(f\"h₃(0) = {a3:.1f}\")\nprint(f\"h₃(1) = {a3 + b3:.1f}\")\nprint(f\"h₃(2) = {a3 + 2*b3:.1f}\")\n\n# final leaf opening \nalpha2 = random.randint(0, 1)       \nleaf   = f_fold2[alpha2]\nprint(f\"\\nVerifier picks  α₂ = {alpha2}   and asks for  f²(α₂) = {leaf:.1f}\")\nassert abs(leaf - (a3 + b3*alpha2)) < 1e-9\nprint(\"Leaf equals h₃(α₂)   ✓\")\n\nprint(\"\\n✅  Sumcheck for n = 3 completed – all rounds verified.\\n\")\n\n\n\nn = 3\nX = sp.symbols(f'X0:{n}')                       # (X0, X1, X2)\nf_tilde = sum(coeff * eq_tilde(bits(i, n), X)\n              for i, coeff in enumerate(f_vec))\n\n\n\n# 2. Evaluate it at the verifier’s random challenges\npoly_val = f_tilde.subs({X[0]: alpha0,\n                         X[1]: alpha1,\n                         X[2]: alpha2})\n\nprint(\"────────────────────────────────────────────\")\nprint(f\"f̃(α₀, α₁, α₂) = f̃({alpha0}, {alpha1}, {alpha2})  =  {poly_val}\")\nprint(f\"leaf opened in the final Sumcheck round         =  {leaf}\")\nprint(\"────────────────────────────────────────────\")\nassert poly_val == leaf\nprint(\"✅  They match – Sumcheck transcript is fully consistent.\\n\")\n\n\n","type":"content","url":"/basefold/basefold-notebook#what-does-sumcheck-prove","position":3},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"What’s still missing after Sumcheck?"},"type":"lvl2","url":"/basefold/basefold-notebook#whats-still-missing-after-sumcheck","position":4},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"What’s still missing after Sumcheck?"},"content":"At the end of the previous section the verifier is convinced that\\text{leaf} = \\sum_{b \\in \\{0,1\\}^3} \\text{f\\_vec}[b] \\cdot \\text{eq}(b, \\alpha)\n\n…but the verifier has never seen f_vec itself.\nA cheating prover could:\n\nRun Sumcheck with any fake vector f_fake.\n\nCompute leaf_fake = Σ f_fake[b]·eq(b, α) (easy because α is known).\n\nOpen that scalar in the last round.\n\nSumcheck would verify perfectly – yet the statement\n“leaf equals your polynomial at α” might be false.\nWe therefore need a separate mechanism that binds the prover to\na single multilinear polynomial before Sumcheck starts.\n\n#  Cheating prover with *same* total sum  S  but different vector\nimport random, math\n\n# Choose a fake vector with the same sum \nS_real = sum(f_vec)                        # 36 for [1…8]\n\nprint(\"Honest vector :\", f_vec)\nn = 3\nX = sp.symbols(f'X0:{n}')                       # (X0, X1, X2)\nf_tilde = sum(coeff * eq_tilde(bits(i, n), X)\n              for i, coeff in enumerate(f_vec))\n\n# 2. Evaluate it at the verifier’s random challenges\npoly_val = f_tilde.subs({X[0]: alpha0,\n                         X[1]: alpha1,\n                         X[2]: alpha2})\n\nprint(\"Expanded polynomial:\")\nsp.pretty_print(sp.expand(f_tilde))\nprint(\"\\n\")\nf_fake_same_sum = list(reversed(f_vec))    # [8,7,6,5,4,3,2,1]\ndelta = sum(f_fake_same_sum) - S_real      # here 36 - 36 = 0\n\n\n\nassert sum(f_fake_same_sum) == S_real and f_fake_same_sum != f_vec\n\nprint(\"Fake vector   :\", f_fake_same_sum, \"(same sum, different values)\")\n\nfake_tilde = sum(coeff * eq_tilde(bits(i, n), X)\n              for i, coeff in enumerate(f_fake_same_sum))\n\n# Evaluate it at the verifier’s random challenges\npoly_val = fake_tilde.subs({X[0]: alpha0,\n                         X[1]: alpha1,\n                         X[2]: alpha2})\n\nprint(\"Expanded polynomial:\")\nsp.pretty_print(sp.expand(fake_tilde))\nprint(\"\\n\")\n\n\nprint(\"Total sum S   :\", S_real, \"\\n\")\n\ndef line_triplet(v0, v1):\n    return (v0, v1, v0 + 2*(v1 - v0))\n\ndef sumcheck_transcript(vec, α0, α1):\n    g1_0, g1_1 = sum(vec[:4]), sum(vec[4:])\n    h1 = line_triplet(g1_0, g1_1)\n\n    even, odd = vec[:4], vec[4:]\n    f_fold = [(1-α0)*e + α0*o for e,o in zip(even, odd)]\n\n    g2_0, g2_1 = f_fold[0]+f_fold[1], f_fold[2]+f_fold[3]\n    h2 = line_triplet(g2_0, g2_1)\n\n    even2, odd2 = f_fold[:2], f_fold[2:]\n    f_fold2 = [(1-α1)*e + α1*o for e,o in zip(even2, odd2)]\n\n    g3_0, g3_1 = f_fold2\n    h3 = line_triplet(g3_0, g3_1)\n\n    α2   = random.randint(0, 1)     \n    leaf = eval_line(h3, α2) \n    return h1, h2, h3, α2, leaf\n\ndef eval_line(h, x):\n    return h[0] + (h[1]-h[0])*x\n\ndef verifier_accepts(h1,h2,h3, α0,α1,α2, leaf, S):\n    \"\"\"Return True iff all 4 Sumcheck equalities hold.\"\"\"\n    return (\n        math.isclose(h1[0]+h1[1], S) and\n        math.isclose(h2[0]+h2[1], eval_line(h1, α0)) and\n        math.isclose(h3[0]+h3[1], eval_line(h2, α1)) and\n        math.isclose(leaf,          h3[α2])\n    )\n\nh1_h, h2_h, h3_h, α2_h, leaf_h = sumcheck_transcript(f_vec,  alpha0, alpha1)\nh1_c, h2_c, h3_c, α2_c, leaf_c = sumcheck_transcript(f_fake_same_sum,\n                                                     alpha0, alpha1)\n\nprint(\"Verifier challenges: α0 =\", alpha0, \"α1 =\", alpha1, \"α2 =\", α2_c)\nprint(\"Honest leaf =\", leaf_h, \"   Fake leaf =\", leaf_c, \"\\n\")\n\ndef poly_str(h):\n    \"\"\"Return 'a + b·X' string for a degree-1 poly given (h0,h1, _).\"\"\"\n    a, b = h[0], h[1] - h[0]\n    return f\"{a:.1f}  +  {b:.1f}·X\"\n\ndef run_and_explain(tag, h1,h2,h3, α0,α1,α2, leaf, S):\n    print(f\"\\n===== Verifier run on {tag} transcript =====\")\n    print(\"Prover’s messages:\")\n    print(\"  h₁ values =\", h1, \"  ⇒  h₁(X) =\", poly_str(h1))\n    print(\"  h₂ values =\", h2, \"  ⇒  h₂(X) =\", poly_str(h2))\n    print(\"  h₃ values =\", h3, \"  ⇒  h₃(X) =\", poly_str(h3))\n    print(\"  leaf =\", leaf, \"\\n\")\n\n    # --- algebraic checks ----------------------------------\n    check1 = math.isclose(h1[0] + h1[1], S)\n    check2 = math.isclose(h2[0] + h2[1], eval_line(h1, α0))\n    check3 = math.isclose(h3[0] + h3[1], eval_line(h2, α1))\n    check4 = math.isclose(leaf        , h3[α2])\n\n    print(f\"①  h₁(0)+h₁(1) = {h1[0]+h1[1]:>8}    vs  S = {S:<8}        → {check1}\")\n    print(f\"②  h₂(0)+h₂(1) = {h2[0]+h2[1]:>8}    vs  h₁(α₀) = {eval_line(h1,α0):<8}  → {check2}\")\n    print(f\"③  h₃(0)+h₃(1) = {h3[0]+h3[1]:>8}    vs  h₂(α₁) = {eval_line(h2,α1):<8}  → {check3}\")\n    print(f\"④  leaf        = {leaf:>8}    vs  h₃(α₂) = {h3[α2]:<8}       → {check4}\")\n\n    all_pass = check1 and check2 and check3 and check4\n    print(\"Result:\", \"✔ PASSED\" if all_pass else \"✖ REJECTED\")\n    return all_pass\n\nok_honest = run_and_explain(\"HONEST\",\n                            h1_h,h2_h,h3_h,\n                            alpha0,alpha1,α2_h,\n                            leaf_h, S_real)\n\nok_fake   = run_and_explain(\"FAKE  \",\n                            h1_c,h2_c,h3_c,\n                            alpha0,alpha1,α2_c,\n                            leaf_c, S_real)   \nprint(\"Verifier result on HONEST transcript :\", ok_honest)\nprint(\"Verifier result on FAKE   transcript :\", ok_fake, \"\\n\")\n\ndef eval_mle(evals, u):\n    return sum(e*eq_tilde(bits_of(i,3), u) for i,e in enumerate(evals))\n\nu_test = (0,0,0)\nprint(f\"At point u = {u_test}:\")\nprint(\"  honest f̃(u) =\", eval_mle(f_vec, u_test))\nprint(\"  fake   f̃(u) =\", eval_mle(f_fake_same_sum, u_test),\n      \"← different but still passed Sumcheck!\")\n\n\n","type":"content","url":"/basefold/basefold-notebook#whats-still-missing-after-sumcheck","position":5},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Why add a (mock) PCS?"},"type":"lvl2","url":"/basefold/basefold-notebook#why-add-a-mock-pcs","position":6},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Why add a (mock) PCS?"},"content":"Problem: Sumcheck only checks algebra; the verifier never sees theevaluation vector f. A prover can swap in a different vectorf_fake after learning the challenges and still pass all four checks.\n\nPCS idea: Prover first sends a digest C = Hash(f).After Sumcheck it must open the same digest together with the claimed\nvalue f(α₀,α₁,α₂).\n\nSecurity:\n\nHonest → digest matches and value matches → verifier accepts.\n\nCheater → digest mismatch (or wrong value) → verifier rejects.\n\nThe tiny SHA-256 PCS below demonstrates this:pcs_open prints why it succeeds on the honest vector and fails on the\nfake one.\n\n\nimport hashlib, json, math\n\ndef _canonical(evals):\n    return [int(x) for x in evals]\n\ndef pcs_commit(evals):\n    blob = json.dumps(_canonical(evals), separators=(\",\", \":\")).encode()\n    return hashlib.sha256(blob).hexdigest()\n\ndef pcs_open(evals, point, claimed_val, commitment, *, verbose=True):\n    \"\"\"\n    Return (all_ok, digest_ok, value_ok)\n    • If verbose=True, print a one-line explanation whenever something fails.\n    \"\"\"\n    digest_ok = (pcs_commit(evals) == commitment)\n    if verbose and not digest_ok:\n        print(\"[PCS] digest mismatch \"\n              f\"(got {commitment[:12]}…, expected {pcs_commit(evals)[:12]}… )\")\n\n    poly_val = sum(int(e) * eq_tilde(bits_of(i, 3), point)\n                   for i, e in enumerate(evals))\n    value_ok = math.isclose(poly_val, claimed_val)\n    if verbose and digest_ok and not value_ok:\n        print(\"[PCS] value mismatch \"\n              f\"(claimed {claimed_val}, recomputed {poly_val})\")\n\n    if verbose and digest_ok and value_ok:\n        print(\"[PCS] ✔ digest & value both correct\")\n\n    return digest_ok and value_ok, digest_ok, value_ok\n\nprint(\" Honest evaluation vector  :\", f_vec)\nprint(\" Fake   evaluation vector  :\", f_fake_same_sum, \"(same Σ, different order)\\n\")\n\ncommit_honest = pcs_commit(f_vec)\ncommit_fake   = pcs_commit(f_fake_same_sum)\nprint(\"Honest commitment digest :\", commit_honest)\nprint(\"Fake   commitment digest :\", commit_fake)\nprint(\"   Digests equal ? \", commit_honest == commit_fake, \"\\n\")\n\nquery_pt = (alpha0, alpha1, α2_h)               \nleaf_h   = eval_mle(f_vec , query_pt)          \nleaf_f   = eval_mle(f_fake_same_sum, query_pt)        \n\n\nok_honest, dig_honest, val_honest = pcs_open(f_vec , query_pt, leaf_h, commit_honest)\nok_fake  , dig_fake , val_fake  = pcs_open(f_fake_same_sum, query_pt, leaf_f, commit_honest)\n\nprint(\"PCS-open on HONEST data :\", ok_honest)\nprint(\"PCS-open on FAKE  data  :\", ok_fake, \"\\n\",\n      \"← fails (digest mismatch)\")\n\n","type":"content","url":"/basefold/basefold-notebook#why-add-a-mock-pcs","position":7},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Fact 1 — “Evaluation = Sum”"},"type":"lvl2","url":"/basefold/basefold-notebook#fact-1-evaluation-sum","position":8},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Fact 1 — “Evaluation = Sum”"},"content":"For a multilinear-extension polynomial written in evaluation form:\\tilde{f}(\\mathbf{X}) = \\sum_{i \\in {0,1}^n} a_i,E_i(\\mathbf{X}), \\quad\nE_i(\\mathbf{X}) = \\prod_{j=0}^{n-1}\n\\left( \\mathrm{bits}(i)_j,X_j + (1 - \\mathrm{bits}(i)_j)(1 - X_j) \\right)\n\na single point-evaluation can be rewritten as an inner product:\\boxed{\n\\tilde{f}(\\mathbf{u}) = \\sum_{i \\in {0,1}^n} a_i,\\underbrace{E_i(\\mathbf{u})}{=,eq(\\mathbf{u}, i)}\n= \\left\\langle\n\\underbrace{(a_0, \\dots, a{2^n - 1})}{f{\\text{vec}}},\n\\underbrace{(eq(\\mathbf{u}, 0), \\dots, eq(\\mathbf{u}, 2^n - 1))}{w{\\text{vec}}}\n\\right\\rangle\n}•\tLeft-hand side — the usual “plug-and-chug” evaluation.\n•\tRight-hand side — a length-$2^n$ sum, exactly the form the Sumcheck protocol can certify.\n\n⸻\n\nIn the notebook below we will:\n\nbuild the equality-function vector w_{\\text{vec}} for the verifier’s random point \\mathbf{u} = (\\alpha_0, \\alpha_1, \\alpha_2);\n\ncompute the inner product \\langle f_{\\text{vec}}, w_{\\text{vec}} \\rangle;\n\nshow it equals the direct polynomial value computed earlier.\n\nThat makes the algebraic bridge between an MLE evaluation and a Sumcheck sum concrete.\n\n# Fact 1 : a single evaluation is an inner product \nu_point = (alpha0, alpha1, alpha2)                     \n\ntry:\n    _ = poly_honest      \nexcept NameError:\n    poly_honest = sp.expand(sum(\n        int(f) * eq_tilde(bits_of(i, 3), X)     \n        for i, f in enumerate(f_vec)\n    ))\n\nprint(\"\\nFull multilinear extension  f̃(X₀,X₁,X₂):\")\nsp.pretty_print(poly_honest)\n\n\n# Build equality-function vector  w  (length 8)\nw_vec = [eq_tilde(bits_of(i, 3), u_point) for i in range(8)]\n\n\nprint(\"Verifier’s point  u =\", u_point, \"\\n\")\nprint(\" i   bits       f_vec[i]      w[i]   contribution  f[i]·w[i]\")\nprint(\"───  ─────      ─────────     ─────  ────────────────────────\")\n\ncontribs = []\nfor i, (f, w) in enumerate(zip(f_vec, w_vec)):\n    c = f * w\n    contribs.append(c)\n    print(f\"{i:2}   {bits_of(i,3)}     {f:>2}        {w:>5}        {c:>6}\")\n\ninner_prod = sum(contribs)\npoly_val   = eval_mle(f_vec, u_point)                  # direct evaluation\n\nprint(\"\\nΣ contributions =\", inner_prod)\nprint(\"Direct polynomial value f̃(u) =\", poly_val)\n\nassert inner_prod == poly_val\nprint(\"\\n✔  f̃(u) equals the length-8 inner product ⟨f_vec , w_vec⟩.\")\n\n\n","type":"content","url":"/basefold/basefold-notebook#fact-1-evaluation-sum","position":9},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Fact #2 — Sumcheck fold ≙ FRI split-fold"},"type":"lvl2","url":"/basefold/basefold-notebook#fact-2-sumcheck-fold-fri-split-fold","position":10},{"hierarchy":{"lvl1":"What does Sumcheck prove?","lvl2":"Fact #2 — Sumcheck fold ≙ FRI split-fold"},"content":"At every round both protocols take a length-2m vector, cut it intoeven and odd halves, and compute:\\texttt{fold}(\\alpha) = (1 - \\alpha)\\,\\texttt{even} + \\alpha\\,\\texttt{odd}\n\nSumcheck uses it to reduce the multilinear inner-product dimension.\n\nFRI uses the same map to reduce a Reed–Solomon codeword.\n\nBelow we fold the honest evaluation vector two ways:\n\nwith the hand-written Sumcheck comprehension already in the notebook,\n\nuse folding code from basefold_rs_pcs.py, the helper the realFRI implementation calls.\n\nThey land on exactly the same 4-entry vector, confirming the equivalence.\n\n\n\n'''\nThe actual folding from the implementation itself : \n\n            # fold f_code\n            f_code_folded = [(Field(1)-alpha) * (f_code[2*j] + f_code[2*j+1]) / 2 \n                      + alpha * (f_code[2*j] - f_code[2*j+1]) / (2 * coset * twiddles[j]) \n                      for j in range(len(f_code)//2)]\n'''\n\ndef fri_split_fold(even, odd, alpha):\n    \"\"\"\n    Return (1-alpha)*even + alpha*odd   element-wise, exactly what FRI\n    does to halve a Reed–Solomon codeword.  Works for plain lists of\n    field elements as used in this demo.\n    \"\"\"\n    return [(1 - alpha) * e + alpha * o for e, o in zip(even, odd)]\n\n\neven = f_vec[:4]          # indices 000,001,010,011  (X₀=0)\nodd  = f_vec[4:]          # indices 100,101,110,111  (X₀=1)\nα    = alpha0             # verifier’s first challenge\n\nprint(\"even =\", even)\nprint(\"odd  =\", odd)\nprint(\"α =\", α, \"\\n\")\n\n# Sumcheck’s explicit comprehension\nsumcheck_fold = [(1-α)*e + α*o for e, o in zip(even, odd)]\nprint(\"Sumcheck fold :\", sumcheck_fold)\n\n# FRI’s comprehension\npcs = BASEFOLD_RS_PCS(MerkleTree, debug=0)\nfri_fold = fri_split_fold(even, odd, α)  \nprint(\"FRI      fold :\", fri_fold, \"\\n\")\n\nassert sumcheck_fold == fri_fold\nprint(\"✔  Identical output ⇒ both protocols share the very same linear map.\")\n\n\n","type":"content","url":"/basefold/basefold-notebook#fact-2-sumcheck-fold-fri-split-fold","position":11},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS"},"type":"lvl1","url":"/basefold/basefold-notebook#step-4-gluing-fri-sumcheck-into-one-pcs","position":12},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS"},"content":"Goal: prove the public statement f(u) = v without revealing thewhole evaluation vector.","type":"content","url":"/basefold/basefold-notebook#step-4-gluing-fri-sumcheck-into-one-pcs","position":13},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"1 · Re-phrase the claim"},"type":"lvl2","url":"/basefold/basefold-notebook#id-1-re-phrase-the-claim","position":14},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"1 · Re-phrase the claim"},"content":"Pick the same random challenges\\alpha = (\\alpha_0, \\dots, \\alpha_{n-1}) that will drive bothsub-protocols.\n\nLet w := f(\\alpha).\n\nNow proving f(u) = v is equivalent to proving these two facts:\n\nFRI-fact f(\\alpha) = w\n\nSumcheck-fact v = \\sum_b f(b) \\, eq(u, b)","type":"content","url":"/basefold/basefold-notebook#id-1-re-phrase-the-claim","position":15},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"2 · One transcript, two jobs"},"type":"lvl2","url":"/basefold/basefold-notebook#id-2-one-transcript-two-jobs","position":16},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"2 · One transcript, two jobs"},"content":"round\n\nprover message\n\nα used by\n\ncommit\n\nRS–codeword → Merkle root C_0\n\n—\n\n0\n\nsend h_1, fold with \\alpha_0\n\nFRI & Sumcheck\n\n1\n\nsend h_2, fold with \\alpha_1\n\nFRI & Sumcheck\n\n…\n\n…\n\n…\n\nn\n\nsend h_n; open one code symbol = w\n\nFRI constant & Sumcheck leaf\n\nFRI + Merkle paths ⇒ verifier accepts f(\\alpha) = w\n\nSumcheck, fed with the same \\alpha’s and that single scalarw, ⇒ verifier accepts \\sum f(b) \\, eq(u, b) = v⇒ Algebra then forces f(u) = v","type":"content","url":"/basefold/basefold-notebook#id-2-one-transcript-two-jobs","position":17},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"3 · Soundness intuition"},"type":"lvl2","url":"/basefold/basefold-notebook#id-3-soundness-intuition","position":18},{"hierarchy":{"lvl1":"Step 4 — Gluing FRI + Sumcheck into one PCS","lvl2":"3 · Soundness intuition"},"content":"Because every fold is checked twice (FRI and Sumcheck), a cheatingprover must satisfy both or be caught with probability\\approx 1 / |F| per round. The verifier inspects onlyO(\\log n) field elements.\n\nShared randomness = one proofFRI authenticates one hidden evaluation w;Sumcheck converts that single fact into the original claimf(u) = v.\n\n","type":"content","url":"/basefold/basefold-notebook#id-3-soundness-intuition","position":19},{"hierarchy":{"lvl1":"End-to-end picture for (n = 3)"},"type":"lvl1","url":"/basefold/basefold-notebook#end-to-end-picture-for-n-3","position":20},{"hierarchy":{"lvl1":"End-to-end picture for (n = 3)"},"content":"flowchart TD\n    %% Style tweaks\n    classDef b fill:#f0f0ff,stroke:#9aa;\n    classDef s fill:#fff8dc,stroke:#d9a;\n\n    %% Commit phase\n    A0([\"**8-entry eval&nbsp;vector**<br>f = [1..8]\"]):::b\n    A1([\"Reed–Solomon encode →<br>codeword c₀ (len = 8·ρ)\"]):::b\n    A2([\"Merkle-root<br>**C₀**\"]):::b\n\n    A0 -->|encode| A1 -->|Merkle| A2\n    subgraph G1[ Prover side ]\n    A0;A1;A2\n    end\n\n    %% Shared loop\n    B0((\"k = 3 rounds\")):::s\n\n    %% Round boxes\n    R1[\"Round 0<br>split even/odd<br>fold with α₀\"]\n    R2[\"Round 1<br>split even/odd<br>fold with α₁\"]\n    R3[\"Round 2<br>split even/odd<br>fold with α₂ (last)\"]\n\n    %% Folding arrows\n    A2 --> R1 --> R2 --> R3\n\n    %% Sumcheck track\n    subgraph Sumcheck[ Sumcheck track ]\n        direction LR\n        S8[\"f (8)\"]-->S4[\"f¹ (4)\"]-->S2[\"f² (2)\"]-->S1[\"leaf (1)\"]\n    end\n\n    %% FRI track\n    subgraph FRI[ FRI track ]\n        direction LR\n        C8[\"c₀ (8·ρ)\"]-->C4[\"c₁\"]-->C2[\"c₂\"]-->C1[\"constant w\"]\n    end\n\n    %% α arrows\n    R1 -- same α₀ --> C4 & S4\n    R2 -- same α₁ --> C2 & S2\n    R3 -- same α₂ --> C1 & S1\n\n    %% Verifier checks\n    V1[\"Merkle paths + FRI\\n⇒ accept f(α)=w?\"]:::b\n    V2[\"Sumcheck equalities\\n⇒ accept Σf eq(u,·)=v?\"]:::b\n    C1 --> V1\n    S1 --> V2\n    V1 --> V2\n\n    %% Final statement\n    V2 --> V3([\"✓ conclude  f(u)=v\"]):::b\n\n# Basefold PCS demo  (n = 3, f = [1…8]) \nimport basefold_rs_pcs as pcs_mod              \nfrom merlin.merlin_transcript import MerlinTranscript\nfrom utils import inner_product               \n\nField            = pcs_mod.Field              \nMerkleTree       = pcs_mod.MerkleTree\nBASEFOLD_RS_PCS  = pcs_mod.BASEFOLD_RS_PCS\nMLEPolynomial    = pcs_mod.MLEPolynomial\n\n# public instance \nevals   = [Field(i) for i in range(1, 9)]          # f = [1,2,3,4,5,6,7,8]\nu_point = [Field(alpha0), Field(alpha1), Field(alpha2)]   # the same α’s\n\nMLEPolynomial.set_field_type(Field)\nf_mle = MLEPolynomial(evals, 3)\n\n# v = f(u)\neq_vec = MLEPolynomial.eqs_over_hypercube(u_point)\nv_true = inner_product(evals, eq_vec, Field.zero())\nassert f_mle.evaluate(u_point) == v_true\n\nprint(f\"Public claim :  f({u_point}) = v = {v_true}\\n\")\n\n# set-up prover / verifier objects \npcs = BASEFOLD_RS_PCS(MerkleTree, debug=2)     \npcs.security_bits = 32 \ntr  = MerlinTranscript(b\"basefold-rs-demo\")\n\n# commitment\nf_cm = pcs.commit(f_mle)\nprint(\"Commitment root :\", f_cm.cm, \"…\\n\")\n\n# prover produces argument \nprint(\"🕐  Prover: generating proof …\")\nv_proved, argument = pcs.prove_eval(f_cm, f_mle, u_point, tr.fork(b\"prove\"))\nprint(\"ℹ️  Proof generated.\\n\")\nassert v_proved == v_true\n\n# verifier checks\nprint(\"🕐  Verifier: checking proof …\")\nok = pcs.verify_eval(f_cm, u_point, v_true, argument, tr.fork(b\"verify\"))\nprint(\"✅  Proof verified =\", ok)\n","type":"content","url":"/basefold/basefold-notebook#end-to-end-picture-for-n-3","position":21},{"hierarchy":{"lvl1":"Basefold Optimization"},"type":"lvl1","url":"/basefold/basefold-opt","position":0},{"hierarchy":{"lvl1":"Basefold Optimization"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nLast updated: 2025-06-10","type":"content","url":"/basefold/basefold-opt","position":1},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Protocol Review"},"type":"lvl2","url":"/basefold/basefold-opt#protocol-review","position":2},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Protocol Review"},"content":"For any multilinear polynomial \\tilde{f}(X_0, X_1,\\ldots, X_{n-1})\\in F^{\\leq 1}[X_0, X_1,\\ldots, X_{n-1}] with n variables (indeterminates), if we want to prove that its evaluation at an arbitrary point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}) is correct, the Basefold protocol [ZCF23] provides an elegant solution. Its core idea is to use the Sumcheck protocol to prove \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}), and then in the last step of the Sumcheck protocol, the Verifier needs to verify the evaluation of \\tilde{f}(r_0, r_1, \\ldots, r_{n-1}). Basefold does not rely on another MLE PCS to complete this final step of proof, but instead uses a FRI protocol to assist. The original purpose of the FRI protocol is to prove the proximity of a codeword, i.e., that its distance from the correct codeword does not exceed a security parameter \\delta. However, in the last step of the FRI protocol, the Prover sends the message corresponding to the folded codeword. If folded sufficiently, the final message is a constant (polynomial). If the Prover is honest, this value happens to be equal to the value of \\tilde{f}(r_0, r_1, \\ldots, r_{n-1}), which perfectly complements the Sumcheck situation.\n\nLet’s go through the protocol details. First, \\tilde{f}(\\vec{X}) can be rewritten as the following equation:\\tilde{f}(\\vec{X}) = \\sum_{\\vec{b}\\in\\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot eq(\\vec{X}, \\vec{b})\n\nWe can see that the right side of the equation is a sum, which meets the requirements of the Sumcheck protocol. The Sumcheck protocol can prove the following inner product equation:v = \\sum_{\\vec{b}\\in\\{0,1\\}^n} \\tilde{f}(\\vec{b}) \\cdot \\tilde{g}(\\vec{b})","type":"content","url":"/basefold/basefold-opt#protocol-review","position":3},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Inner Product Proof Based on Sumcheck Protocol","lvl2":"Protocol Review"},"type":"lvl3","url":"/basefold/basefold-opt#inner-product-proof-based-on-sumcheck-protocol","position":4},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Inner Product Proof Based on Sumcheck Protocol","lvl2":"Protocol Review"},"content":"Let’s review how to use the Sumcheck protocol to prove inner products. We assume that two vectors \\vec{f} and \\vec{g} of length 2^n correspond to two n-variable multilinear polynomials, \\tilde{f} and \\tilde{g}. Next, we explain how the Prover and Verifier prove the following goal:s = \\sum_{\\vec{b}\\in\\{0,1\\}^n} \\tilde{f}(b_0, b_1, \\ldots, b_{n-1}) \\cdot \\tilde{g}(b_0, b_1, \\ldots, b_{n-1})\n\nRound 1: The Prover constructs a degree-2 univariate polynomial h^{(0)}(X) and sends its evaluations at X=0,1,2, i.e., (h^{(0)}(0), h^{(0)}(1), h^{(0)}(2)).h^{(0)}(X) = \\sum_{\\vec{b}\\in\\{0,1\\}^{n-1}} \\tilde{f}(X, b_1, b_2, \\ldots, b_{n-1}) \\cdot \\tilde{g}(X, b_1, b_2, \\ldots, b_{n-1})\n\nThe Verifier checks if h^{(0)}(0)+h^{(0)}(1)\\overset{?}{=}v. If true, it responds with a random challenge r_0\\in\\mathbb{F}_q, and the proof goal is transformed into a new goal:h^{(0)}(r_0) = \\sum_{\\vec{b}\\in\\{0,1\\}^{n-1}} \\tilde{f}(r_0, b_1, b_2, \\ldots, b_{n-1}) \\cdot \\tilde{g}(r_0, b_1, b_2, \\ldots, b_{n-1})\n\nNote that \\tilde{f}(r_0, X_1, \\ldots, X_{n-1}) and \\tilde{g}(r_0, X_1, \\ldots, X_{n-1}) are still multilinear polynomials. We denote them as \\tilde{f}^{(1)}(X_1, \\ldots, X_{n-1}) and \\tilde{g}^{(1)}(X_1, \\ldots, X_{n-1}):\\begin{aligned}\n\\tilde{f}^{(1)}(X_1, \\ldots, X_{n-1}) &= \\tilde{f}(r_0, X_1, \\ldots, X_{n-1}) \\\\\n\\tilde{g}^{(1)}(X_1, \\ldots, X_{n-1}) &= \\tilde{g}(r_0, X_1, \\ldots, X_{n-1})\n\\end{aligned}\n\nSo the new proof goal can be written as:h^{(0)}(r_0) = v^{(1)} \\overset{?}{=} \\sum_{\\vec{b}\\in\\{0,1\\}^{n-1}} \\tilde{f}^{(1)}(b_1, \\ldots, b_{n-1}) \\cdot \\tilde{g}^{(1)}(b_1, \\ldots, b_{n-1})\n\nThis way, Sumcheck can be seen as a recursive protocol, with each call halving the length of the sum.\nThe Prover and Verifier repeat this process until the proof length of Sumcheck becomes 1.\n\nIn round n, the Prover constructs h^{(n-1)}(X) and sends (h^{(n-1)}(0), h^{(n-1)}(1), h^{(n-1)}(2)):h^{(n-1)}(X) = \\tilde{f}^{(n-1)}(X) \\cdot \\tilde{g}^{(n-1)}(X)\n\nThe Verifier checks if h^{(n-1)}(0)+h^{(n-1)}(1)\\overset{?}{=}v^{(n-1)}. If true, it responds with a random challenge r_{n-1}\\in\\mathbb{F}_q, and the proof goal is transformed into a new goal:h^{(n-1)}(r_{n-1}) \\overset{?}{=} \\tilde{f}^{(n-1)}(r_{n-1}) \\cdot \\tilde{g}^{(n-1)}(r_{n-1})\n\nThen the Prover doesn’t need to send any more messages. The Verifier directly calculates the value of h^{(n-1)}(r_{n-1}), then calls the oracles of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) and \\tilde{g}(X_0, X_1, \\ldots, X_{n-1}) to get the values of \\tilde{f}^{(n-1)}(r_{n-1}) and \\tilde{g}^{(n-1)}(r_{n-1}), and then verifies if the equation holds:\\begin{aligned}\nf^{(n-1)}(r_{n-1}) &= \\tilde{f}(r_0, r_1, \\ldots, r_{n-1}) \\\\\ng^{(n-1)}(r_{n-1}) &= \\tilde{g}(r_0, r_1, \\ldots, r_{n-1}) \\\\\n\\end{aligned}\n\nTypically, in the last step of Sumcheck, the oracles of \\tilde{f} and \\tilde{g} that the Verifier relies on are implemented using Polynomial Commitment. That is, before Sumcheck begins, the Prover first sends their commitments, and then in the last step of Sumcheck, the Prover sends the values of \\tilde{f}(r_0, r_1, \\ldots, r_{n-1}) and \\tilde{g}(r_0, r_1, \\ldots, r_{n-1}), along with the corresponding proofs of Multilinear Polynomial Evaluation.","type":"content","url":"/basefold/basefold-opt#inner-product-proof-based-on-sumcheck-protocol","position":5},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Proximity Proof Based on FRI Protocol","lvl2":"Protocol Review"},"type":"lvl3","url":"/basefold/basefold-opt#proximity-proof-based-on-fri-protocol","position":6},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Proximity Proof Based on FRI Protocol","lvl2":"Protocol Review"},"content":"TODO","type":"content","url":"/basefold/basefold-opt#proximity-proof-based-on-fri-protocol","position":7},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Basefold Proof Process Based on Multilinear Basis","lvl2":"Protocol Review"},"type":"lvl3","url":"/basefold/basefold-opt#basefold-proof-process-based-on-multilinear-basis","position":8},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Basefold Proof Process Based on Multilinear Basis","lvl2":"Protocol Review"},"content":"Let’s go through the Prover’s proof process from an implementation perspective. To help readers understand, we assume \\tilde{f} is a three-variable multilinear polynomial, and the protocol has a total of n=3 rounds. The Prover first does one round of Sumcheck, then reuses the random numbers from Sumcheck to do one round of FRI protocol, which is equivalent to completing one round of the Basefold protocol; then continues until the proof length of Sumcheck becomes 1.\\begin{aligned}\n v &= \\sum_{(b_0,b_1,b_2)\\in\\{0,1\\}^3} \\tilde{f}(b_0, b_1, b_2) \\cdot eq((u_0, u_1, u_2), (b_0, b_1, b_2)) \\\\\n &= \\sum_{i=0}^{7} a_i\\cdot w_i\n\\end{aligned}\n\nHere v= \\tilde{f}(u_0, u_1, u_2), vector \\vec{a}=(a_0, a_1, \\cdots, a_{7}) is the evaluation of \\tilde{f}(\\vec{X}) on the Boolean Hypercube, and \\vec{w}=(w_0, w_1, \\cdots, w_{7}) is the evaluation of eq(\\vec{u}, \\vec{X}) on the Boolean Hypercube:\\begin{aligned}\nw_0 &= (1-u_0)(1-u_1)(1-u_2) \\\\\nw_1 &= u_0(1-u_1)(1-u_2) \\\\\nw_2 &= (1-u_0)u_1(1-u_2) \\\\\nw_3 &= u_0u_1(1-u_2) \\\\\nw_4 &= (1-u_0)(1-u_1)u_2 \\\\\nw_5 &= u_0(1-u_1)u_2 \\\\\nw_6 &= (1-u_0)u_1u_2 \\\\\nw_7 &= u_0u_1u_2 \\\\\n\\end{aligned}\n\nThe Prover needs to prove the correctness of this inner product calculation. That is:v = a_0w_0 + a_1w_1 + a_2w_2 + a_3w_3 + a_4w_4 + a_5w_5 + a_6w_6 + a_7w_7\n\nThe Prover maintains two arrays of length 2^n, storing \\vec{a} and \\vec{w} respectively, corresponding to the Evaluations representations of \\tilde{f} and eq.\\begin{aligned}\na_i &= \\tilde{f}(i_0, i_1, i_2) \\\\\nw_i &= eq((u_0, u_1, u_2), (i_0, i_1, i_2))\n\\end{aligned}\n\nIn the first round of the Sumcheck protocol, the Prover calculates h^{(0)}(X). This is a degree-2 univariate polynomial. We only need to get its evaluations at at least three different points to uniquely represent this polynomial. To optimize computation, we specifically choose the points X=0,1,2.h^{(0)}(X) = \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot \\tilde{g}(X, b_1, b_2)\n\nAnd h^{(0)}(0) happens to be equal to the sum of the terms at even positions in the inner product sum, while h^{(0)}(1) happens to be equal to the sum of the terms at odd positions in the inner product sum, represented as follows:\\begin{aligned}\nh^{(0)}(0) &= a_0w_0 + a_2w_2 + a_4w_4 + a_6w_6  \\\\\nh^{(0)}(1) &= a_1w_1 + a_3w_3 + a_5w_5 + a_7w_7 \\\\\n\\end{aligned}\n\nTherefore, the Prover only needs two passes of 2^{n-1} multiplications and 2^{n-1} additions (a total of 2^n multiplications and 2^{n} additions) to calculate h^{(0)}(0) and h^{(0)}(1). But how to calculate h^{(0)}(2)? Let’s first prove the following equation, which we will use repeatedly:\\tilde{f}(X_0+1, X_1, X_2) = \\tilde{f}(X_0, X_1, X_2) + \n\\tilde{f}(1, X_1, X_2) - \\tilde{f}(0, X_1, X_2)\n\nGeneralizing this, we can get the following expression:\\tilde{f}(\\vec{Y}, X_k+1, \\vec{Z}) = \\tilde{f}(\\vec{Y}, X_k, \\vec{Z}) + \n\\tilde{f}(\\vec{Y}, 1, \\vec{Z}) - \\tilde{f}(\\vec{Y}, 0, \\vec{Z})\n\nThen, according to the properties of Multilinear Polynomials, we can calculate the value of h^{(0)}(2):\\begin{aligned}\nh^{(0)}(2) &= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(2, b_1, b_2) \\cdot \\tilde{w}(2, b_1, b_2)  \\\\\n&=  \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\Big( 2\\cdot\\tilde{f}(1, b_1, b_2) - \\tilde{f}(0, b_1, b_2) \\Big)\\cdot \\Big( 2\\cdot\\tilde{w}(1, b_1, b_2)  - \\tilde{w}(0, b_1, b_2) \\Big)\n\\end{aligned}\n\nThis requires the Prover to perform 2^{n-1} multiplications and 5\\cdot 2^{n-1} additions. Then the Prover sends h^{(0)}(0), h^{(0)}(1), h^{(0)}(2) to the Verifier, and the Verifier checks if the following equation holds:h^{(0)}(0)+h^{(0)}(1) \\overset{?}{=} v\n\nIf it holds, the Verifier replies with a random number r_0\\in\\mathbb{F}_q, and the proof goal is reduced to a new proof goal:\\begin{aligned}\nh^{(0)}(r_0) &= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(r_0, b_1, b_2) \\cdot \\tilde{w}(r_0, b_1, b_2) \\\\\n& = \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}^{(1)}(b_1,b_2) \\cdot \\tilde{w}^{(1)}(b_1,b_2)\n\\end{aligned}\n\nHere, the Evaluations representations of f^{(1)}(X_1, X_2) and w^{(1)}(X_1, X_2) on the two-dimensional Boolean Hypercube can actually be calculated directly from the Evaluations of \\tilde{f} and \\tilde{w}:\\begin{aligned}\na^{(1)}_0 &= \\tilde{f}^{(1)}(0, 0) = (1-r_0)\\cdot\\tilde{f}(0, 0, 0) + r_0\\cdot\\tilde{f}(1, 0, 0) \\\\\na^{(1)}_1 &= \\tilde{f}^{(1)}(0, 1) = (1-r_0)\\cdot\\tilde{f}(0, 0, 1) + r_0\\cdot\\tilde{f}(1, 0, 1) \\\\\na^{(1)}_2 &= \\tilde{f}^{(1)}(1, 0) = (1-r_0)\\cdot\\tilde{f}(0, 1, 0) + r_0\\cdot\\tilde{f}(1, 1, 0) \\\\\na^{(1)}_3 &= \\tilde{f}^{(1)}(1, 1) = (1-r_0)\\cdot\\tilde{f}(0, 1, 1) + r_0\\cdot\\tilde{f}(1, 1, 1) \\\\\n\\end{aligned}\n\nSo this requires the Prover to perform 2^{n-1} multiplications and 2^n additions to get the Evaluations representation of f^{(1)}(X_1, X_2), recorded in (a^{(1)}_0, a^{(1)}_1, a^{(1)}_2, a^{(1)}_3):\\begin{aligned}\na^{(1)}_0 &= (1-r_0)\\cdot a_0 + r_0\\cdot a_1 \\\\\na^{(1)}_1 &= (1-r_0)\\cdot a_2 + r_0\\cdot a_3 \\\\\na^{(1)}_2 &= (1-r_0)\\cdot a_4 + r_0\\cdot a_5 \\\\\na^{(1)}_3 &= (1-r_0)\\cdot a_6 + r_0\\cdot a_7 \\\\\n\\end{aligned}\n\nIn addition, the Prover also needs to calculate the Evaluations representation of \\tilde{w}^{(1)}(X_1,X_2) in the same way (2^{n-1} multiplications and 2^n additions):\\begin{aligned}\nw^{(1)}_0 &= \\tilde{w}^{(1)}(0, 0) = (1-r_0)\\cdot\\tilde{w}(0, 0, 0) + r_0\\cdot\\tilde{w}(1, 0, 0) \\\\\nw^{(1)}_1 &= \\tilde{w}^{(1)}(0, 1) = (1-r_0)\\cdot\\tilde{w}(0, 0, 1) + r_0\\cdot\\tilde{w}(1, 0, 1) \\\\\nw^{(1)}_2 &= \\tilde{w}^{(1)}(1, 0) = (1-r_0)\\cdot\\tilde{w}(0, 1, 0) + r_0\\cdot\\tilde{w}(1, 1, 0) \\\\\nw^{(1)}_3 &= \\tilde{w}^{(1)}(1, 1) = (1-r_0)\\cdot\\tilde{w}(0, 1, 1) + r_0\\cdot\\tilde{w}(1, 1, 1) \\\\\n\\end{aligned}\n\nThese two parts of folding calculations for Multilinear Polynomials together require a total of 2^n multiplications and 2\\cdot 2^n additions. The calculation results are stored in two arrays \\vec{a}^{(1)} and \\vec{w}^{(1)} of length 2^{n-1}.\n\nNext, the Prover and Verifier complete the first round of the FRI protocol (Commit-phase), which is to calculate the RS Code of the folded univariate polynomial. Below is the univariate polynomial \\hat{f}(X) corresponding to \\tilde{f}(X_0, X_1, X_2). Please note that its coefficient form corresponds to the Evaluations representation of \\tilde{f}(X), which is different from the form in the Basefold paper [ZCF23]:\\hat{f}(X) = a_0 + a_1X + a_2X^2 + a_3X^3 + a_4X^4 + a_5X^5 + a_6X^6 + a_7X^7\n\nThe Prover has already calculated the Evaluations representation of the folded \\tilde{f}^{(1)}(X_1, X_2), i.e., \\vec{a}^{(1)}, in the first round of Sumcheck. It also corresponds to a univariate polynomial \\hat{f}^{(1)}(X), whose coefficient form is:\\begin{aligned}\n\\hat{f}^{(1)}(X) &= a^{(1)}_0 + a^{(1)}_1X + a^{(1)}_2X^2 + a^{(1)}_3X^3\n\\end{aligned}\n\nNext, the Prover needs to calculate the RS Code of \\tilde{f}^{(1)}(X). If calculated directly, this would require the Prover to perform (n2^{n-1}) multiplications. Fortunately, since RS Code is a “linear code”, the Prover can directly fold on the RS Code of \\hat{f}(X) using (1-r_0, r_0) to obtain the RS Code of \\hat{f}^{(1)}(X).\n\nLet’s first review the familiar polynomial decomposition of \\hat{f}(X):\\begin{aligned}\n\\hat{f}(X) &= f_e(X^2) + X\\cdot f_o(X^2) \\\\\n\\hat{f}(-X) &= f_e(X^2) - X\\cdot f_o(X^2)\n\\end{aligned}\n\nWhere f_e(X) is the polynomial composed of even terms, and f_o(X) is the polynomial composed of odd terms:\\begin{aligned}\nf_e(X) &= a_0 + a_2X + a_4X^2 + a_6X^3 \\\\\nf_o(X) &= a_1 + a_3X + a_5X^2 + a_7X^3\n\\end{aligned}\n\nAlso because\\begin{aligned}\n\\hat{f}^{(1)}(X^2) &= (1-r_0)\\cdot f_e(X^2) + r_0\\cdot f_o(X^2) \\\\\n& = (1-r_0)\\cdot\\frac{\\hat{f}(X)+\\hat{f}(-X)}{2} + r_0\\cdot\\frac{\\hat{f}(X)-\\hat{f}(-X)}{2}\n\\end{aligned}\n\nTherefore, the Prover only needs to perform the same folding operation on the RS Code of \\hat{f}(X) to obtain the RS Code of \\hat{f}^{(1)}(X):\\mathsf{encode}(\\hat{f}^{(1)})_i = (1-r_0)\\cdot\\mathsf{encode}(\\hat{f})_i + r_0\\cdot\\mathsf{encode}(\\hat{f})_{(i+N/2)}, \\quad i\\in[0, N)\n\nHere, N is the length after encoding a message of length 2^n, where \\rho=2^n/N is the code rate.\n\nAt this point, the Prover sends the Merkle Root of \\mathsf{encode}(\\hat{f}^{(1)}) as a commitment, then the Prover and Verifier proceed to the second round of Sumcheck, repeating the above process until the sum length of the Sumcheck protocol becomes 1. At this time, the accompanying FRI protocol has also folded the polynomial to a constant polynomial \\hat{f}^{(3)}(X). If the Prover is honest, then the constant term of \\hat{f}^{(3)}(X) happens to be \\tilde{f}(r_0, r_1, r_2):\\hat{f}^{(3)}(X) = \\tilde{f}(r_0, r_1, r_2)\n\nThis value can be used by the Verifier for verification:v^{(3)} \\overset{?}{=} \\hat{f}^{(3)}(0)\\cdot \\tilde{eq}((u_0, u_1, u_2), (r_0, r_1, r_2))\n\nAt this point, the Verifier needs to calculate \\tilde{eq}((u_0, u_1, u_2), (r_0, r_1, r_2)) to complete the final verification step, which requires 2n multiplications:\\begin{aligned}\n\\tilde{eq}((u_0, u_1, u_2), (r_0, r_1, r_2)) &= \\prod_{i=0}^{2} \\Big( (1-r_i)\\cdot(1-u_i) + r_i\\cdot u_i \\Big) \\\\\n&= \\prod_{i=0}^{2} \\Big( 1 + 2r_i\\cdot u_i - r_i -u_i \\Big)\n\\end{aligned}\n\nFor the Query-phase part of FRI, we omit it here.","type":"content","url":"/basefold/basefold-opt#basefold-proof-process-based-on-multilinear-basis","position":9},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Sumcheck Performance Analysis","lvl2":"Protocol Review"},"type":"lvl3","url":"/basefold/basefold-opt#sumcheck-performance-analysis","position":10},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Sumcheck Performance Analysis","lvl2":"Protocol Review"},"content":"In the Sumcheck protocol part, the Prover’s total computation is:\n\nCalculate \\vec{w}: 2^n multiplications\n\nCalculate h^{i}(0): 2^n multiplications, 2\\cdot 2^n additions\n\nCalculate h^{i}(1): 2^n multiplications, 2\\cdot 2^n additions\n\nCalculate h^{i}(2): 2^{n} multiplications and 5\\cdot 2^{n} additions\n\nFolding calculation of \\vec{a}: 2^n multiplications and 2\\cdot 2^n additions\n\nFolding calculation of \\vec{w}: 2^n multiplications and 2\\cdot 2^n additions\n\nVerifier’s computation:\n\nVerify h^{(i)}(0)+h^{(i)}(1)\\overset{?}{=}h^{(i-1)}(r_{i-1}): n additions\n\nInterpolate to calculate h^{(i)}(r_i): 4n divisions, 3n multiplications and 9n additions\n\nCalculate \\tilde{eq}(r_0, r_1, \\cdots, r_{n-1}): n multiplications, 4n additions\n\nFinal verification: 1 multiplication","type":"content","url":"/basefold/basefold-opt#sumcheck-performance-analysis","position":11},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Sumcheck Optimization"},"type":"lvl2","url":"/basefold/basefold-opt#sumcheck-optimization","position":12},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Sumcheck Optimization"},"content":"In the above protocol, the Prover needs to send three degree-2 polynomials h^{(0)}(X), h^{(1)}(X), h^{(2)}(X). The most straightforward implementation is for the Prover to send the evaluations of these polynomials at X=0,1,2. The Verifier checks the first two values, then uses the third value to calculate Lagrange Interpolation to get h^{(0)}(r_0).\n\nWe can slightly modify the above protocol to allow the Prover to only send linear polynomials of degree 1. Then the Prover only needs to send the evaluations at two points, and the Verifier only needs to do linear interpolation to calculate the next sum value.\n\nHabock gave an optimized version of the Basefold protocol in [Hab24], which can make h(X) a linear polynomial of degree 1. This optimization technique first appeared in [Gruen24]. Let’s briefly describe this optimization technique.\n\nAccording to the definition of \\tilde{eq}(\\vec{X}, \\vec{Y}), it can be decomposed as:\\tilde{eq}(\\vec{X}_0\\parallel\\vec{X}_1, \\vec{Y}_0\\parallel\\vec{Y}_1) = eq(\\vec{X}_0, \\vec{Y}_0) \\cdot eq(\\vec{X}_1, \\vec{Y}_1)\n\nLet’s first observe the definition of h^{(0)}(X):h^{(0)}(X) = \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot \\tilde{eq}((u_0, u_1, u_2), (X, b_1, b_2))\n\nThe right side of its equation can be rewritten as:\\begin{aligned}\nh^{(0)}(X) &= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot \\tilde{eq}((u_0, u_1, u_2), (X, b_1, b_2)) \\\\\n&= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot eq(u_0, X) \\cdot eq((u_1, u_2), (b_1, b_2)) \\\\\n&= eq(u_0, X) \\cdot \\sum_{(b_1,b_2)\\in\\{0,1\\}^2`}  \\Big( \\tilde{f}(X, b_1, b_2) \\cdot eq((u_1,u_2), (b_1, b_2)) \\Big) \\\\\n\\end{aligned}\n\nWe introduce the notation g^{(0)}(X) to represent the linear polynomial on the right side of the equation except for eq(u_0, X), then h^{(0)}(X) can be represented as the product of two linear polynomials:h^{(0)}(X) = eq(u_0, X) \\cdot g^{(0)}(X)\n\nWe modify the protocol as follows: let the Prover not directly send h_0(X), but send the linear polynomial g_0(X), i.e., g^{(0)}(0), g^{(0)}(1),g^{(0)}(X) = \\sum_{(b_1,b_2)\\in\\{0,1\\}^2} \\Big( \\tilde{f}(X, b_1, b_2) \\cdot eq((u_1,u_2), (b_1, b_2)) \\Big)\n\nAfter receiving the polynomial g^{(0)}(0), g^{(0)}(1), the Verifier can calculate the values of h^{(0)}(0), h^{(0)}(1) on its own, because:\\begin{aligned}\nh^{(0)}(0) &= (1-u_0)\\cdot g^{(0)}(0) \\\\\nh^{(0)}(1) &= u_0 \\cdot g^{(0)}(1) \\\\\n\\end{aligned}\n\nThen the Verifier can verify h^{(0)}(0)+h^{(0)}(1)\\overset{?}{=}v. If the verification passes, the Verifier can calculate h^{(0)}(r_0):h^{(0)}(r_0) = \\Big((1-u_0)(1-r_0)+u_0r_0\\Big) \\cdot g^{(0)}(r_0)\n\nHere g^{(0)}(r_0) can also be calculated from g^{(0)}(0), g^{(0)}(1):g^{(0)}(r_0) = g^{(0)}(0) + \\Big(g^{(0)}(1)-g^{(0)}(0)\\Big)\\cdot r_0\n\nFinally, calculate h_0(r_0). In summary, the Verifier needs one multiplication to verify v, and three multiplications to calculate h^{(0)}(r_0).","type":"content","url":"/basefold/basefold-opt#sumcheck-optimization","position":13},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Sumcheck Optimization"},"type":"lvl3","url":"/basefold/basefold-opt#performance-analysis","position":14},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Sumcheck Optimization"},"content":"The Prover’s total computation is:\n\nCalculate g^{i}(0): 2^n multiplications, 2\\cdot 2^n additions\n\nCalculate g^{i}(1): 2^n multiplications\n\nFolding calculation of \\vec{a}: 2^n multiplications and 2\\cdot 2^n additions\n\nFolding calculation of \\vec{w}: 2^n multiplications and 2\\cdot 2^n additions\n\nVerifier’s computation:\n\nCalculate h^{(i)}(0): n multiplications, n additions\n\nCalculate h^{(i)}(1): n multiplications, n additions\n\nVerify h^{(i)}(0)+h^{(i)}(1): n additions\n\nCalculate h^{(i)}(r_i): 3n multiplications, 6n additions\n\nCalculate \\tilde{eq}(r_0, r_1, \\cdots, r_{n-1}): n multiplications, 4n additions\n\nFinal verification: 1 multiplication","type":"content","url":"/basefold/basefold-opt#performance-analysis","position":15},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Further Optimization of the Verifier"},"type":"lvl2","url":"/basefold/basefold-opt#further-optimization-of-the-verifier","position":16},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Further Optimization of the Verifier"},"content":"Let’s observe the definition of g^{(0)}(X) again:g^{(0)}(X) = \\sum_{(b_1,b_2)\\in\\{0,1\\}^2} \\Big( \\tilde{f}(X, b_1, b_2) \\cdot eq((u_1,u_2), (b_1, b_2)) \\Big)\n\nThis sum happens to be equal to \\tilde{f}(X, u_1, u_2), so we can review the above optimization protocol from another perspective:\n\nIn the first step, the Prover sends g^{(0)}(0), g^{(0)}(1), which is actually sending:\\begin{aligned}\ng^{(0)}(0) &= \\tilde{f}(0, u_1, u_2) \\\\\ng^{(0)}(1) &= \\tilde{f}(1, u_1, u_2) \\\\\n\\end{aligned}\n\nThe Verifier checks the correctness of h^{(0)}(X), which is equivalent to verifying the correctness of g^{(0)}(X):h^{(0)}(u_0) = eq(u_0, u_0) \\cdot g^{(0)}(u_0) = g^{(0)}(u_0) = g^{(0)}(0) + \\Big(g^{(0)}(1)-g^{(0)}(0)\\Big)\\cdot u_0 \\overset{?}{=} v\n\nThen the Verifier calculates g^{(0)}(r_0) as the sum value for the next round of Sumcheck.g^{(0)}(r_0) = g^{(0)}(0) + \\Big(g^{(0)}(1)-g^{(0)}(0)\\Big)\\cdot r_0\n\nTherefore, it seems we no longer need to introduce h^{(i)}(X), but can directly use g^{(i)}(X). This way, in each round of the Sumcheck protocol, the Prover needs to send g^{(i)}(0), g^{(i)}(1), and the Verifier only needs to perform two multiplications, one to calculate g^{(i)}(u_i), and one to calculate g^{(i)}(r_i). And in the last step of Sumcheck, the Verifier only needs to verify:g^{(n-1)}(r_{n-1}) \\overset{?}{=} \\tilde{f}(r_0, r_1, \\cdots, r_{n-1})\n\nIt’s not hard to see that in the i-th round, if the Prover is honest, g^{(i)}(X) should be exactly equal to \\tilde{f}^{(i)}(r_0, r_1, \\cdots, r_{i-1}, X, u_{i+1}, \\cdots, u_{n-1}). Let’s rewrite the transformed Sumcheck protocol:\n\nRound 1: The Prover sends \\tilde{f}(0, u_1, u_2), \\tilde{f}(1, u_1, u_2), the Verifier verifies:(1-u_0)\\cdot \\tilde{f}(0, u_1, u_2) + u_0\\cdot \\tilde{f}(1, u_1, u_2) \\overset{?}{=} \\tilde{f}(u_0, u_1, u_2) = v\n\nThe Verifier responds with r_0\\in F, and calculates g^{(0)}(r_0) = \\tilde{f}(r_0, u_1, u_2) as the new sum value v^{(1)}:\\tilde{f}(r_0, u_1, u_2) = (1-r_0)\\cdot \\tilde{f}(0, u_1, u_2) + r_0\\cdot \\tilde{f}(1, u_1, u_2)\n\nRound 2: The Prover sends \\tilde{f}(r_0, 0, u_2), \\tilde{f}(r_0, 1, u_2), the Verifier verifies:(1-u_1)\\cdot \\tilde{f}(r_0, 0, u_2) + u_1\\cdot \\tilde{f}(r_0, 1, u_2) \\overset{?}{=} \\tilde{f}(r_0, u_1, u_2) = v^{(1)}\n\nThe Verifier responds with r_1\\in F, and calculates g^{(1)}(r_1) = \\tilde{f}(r_0, r_1, u_2) as the new sum value:\\tilde{f}(r_0, r_1, u_2) = (1-r_1)\\cdot \\tilde{f}(r_0, 0, u_2) + r_1\\cdot \\tilde{f}(r_0, 1, u_2)\n\nRound 3: The Prover sends \\tilde{f}(r_0, r_1, 0), \\tilde{f}(r_0, r_1, 1), the Verifier verifies:(1-u_2)\\cdot \\tilde{f}(r_0, r_1, 0) + u_2\\cdot \\tilde{f}(r_0, r_1, 1) \\overset{?}{=} \\tilde{f}(r_0, r_1, u_2) = v^{(2)}\n\nThe Verifier responds with r_2\\in F, and calculates g^{(2)}(r_2) = \\tilde{f}(r_0, r_1, r_2) as the new sum value:\\tilde{f}(r_0, r_1, r_2) = (1-r_2)\\cdot \\tilde{f}(r_0, r_1, 0) + r_2\\cdot \\tilde{f}(r_0, r_1, 1)\n\nVerification step: The Verifier verifies the following equation through the \\tilde{f}(X_0, X_1, X_2) Oracle:g^{(2)}(r_2) \\overset{?}{=} \\tilde{f}(r_0, r_1, r_2)\n\nThis way, the Verifier only needs to perform two multiplications in each round. The first multiplication is to verify the correctness of the sum value v^{(i)}, and the second multiplication is to calculate g^{(i)}(r_i), totaling 2n multiplications.","type":"content","url":"/basefold/basefold-opt#further-optimization-of-the-verifier","position":17},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Prover’s Computation Optimization"},"type":"lvl2","url":"/basefold/basefold-opt#provers-computation-optimization","position":18},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Prover’s Computation Optimization"},"content":"In the three rounds, the Prover needs to send g^{(0)}(0) and g^{(0)}(1), g^{(1)}(0) and g^{(1)}(1), g^{(2)}(0) and g^{(2)}(1) in turn.\\begin{aligned}\ng^{(0)}(0) &= \\tilde{f}(0, u_1, u_2) \\\\\ng^{(0)}(1) &= \\tilde{f}(1, u_1, u_2) \\\\\ng^{(1)}(0) &= \\tilde{f}(r_0, 0, u_2) \\\\\ng^{(1)}(1) &= \\tilde{f}(r_0, 1, u_2) \\\\\ng^{(2)}(0) &= \\tilde{f}(r_0, r_1, 0) \\\\\ng^{(2)}(1) &= \\tilde{f}(r_0, r_1, 1) \\\\\n\\end{aligned}\n\nIn this section, we’ll look at how the Prover can efficiently calculate g^{(i)}(0), g^{(i)}(1).\n\nAs mentioned earlier, the Prover maintains an array \\vec{a} of length 2^n:\\begin{aligned}\na_0 &= \\tilde{f}(0, 0, 0) \\\\\na_1 &= \\tilde{f}(1, 0, 0) \\\\\na_2 &= \\tilde{f}(0, 1, 0) \\\\\na_3 &= \\tilde{f}(1, 1, 0) \\\\\na_4 &= \\tilde{f}(0, 0, 1) \\\\\na_5 &= \\tilde{f}(1, 0, 1) \\\\\na_6 &= \\tilde{f}(0, 1, 1) \\\\\na_7 &= \\tilde{f}(1, 1, 1) \\\\\n\\end{aligned}\n\nIt’s folded in each round of Sumcheck. For example, after the first round of folding (with r_0 as the folding factor), the Prover gets:\\begin{aligned}\na^{(1)}_0 &= \\tilde{f}^{(1)}(0, 0) = (1-r_0)\\cdot\\tilde{f}(0, 0, 0) + r_0\\cdot\\tilde{f}(1, 0, 0) \\\\\na^{(1)}_1 &= \\tilde{f}^{(1)}(0, 1) = (1-r_0)\\cdot\\tilde{f}(0, 0, 1) + r_0\\cdot\\tilde{f}(1, 0, 1) \\\\\na^{(1)}_2 &= \\tilde{f}^{(1)}(1, 0) = (1-r_0)\\cdot\\tilde{f}(0, 1, 0) + r_0\\cdot\\tilde{f}(1, 1, 0) \\\\\na^{(1)}_3 &= \\tilde{f}^{(1)}(1, 1) = (1-r_0)\\cdot\\tilde{f}(0, 1, 1) + r_0\\cdot\\tilde{f}(1, 1, 1) \\\\\n\\end{aligned}\n\nTo efficiently calculate g^{(i)}(0), g^{(i)}(1), we let the Prover do some pre-computation before the Sumcheck protocol starts, obtaining a vector \\vec{d} of length 2^n-1.\n\nThis vector is the result of folding the vector \\vec{a} (with u_2, u_1, u_0 as folding factors). We first fold the vector \\vec{a} using u_2 to get d_0, d_1, d_2, d_3:\\begin{aligned}\nd_0 &= (1-u_2)\\cdot a_0 + u_2\\cdot a_4 = \\tilde{f}(0, 0, u_2)\\\\\nd_1 &= (1-u_2)\\cdot a_1 + u_2\\cdot a_5 = \\tilde{f}(1, 0, u_2)\\\\\nd_2 &= (1-u_2)\\cdot a_2 + u_2\\cdot a_6 = \\tilde{f}(0, 1, u_2)\\\\\nd_3 &= (1-u_2)\\cdot a_3 + u_2\\cdot a_7 = \\tilde{f}(1, 1, u_2)\\\\\n\\end{aligned}\n\nThen the Prover performs folding again on d_0, d_1, d_2, d_3 using u_1 as the folding factor to get d_4, d_5:\\begin{aligned}\nd_4 &= (1-u_1)\\cdot d_0 + u_1\\cdot d_2 = \\tilde{f}(0, u_1, u_2)\\\\\nd_5 &= (1-u_1)\\cdot d_1 + u_1\\cdot d_3 = \\tilde{f}(1, u_1, u_2)\\\\\n\\end{aligned}\n\nFinally, the Prover performs folding on d_4, d_5 using u_0 as the folding factor to get d_6:d_6 = (1-u_0)\\cdot d_4 + u_0\\cdot d_5 = \\tilde{f}(u_0, u_1, u_2)\n\nObserve that the end value d_6 of the vector (d_0, d_1, d_2, d_3, d_4, d_5, d_6) happens to be \\tilde{f}(u_0, u_1, u_2), and the second-to-last and third-to-last elements happen to be g^{(0)}(0), g^{(0)}(1):g^{(0)}(0) = d_4 \\\\\ng^{(0)}(1) = d_5 \\\\\n\nIf the Prover removes d_6 and folds the remaining vector (d_0, d_1, d_2, d_3, d_4, d_5) along with \\vec{a} (using r_0 as the folding factor), we can get:\\begin{aligned}\nd'_0 &= (1-r_0)\\cdot d_0 + r_0\\cdot d_1 = \\tilde{f}(r_0, 0, u_2) \\\\\nd'_1 &= (1-r_0)\\cdot d_2 + r_0\\cdot d_3 = \\tilde{f}(r_0, 1, u_2) \\\\\nd'_2 &= (1-r_0)\\cdot d_4 + r_0\\cdot d_5 = \\tilde{f}(r_0, u_1, u_2) \\\\\n\\end{aligned}\n\nWe surprisingly find that d'_2 happens to be the value of g^{(0)}(r_0), and d'_0, d'_1 happen to be the values of g^{(1)}(0), g^{(1)}(1).\n\nNext, in the second round of Sumcheck, the Prover folds (d'_0, d'_1) along with \\vec{a}' using r_1 to get the value of d''_0:d''_0 = (1-r_1)\\cdot d'_0 + r_1\\cdot d'_1 = \\tilde{f}(r_0, r_1, u_2)\n\nThis happens to be the value of g^{(1)}(r_1).\n\nThen in the third round of Sumcheck, the Prover needs to send the values of g^{(2)}(0), g^{(2)}(1), but the vector \\vec{d} has already been folded and disappeared. However, at this time, the Prover has the folded vector \\vec{a}:\\begin{aligned}\na^{(2)}_0 &= \\tilde{f}^{(2)}(0) = \\tilde{f}(r_0, r_1, 0) \\\\\na^{(2)}_1 &= \\tilde{f}^{(2)}(1) = \\tilde{f}(r_0, r_1, 1) \\\\\n\\end{aligned}\n\nThese two values happen to be equal to the values of g^{(2)}(0), g^{(2)}(1).","type":"content","url":"/basefold/basefold-opt#provers-computation-optimization","position":19},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Prover’s Computation Optimization"},"type":"lvl3","url":"/basefold/basefold-opt#performance-analysis-1","position":20},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Prover’s Computation Optimization"},"content":"To summarize, we only need to let the Prover pre-compute the vector \\vec{d} before Sumcheck starts. It stores all the intermediate results of \\tilde{f}(X_0, X_1, X_2) after Partial Evaluation by substituting X_2=u_2, X_1=u_1, X_0=u_0 in turn. And this vector is folded along with the vector \\vec{a} using the same random challenge numbers, so the values of g^{(i)}(0), g^{(i)}(1), g^{(i)}(r_i) can be efficiently calculated. The pre-computation requires 2^n-1 multiplications, and the folding requires a total of 2^n-2 multiplications.\n\nProver’s computation:\n\nPre-compute \\vec{d}: 2^n multiplications and 2\\cdot 2^n additions\n\nFold \\vec{a}: 2^n multiplications and 2\\cdot 2^n additions\n\nFold \\vec{d}: 2^n multiplications and 2\\cdot 2^n additions\n\nVerifier’s computation:\n\nVerify g^{(i)}(0), g^{(i)}(1): n multiplications and 2n additions\n\nCalculate g^{(i)}(r_i): n multiplications and 2n additions","type":"content","url":"/basefold/basefold-opt#performance-analysis-1","position":21},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Further Optimization of the Verifier"},"type":"lvl2","url":"/basefold/basefold-opt#further-optimization-of-the-verifier-1","position":22},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Further Optimization of the Verifier"},"content":"In the above protocol, the Prover needs to send g^{(i)}(0), g^{(i)}(1), then the Verifier verifies:g^{(i)}(0) +  (g^{(i)}(1) - g^{(i)}(0))\\cdot u_i \\overset{?}{=} g^{(i-1)}(r_{i-1}) = g^{(i)}(u_i)\n\nSo actually, the Prover only needs to send one value g^{(i)}(0) in each round of Sumcheck, then the Verifier can calculate g^{(i)}(1) backwards based on the “verification equation”:g^{(i)}(1) = \\frac{g^{(i-1)}(r_{i-1}) - g^{(i)}(0)}{u_i} + g^{(i)}(0)\n\nThis way, the Verifier uses one division calculation to exchange for a reduction in communication (by half). But note that the computational cost of finite field division (finding inverse) is significantly higher than multiplication, with a complexity of at least o(\\log{q}). But anyway, we can further reduce the communication, i.e., the Proof size, through this method.\n\nIn fact, it’s not hard to see that this relatively costly division can be optimized away. This idea comes from Deepfold []. We let the Prover send g^{(i)}(u_i+1) instead of g_i(0) or g_i(1) in each round of Sumcheck:g^{(i)}(u_i+1) = g^{(i)}(u_i) + g^{(i)}(1) - g^{(i)}(0)\n\nAgain, g^{(i)}(u_i) is the tail element after each folding of the vector \\vec{d}, so its computation cost is already included in the folding calculation of \\vec{d}. Therefore, the Prover’s cost is just two additional additions in each round.\n\nThen, because the Verifier already has g^{(i)}(u_{i})=g^{(i-1)}(r_{i-1}) at this time, the Verifier can get g^{(i)}(r_i) without division:g^{(i)}(r_i) = g^{(i)}(u_i) + \\Big(g^{(i)}(u_i+1)-g^{(i)}(u_i)\\Big)\\cdot (r_i - u_i)\n\nThe computational cost is only three additions and one multiplication.","type":"content","url":"/basefold/basefold-opt#further-optimization-of-the-verifier-1","position":23},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Further Optimization of the Verifier"},"type":"lvl3","url":"/basefold/basefold-opt#performance-analysis-2","position":24},{"hierarchy":{"lvl1":"Basefold Optimization","lvl3":"Performance Analysis","lvl2":"Further Optimization of the Verifier"},"content":"Prover’s computation:\n\nPre-compute \\vec{d}: 2^n multiplications and 2\\cdot 2^n additions\n\nFold \\vec{a}: 2^n multiplications and 2\\cdot 2^n additions\n\nFold \\vec{d}: 2^n multiplications and 2\\cdot 2^n additions\n\nCalculate g^{(i)}(u_i+1): 2n additions\n\nVerifier’s computation:\n\nCalculate g^{(i)}(r_i): n multiplications and 3n additions","type":"content","url":"/basefold/basefold-opt#performance-analysis-2","position":25},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Understanding Sumcheck Optimization from Another Perspective"},"type":"lvl2","url":"/basefold/basefold-opt#understanding-sumcheck-optimization-from-another-perspective","position":26},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"Understanding Sumcheck Optimization from Another Perspective"},"content":"After multiple steps of optimization above, we have reached a relatively ideal state. Let’s see if we can understand this concise protocol more easily.\n\nWe can view the above optimized protocol as a Recursive Split-and-fold style sum proof. Because \\tilde{f}(u_0, u_1, u_2) itself is a sum of 8 terms:\\begin{aligned}\n\\tilde{f}(u_0, u_1, u_2) &= a_0(1-u_0)(1-u_1)(1-u_2) + a_1u_0(1-u_1)(1-u_2) \\\\\n&+ a_2(1-u_0)u_1(1-u_2) + a_3u_0u_1(1-u_2) \\\\\n&+ a_4(1-u_0)u_1u_2 + a_5u_0(1-u_1)u_2 \\\\\n&+ a_6u_0u_1(1-u_2) + a_7u_0u_1u_2  \\\\\n& = (1-u_0)\\cdot \\Big(a_0(1-u_1)(1-u_2) + a_2u_1(1-u_2) + a_4(1-u_1)u_2 + a_6u_1u_2 \\Big) \\\\\n&+ u_0\\cdot \\Big(a_1(1-u_1)(1-u_2) + a_3u_1(1-u_2) + a_5(1-u_1)u_2 + a_7u_1u_2 \\Big) \\\\\n\\end{aligned}\n\nIn the first round, the Prover sends the sum of 4 terms in the left bracket and the sum of 4 terms in the right bracket, recorded as g_0(0) and g_0(1), and sends them to the Verifier:\\begin{aligned}\ng_0(0) &= a_0(1-u_1)(1-u_2) + a_2u_1(1-u_2) + a_4(1-u_1)u_2 + a_6u_1u_2 \\\\\ng_0(1) &= a_1(1-u_1)(1-u_2) + a_3u_1(1-u_2) + a_5(1-u_1)u_2 + a_7u_1u_2 \\\\\n\\end{aligned}\n\nOr\\begin{aligned}\ng_0(0) &= \\tilde{f}(0, u_1, u_2) \\\\\ng_0(1) &= \\tilde{f}(1, u_1, u_2) \\\\\n\\end{aligned}\n\nThen the Verifier first verifies whether the result of the inner product of (g_0(0), g_0(1)) and (1-u_0, u_0) is equal to the sum v to be proved,(1-u_0)\\cdot g_0(0) + u_0\\cdot g_0(1) \\overset{?}{=} \\tilde{f}(u_0, u_1, u_2)\n\nThis way, the Prover and Verifier have reduced the proof of a sum of length 8 to two proofs of sums of length 4. And because these two new sums actually have the same structure, that is, they are both the result of the inner product of the odd and even term coefficients of the polynomial with the same vector (1-u_1,u_1)\\otimes(1-u_2,u_2),\\begin{aligned}\ng^{(0)}(0) &= \\langle(a_0, a_2, a_4, a_6), (1-u_1, u_1)\\otimes(1-u_2, u_2)\\rangle \\\\\ng^{(0)}(1) &= \\langle(a_1, a_3, a_5, a_7), (1-u_1, u_1)\\otimes(1-u_2, u_2)\\rangle \\\\\n\\end{aligned}\n\nSo the Verifier can give a random number r_0, using it to merge (Batching) these two new sum proofs together. Coincidentally, the merged sum value of length 4 happens to be g^{(0)}(r_0), and it equals \\tilde{f}(r_0, u_1, u_2). Of course, this is not a coincidence, because we deliberately use (1-r_0, r_0) this Multilinear Basis to fold g^{(0)}(0), g^{(0)}(1), with the purpose of keeping it equal to g^{(0)}(r_0):(1-r_0)\\cdot g_0(0) + r_0\\cdot g_0(1) = \\tilde{f}(r_0, u_1, u_2)\n\nThis way, the Prover and Verifier next prove that the sum value of the merged vector of length 4 equals \\tilde{f}(r_0, u_1, u_2), that is(1-r_0)\\cdot g^{(0)}(0) + r_0\\cdot g^{(0)}(1) \\overset{?}{=} a'_0w'_0 + a'_1w'_1 + a'_2w'_2 + a'_3w'_3\n\nAnd so on, this protocol is consistent with the idea of the Sumcheck protocol, which is to split a sum equation into sums of different parts, and then use random numbers to merge these different sum segments together to prove. Until the last round, the sum proof is Reduced to the polynomial evaluation proof, and the Prover and Verifier use other tools to supplement the last part of the proof.\n\nThe original Sumcheck in Basefold protocol did not utilize the internal structure of the sum, but adopted a more general proof of inner product sum. Therefore, the concise protocol introduced in this article fully utilizes the internal structure of the sum terms.","type":"content","url":"/basefold/basefold-opt#understanding-sumcheck-optimization-from-another-perspective","position":27},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"References"},"type":"lvl2","url":"/basefold/basefold-opt#references","position":28},{"hierarchy":{"lvl1":"Basefold Optimization","lvl2":"References"},"content":"[GLH+24] Yanpei Guo, Xuanming Liu, Kexi Huang, Wenjie Qu, Tianyang Tao, and Jiaheng Zhang. “DeepFold: Efficient Multilinear Polynomial Commitment from Reed-Solomon Code and Its Application to Zero-knowledge Proofs.” Cryptology ePrint Archive (2024).\n\n[ACFY24] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\n[Hab24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).","type":"content","url":"/basefold/basefold-opt#references","position":29},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding"},"type":"lvl1","url":"/coding-theory/list-decoding-algorithms","position":0},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding"},"content":"The notebook follows the explanation of encoding a message and then build the entire list decoding algorithm to recover messages from a large number of errors.\nThis notebook provides a hands-on exploration of Reed-Solomon error-correcting codes, with a focus on the powerful list decoding technique. We will start from the first principles of encoding a message and then build the entire list decoding algorithm to recover messages from a large number of errors.","type":"content","url":"/coding-theory/list-decoding-algorithms","position":1},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"The Core Idea"},"type":"lvl3","url":"/coding-theory/list-decoding-algorithms#the-core-idea","position":2},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"The Core Idea"},"content":"The message can be considered a secret curve on the graph. To send it, you pick several points on that curve.\n\nEncoding: Sending the coordinates of those points.\n\nThe Problem: Some points get moved during transmission due to noise (errors).\n\nDecoding: Figuring out the original secret curve from the noisy, corrupted points.\n\nWhen there are too many errors, there might be more than one plausible original curve. Instead of giving up, a list decoder returns a short list of all possible candidates.\n\nAll our calculations will be performed in the finite field GF(257).\n\nimport numpy as np\nimport math\nfrom sage.all import *\n\ndef add(x, y):\n    return (x + y) % 257\n\ndef mul(x, y):\n    return (x * y) % 257\n\ndef sub(x, y):\n    return (x - y) % 257\n\ndef power(x, y):\n    return (x ** y) % 257\n\nprint(\"Setup Complete! Helper functions are defined for arithmetic in GF(257).\")\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#the-core-idea","position":3},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 1: Encoding - Turning Messages into Codewords"},"type":"lvl2","url":"/coding-theory/list-decoding-algorithms#part-1-encoding-turning-messages-into-codewords","position":4},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 1: Encoding - Turning Messages into Codewords"},"content":"Before we decode, let’s quickly review the encoding process. A message is first converted into a polynomial, which is then evaluated at multiple points to create the final codeword.\n\nMessage to Polynomial: A message like “cd” (length k=2) is converted to a list of its ASCII values [99, 100]. This becomes the polynomial P(X) = 100 + 99X.\n\nPolynomial to Codeword: This polynomial is evaluated at n distinct points (e.g., x = 0, 1, \\dots, n-1) to produce an n-symbol codeword.\n\nThe following code implements this process.\n\ndef create_matrix(k, n):\n    A = []\n    for i in range(k):\n        temp_arr = []\n        for j in range(n):\n            temp_arr.append(power(j, i))\n        A.append(temp_arr)\n    return A\n\ndef create_coef_arr(msg):\n    return [ord(m) for m in msg]\n\ndef matrix_mul(m1, m2, k, n):\n    res = np.zeros(n, dtype=int); j = 0\n    while j < n:\n        s = 0\n        for t in range(k): s = add(s, mul(m1[t], m2[t][j]))\n        res[j] = s; j += 1\n    return res\n\ndef encode(msg, n):\n    k = len(msg)\n    M = create_coef_arr(msg)\n    M.reverse()\n    A = create_matrix(k, n)\n    RS_Code = matrix_mul(M, A, k, n)\n    return RS_Code\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#part-1-encoding-turning-messages-into-codewords","position":5},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 2: Simulating a Noisy Channel"},"type":"lvl2","url":"/coding-theory/list-decoding-algorithms#part-2-simulating-a-noisy-channel","position":6},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 2: Simulating a Noisy Channel"},"content":"The decoder’s job is to fix errors. To test it, we need a function to simulate a noisy channel by corrupting the encoded message. This mimics the random noise described in Shannon’s model\n\ndef add_errors(msg, err, n):\n    err_msg = [m for m in msg]\n    # Use random positions for a more realistic simulation\n    error_positions = np.random.choice(n, err, replace=False)\n    for i in error_positions:\n        e = randrange(257)\n        if msg[i] != e:\n            err_msg[i] = e\n        else:\n            err_msg[i] = add(e, 1) # Ensure the value changes\n    return err_msg\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#part-2-simulating-a-noisy-channel","position":7},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 3: The Basic List Decoding Algorithm"},"type":"lvl2","url":"/coding-theory/list-decoding-algorithms#part-3-the-basic-list-decoding-algorithm","position":8},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 3: The Basic List Decoding Algorithm"},"content":"Here we implement the Basic List-Decoder (Algorithm 12.2.1) from the textbook.","type":"content","url":"/coding-theory/list-decoding-algorithms#part-3-the-basic-list-decoding-algorithm","position":9},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"The Two-Step Strategy","lvl2":"Part 3: The Basic List Decoding Algorithm"},"type":"lvl3","url":"/coding-theory/list-decoding-algorithms#the-two-step-strategy","position":10},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"The Two-Step Strategy","lvl2":"Part 3: The Basic List Decoding Algorithm"},"content":"The algorithm transforms the algebraic problem of error-correction into a geometric one:\n\nInterpolation Step: Find a non-zero, two-variable polynomial, Q(X,Y), that passes through all of our noisy received points (\\alpha_i, y_i). To ensure a solution exists, the algorithm puts separate bounds on the maximum degree of the X and Y variables.\n\nRoot-Finding Step: The original message polynomial, P(X), is “hidden” inside Q(X,Y) as a factor of the form Y - P(X). This step factors Q(X,Y) to find these special factors and identify the candidate message polynomials.\n\n### Step 1: Interpolation ###\ndef xy_matrix(a_arr, y_arr, n, deg_a, deg_y):\n    A = []\n    for i in range(n):\n        temp_arr = []\n        for j in range(deg_a):\n            for t in range(deg_y): temp_arr.append(mul(power(a_arr[i], j), power(y_arr[i], t)))\n        A.append(temp_arr)\n    return A\n\ndef get_q(a_arr, y_arr, n, deg_a, deg_y):\n    deg_a_real, deg_y_real = deg_a + 1, deg_y + 1\n    M = MatrixSpace(GF(257), n, deg_a_real * deg_y_real)\n    A = M(xy_matrix(a_arr, y_arr, n, deg_a_real, deg_y_real))\n    print(f\"Interpolation: Solving for Q's coefficients by finding the null space of a {A.nrows()}x{A.ncols()} matrix.\")\n    kernel_basis = A.right_kernel_matrix(basis=\"computed\")\n    return kernel_basis\n\ndef gen_poly(q_arr, deg_a, deg_y):\n    x, y = PolynomialRing(GF(257), 2, ['x','y']).gens(); f = 0\n    deg_a_real, deg_y_real = deg_a + 1, deg_y + 1\n    for i in range(deg_a_real):\n        for j in range(deg_y_real):\n            k = deg_y_real * i + j\n            f += q_arr[k] * x**i * y**j\n    return f\n\n### Step 2: Root-Finding ###\ndef create_list(Q):\n    if Q == 0: return []\n    L = []\n    x, y = Q.parent().gens() # Get the polynomial variables x and y\n    \n    try:\n        factors = [g[0] for g in list(Q.factor())]\n        print(f\"\\nFactoring Q(X,Y) found {len(factors)} factor(s).\")\n        for i, fact in enumerate(factors):\n            fact_str = str(fact)\n            if len(fact_str) > 70: fact_str = fact_str[:70] + \"...\"\n            print(f\"  - Factor {i+1}: {fact_str}\")\n\n            # Robustly check if the factor is of the form c*(Y - P(X))\n            if fact.degree(y) == 1:\n                # Factor is of the form A(x)*y + B(x)\n                A = fact.coefficient({y: 1})\n                B = fact.coefficient({y: 0})\n\n                # A(x) must be a non-zero constant for it to be a Y-P(X) factor\n                if A.is_constant() and A != 0:\n                    # P(X) = -B(X) / A\n                    P_X = -B / A\n                    print(f\"    --> Found valid candidate P(X) = {P_X}\")\n                    L.append(P_X)\n    except Exception as e:\n        print(f\"Factoring failed: {e}\")\n    return L\n\n### Step 3: Filtering ###\ndef filter_candidates(candidates, noisy_msg, n, k):\n    good_candidates = []\n    print(\"\\n--- Filtering Candidates by Agreement ---\")\n    \n    for p in candidates:\n        if p.degree() >= k:\n            print(f\"  - Candidate {p} rejected (degree {p.degree()} is too high; expected < {k})\")\n            continue\n            \n        p_coeffs = p.coefficients()\n        if len(p_coeffs) > k: continue\n\n        p_coeffs.reverse()\n        \n        p_codeword = matrix_mul(p_coeffs, create_matrix(len(p_coeffs), n), len(p_coeffs), n)\n        distance = np.sum(p_codeword != noisy_msg)\n        \n        print(f\"  - Candidate {p} has {distance} disagreements.\")\n        good_candidates.append({'poly': p, 'dist': distance})\n\n    # Find the candidate(s) with the minimum distance to the noisy message\n    if not good_candidates:\n        return []\n    \n    min_dist = min(c['dist'] for c in good_candidates)\n    print(f\"\\nMinimum distance found among candidates is {min_dist}.\")\n    \n    # Return all candidates that achieve this minimum distance\n    final_list = [c['poly'] for c in good_candidates if c['dist'] == min_dist]\n    print(f\"Accepting all candidates with this distance: {final_list}\")\n    return final_list\n\n### Top-Level Function ###\ndef list_decoding(encoded_msg, n, k, err):\n    a_array = [i for i in range(n)]\n    deg_a = int(math.sqrt(n * (k - 1)))\n    deg_y = int(math.sqrt(n / (k - 1)))\n\n    q_kernel_basis = get_q(a_array, encoded_msg, n, deg_a, deg_y)\n    if q_kernel_basis.nrows() == 0: return []\n    \n    q_coeffs = list(q_kernel_basis[0])\n    q_poly = gen_poly(q_coeffs, deg_a, deg_y)\n    \n    candidate_polynomials = create_list(q_poly)\n    filtered_polynomials = filter_candidates(candidate_polynomials, encoded_msg, n, k)\n    \n    return filtered_polynomials\n\n### Final Conversion ###\ndef poly_to_str(poly_arr, k):\n    decoded_msg_arr = []\n    for f_i in poly_arr:\n        coefs_array = f_i.coefficients()\n        \n        while len(coefs_array) < k: coefs_array.append(0)\n        coefs_array = coefs_array[:k]\n        \n        msg_coeffs = [int(c) for c in coefs_array]\n        msg = [chr(c) for c in msg_coeffs if 0 <= c < 256]\n        decoded_msg_arr.append(''.join(msg))\n    return decoded_msg_arr\n\ndef RS_demonstration(msg, err, n):\n    k = len(msg)\n    print(\"=\"*60)\n    print(f\"--- Running Test: msg='{msg}', n={n}, k={k}, errors={err} ---\")\n    \n    unique_decoding_limit = math.floor((n - k + 1) / 2)\n    print(f\"Unique decoding can handle less than {unique_decoding_limit} errors.\")\n    if err >= unique_decoding_limit:\n        print(\"NOTE: Number of errors meets or exceeds the unique decoding limit. List decoding is required.\")\n    print(\"-\"*60 + \"\\n\")\n    \n    encoded_msg = encode(msg, n)\n    print(\"Original Codeword (first 30 symbols):\\n\", encoded_msg[:30])\n\n    encoded_msg_with_errors = add_errors(encoded_msg, err, n)\n    print(\"\\nNoisy Codeword (first 30 symbols):\\n\", np.array(encoded_msg_with_errors[:30]))\n    \n    print(\"\\n--- Starting List Decoding ---\\n\")\n    poly_list = list_decoding(encoded_msg_with_errors, n, k, err)\n\n    final_messages = poly_to_str(poly_list, k)\n    print(\"\\n--- Final Results ---\")\n    print(\"Output Polynomials List:\\n\", poly_list)\n    print(\"\\nDecoded Possible Messages:\\n\", final_messages)\n\n    if msg in final_messages:\n        print(\"\\nSUCCESS: Original message was recovered in the list!\")\n    else:\n        print(\"\\nFAILURE: Could not recover original message.\")\n    print(\"=\"*60 + \"\\n\\n\")\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#the-two-step-strategy","position":11},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 4: Demonstrations"},"type":"lvl2","url":"/coding-theory/list-decoding-algorithms#part-4-demonstrations","position":12},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 4: Demonstrations"},"content":"We will now run a series of tests to showcase the algorithm’s performance, including cases with both high and low code rates.\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#part-4-demonstrations","position":13},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"4.1 High-Rate Codes (e.g., R = 0.125)","lvl2":"Part 4: Demonstrations"},"type":"lvl3","url":"/coding-theory/list-decoding-algorithms#id-4-1-high-rate-codes-e-g-r-0-125","position":14},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"4.1 High-Rate Codes (e.g., R = 0.125)","lvl2":"Part 4: Demonstrations"},"content":"For codes with a relatively high rate, the unique decoding bound is often better than the guarantee for the Basic List-Decoder. The following tests confirm that the algorithm works when the number of errors is low, but fails when the number of errors exceeds its mathematical limit.\n\nTest 1: A working high-rate case\n\nMessage: “abcde” (k=5)\n\nCodeword Length: n=40\n\nErrors: err=10\n\nRate: R = 5/40 = 0.125\n\nAnalysis: The number of actual agreements is t = 40 - 10 = 30. The required number of agreements for this algorithm is t > 2\\sqrt{40 \\times (5-1)} \\approx 25.3. Since 30 > 25.3, this test should succeed.\n\nRS_demonstration(msg=\"abcde\", err=10, n=40)\n\nTest 2: A failing high-rate case\n\nMessage: “hello” (k=5)\n\nCodeword Length: n=20\n\nErrors: err=8\n\nRate: R = 5/20 = 0.25\n\nAnalysis: The number of actual agreements is t = 20 - 8 = 12. The required number of agreements is t > 2\\sqrt{20 \\times (5-1)} \\approx 17.9. [cite_start]Since 12 is not greater than 17.9, the algorithm is not guaranteed to work and is expected to fail[cite: 267].\n\nRS_demonstration(msg=\"hello\", err=8, n=20)\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#id-4-1-high-rate-codes-e-g-r-0-125","position":15},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"4.2 Low-Rate Codes (R < 0.07)","lvl2":"Part 4: Demonstrations"},"type":"lvl3","url":"/coding-theory/list-decoding-algorithms#id-4-2-low-rate-codes-r-0-07","position":16},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl3":"4.2 Low-Rate Codes (R < 0.07)","lvl2":"Part 4: Demonstrations"},"content":"This is the region where the Basic List-Decoder’s guarantee shines, surpassing the unique decoding bound. We will choose a number of errors that is too high for unique decoding but should be correctable by our list decoder.\n\nTest 3: A working low-rate case\n\nMessage: “data” (k=4)\n\nCodeword Length: n=67\n\nErrors: err=33\n\nRate: R = 4/67 \\approx 0.0597\n\nAnalysis: The unique decoding limit is less than \\lfloor(67-4+1)/2\\rfloor = 32 errors. We have 33 errors, so unique decoding would fail. The Basic List-Decoder’s limit is t > 2\\sqrt{67 \\times (4-1)} \\approx 28.3. We have t = 67 - 33 = 34 agreements. Since 34 > 28.3, this should succeed.\n\nRS_demonstration(msg=\"data\", err=33, n=67)\n\nTest 4: A working very low-rate case\n\nMessage: “log” (k=3)\n\nCodeword Length: n=150\n\nErrors: err=90\n\nRate: R = 3/150 = 0.02\n\nAnalysis: The unique decoding limit is less than \\lfloor(150-3+1)/2\\rfloor = 74 errors. We have 90 errors. The Basic List-Decoder’s limit is t > 2\\sqrt{150 \\times (3-1)} \\approx 34.6. We have t = 150 - 90 = 60 agreements. Since 60 > 34.6, this should succeed.\n\nRS_demonstration(msg=\"log\", err=90, n=150)\n\n","type":"content","url":"/coding-theory/list-decoding-algorithms#id-4-2-low-rate-codes-r-0-07","position":17},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 5: Conclusion"},"type":"lvl2","url":"/coding-theory/list-decoding-algorithms#part-5-conclusion","position":18},{"hierarchy":{"lvl1":"Guide to Reed-Solomon List Decoding","lvl2":"Part 5: Conclusion"},"content":"This notebook has successfully implemented and demonstrated the Basic List-Decoder (Algorithm 12.2.1).\n\nAs the demonstrations show:\n\nThe algorithm correctly decodes messages when the number of errors is within its mathematical performance guarantee (t > 2\\sqrt{n(k-1)}).\n\nIt fails, as expected, when this condition is not met, particularly for higher-rate codes.\n\nCrucially, it succeeds in correcting a large fraction of errors for low-rate codes, outperforming the guarantee of unique decoding in the exact region predicted by the theory.\n\nThis confirms the properties of this foundational list decoding algorithm and highlights the motivation for the more advanced versions (Weighted-Degree and Multiplicity) which provide superior performance across all code rates.","type":"content","url":"/coding-theory/list-decoding-algorithms#part-5-conclusion","position":19},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon"},"type":"lvl1","url":"/coding-theory/list-decoding","position":0},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon"},"content":"","type":"content","url":"/coding-theory/list-decoding","position":1},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon"},"type":"lvl1","url":"/coding-theory/list-decoding#list-decoding-bridging-the-gap-between-hamming-and-shannon","position":2},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon"},"content":"","type":"content","url":"/coding-theory/list-decoding#list-decoding-bridging-the-gap-between-hamming-and-shannon","position":3},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"Two Perspectives on Noise: Hamming vs. Shannon"},"type":"lvl2","url":"/coding-theory/list-decoding#two-perspectives-on-noise-hamming-vs-shannon","position":4},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"Two Perspectives on Noise: Hamming vs. Shannon"},"content":"In coding theory, we have two fundamental views on communication over a noisy channel:\n\nShannon theory. This probabilistic model shows that for a channel with random noise (like the q-ary Symmetric Channel, or qSC_p), we can achieve reliable communication for any rateR < 1 - H_q(p).\n\nThis promises successful communication even with a relatively high fraction of errors.\n\nHamming theory. This model takes a more pessimistic, worst-case view. It provides a 100% guarantee of error correction, but only for a much smaller fraction of errors.\n\nThere is a significant gap between the number of errors these two theories can handle. This section explores the mathematical and geometric reasons for the strict limits of the Hamming world, which sets the stage for list decoding as a bridge between the two.","type":"content","url":"/coding-theory/list-decoding#two-perspectives-on-noise-hamming-vs-shannon","position":5},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"The Quantitative Limit of Unique Decoding"},"type":"lvl2","url":"/coding-theory/list-decoding#the-quantitative-limit-of-unique-decoding","position":6},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"The Quantitative Limit of Unique Decoding"},"content":"Let’s define our terms from the Hamming perspective. A code has a rate R = k/n (message length / codeword length) and a relative distance \\delta = d/n (minimum distance / codeword length).\n\nThe central tenet of unique decoding is that it can correct a fraction of errors up to half the relative distance. By the Singleton bound, we know that a code’s relative distance is limited by its rate:\\delta \\le 1 - R.\n\nThis directly implies that the fraction of correctable errors, p, must satisfy the following condition:p \\le \\frac{1 - R}{2}.\n\nThis is the hard barrier for unique decoding. If the fraction of errors exceeds this, the worst-case guarantee is broken. Reed–Solomon codes are optimal in this regard, as they can achieve this bound.","type":"content","url":"/coding-theory/list-decoding#the-quantitative-limit-of-unique-decoding","position":7},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"Visualizing the Breakdown: The “Bad Examples”"},"type":"lvl2","url":"/coding-theory/list-decoding#visualizing-the-breakdown-the-bad-examples","position":8},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"Visualizing the Breakdown: The “Bad Examples”"},"content":"The reason for this strict limit is best understood visually. Consider the figure\n\n\nwhere c_1, c_2, c_3, c_4 are valid codewords.\n\nThe decoder fails for received words that fall into the “bad examples” region (the area with dotted lines). This happens in two key scenarios:\n\nAmbiguity (point y). The received word y has been corrupted such that it lies exactly halfway between two codewords, say c_1 and c_4. Its distance to both is exactly \\delta/2. Since there is no unique closest codeword, the decoder must give up.\n\nDecoding failure (point z). The received word z does not fall within the \\delta/2 decoding radius of any codeword. It exists in the interstitial space between them, and the decoder again declares a failure.","type":"content","url":"/coding-theory/list-decoding#visualizing-the-breakdown-the-bad-examples","position":9},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"The Path Forward: List Decoding"},"type":"lvl2","url":"/coding-theory/list-decoding#the-path-forward-list-decoding","position":10},{"hierarchy":{"lvl1":"List Decoding: Bridging the gap between Hamming and Shannon","lvl2":"The Path Forward: List Decoding"},"content":"The unique decoding model is pessimistic because the number of these “bad examples” is insignificant compared to the total volume of possible received words. However, the model’s strict requirement for a unique answer forces it to fail even in these rare cases.\n\nTo overcome this, we relax the demand for a single candidate. This leads to list decoding, a paradigm where the decoder, instead of failing, returns a short list of all plausible codewords.\n\n","type":"content","url":"/coding-theory/list-decoding#the-path-forward-list-decoding","position":11},{"hierarchy":{"lvl1":"The Core Concept of List Decoding"},"type":"lvl1","url":"/coding-theory/list-decoding#the-core-concept-of-list-decoding","position":12},{"hierarchy":{"lvl1":"The Core Concept of List Decoding"},"content":"","type":"content","url":"/coding-theory/list-decoding#the-core-concept-of-list-decoding","position":13},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"A New Paradigm: From One to Many"},"type":"lvl2","url":"/coding-theory/list-decoding#a-new-paradigm-from-one-to-many","position":14},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"A New Paradigm: From One to Many"},"content":"In the previous section, we saw how the strict requirement of a single, unique answer forced the decoder to fail even in scenarios where a received word was close to a small number of valid codewords. To overcome this limitation, we turn to a relaxed notion of decoding called list decoding.\n\nInstead of outputting a single candidate for the message, a list-decoding algorithm is allowed to output a short list of all plausible messages. This notion is formally parameterized by two values:\n\n\\rho (rho): The fraction of errors we wish to correct. This defines the radius of our search.\n\nL: A number representing the maximum allowed size of the output list.","type":"content","url":"/coding-theory/list-decoding#a-new-paradigm-from-one-to-many","position":15},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"Formal Definition"},"type":"lvl2","url":"/coding-theory/list-decoding#formal-definition","position":16},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"Formal Definition"},"content":"The concept of list decodability is fundamentally a combinatorial property of a code. It guarantees that no single point in the entire space of possible received words is “too close” to a large number of codewords simultaneously. The formal definition is as follows:\n\nCombinatorial List Decoding:Given 0 \\le \\rho \\le 1 and L \\ge 1, a code C \\subseteq \\Sigma^n is (\\rho, L)-list decodable if for every received word y \\in \\Sigma^n, we have:\\left|\\{\\, c \\in C \\mid \\Delta(y, c) \\le \\rho n \\,\\}\\right| \\le L .\n\nLet’s break this down:\n\nThis is a worst-case definition that must hold for every possible received word y.\n\n\\Delta(y, c) is the Hamming distance between the received word y and a codeword c.\n\nThe set \\{\\ldots\\} contains all codewords from the code C that are inside a Hamming ball of radius \\rho n centered at y.\n\nThe definition simply states that the size of this set can never be larger than L.","type":"content","url":"/coding-theory/list-decoding#formal-definition","position":17},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"The List-Decoding Algorithm and Guarantee"},"type":"lvl2","url":"/coding-theory/list-decoding#the-list-decoding-algorithm-and-guarantee","position":18},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"The List-Decoding Algorithm and Guarantee"},"content":"While the definition above is about the code’s structure, a list-decoding algorithm works as follows: given an error parameter \\rho, a code C, and a received word y, the algorithm’s task is to find and output all codewords that are within a relative Hamming distance \\rho of y.\n\nThis provides a powerful guarantee:\n\nIf the fraction of errors that actually occurred during transmission was at most \\rho, then the transmitted codeword is guaranteed to be in the algorithm’s output list.\n\nOf course, the choice of L is critical. If we set L = 1, we simply recover the notion of unique decoding. If we allow L to be exponentially large, the concept becomes trivial. Therefore, our focus is on cases where L is a small constant or, more generally, grows polynomially with the block length n.","type":"content","url":"/coding-theory/list-decoding#the-list-decoding-algorithm-and-guarantee","position":19},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"Practical Utility"},"type":"lvl2","url":"/coding-theory/list-decoding#practical-utility","position":20},{"hierarchy":{"lvl1":"The Core Concept of List Decoding","lvl2":"Practical Utility"},"content":"A natural question arises: if the decoder returns a list with more than one item, how do we recover the single correct message? There are two primary approaches to this problem:\n\nDeclare a decoding error if the list size is greater than 1.This still represents a significant gain over unique decoding. For most error patterns, the list size is one with high probability, meaning we can successfully decode many more error patterns than the strict d/2 limit allows.\n\nUse side information to select the correct message.If the decoder has access to some external information, it can use that to “prune” the list. Informally, to pick the correct item from a list of size L, one needs approximately O(\\log L) extra bits of information. This is especially useful in applications like complexity theory, where maximizing the rate of the code is not the primary objective.\n\n","type":"content","url":"/coding-theory/list-decoding#practical-utility","position":21},{"hierarchy":{"lvl1":"The Johnson Bound"},"type":"lvl1","url":"/coding-theory/list-decoding#the-johnson-bound","position":22},{"hierarchy":{"lvl1":"The Johnson Bound"},"content":"","type":"content","url":"/coding-theory/list-decoding#the-johnson-bound","position":23},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"Exceeding the Unique Decoding Limit"},"type":"lvl2","url":"/coding-theory/list-decoding#exceeding-the-unique-decoding-limit","position":24},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"Exceeding the Unique Decoding Limit"},"content":"From our knowledge of list decoding, we encounter a critical question: can list decoding correct more than the \\delta/2 fraction of errors imposed by the unique-decoding barrier?\n\nThe Johnson Bound answers this question with a definitive yes. It provides a new, larger radius p up to which a code is guaranteed to be list-decodable with a polynomial-sized list. This is the first major result that quantitatively shows the power of list decoding over unique decoding. The bound is defined by a specific function, J_q(\\delta).","type":"content","url":"/coding-theory/list-decoding#exceeding-the-unique-decoding-limit","position":25},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"The Johnson Bound Theorem"},"type":"lvl2","url":"/coding-theory/list-decoding#the-johnson-bound-theorem","position":26},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"The Johnson Bound Theorem"},"content":"The theorem formally states the relationship between a code’s relative distance \\delta and the fraction of errors p it can handle with list decoding.\n\nTheorem 7.3.1 (Johnson Bound)Let C \\subset \\{0,1\\}^n be a code of distance \\delta. If p < J_q(\\delta), then C is a (p,q\\delta n)-list-decodable code. The function J_q(\\delta) is defined asJ_q(\\delta)=\\left(1-\\frac{1}{q}\\right)\\!\\left(1-\\sqrt{1-\\frac{q\\delta}{q-1}}\\right).\n\nFor the important binary case where q=2, this simplifies toJ_2(\\delta)=\\frac12\\!\\left(1-\\sqrt{1-2\\delta}\\right).\n\nThis theorem guarantees that for any error fraction p less than this Johnson bound, the number of codewords in any Hamming ball of radius pn will be small (at most 2\\delta n).","type":"content","url":"/coding-theory/list-decoding#the-johnson-bound-theorem","position":27},{"hierarchy":{"lvl1":"The Johnson Bound","lvl3":"3.3 Proof Sketch","lvl2":"The Johnson Bound Theorem"},"type":"lvl3","url":"/coding-theory/list-decoding#id-3-3-proof-sketch","position":28},{"hierarchy":{"lvl1":"The Johnson Bound","lvl3":"3.3 Proof Sketch","lvl2":"The Johnson Bound Theorem"},"content":"The full proof of the Johnson Bound involves a clever double-counting argument. Instead of reproducing all the mathematical steps, here is a high-level sketch:\n\nSetup: Assume for contradiction that there is a received word y with a list of M > 2\\delta n candidate codewords c_1,\\dots,c_M in its Hamming ball.\n\nDouble Counting: Define a quantity S that counts the total number of agreements between pairs of these candidate codewords.\n\nUpper and Lower Bounds:\n\nUpper bound — obtained by analyzing the average number of errors.\n\nLower bound — based on the fact that the distance between any two candidate codewords is at least \\delta.\n\nThe Contradiction: Combining these two bounds yields an inequality that contradicts the initial assumption that M could be large. Consequently, M \\le 2\\delta n, proving the theorem.","type":"content","url":"/coding-theory/list-decoding#id-3-3-proof-sketch","position":29},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"Code and Plot"},"type":"lvl2","url":"/coding-theory/list-decoding#code-and-plot","position":30},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"Code and Plot"},"content":"The best way to see the improvement is to compare the Johnson Bound to the unique-decoding bound (\\delta/2). The following code computes both bounds and generates a plot visualizing the trade-off between relative distance \\delta and the correctable fraction of errors p.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unique_decoding_bound(delta):\n    return delta / 2\n\ndef johnson_bound_binary(delta):\n    if 1 - 2 * delta < 0:\n        return np.nan\n    return 0.5 * (1 - np.sqrt(1 - 2 * delta))\n\ndelta_values = np.linspace(0, 0.5, 100)\nud_bound_values = [unique_decoding_bound(d) for d in delta_values]\njb_bound_values = [johnson_bound_binary(d) for d in delta_values]\n\nplt.figure(figsize=(8, 6))\nplt.plot(delta_values, jb_bound_values, '-', label='Johnson Bound (List Decoding)', color='blue')\nplt.plot(delta_values, ud_bound_values, ':', label='Unique Decoding Bound', color='red')\nplt.title('Johnson Bound vs. Unique Decoding Bound')\nplt.xlabel('Relative Distance (δ)')\nplt.ylabel('Correctable Fraction of Errors (ρ)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nThe plot clearly shows that the Johnson Bound (solid line) always lies above the unique-decoding bound (dotted line), demonstrating that list decoding can always handle a strictly larger fraction of errors.\n\nWe can also visualize the trade-off in terms of Rate (R) versus Fraction of Errors (p). Assuming the Singleton Bound (\\delta = 1-R), both bounds can be plotted in the (R,p) plane to highlight their differences.\n\nrate_values = np.linspace(0, 1, 100)\nsingleton_bound_p = 1 - rate_values\nunique_decoding_p = (1 - rate_values) / 2\njohnson_bound_p = 1 - np.sqrt(rate_values)\n\nplt.figure(figsize=(8, 6))\nplt.plot(rate_values, singleton_bound_p, '--', label='Singleton Bound (p = 1-R)')\nplt.plot(rate_values, johnson_bound_p, '-', label='Johnson Bound (p ≈ 1-√R)')\nplt.plot(rate_values, unique_decoding_p, ':', label='Unique Decoding Bound (p = (1-R)/2)')\n\nplt.title('Trade-off between Rate and Correctable Errors')\nplt.xlabel('Rate (R)')\nplt.ylabel('Fraction of Errors (p)')\nplt.ylim(0, 1.05)\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/coding-theory/list-decoding#code-and-plot","position":31},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"A Key Property of the Johnson Bound"},"type":"lvl2","url":"/coding-theory/list-decoding#a-key-property-of-the-johnson-bound","position":32},{"hierarchy":{"lvl1":"The Johnson Bound","lvl2":"A Key Property of the Johnson Bound"},"content":"This figure perfectly illustrates the gap between the different decoding methods: the Johnson Bound carves out a significant new region of correctable errors that unique decoding cannot reach.\n\nLemmaLet q \\ge 2 be an integer and let 0 \\le x \\le 1 - \\tfrac1q. ThenJ_q(x) \\;\\ge\\; 1-\\sqrt{1-x} \\;\\ge\\; \\frac{x}{2}.\n\nThe proof relies on comparing derivatives of the functions and showing that J_q(x) grows more rapidly than the other terms. Together with Theorem 7.3.1, this confirms that for any \\delta>0, list decoding corrects a strictly larger fraction of errors than unique decoding.\n\n","type":"content","url":"/coding-theory/list-decoding#a-key-property-of-the-johnson-bound","position":33},{"hierarchy":{"lvl1":"List-Decoding Capacity"},"type":"lvl1","url":"/coding-theory/list-decoding#list-decoding-capacity","position":34},{"hierarchy":{"lvl1":"List-Decoding Capacity"},"content":"","type":"content","url":"/coding-theory/list-decoding#list-decoding-capacity","position":35},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"The Ultimate Limit of List Decoding"},"type":"lvl2","url":"/coding-theory/list-decoding#the-ultimate-limit-of-list-decoding","position":36},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"The Ultimate Limit of List Decoding"},"content":"So far, we’ve established that list decoding can correct more errors than unique decoding.But what is the ultimate limit?The concept of List-Decoding Capacity provides the answer by defining the optimal trade-off between the code’s rate R and the fraction of errors p it can handle.\n\nThis result shows that by allowing a decoder to output a list of candidates, we can achieve the same rate of communication promised by Shannon’s theory for random noise, but with a worst-case error guarantee.","type":"content","url":"/coding-theory/list-decoding#the-ultimate-limit-of-list-decoding","position":37},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"The Capacity Theorem"},"type":"lvl2","url":"/coding-theory/list-decoding#the-capacity-theorem","position":38},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"The Capacity Theorem"},"content":"The following theorem establishes a sharp threshold for list decoding.It shows that for any rate below a specific capacity limit, a good list-decodable code exists.Conversely, for any rate above this limit, no code can be reliably list-decoded.\n\nTheorem 7.4.1 (List-Decoding Capacity)Let q \\ge 2, 0 \\le p < 1 - 1/q, and \\varepsilon > 0 be a small enough real number.Then the following holds for codes of large enough block length n:\n\n(i) Achievability: If R \\le 1 - H_q(p) - \\varepsilon, then there exists a (p,\\;O(1/\\varepsilon))-list-decodable code.\n\n(ii) Converse: If R \\ge 1 - H_q(p) + \\varepsilon, then every (p,L)-list-decodable code must have L \\ge q^{\\Omega(\\varepsilon n)}.\n\nHere, H_q(p) is the q-ary entropy function, which generalizes the binary entropy we saw in the Shannon notebook.","type":"content","url":"/coding-theory/list-decoding#the-capacity-theorem","position":39},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"Significance and Benefits"},"type":"lvl2","url":"/coding-theory/list-decoding#significance-and-benefits","position":40},{"hierarchy":{"lvl1":"List-Decoding Capacity","lvl2":"Significance and Benefits"},"content":"Bridging Two Worlds: The list-decoding capacity is 1 - H_q(p).This exactly matches the Shannon capacity for a channel with random noise.This means list decoding bridges the gap between the pessimistic worst-case guarantees of the Hamming world and the optimistic possibilities of the Shannon world.\n\nSuperior to Unique Decoding: For unique decoding, the rate is limited by R = 1 - 2p.For list decoding, the rate is R = 1 - H_q(p).Since H_q(p) is always much smaller than 2p (for p > 0), list decoding allows for building codes with much better rates for the same error-correction capability.\n\nProof by Randomness: The proof of the achievability part (i) uses the probabilistic method.It shows that a randomly chosen code will have the desired list-decoding properties with very high probability.\n\n","type":"content","url":"/coding-theory/list-decoding#significance-and-benefits","position":41},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol"},"type":"lvl1","url":"/coding-theory/list-decoding#how-list-decoding-secures-the-fri-protocol","position":42},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol"},"content":"","type":"content","url":"/coding-theory/list-decoding#how-list-decoding-secures-the-fri-protocol","position":43},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"What is FRI? A High-Level View"},"type":"lvl2","url":"/coding-theory/list-decoding#what-is-fri-a-high-level-view","position":44},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"What is FRI? A High-Level View"},"content":"FRI stands for Fast Reed-Solomon Interactive Oracle Proof of Proximity.At its core, FRI is a protocol that allows a Prover to convince a Verifier of a powerful statement: that a function they have committed to is “close” to being a low-degree polynomial, without the Verifier having to read the entire function.\n\nThe core mechanism of FRI is recursive: it takes a large set of function evaluations (a Reed-Solomon codeword) and repeatedly “folds” it, generating new, smaller codewords at each step.The Verifier performs consistency checks between these layers. The final layer is a single constant that the Verifier can check directly.","type":"content","url":"/coding-theory/list-decoding#what-is-fri-a-high-level-view","position":45},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"The Prover, The Verifier, and The Challenge"},"type":"lvl2","url":"/coding-theory/list-decoding#the-prover-the-verifier-and-the-challenge","position":46},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"The Prover, The Verifier, and The Challenge"},"content":"The Prover’s Commitment: The Prover begins with a computational trace, which can be represented as a polynomial P(x). They evaluate this polynomial over a large domain to create a Reed-Solomon codeword, C, and commit to this codeword.\n\nThe Verifier’s Goal: The Verifier wants to check that the committed codeword C is indeed a valid codeword from the code of low-degree polynomials. However, they want to do this very efficiently, by querying only a few positions of C.\n\nThe Cheater’s Strategy: A malicious Prover might commit to a codeword C' that is not a low-degree polynomial but is cleverly constructed to look like one on a few positions.","type":"content","url":"/coding-theory/list-decoding#the-prover-the-verifier-and-the-challenge","position":47},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"Soundness: Where List Decoding Comes In"},"type":"lvl2","url":"/coding-theory/list-decoding#soundness-where-list-decoding-comes-in","position":48},{"hierarchy":{"lvl1":"How List Decoding Secures the FRI Protocol","lvl2":"Soundness: Where List Decoding Comes In"},"content":"The security of the FRI protocol against a malicious Prover is guaranteed by the principles of list decoding. The argument is as follows:\n\nA commitment from a cheating Prover can be seen as a “received word” y that is, by design, far from the code \\mathcal{C} of true low-degree polynomials.The Verifier’s random checks are equivalent to sampling positions of this received word y and a potential codeword c to see if they agree.\n\nList-decoding theory gives us hard bounds on this scenario.We know that if a received word y is far from the entire code \\mathcal{C}, it cannot have a high level of agreement with any single codeword c \\in \\mathcal{C}.If it did, it would imply y is “close” to c, which contradicts our assumption that the Prover is cheating.\n\nThe parameters of the FRI protocol are specifically chosen such that a cheating Prover would need their commitment to agree with a low-degree polynomial on an impossibly high number of positions to pass the Verifier’s checks.The list-decoding bounds we’ve discussed prove that this is not possible, ensuring that any attempt to cheat will be detected with very high probability.","type":"content","url":"/coding-theory/list-decoding#soundness-where-list-decoding-comes-in","position":49},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes"},"type":"lvl1","url":"/coding-theory/reed-solomon-codes","position":0},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes"},"content":"","type":"content","url":"/coding-theory/reed-solomon-codes","position":1},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl2","url":"/coding-theory/reed-solomon-codes#part-1-polynomials-and-finite-fields","position":2},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"","type":"content","url":"/coding-theory/reed-solomon-codes#part-1-polynomials-and-finite-fields","position":3},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Polynomials and Finite Fields","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#polynomials-and-finite-fields","position":4},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Polynomials and Finite Fields","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"The magic behind Reed-Solomon codes lies in the algebra of polynomials over finite fields. Before we can build the codes, we need to build our tools. This section covers the essentials of creating and working with these special mathematical objects.","type":"content","url":"/coding-theory/reed-solomon-codes#polynomials-and-finite-fields","position":5},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"A Quick Recap of Polynomials","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#a-quick-recap-of-polynomials","position":6},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"A Quick Recap of Polynomials","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"Let’s quickly refresh some basic definitions. We’ll be working with a finite field, which you can think of as a set of numbers where addition, subtraction, multiplication, and division are all well-defined. The simplest example is F_p = \\{0, 1, \\dots, p - 1\\} where p is a prime number, and all operations are done modulo p.\n\nA polynomial F(X) over a field F_q is an expression of the formF(X) = f_d X^d + \\dots + f_1 X + f_0\n\n,where the coefficients f_i are all elements of F_q.\n\nThe degree of the polynomial, \\text{deg}(F), is the highest power of X with a non-zero coefficient.\n\nEvaluation means plugging in a value \\alpha \\in F_q for X to get a result F(\\alpha) \\in F_q.An \\alpha is a root of F(X) if F(\\alpha) = 0.\n\nDegree Mantra :a non-zero polynomial of degree t over a field can have at most t distinct roots.\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#a-quick-recap-of-polynomials","position":7},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Irreducibility and Field Extensions","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#irreducibility-and-field-extensions","position":8},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Irreducibility and Field Extensions","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"An irreducible polynomial is a polynomial that cannot be factored into two non-constant polynomials of smaller degree.\n\nFor example, over \\mathbb{F}_2, the polynomial X^2 + X + 1 is irreducible because its only possible factors are X and X + 1, and neither divides it. However, X^2 + 1 is reducible over \\mathbb{F}_2 since X^2 + 1 = (X + 1)(X + 1).\n\nThe most powerful application of irreducible polynomials is creating field extensions. If we have a prime field \\mathbb{F}_p and an irreducible polynomial E(X) of degree s over that field, we can construct a new, larger field:\\mathbb{F}_{p^s} \\triangleq \\mathbb{F}_p[X]/E(X)\n\nThis new field contains all polynomials over \\mathbb{F}_p with degree less than s. This means every element looks like\n\na_{s - 1} X^{s - 1} + \\dots + a_1 X + a_0.\n\nSince each of the s coefficients can be any of the p values from \\mathbb{F}_p, the new field has exactly p^s elements.\n\nArithmetic in this field works as follows:\n\nAddition: Standard polynomial addition, with coefficients added modulo p.\n\nMultiplication: Standard polynomial multiplication, followed by taking the remainder of the division by the irreducible polynomial E(X).","type":"content","url":"/coding-theory/reed-solomon-codes#irreducibility-and-field-extensions","position":9},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Example: Constructing \\mathbb{F}_{7^2} ","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#example-constructing-mathbb-f-7-2","position":10},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Example: Constructing \\mathbb{F}_{7^2} ","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"Let’s work over our chosen field, \\mathbb{F}_7. The polynomial E(X) = X^2 + 3 is irreducible over \\mathbb{F}_7 (you can check it has no roots in \\mathbb{F}_7). We can use it to construct the field \\mathbb{F}_{7^2}, which has 7^2 = 49 elements. The elements are all linear polynomials of the form aX + b where a, b \\in \\mathbb{F}_7.\n\nLet’s take two elements in this field: A(X) = 2X + 5 and B(X) = 3X + 1.\n\nAddition: A(X) + B(X) = (2 + 3)X + (5 + 1) = 5X + 6.\n\nMultiplication:A(X) \\cdot B(X) = (2X + 5)(3X + 1) = 6X^2 + 17X + 5.\n\nFirst, reduce coefficients mod 7: 6X^2 + 3X + 5.\n\nNow, find the remainder when dividing by E(X) = X^2 + 3. From X^2 + 3 = 0, we know X^2 \\equiv -3 \\equiv 4 \\pmod{E(X)}, 7.\n\nSubstitute this in: 6(4) + 3X + 5 = 24 + 3X + 5 = 29 + 3X.\n\nFinally, reduce the coefficients again: 1 + 3X.\n\nSo, in \\mathbb{F}_{7^2},(2X + 5) \\cdot (3X + 1) = 3X + 1.\n\nimport numpy as np\nfrom numpy.polynomial import polynomial as P\n\ndef poly_add_mod(p1, p2, q):\n    p_add = P.polyadd(p1.coef, p2.coef)\n    return P.Polynomial(p_add % q)\n\ndef poly_mul_mod(p1, p2, q):\n    p_mul = P.polymul(p1.coef, p2.coef)\n    return P.Polynomial(p_mul % q)\n\ndef poly_divmod_mod(p1, p2, q):\n    \"\"\"\n    Performs polynomial division over F_q.\n    Returns quotient and remainder.\n    \"\"\"\n    # Make copies to avoid modifying original objects\n    p1_coef = np.copy(p1.coef).astype(int)\n    p2_coef = np.copy(p2.coef).astype(int)\n\n    if len(p1_coef) < len(p2_coef):\n        return P.Polynomial([0]), p1\n\n    # Find multiplicative inverse of leading coefficient of divisor\n    lead_inv = pow(int(p2_coef[-1]), q - 2, q)\n    \n    quotient = np.zeros(len(p1_coef) - len(p2_coef) + 1, dtype=int)\n\n    while len(p1_coef) >= len(p2_coef):\n        deg_diff = len(p1_coef) - len(p2_coef)\n        \n        coef = (p1_coef[-1] * lead_inv) % q\n        quotient[deg_diff] = coef\n\n        term = (p2_coef * coef) % q\n        \n        # Ensure 'term' is aligned with the part of p1_coef it's being subtracted from\n        sub_len = len(p1_coef) - deg_diff\n        p1_coef[-sub_len:] = (p1_coef[-sub_len:] - term) % q\n        \n        # Remove leading zeros from the remainder\n        while len(p1_coef) > 0 and p1_coef[-1] == 0:\n            p1_coef = p1_coef[:-1]\n\n    if len(p1_coef) == 0:\n        remainder_poly = P.Polynomial([0])\n    else:\n        remainder_poly = P.Polynomial(p1_coef)\n        \n    return P.Polynomial(quotient), remainder_poly\n\n\n# ---- Example in GF(7^2) ----\nq = 7\n# E(X) = X^2 + 3, which is irreducible over F_7\nE = P.Polynomial([3, 0, 1]) \n\nA = P.Polynomial([5, 2]) # 2X + 5\nB = P.Polynomial([1, 3]) # 3X + 1\n\nprint(f\"Field: F_{q}\")\nprint(f\"Irreducible Polynomial E(X): {E}\")\nprint(\"-\" * 20)\nprint(f\"A(X) = {A}\")\nprint(f\"B(X) = {B}\")\nprint(\"-\" * 20)\n\n\n# Addition in GF(7^2)\nC_add = poly_add_mod(A, B, q)\nprint(f\"A(X) + B(X) = {C_add}\")\n\n# Multiplication in GF(7^2)\nC_mul_prod = poly_mul_mod(A, B, q)\n_, C_mul_rem = poly_divmod_mod(C_mul_prod, E, q)\n\nprint(f\"A(X) * B(X) = {C_mul_prod}\")\nprint(f\"  ... mod E(X) => {C_mul_rem}\")\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#example-constructing-mathbb-f-7-2","position":11},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Finding an Irreducible Polynomial","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#finding-an-irreducible-polynomial","position":12},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Finding an Irreducible Polynomial","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"To construct these fields, we need a reliable way to find an irreducible polynomial of a given degree s. While there are deterministic ways, a simple and effective method is a randomized algorithm:\n\nGenerate a random monic polynomial F(X) of degree s with coefficients in \\mathbb{F}_q.\n\nTest if F(X) is irreducible.\n\nIf it is, you’re done! If not, go back to step 1.\n\nThe key is the test in step 2. A polynomial F(X) of degree s is irreducible over \\mathbb{F}_q iff it satisfies two conditions:\n\nF(X) divides X^{q^{\\,s}} - X.\n\nFor every prime factor d of s, the greatest common divisor\n\\gcd\\!\\big(F(X),\\, X^{q^{\\,s/d}} - X\\big) is 1.","type":"content","url":"/coding-theory/reed-solomon-codes#finding-an-irreducible-polynomial","position":13},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Condition 1: F(X) must divide X^{q^{s}} - X","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#condition-1-f-x-must-divide-x-q-s-x","position":14},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Condition 1: F(X) must divide X^{q^{s}} - X","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"This is the “Belonging” Test. The polynomial X^{q^{s}} - X is very special: its roots are all the elements of the field \\mathbb{F}_{q^s}. If F(X) is genuinely an irreducible polynomial of degree s, its roots must live in \\mathbb{F}_{q^s}. Therefore, F(X) must be a factor of the polynomial that defines the entire field. This check confirms that F(X)’s roots are in the correct target field.","type":"content","url":"/coding-theory/reed-solomon-codes#condition-1-f-x-must-divide-x-q-s-x","position":15},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Condition 2: \\gcd(F(X),\\, X^{q^{\\,s/d}} - X) = 1","lvl2":"Part 1: Polynomials and Finite Fields"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#condition-2-gcd-f-x-x-q-s-d-x-1","position":16},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Condition 2: \\gcd(F(X),\\, X^{q^{\\,s/d}} - X) = 1","lvl2":"Part 1: Polynomials and Finite Fields"},"content":"This is the “Minimality” Test. The field \\mathbb{F}_{q^s} contains smaller subfields, like \\mathbb{F}_{q^{s/d}}, for every prime factor d of s. If the \\gcd in this test is not 1, it means F(X) shares roots with a polynomial that defines a smaller subfield. This would imply F(X) is reducible, with factors belonging to that smaller field. An irreducible polynomial of degree s must be “native” to \\mathbb{F}_{q^s}, so this test ensures its roots aren’t secretly from a smaller subfield.\n\nIn essence: Condition 1 checks that the roots are in the right field, while Condition 2 checks they aren’t in any smaller field. Together, they prove that F(X) must be irreducible of degree s.\n\nThis gives us a concrete algorithm to find our building blocks.\n\n\ndef get_prime_factors(n):\n    factors = set()\n    d = 2\n    temp_n = n\n    while d * d <= temp_n:\n        if temp_n % d == 0:\n            factors.add(d)\n            while temp_n % d == 0:\n                temp_n //= d\n        d += 1\n    if temp_n > 1:\n        factors.add(temp_n)\n    return factors\n\ndef poly_gcd_mod(p1, p2, q):\n    a, b = p1, p2\n    while b.degree() > -1 and np.any(b.coef != 0): \n        _, r = poly_divmod_mod(a, b, q)\n        a, b = b, r\n    # Normalize to make it monic\n    if a.degree() > -1:\n        lead_inv = pow(int(a.coef[-1]), q - 2, q)\n        a = P.Polynomial((a.coef * lead_inv) % q)\n    return a\n\ndef poly_pow_mod(base, exp, mod_poly, q):\n    res = P.Polynomial([1])\n    base_rem = poly_divmod_mod(base, mod_poly, q)[1]\n\n    while exp > 0:\n        if exp % 2 == 1:\n            res_prod = poly_mul_mod(res, base_rem, q)\n            res = poly_divmod_mod(res_prod, mod_poly, q)[1]\n        \n        base_rem_sq = poly_mul_mod(base_rem, base_rem, q)\n        base_rem = poly_divmod_mod(base_rem_sq, mod_poly, q)[1]\n        exp //= 2\n    return res\n\ndef is_irreducible(F, q, s):\n    \"\"\"\n    Tests if a polynomial F of degree s is irreducible over F_q.\n    \"\"\"\n    # 1. Test if F(X) divides X^(q^s) - X\n    # This is equivalent to X^(q^s) = X (mod F(X))\n    X = P.Polynomial([0, 1])\n    # Use modular exponentiation for efficiency\n    x_pow = poly_pow_mod(X, q**s, F, q)\n    \n    rem = poly_divmod_mod(x_pow - X, F, q)[1]\n    if rem.degree() > -1 and np.any(rem.coef != 0):\n        return False # F(X) does not divide X^(q^s) - X\n\n    # 2. Test gcd condition for all prime factors of s\n    prime_factors_s = get_prime_factors(s)\n    for d in prime_factors_s:\n        exp = q**(s // d)\n        x_pow = poly_pow_mod(X, exp, F, q)\n        gcd = poly_gcd_mod(F, x_pow - X, q)\n        if gcd.degree() > 0:\n            return False # gcd is not 1\n\n    return True\n\ndef find_irreducible_poly(q, s):\n    \"\"\"Finds a random monic irreducible polynomial of degree s over F_q.\"\"\"\n    print(f\"\\nSearching for a monic irreducible polynomial of degree {s} over F_{q}...\")\n    while True:\n        # Generate a random monic polynomial of degree s\n        coeffs = np.random.randint(0, q, s)\n        coeffs = np.append(coeffs, 1) # Make it monic\n        F = P.Polynomial(coeffs)\n        \n        if is_irreducible(F, q, s):\n            print(f\"Found one: {F}\")\n            return F\n\n# --- Example of finding a polynomial ---\nfind_irreducible_poly(q=7, s=2)\nfind_irreducible_poly(q=2, s=4);\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#condition-2-gcd-f-x-x-q-s-d-x-1","position":17},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"type":"lvl2","url":"/coding-theory/reed-solomon-codes#part-2-constructing-reed-solomon-codes","position":18},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"content":"Now we get to the core of the topic. A Reed-Solomon (RS) code is created through a beautifully simple process: messages\nare turned into polynomials, and codewords are generated by evaluating those polynomials at several points.","type":"content","url":"/coding-theory/reed-solomon-codes#part-2-constructing-reed-solomon-codes","position":19},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"How Reed–Solomon Encoding Works","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#how-reed-solomon-encoding-works","position":20},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"How Reed–Solomon Encoding Works","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"content":"The definition of an RS code gives us a clear recipe for encoding:\n\nStart with the parameters:\n\nA finite field, \\mathbb{F}_q. We’ll stick with \\mathbb{F}_7.\n\nA message length k.\n\nA block length (codeword length) n. We must have k \\le n.\n\nMap Message to Polynomial:Take a message \\mathbf{m} = (m_0, m_1, \\dots, m_{k-1}), which is a vector of k symbols from \\mathbb{F}_q.We treat these symbols as coefficients to form a message polynomial of degree at most k-1:f_{\\mathbf{m}}(X) = m_0 + m_1 X + \\cdots + m_{k-1} X^{k-1}.\n\nEvaluate:Choose n distinct evaluation points (\\alpha_1,\\alpha_2,\\dots,\\alpha_n) from \\mathbb{F}_q.The final codeword is the evaluation of the message polynomial at each of these points:\\text{Codeword}=(f_{\\mathbf{m}}(\\alpha_1),\\, f_{\\mathbf{m}}(\\alpha_2),\\, \\dots,\\, f_{\\mathbf{m}}(\\alpha_n)).","type":"content","url":"/coding-theory/reed-solomon-codes#how-reed-solomon-encoding-works","position":21},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Example: An RS code over \\mathbb{F}_7","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#example-an-rs-code-over-mathbb-f-7","position":22},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Example: An RS code over \\mathbb{F}_7","lvl2":"Part 2: Constructing Reed–Solomon Codes"},"content":"Define an RS code with parameters [n,k]=[5,3] over \\mathbb{F}_7.\n\nMessage: A vector of k=3 symbols, e.g., \\mathbf{m}=(6,1,2).\n\nPolynomial: This message maps to the polynomial f(X)=2X^2 + X + 6.\n\nEvaluation Points: We need n=5 distinct points from \\mathbb{F}_7. Let’s choose the set \\{1,2,3,4,5\\}.\n\nEncoding: Calculate f(1), f(2), f(3), f(4), f(5) modulo 7.\n\nf(1)=2(1)^2+1+6=9 \\equiv 2 \\pmod{7}\n\nf(2)=2(2)^2+2+6=16 \\equiv 2 \\pmod{7}\n\n…and so on.\n\nThe resulting 5-symbol vector is our codeword. (For the example above, it is (2,2,6,0,5).)\n\nimport numpy as np\nfrom numpy.polynomial import polynomial as P\n\n# --- RS Encoder ---\n\ndef rs_encoder(message_vec, n, q, eval_points):\n    \"\"\"\n    Encodes a message vector into a Reed-Solomon codeword.\n    \n    Args:\n        message_vec (list or np.array): The k-element message from F_q.\n        n (int): The block length of the code.\n        q (int): The size of the finite field.\n        eval_points (list or np.array): The n distinct evaluation points.\n        \n    Returns:\n        np.array: The n-element codeword.\n    \"\"\"\n    if len(eval_points) != n:\n        raise ValueError(\"The number of evaluation points must be equal to n.\")\n    \n    message_poly = P.Polynomial(message_vec)\n    \n    print(f\"Message m = {message_vec}  -->  Polynomial f(X) = {message_poly}\")\n    \n    # Evaluate the polynomial at each point in the evaluation set\n    codeword = []\n    for alpha in eval_points:\n        val = message_poly(alpha) % q\n        codeword.append(int(val))\n        \n    return np.array(codeword)\n\n# --- Example of Encoding ---\nq = 7\nn = 5\nk = 3\n# Choose n distinct evaluation points from F_7\neval_points = [1, 2, 3, 4, 5] \n\n# A message m is a vector of k=3 elements from F_7\nm1 = [6, 1, 2] # Represents f(X) = 6 + 1*X + 2*X^2\n\nprint(f\"--- Encoding with RS[{n}, {k}] over F_{q} ---\")\nc1 = rs_encoder(m1, n, q, eval_points)\nprint(f\"Evaluation Points: {eval_points}\")\nprint(f\"Resulting Codeword c1 = {c1}\")\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#example-an-rs-code-over-mathbb-f-7","position":23},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl2","url":"/coding-theory/reed-solomon-codes#properties-of-reed-solomon-codes","position":24},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl2":"Properties of Reed–Solomon Codes"},"content":"RS codes are widely used because they have excellent, provable\nproperties.\n\nLinearity: RS codes are linear codes. This means\nthat if you add two messages and then encode the sum, you get the\nsame result as if you first encode each message and then add the\nresulting codewords:\\mathrm{Encode}(\\mathbf m_1 + \\mathbf m_2)\n\\;=\\;\n\\mathrm{Encode}(\\mathbf m_1) + \\mathrm{Encode}(\\mathbf m_2).\n\nDistance: The minimum distance\nof an RS code isd = n - k + 1.\n\nThis is the largest possible distance for any linear code with\nparameters n and k, a limit known as the Singleton Bound.\nBecause RS codes achieve this bound, they are considered\noptimal in terms of their distance and are known as\nMaximum Distance Separable (MDS) codes.\n\nThe proof is elegant: consider two different messages\n\\mathbf m_1 and \\mathbf m_2, and their polynomials\nf_1(X) and f_2(X). The number of places their codewords\nagree is the number of roots of the difference polynomialg(X) = f_1(X) - f_2(X).\n\nSince the degree of g(X) is at most k-1, it can have at most\nk-1 roots. Therefore, the codewords can agree in at most\nk-1 positions, meaning they must differ in at leastn - (k - 1) = n - k + 1\n\npositions.\n\nLet’s verify these properties with our code.\n\ndef hamming_distance(v1, v2):\n    if len(v1) != len(v2):\n        raise ValueError(\"Vectors must have the same length.\")\n    return np.sum(v1 != v2)\n\n# --- Verifying Properties ---\n\n# 1. Linearity\nprint(\"\\n--- Verifying Linearity ---\")\nm2 = [1, 3, 1] \nprint(\"Recalling our initial messages and codewords:\")\nprint(f\"m1 = {m1}\")\nc1 = rs_encoder(m1, n, q, eval_points) \nprint(f\"--> c1 = {c1}\\n\")\n\nprint(f\"m2 = {m2}\")\nc2 = rs_encoder(m2, n, q, eval_points)\nprint(f\"--> c2 = {c2}\\n\")\n\n\n# Encode(m1 + m2)\nm_sum_np = (np.array(m1) + np.array(m2)) % q\nm_sum = m_sum_np.tolist() \n\nprint(\"---\")\nprint(\"Encoding the sum of messages m1+m2:\")\nc_sum = rs_encoder(m_sum, n, q, eval_points)\nprint(f\"Result: {c_sum}\\n\")\n\n# Encode(m1) + Encode(m2)\nc1_plus_c2 = (c1 + c2) % q\nprint(\"Adding the codewords c1+c2:\")\nprint(f\"Result: {c1_plus_c2}\\n\")\n\nassert np.array_equal(c_sum, c1_plus_c2), \"Linearity test failed!\"\nprint(\"Linearity holds: Encode(m1+m2) == Encode(m1) + Encode(m2)\")\n\n\n# 2. Distance\nprint(\"\\n--- Verifying Distance ---\")\nd_theory = n - k + 1\nprint(f\"Theoretical minimum distance d = n-k+1 = {n}-{k}+1 = {d_theory}\")\n\ndist_c1_c2 = hamming_distance(c1, c2)\nprint(f\"\\nActual distance between c1 and c2: {dist_c1_c2}\")\nassert dist_c1_c2 >= d_theory\n\n# Let's try another random message\nm3 = [5, 4, 1]\nprint(\"\\nComparing c1 with a new codeword c3:\\n\")\nc3 = rs_encoder(m3, n, q, eval_points)\nprint(f\"--> c3 = {c3}\")\ndist_c1_c3 = hamming_distance(c1, c3)\nprint(f\"Actual distance between c1 and c3: {dist_c1_c3}\")\nassert dist_c1_c3 >= d_theory\n\nprint(\"\\nConstructing a pair with the minimum possible distance.\")\n\nc_zero = rs_encoder(m_zero, n, q, eval_points)\nprint(f\"\\n--> Codeword for all-zero message: {c_zero}\")\nc_diff = rs_encoder(m_diff, n, q, eval_points)\nprint(f\"\\n--> Codeword for difference message: {c_diff}\")\n\ndist_min = hamming_distance(c_zero, c_diff)\nprint(f\"\\nThe distance is {dist_min}. This matches the theoretical minimum d_min = {d_theory}.\")\nassert dist_min == d_theory\n\n\nprint(\"\\nDistance property holds and we have demonstrated the minimum case.\")\n\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#properties-of-reed-solomon-codes","position":25},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#the-missing-link-why-do-rs-codes-need-field-extensions","position":26},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"content":"So far, we’ve introduced two major concepts:\n\nField Extensions F_{p^s}: A way to construct large fields with q = p^s elements.\n\nReed–Solomon Codes: A method of encoding that works over any finite field F_q.\n\nBut why are they so important to each other? The connection lies in the length of the codeword.","type":"content","url":"/coding-theory/reed-solomon-codes#the-missing-link-why-do-rs-codes-need-field-extensions","position":27},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl4":"The Problem: The Alphabet Size Constraint","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl4","url":"/coding-theory/reed-solomon-codes#the-problem-the-alphabet-size-constraint","position":28},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl4":"The Problem: The Alphabet Size Constraint","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"content":"The Reed–Solomon encoding process requires us to choose n distinct evaluation points from our field F_q. This leads to a fundamental constraint: the block length n must be less than or equal to the field size q (n \\le q).\n\nIf we only use simple prime fields, our codes are forced to be very short. For example, in our notebook we used F_7. The longest possible RS code we could build over this field is just 7 symbols long. This is far too short for most real-world applications like data storage or digital communication, where we need to handle thousands of bits of data.","type":"content","url":"/coding-theory/reed-solomon-codes#the-problem-the-alphabet-size-constraint","position":29},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl4":"The Solution: Creating Large Alphabets","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl4","url":"/coding-theory/reed-solomon-codes#the-solution-creating-large-alphabets","position":30},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl4":"The Solution: Creating Large Alphabets","lvl3":"The Missing Link: Why Do RS Codes Need Field Extensions?","lvl2":"Properties of Reed–Solomon Codes"},"content":"This is where field extensions become essential. They are the tool we use to construct massive alphabets from small, simple ones.\n\nThe most common choice in practice is to start with the simplest field, F_2 = \\{0,1\\}, and build an extension. For example, by choosing an irreducible polynomial of degree 8 over F_2, we can construct the field extension F_{2^8}.\n\nThis new field has q = 2^8 = 256 elements. Now, our alphabet size is 256. This means we can create RS codes with a block length up to n = 255 (using the non-zero elements as evaluation points).\n\nCrucially, each “symbol” in F_{2^8} corresponds to a block of 8 bits, which is exactly one byte. So, we can think of an RS code over F_{2^8} as a code that operates on bytes instead of individual bits.\n\nThe real-world example is the RS[255, 223] code over F_{2^8}:\n\nIt takes a message of k = 223 bytes.\n\nIt adds n - k = 32 parity bytes.\n\nIt produces a codeword of n = 255 bytes.\n\nThis code can correct up to (n - k)/2 = 16 byte errors anywhere in the codeword.\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#the-solution-creating-large-alphabets","position":31},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Part 3: The MDS Property","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#part-3-the-mds-property","position":32},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Part 3: The MDS Property","lvl2":"Properties of Reed–Solomon Codes"},"content":"We’ve seen that Reed–Solomon codes are optimal with respect to their distance, meeting the Singleton Bound. This class of codes has a special name.\n\nMDS Codes: An [n, k, d]_q code is called Maximum Distance Separable (MDS) if its parameters satisfy d = n - k + 1.\n\nAs we proved, Reed–Solomon codes are MDS codes. This property implies something profound about their structure, which makes them appear very “random” and uniform.","type":"content","url":"/coding-theory/reed-solomon-codes#part-3-the-mds-property","position":33},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"The Core Property of MDS Codes","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#the-core-property-of-mds-codes","position":34},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"The Core Property of MDS Codes","lvl2":"Properties of Reed–Solomon Codes"},"content":"The key property of MDS codes (stated in Proposition 5.3.3) is a powerful statement about their uniformity. Let’s use our running example of an RS[5, 3] code over the field \\mathbb{F}_7 to understand it.\n\nFor this code, we have:\n\nq = 7 (our alphabet is \\{0, 1, 2, 3, 4, 5, 6\\})\n\nk = 3 (our messages are 3 symbols long)\n\nn = 5 (our codewords are 5 symbols long)\n\nThere are q^k = 7^3 = 343 possible messages we can encode, ranging from (0, 0, 0), (0, 0, 1), all the way to (6, 6, 6). Each of these 343 messages is encoded into a unique codeword of length 5.\n\nImagine a table with 343 rows and 5 columns. Each row is a different codeword.\n\nMessage\n\nCodeword (5 columns)\n\n(0,0,0)\n\n(c_{1,1}, c_{1,2}, c_{1,3}, c_{1,4}, c_{1,5})\n\n(0,0,1)\n\n(c_{2,1}, c_{2,2}, c_{2,3}, c_{2,4}, c_{2,5})\n\n...\n\n...\n\n(6,6,6)\n\n(c_{343,1}, c_{343,2}, c_{343,3}, c_{343,4}, c_{343,5})\n\nThe MDS property guarantees the following: if you pick any k = 3 columns from this table (say, columns 2, 3, and 5) and look down the 343 rows, the list of 3-symbol vectors you see will be a perfect list of all 343 possible combinations, from (0,0,0) to (6,6,6), with each appearing exactly once.\n\nThis remarkable uniformity is what makes MDS codes so special. It means no information is “clumped together”; it is spread out as evenly as possible across the codeword.","type":"content","url":"/coding-theory/reed-solomon-codes#the-core-property-of-mds-codes","position":35},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"How is this possible?","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#how-is-this-possible","position":36},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"How is this possible?","lvl2":"Properties of Reed–Solomon Codes"},"content":"This property is a direct result of the code’s polynomial structure. It guarantees that for any choice of k positions and any desired k-symbol target vector \\mathbf{v}, we can find a unique message polynomial f(X) that “hits” those values. This requires solving a system of linear equations where the matrix is an invertible Vandermonde matrix, which always has a unique solution.\n\n","type":"content","url":"/coding-theory/reed-solomon-codes#how-is-this-possible","position":37},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Demonstrating the MDS Property","lvl2":"Properties of Reed–Solomon Codes"},"type":"lvl3","url":"/coding-theory/reed-solomon-codes#demonstrating-the-mds-property","position":38},{"hierarchy":{"lvl1":"An Introduction to Reed-Solomon Codes","lvl3":"Demonstrating the MDS Property","lvl2":"Properties of Reed–Solomon Codes"},"content":"Let’s prove this with code. We will take our RS[5, 3]_7 code and challenge it: can we find a message that produces the specific sub-vector [4, 0, 1] at positions 0, 2, 4?\n\nThis means we need to find a message\\textbf{m} = (m_0, m_1, m_2)such that its polynomial f(X) satisfies:\n\nf(\\alpha_0) = f(1) = 4\n\nf(\\alpha_2) = f(3) = 0\n\nf(\\alpha_4) = f(5) = 1\n\nimport numpy as np\nfrom numpy.polynomial import polynomial as P\n\n# --- Helper function for modular inverse of a matrix ---\ndef matrix_inverse_mod(matrix, q):\n    \"\"\"Calculates the inverse of a matrix in a finite field F_q.\"\"\"\n    det = int(np.round(np.linalg.det(matrix)))\n    det_inv = pow(det, q - 2, q)\n    adjugate = np.round(det * np.linalg.inv(matrix)).astype(int)\n    return (adjugate * det_inv) % q\n\ndef find_message_for_projection(target_indices, target_vector, eval_points, q, k):\n    \"\"\"\n    Finds the unique message that produces a target vector at specific indices.\n    Also returns the Vandermonde matrix used.\n    \"\"\"\n    sub_eval_points = [eval_points[i] for i in target_indices]\n    \n    # Construct the k x k Vandermonde matrix\n    V = np.vander(sub_eval_points, k, increasing=True).astype(int)\n    \n    # Find the inverse of the Vandermonde matrix mod q\n    V_inv = matrix_inverse_mod(V, q)\n    \n    # Solve for the message m = V_inv * v\n    message_np = (V_inv @ np.array(target_vector)) % q\n    message = message_np.tolist()\n    \n    return message, V\n\n# --- Setup from before ---\nq = 7\nn = 5\nk = 3\neval_points = [1, 2, 3, 4, 5]\n\n# --- The Challenge ---\ntarget_indices = [0, 2, 4] \ntarget_vector = [4, 0, 1]\n\nprint(\"Challenge: Find `m` for a specific projection.\")\nprint(f\"Target vector v = {target_vector} at indices = {target_indices}\")\n\nfound_message, V_matrix = find_message_for_projection(target_indices, target_vector, eval_points, q, k)\n\n# --- Show the equation V * m = v with numbers ---\nprint(\"\\n--- Verifying the equation V * m = v ---\\n\")\nprint(\"V (Vandermonde matrix):\")\nprint(V_matrix)\nprint(f\"\\nm (Found message): {found_message}\")\nprint(f\"\\nv (Target vector): {target_vector}\")\n\n# Check the multiplication\nresult_vector = (V_matrix @ np.array(found_message)) % q\nprint(f\"\\nResult of V * m (mod {q}): {result_vector.tolist()}\")\nassert np.array_equal(result_vector, target_vector)\n\n\n# --- Final verification by encoding ---\nprint(\"\\n--- Verifying by encoding the found message ---\")\nfound_codeword = rs_encoder(found_message, n, q, eval_points)\nprint(f\"Full codeword c = {found_codeword}\")\n\n# Check if the codeword matches the target at the specified indices\nprojection = [int(found_codeword[i]) for i in target_indices] # Fix for np.int64\nprint(f\"Codeword projection at indices {target_indices}: {projection}\")\n\nassert projection == target_vector\nprint(\"\\n Success! The property holds. A unique message was found.\")","type":"content","url":"/coding-theory/reed-solomon-codes#demonstrating-the-mds-property","position":39},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise"},"type":"lvl1","url":"/coding-theory/shannon-theorem","position":0},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise"},"content":"So far, we have looked at codes from the Hamming perspective, where we assume a “worst-case” scenario: up to a certain number of errors (t) will occur, and our code must be able to handle it. This is an adversarial model.\n\nThis notebook introduces a different perspective. Instead of a worst-case scenario, Shannon modeled noise stochastically, or probabilistically. He viewed communication as a random process where errors happen with a certain likelihood, not with a guarantee.","type":"content","url":"/coding-theory/shannon-theorem","position":1},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"The Communication Process"},"type":"lvl2","url":"/coding-theory/shannon-theorem#the-communication-process","position":2},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"The Communication Process"},"content":"Shannon’s framework separates the problem of communication into two distinct parts, as shown in the general model below.\n\nSource Coding (Compression): This part deals with the message itself before noise is a factor. The goal of the Source Encoder is to remove redundancy from the original data to make it as compact as possible. The theoretical limit of this compression is related to the entropy of the source. Think of this as zipping a file.\n\nChannel Coding (Error Correction): This is the part we’ve been focused on. After the message is compressed, the Channel Encoder adds “smart” redundancy back in. This redundancy is not a simple repetition; it’s structured (like in an RS code) to protect the data from errors that occur when it’s sent over the noisy Channel.\n\nShannon showed that these two problems can be studied separately. For the rest of this chapter, we will ignore source coding and focus exclusively on channel coding in a world where the noise is random.\n\n","type":"content","url":"/coding-theory/shannon-theorem#the-communication-process","position":3},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl2","url":"/coding-theory/shannon-theorem#shannons-noise-models-from-bsc-to-channel-capacity","position":4},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"Shannon’s framework begins with a formal model of a noisy channel. It consists of three key parts:\n\nInput alphabet \\,\\mathcal{X}\\, — the set of symbols we can send.\n\nOutput alphabet \\,\\mathcal{Y}\\, — the set of symbols we might receive.\n\nTransition matrix \\,M\\, — contains crossover probabilities. The entry M(x,y) is the probability of receiving symbol y given that we sent symbol x, written as \\Pr(y \\mid x).\n\nChannels are typically assumed to be memoryless, meaning the noise on each transmitted symbol is an independent event.","type":"content","url":"/coding-theory/shannon-theorem#shannons-noise-models-from-bsc-to-channel-capacity","position":5},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl3","url":"/coding-theory/shannon-theorem#common-stochastic-channels","position":6},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"Let’s look at a specific channel model.","type":"content","url":"/coding-theory/shannon-theorem#common-stochastic-channels","position":7},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Symmetric Channel (BSC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl4","url":"/coding-theory/shannon-theorem#binary-symmetric-channel-bsc","position":8},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Symmetric Channel (BSC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"This is the most fundamental model. The input and output alphabets are both binary:\n\\,\\mathcal{X} = \\mathcal{Y} = \\{0,1\\}.\n\nA bit is transmitted correctly (0 stays 0, 1 stays 1) with probability 1 - p.\n\nA bit is flipped (0 becomes 1, 1 becomes 0) with probability p. This p is the crossover probability.\n\nThe channel can be visualized as:\n\nThe corresponding transition matrix is:M = \\begin{pmatrix}\n1 - p & p \\\\\np & 1 - p\n\\end{pmatrix}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_bsc(codeword, p):\n    \"\"\"Simulates passing a codeword through a Binary Symmetric Channel.\"\"\"\n    received_word = []\n    for bit in codeword:\n        if np.random.rand() < p:\n            # The bit is flipped\n            received_word.append(1 - bit)\n        else:\n            # The bit is transmitted correctly\n            received_word.append(bit)\n    return np.array(received_word)\n\n# Example\noriginal_codeword = np.array([0, 1, 0, 1, 1, 1, 0])\ncrossover_prob = 0.1\nreceived_codeword = simulate_bsc(original_codeword, crossover_prob)\n\nprint(f\"Original codeword: {original_codeword}\")\nprint(f\"Received codeword: {received_codeword} (after BSC with p={crossover_prob})\")\n\n","type":"content","url":"/coding-theory/shannon-theorem#binary-symmetric-channel-bsc","position":9},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"q-ary Symmetric Channel (qSC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl4","url":"/coding-theory/shannon-theorem#q-ary-symmetric-channel-qsc","position":10},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"q-ary Symmetric Channel (qSC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"This model generalizes the BSC to an alphabet of any size q \\ge 2.\n\nA symbol is transmitted correctly with probability 1 - p.\n\nIf an error occurs, the original symbol is transformed into any of the other q - 1 incorrect symbols with equal probability \\,\\frac{p}{q-1}.","type":"content","url":"/coding-theory/shannon-theorem#q-ary-symmetric-channel-qsc","position":11},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Erasure Channel (BEC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl4","url":"/coding-theory/shannon-theorem#binary-erasure-channel-bec","position":12},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Erasure Channel (BEC)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"In the BEC, bits are never flipped, but they can be completely lost or “erased”.\n\nThe input alphabet is \\mathcal{X} = \\{0,1\\}.\n\nThe output alphabet is \\mathcal{Y} = \\{0,1,?\\}, where “?” denotes an erasure.\n\nA bit is transmitted correctly with probability 1 - a.\n\nA bit is erased with probability a.","type":"content","url":"/coding-theory/shannon-theorem#binary-erasure-channel-bec","position":13},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Input Additive Gaussian White Noise (BIAGWN)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl4","url":"/coding-theory/shannon-theorem#binary-input-additive-gaussian-white-noise-biagwn","position":14},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Binary Input Additive Gaussian White Noise (BIAGWN)","lvl3":"Common Stochastic Channels","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"This is a crucial model for continuous channels, like radio waves.\n\nThe input is binary, typically \\mathcal{X}=\\{-1,1\\}.\n\nThe output \\mathcal{Y} is the set of all real numbers (\\mathbb{R}).\n\nThe channel adds random noise drawn from a Gaussian (Normal) distribution to the input.The probability density of receiving y given x is\n\\Pr(y \\mid x) \\;=\\; \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right).\n\n","type":"content","url":"/coding-theory/shannon-theorem#binary-input-additive-gaussian-white-noise-biagwn","position":15},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Vanishing Error Probability","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl3","url":"/coding-theory/shannon-theorem#vanishing-error-probability","position":16},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Vanishing Error Probability","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"In the Hamming model, we could guarantee correction for up to t errors. In a stochastic model, this is impossible. There is always a tiny, non-zero probability that extreme noise could flip more than t bits and make one codeword look like another.\n\nSo, the goal shifts. We no longer aim for a 100% guarantee. Instead, we want to design codes where the probability of a decoding error can be made arbitrarily small. For a code of length n, we want the decoding error probability P(\\text{error}) to approach 0 as n \\to \\infty.","type":"content","url":"/coding-theory/shannon-theorem#vanishing-error-probability","position":17},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Shannon’s Noisy-Channel Coding Theorem","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"type":"lvl3","url":"/coding-theory/shannon-theorem#shannons-noisy-channel-coding-theorem","position":18},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Shannon’s Noisy-Channel Coding Theorem","lvl2":"Shannon’s Noise Models: From BSC to Channel Capacity"},"content":"This leads us to one of the most important results in information theory. Shannon’s theorem provides a sharp, definitive answer to the question of how much information can be sent reliably over a noisy channel.\n\nThe theorem introduces the concept of Channel Capacity (C). This is a single number, a fundamental property of a channel (like its BSC crossover probability p), which represents the maximum rate at which information can be transmitted over that channel with arbitrarily low error probability.\n\nShannon’s Theorem states:\n\nAchievability (R < C): For any rate R less than the channel capacity C, there exists a coding scheme that allows us to communicate with a probability of error that can be made arbitrarily close to zero (by using long enough codewords).\n\nConverse (R > C): For any rate R greater than the channel capacity C, it is impossible to achieve reliable communication. The probability of error will always be bounded away from zero, no matter how clever our coding scheme is.\n\nThis theorem establishes a fundamental speed limit for communication. For the BSC with crossover probability p, the capacity is given by C = 1 - H(p), where H(p) is the binary entropy function.\n\ndef binary_entropy(p):\n    \"\"\"Calculates the binary entropy H(p).\"\"\"\n    if p == 0 or p == 1:\n        return 0\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\ndef bsc_capacity(p):\n    \"\"\"Calculates the capacity of a BSC with crossover probability p.\"\"\"\n    return 1 - binary_entropy(p)\n\n# --- Plotting the Capacity ---\np_values = np.linspace(0.0, 1.0, 100)\ncapacities = [bsc_capacity(p) for p in p_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(p_values, capacities)\nplt.title(\"Capacity of the Binary Symmetric Channel (BSC)\")\nplt.xlabel(\"Crossover Probability (p)\")\nplt.ylabel(\"Channel Capacity C (bits per channel use)\")\nplt.grid(True)\nplt.axvline(x=0.5, color='r', linestyle='--', label='p=0.5 (Useless Channel)')\nplt.legend()\nplt.show()\n\n","type":"content","url":"/coding-theory/shannon-theorem#shannons-noisy-channel-coding-theorem","position":19},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"type":"lvl2","url":"/coding-theory/shannon-theorem#shannons-theorem-for-the-bsc-the-precise-statement","position":20},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"content":"We’ve discussed channel capacity in general. Now we state Shannon’s theorem for the Binary Symmetric Channel (BSC) with crossover probability p.\n\nRecall: the BSC capacity is C = 1 - H(p), where H(p) is the binary entropy.\n\nThe theorem has two parts: achievability (what’s possible below capacity) and the converse (what’s impossible above capacity).","type":"content","url":"/coding-theory/shannon-theorem#shannons-theorem-for-the-bsc-the-precise-statement","position":21},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"type":"lvl3","url":"/coding-theory/shannon-theorem#theorem-6-3-1-shannons-capacity-theorem-for-the-bsc","position":22},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"content":"","type":"content","url":"/coding-theory/shannon-theorem#theorem-6-3-1-shannons-capacity-theorem-for-the-bsc","position":23},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Part 1: Achievability (Coding Below Capacity)","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"type":"lvl4","url":"/coding-theory/shannon-theorem#part-1-achievability-coding-below-capacity","position":24},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Part 1: Achievability (Coding Below Capacity)","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"content":"If the message length k satisfies k \\le n \\cdot [1 - H(p) - \\varepsilon], then for sufficiently large block length n there exist encoding and decoding functions E, D such that for every message m:\\Pr_{e \\sim \\mathrm{BSC}_p}\\big[\\, D(E(m) + e) \\ne m \\,\\big] \\le 2^{-\\delta n}.\n\nPlain English for the probability expression:\n\n\\Pr_{e \\sim \\mathrm{BSC}_p}[\\cdot]: “The probability that…”, where e is random BSC noise.\n\nE(m): encode the message m to a codeword.\n\n+\\,e: the channel corrupts that codeword by adding noise e (bit flips).\n\nD(\\cdot): decode the received, noisy word.\n\n\\ne m: the decoded message differs from the original.\n\nThe right side, 2^{-\\delta n}, shrinks exponentially in n.\n\nTakeaway: If your rate is just below capacity, you can design codes whose error probability becomes extremely small as n grows.","type":"content","url":"/coding-theory/shannon-theorem#part-1-achievability-coding-below-capacity","position":25},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Part 2: The Converse (Trying to Code Above Capacity)","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"type":"lvl4","url":"/coding-theory/shannon-theorem#part-2-the-converse-trying-to-code-above-capacity","position":26},{"hierarchy":{"lvl1":"Shannon’s Theorem and Probabilistic Noise","lvl4":"Part 2: The Converse (Trying to Code Above Capacity)","lvl3":"Theorem 6.3.1 (Shannon’s Capacity Theorem for the BSC)","lvl2":"Shannon’s Theorem for the BSC: The Precise Statement"},"content":"If the message length k satisfies k \\ge n \\cdot [1 - H(p) + \\varepsilon], then for any encoding/decoding functions E, D, there exists some message m such that\\Pr_{e \\sim \\mathrm{BSC}_p}\\big[\\, D(E(m) + e) \\ne m \\,\\big] > \\tfrac{1}{2}.\n\nPlain English: This probability is greater than 50% for at least one message.\n\nTakeaway: If you transmit faster than capacity, no matter the code, there will always be at least one message that is more likely than not to be decoded incorrectly.\n\nConclusion: Together, these parts pin down a sharp communication limit. Below capacity: reliable communication is achievable. Above capacity: reliability is impossible.","type":"content","url":"/coding-theory/shannon-theorem#part-2-the-converse-trying-to-code-above-capacity","position":27},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes"},"type":"lvl1","url":"/coding-theory/unique-decoding-algorithm","position":0},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes"},"content":"This notebook provides the explanation and implementation of the Welch-Berlekamp algorithm for the unique decoding of Reed-Solomon codes. We will start by defining the unique decoding problem, develop the intuition behind the algorithm using a geometric perspective, and finally build and demonstrate a working Python implementation.","type":"content","url":"/coding-theory/unique-decoding-algorithm","position":1},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Unique Decoding Problem"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#the-unique-decoding-problem","position":2},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Unique Decoding Problem"},"content":"A Reed-Solomon code is created by taking a message, treating it as the coefficients of a message polynomial P(X) of degree less than k, and then evaluating that polynomial at n distinct points (\\alpha_1, \\dots, \\alpha_n) to create a codeword.\n\nWhen this codeword is transmitted, a noisy channel may introduce errors, resulting in a received word y = (y_1, \\dots, y_n). The number of errors, e, is the number of positions where the received word differs from the original codeword.\n\nThe goal of unique decoding is to recover the one and only original polynomial P(X) from the noisy received word y. This is only guaranteed to be possible if the number of errors is strictly less than half the minimum distance of the code. For a Reed-Solomon code, this condition is:e < \\frac{n-k+1}{2}\n\nThis notebook will build the Welch-Berlekamp algorithm, an efficient method for solving this exact problem.\n\n","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-unique-decoding-problem","position":3},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"A Geometric View of the Problem"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#a-geometric-view-of-the-problem","position":4},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"A Geometric View of the Problem"},"content":"To better visualize the problem, we can perform a “syntactic shift” and think of the received word y not as a vector, but as a collection of n points in a 2D plane: \\{(\\alpha_1, y_1), (\\alpha_2, y_2), \\dots, (\\alpha_n, y_n)\\}.\n\nThe image above shows an example of a received word with n=14 points and k=2. The original message was a line (a polynomial of degree k-1=1).\n\nThe original, uncorrupted codeword would consist of points that all lie perfectly on the curve defined by the message polynomial P(X). The effect of noise is to knock some of these points off the curve. The decoder’s job, in this geometric view, is to find the unique curve of degree less than k that passes through the maximum number of these received points.\n\nThe above image illustrates this. The correct polynomial, P(X)=X, passes through the subset of “correct” points, while the “error” points lie scattered off the line.\n\n","type":"content","url":"/coding-theory/unique-decoding-algorithm#a-geometric-view-of-the-problem","position":5},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#the-core-idea-reverse-engineering-a-solution","position":6},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"The Welch-Berlekamp algorithm is designed using a “reverse engineering” approach. We start by assuming we magically know the solution—both the original polynomial P(X) and the locations of the errors—and derive a mathematical property. Then, we use that property to build an algorithm that finds P(X) without knowing the error locations beforehand.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-core-idea-reverse-engineering-a-solution","position":7},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Error-Locator Polynomial, E(X)","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-error-locator-polynomial-e-x","position":8},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Error-Locator Polynomial, E(X)","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"Let’s define a special tool called the Error-Locator Polynomial, E(X). This is a polynomial whose roots are the x-coordinates (\\alpha_i) where an error occurred. In other words:E(\\alpha_i) = 0 \\quad \\text{if} \\quad y_i \\neq P(\\alpha_i)\n\nIf there are e errors, we can construct such a polynomial of degree e.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-error-locator-polynomial-e-x","position":9},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Key Equation","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-key-equation","position":10},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Key Equation","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"With this definition, we can establish a key equation that holds true for every single point, whether it’s an error or not:y_i E(\\alpha_i) = P(\\alpha_i) E(\\alpha_i) \\quad \\text{for all } i=1, \\dots, n\n\nThis powerful identity is easy to prove by considering two cases:\n\nIf an error occurred at \\alpha_i: By definition, E(\\alpha_i) = 0. The equation becomes y_i \\cdot 0 = P(\\alpha_i) \\cdot 0, which simplifies to 0=0. The identity holds.\n\nIf no error occurred at \\alpha_i: In this case, we know y_i = P(\\alpha_i). If we multiply both sides of this by E(\\alpha_i), the equality is preserved. The identity also holds.\n\nThis equation is the foundation of the algorithm. The only problem is that it involves the product of two unknowns, P(X) and E(X), making it a difficult quadratic problem to solve directly.\n\n","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-key-equation","position":11},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#the-core-idea-reverse-engineering-a-solution-1","position":12},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"The Welch-Berlekamp algorithm is designed using a clever “reverse engineering” approach. We start by assuming we magically know the solution—both the original polynomial P(X) and the locations of the errors—and derive a mathematical property. Then, we use that property to build an algorithm that finds P(X) without knowing the error locations beforehand.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-core-idea-reverse-engineering-a-solution-1","position":13},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Error-Locator Polynomial, E(X)","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-error-locator-polynomial-e-x-1","position":14},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Error-Locator Polynomial, E(X)","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"Let’s define a special tool called the Error-Locator Polynomial, E(X). This is a polynomial whose roots are the x-coordinates (\\alpha_i) where an error occurred. In other words:E(\\alpha_i) = 0 \\quad \\text{if} \\quad y_i \\neq P(\\alpha_i)\n\nIf there are e errors, we can construct such a polynomial of degree e.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-error-locator-polynomial-e-x-1","position":15},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Key Equation","lvl2":"The Core Idea: Reverse Engineering a Solution"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-key-equation-1","position":16},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Key Equation","lvl2":"The Core Idea: Reverse Engineering a Solution"},"content":"With this definition, we can establish a key equation that holds true for every single point, whether it’s an error or not:y_i E(\\alpha_i) = P(\\alpha_i) E(\\alpha_i) \\quad \\text{for all } i=1, \\dots, n\n\nThis powerful identity is easy to prove by considering two cases:\n\nIf an error occurred at \\alpha_i: By definition, E(\\alpha_i) = 0. The equation becomes y_i \\cdot 0 = P(\\alpha_i) \\cdot 0, which simplifies to 0=0. The identity holds.\n\nIf no error occurred at \\alpha_i: In this case, we know y_i = P(\\alpha_i). If we multiply both sides of this by E(\\alpha_i), the equality is preserved. The identity also holds.\n\nThis equation is the foundation of the algorithm. The only problem is that it involves the product of two unknowns, P(X) and E(X), making it a difficult quadratic problem to solve directly.\n\n","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-key-equation-1","position":17},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Welch-Berlekamp Algorithm"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#the-welch-berlekamp-algorithm","position":18},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"The Welch-Berlekamp Algorithm"},"content":"The genius of Welch-Berlekamp is a linearization trick that transforms the difficult quadratic problem into a simple system of linear equations that we know how to solve efficiently.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-welch-berlekamp-algorithm","position":19},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Linearization Trick","lvl2":"The Welch-Berlekamp Algorithm"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-linearization-trick","position":20},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Linearization Trick","lvl2":"The Welch-Berlekamp Algorithm"},"content":"We define a new “numerator” polynomial, N(X), as the product of our two unknowns:N(X) \\triangleq P(X) \\cdot E(X)\n\nBy substituting this into our key equation, we get a new equation where the unknowns—the coefficients of N(X) and E(X)—appear linearly:N(\\alpha_i) = y_i E(\\alpha_i) \\quad \\text{for all } i=1, \\dots, n\n\nNow, our goal is to find the two polynomials N(X) and E(X) that satisfy this linear system. If we can find them, we can recover the original message polynomial by simply performing a polynomial division:P(X) = \\frac{N(X)}{E(X)}\n\nThis leads to the formal three-step algorithm.","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-linearization-trick","position":21},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Algorithm Steps","lvl2":"The Welch-Berlekamp Algorithm"},"type":"lvl3","url":"/coding-theory/unique-decoding-algorithm#the-algorithm-steps","position":22},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl3":"The Algorithm Steps","lvl2":"The Welch-Berlekamp Algorithm"},"content":"Interpolation (Solve Linear System): Find non-zero polynomials N(X) (of degree at most k+e-1) and E(X) (of degree at most e) that satisfy the system of n linear equations N(\\alpha_i) = y_i E(\\alpha_i) for all received points (\\alpha_i, y_i).\n\nDivision (Find P(X)): If a solution is found and E(X) divides N(X), compute the candidate message polynomial by division: P(X) = N(X) / E(X). If not, the decoding fails.\n\nVerification: Check if the recovered P(X) is a valid solution by ensuring it is “close” to the received word y (i.e., the number of disagreements is at most e). If it is, return P(X).\n\n","type":"content","url":"/coding-theory/unique-decoding-algorithm#the-algorithm-steps","position":23},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"Code Implementation & Demonstration"},"type":"lvl2","url":"/coding-theory/unique-decoding-algorithm#code-implementation-demonstration","position":24},{"hierarchy":{"lvl1":"Guide to Unique Decoding of Reed-Solomon Codes","lvl2":"Code Implementation & Demonstration"},"content":"\n\n# Import necessary libraries\nimport numpy as np\nfrom sage.all import *\n\n# Define the finite field we will work in\nF = GF(257)\n\n# Helper function to convert a message to a polynomial\ndef msg_to_poly(msg_coeffs, R):\n    return R(list(msg_coeffs))\n\n# Helper function to convert a polynomial back to a message\ndef poly_to_msg(poly, k):\n    coeffs = poly.coefficients(sparse=False)\n    # Pad with zeros if necessary\n    while len(coeffs) < k:\n        coeffs.append(0)\n    \n    # NEW: Descriptive print statement\n    print(f\"\\nConverting polynomial coefficients {coeffs} back to message...\")\n    return \"\".join([chr(int(c)) for c in coeffs])\n\n# Main Welch-Berlekamp Decoder Function\ndef welch_berlekamp_decode(points, n, k):\n    print(\"\\n--- Starting Welch-Berlekamp Decoder ---\")\n    \n    e = (n - k + 1) // 2\n    print(f\"This [n={n}, k={k}] code can correct up to e={e} errors.\")\n\n    R_x = PolynomialRing(F, 'x')\n    \n    # Step 1: Solve the Linear System for N(X) and E(X)\n    num_N_coeffs = k + e \n    num_E_coeffs = e \n    num_vars = num_N_coeffs + num_E_coeffs\n\n    M = MatrixSpace(F, n, num_vars)\n    matrix = M()\n    b = vector(F, n)\n\n    for i in range(n):\n        alpha_i, y_i = points[i]\n        \n        # Build matrix row for N(X) and E(X) coefficients\n        for j in range(num_N_coeffs):\n            matrix[i, j] = alpha_i**j\n        for j in range(num_E_coeffs):\n            matrix[i, num_N_coeffs + j] = -y_i * (alpha_i**j)\n        \n        # Build the constant vector `b` from the E_e=1 term\n        b[i] = y_i * (alpha_i**e)\n\n    print(f\"\\nSolving a {n}x{num_vars} linear system for the coefficients...\")\n    \n    try:\n        solution = matrix.solve_right(b)\n    except ValueError:\n        print(\"Decoding Failed: The linear system has no unique solution.\")\n        return None\n\n    N_coeffs = list(solution[:num_N_coeffs])\n    E_coeffs = list(solution[num_N_coeffs:]) + [1] # Add the implicit e_e=1 coefficient\n    \n    N = R_x(N_coeffs)\n    E = R_x(E_coeffs)\n    \n    print(f\"Found polynomials:\\n  N(X) = {N}\\n  E(X) = {E}\")\n    \n    # Step 2: Division to find P(X)\n    if E == 0 or N % E != 0:\n        print(\"\\nDecoding Failed: E(X) does not divide N(X).\")\n        return None\n        \n    P_candidate = N // E\n    print(f\"\\nFound candidate message polynomial P(X) = {P_candidate}\")\n    \n    # Step 3: Verification\n    encoded_candidate = [(alpha, P_candidate(alpha)) for alpha, y in points]\n    disagreements = 0\n    for i in range(n):\n        if points[i][1] != encoded_candidate[i][1]:\n            disagreements += 1\n            \n    print(f\"Verification: Found {disagreements} disagreements with the received word.\")\n    \n    if disagreements <= e:\n        decoded_msg = poly_to_msg(P_candidate, k)\n        print(f\"\\nSUCCESS! Decoded message: '{decoded_msg}'\")\n        return decoded_msg\n    else:\n        print(\"\\nDecoding Failed: Candidate polynomial is too far from the received word.\")\n        return None\n\n# --- Demonstration ---\n\n# Parameters\nmsg = \"abc\"\nk = len(msg)\nn = 7\n\n# Message -> Polynomial\nR_x = PolynomialRing(F, 'x')\nmsg_coeffs = [ord(c) for c in msg]\n\n# NEW: Descriptive print statement\nprint(f\"Original message: '{msg}' (k={k})\")\nprint(f\"Message ASCII coefficients: {msg_coeffs}\")\nP = msg_to_poly(msg_coeffs, R_x)\nprint(f\"Message polynomial P(X): {P}\")\n\n# Encoding\nalphas = [F(i) for i in range(n)]\ncodeword = [(alpha, P(alpha)) for alpha in alphas]\nprint(f\"\\nEncoded codeword (n={n}): {codeword}\")\n\n# Introduce Errors\ne_limit = (n - k + 1) // 2\nnum_errors = 2 # This is <= e_limit, so it's correctable\nnoisy_word = list(codeword)\nnoisy_word[2] = (alphas[2], F(99)) # Corrupt point 2\nnoisy_word[5] = (alphas[5], F(42)) # Corrupt point 5\nprint(f\"Noisy word with {num_errors} errors: {noisy_word}\")\n\n# Decode\nwelch_berlekamp_decode(noisy_word, n, k)","type":"content","url":"/coding-theory/unique-decoding-algorithm#code-implementation-demonstration","position":25},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)"},"content":"Last update: 2025-06-12","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Background"},"type":"lvl2","url":"/#background","position":2},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Background"},"content":"Polynomial Commitment Schemes (PCS) are important components in many zkSNARK (zero-knowledge Succinct Non-interactive ARguments of Knowledge) systems. A Prover can commit to a polynomial and later prove to a Verifier that the value of this polynomial at a publicly disclosed opening point is correct.\n\n\nInitially, schemes like [KZG10] only supported univariate polynomials. Assuming a univariate polynomial with N coefficients, the Prover’s computational complexity was O(N \\log N). Recently, many SNARK proof systems have begun using Multilinear Polynomial Commitment Schemes (MLE-PCS), such as Hyperplonk[CBBZ22]. For a multilinear polynomial with N coefficients, the Prover’s computational complexity can achieve linearity, i.e., O(N). MLE-PCS can not only construct more efficient proof systems, but the representation of multilinear polynomials also brings other benefits, such as efficient split-and-fold on Hypercubes, better support for high-degree constraints, and more flexible decomposition.\n\nThis project MLE-PCS focuses on researching and comparing different multilinear polynomial commitment schemes, including their design, security assumptions, and efficiency.","type":"content","url":"/#background","position":3},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"MLE-PCS Overview"},"type":"lvl2","url":"/#mle-pcs-overview","position":4},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"MLE-PCS Overview"},"content":"For an n-variate linear polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}), there are two forms of representation:\n\nCoefficients form\n\nA multilinear polynomial can be represented in terms of coefficients as follows:\\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) = c_0 + c_1 X_0 + c_2 X_1 + \\ldots + c_{2^n - 1} X_0 X_1 \\cdots X_{n - 1}\n\nwhere \\{c_i\\}_{0  \\le i \\le 2^{n} - 1} are the coefficients of the multilinear polynomial.\n\nEvaluations form\n\nA multilinear polynomial can also be represented by its values on the Boolean Hypercube B_n = \\{0,1\\}^n, as:\\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) = \\sum_{i = 0}^{2^n - 1} \\tilde{f}(\\mathsf{bits}(i)) \\cdot \\tilde{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n - 1}))\n\nHere, \\mathsf{bits}(i) = (i_0, i_1, \\ldots, i_{n-1}) represents the binary representation of i as a vector, with the first component i_0 being the least significant bit in the binary representation, i.e., Little-endian. For example, when n = 3, the binary representation of 3 is 011, so the vector \\mathsf{bits}(3) = (1,1,0). \\tilde{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n - 1})) is the Lagrange polynomial on the Boolean Hypercube B_n = \\{0,1\\}^n, defined as:\\tilde{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n - 1})) = \\prod_{j = 0}^{n - 1} ((1 - i_j)(1 - X_j) + i_j X_j)\n\nIn the MLE-PCS commitment protocol, the Prover first commits to the multilinear polynomial \\tilde{f} to the Verifier. Subsequently, in the Evaluation proof protocol, the Prover proves to the Verifier that the value of \\tilde{f} at a public point \\vec{u} = (u_0, \\ldots, u_{n - 1}) is v, i.e., proving that \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v.\n\nSome MLE-PCS protocols are described in Evaluations Form, while others are described in Coefficients Form. This naturally creates a form conversion problem. For example, if a multilinear polynomial is given in coefficient form, it would need to be converted to Evaluation form using an algorithm similar to FFT to adapt to protocols described in Evaluation form. However, many authors have noted that this FFT conversion is not necessary to adapt to such protocols. Taking the Basefold [ZCF23] protocol as an example, in the original paper [ZCF23], the protocol is described in coefficients, but Ulrich Haböck in paper [H24] described the Basefold protocol in Evaluations form. Based on the original Basefold protocol, only the folding form in the FRI protocol needs to be changed. For more on this conversion, see the note \n\nAn Alternative Folding Method.\n\nThis project describes the basic principles of many MLE-PCS, and for some protocols, we have also supplemented protocol descriptions in alternative forms of multilinear polynomial representation. The table below lists the MLE-PCS covered in this project.\n\nScheme\n\nPaper\n\nNotes\n\nPST13\n\n[XZZPS19]\n\nNotes on Libra-PCS\n\nzeromorph\n\n[KT23]\n\nNotes on Zeromorph, \n\nZeromorph-PCS (Part II)\n\nzeromorph-fri\n\n⭐\n\nZeromorph-PCS: Integration with FRI\n\ngemini\n\n[BCH+22]\n\nGemini-PCS (Part I),\n\nGemini-PCS (Part II),\n\nGemini-PCS (Part III), \n\nGemini-PCS (Part IV)\n\ngemini-fri\n\n⭐\n\nGemini: Interfacing with FRI\n\nhyperKZG\n\nN/A\n\nNotes on HyperKZG\n\nPH23-KZG\n\n[PH23]\n\nThe Missing Protocol PH23-PCS (Part 1), \n\nMissing Protocol PH23-PCS (Part 2)\n\nPH23-fri\n\n⭐\n\nThe Missing Protocol PH23-PCS (Part 4),\n\nThe Missing Protocol PH23-PCS (Part 5)\n\nMercury\n\n[EG25]\n\nMercury Notes: Implementing Constant Proof Size, \n\nMercury Notes: Integration with KZG\n\nSamaritan\n\n[GPS25]\n\n\n\nVirgo\n\n[ZXZS19]\n\nNotes on Virgo-PCS\n\nHyrax\n\n[WTSTW18]\n\nNotes on Hyrax-PCS\n\nBasefold\n\n[ZCF23]\n\nNotes on Basefold (Part I): Foldable Linear Codes, \n\nNotes on Basefold (Part II): IOPP, \n\nNotes on Basefold (Part III): MLE Evaluation Argument\n\nBasefold\n\n⭐\n\nAn Alternative Folding Method\n\nDeepfold\n\n[GLHQTZ24]\n\nNote on DeepFold: Protocol Overview\n\nLigerito\n\n[NA25]\n\nLigerito-PCS Notes\n\nWHIR\n\n[ACFY24b]\n\nNote on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification\n\nFRI-Binius\n\n[DP24]\n\nNotes on FRI-Binius (Part I): Binary Towers, \n\nNotes on Binius (Part II): Subspace Polynomial\n\nGreyhound\n\n[NS24]\n\nGreyhound Commitment\n\nΣ-Check\n\n[GQZGX24]\n\nhttps://​eprint​.iacr​.org​/2024​/1654​.pdf\n\nHyperwolf\n\n[ZGX25]\n\nhttps://​eprint​.iacr​.org​/2025​/922\n\nNOTE: Items marked with “⭐️” in the Remarks column represent new protocol descriptions added in this project.","type":"content","url":"/#mle-pcs-overview","position":5},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"type":"lvl3","url":"/#classification-by-commitment-protocols","position":6},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"content":"All MLE-PCS protocols are extended from Univariate PCS or built directly on them.\n\nFor a univariate polynomial f(X),f(X) = a_0 + a_1 X + a_2 X^2 + \\ldots + a_{N-1}X^{N-1}\n\nDifferent ways of committing to f(X) correspond to different Univariate PCS.\n\nCommitments\n\nAlgebra\n\nSchemes\n\nKZG10\n\nParing Friendly ECC based\n\nPST13(mKZG or Liba-PCS), Zeromorph, Gemini, HyperKZG, PH23-KZG, Mercury, Samaritan\n\nMerkle Tree\n\nLinear code based\n\nLigero, Virgo, Basefold, Deepfold, WHIR, PH23-fri, Zeromorph-fri, Gemini-fri, Ligerito, FRI-Binius\n\nPedersen Commitment\n\nECC based\n\nHyrax, Σ-Check\n\nAjtai Commitment\n\nLattice based\n\nGreyhound, Hyperwolf","type":"content","url":"/#classification-by-commitment-protocols","position":7},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"KZG10","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#kzg10","position":8},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"KZG10","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"content":"KZG10 polynomial commitment requires a Trusted Setup to produce a set of vectors with internal algebraic structure,(G_0, G_1, \\ldots, G_{N-1}, H_0, H_1) = (G, \\gamma G, \\gamma^2 G, \\ldots, \\gamma^{N- 1} G, H, \\gamma H)\n\nHere, \\gamma is a random number generated through Trusted Setup, which must not be leaked after generation. G, H are generators on elliptic curve groups \\mathbb{G}_1, \\mathbb{G}_2 respectively, and there exists a bilinear mapping between them: e: \\mathbb{G}_1\\times \\mathbb{G}_2 \\rightarrow \\mathbb{G}_T.\n\nThe commitment to polynomial f(X) is:\\begin{align}\nC_{f(X)}  & = a_0 G_0 + a_1 G_1 + \\ldots + a_{N - 1}G_{N - 1} \\\\\n & = a_0 G + a_1 \\gamma G + \\ldots + a_{N-1} \\gamma^{N - 1} G \\\\\n & = f(\\gamma) G\n\\end{align}\n\nThe commitment C_{f(X)} is exactly f(\\gamma)G. To construct a PCS using KZG10, such as creating an opening proof for f(\\zeta) = y, one needs to prove that there exists a quotient polynomial q(X) satisfying:f(X) = q(X) \\cdot (X - \\zeta) + y\n\nThe Prover can provide the commitment C_{q(X)} to q(X) as the opening proof for f(\\zeta) = y. Using the additive homomorphic property of commitments and bilinear mapping, the Verifier can verify the divisibility relationship on \\mathbb{G}_T:e(C_{f(X)} - y \\cdot G, H) \\overset{?}{=} e(C_{q(X)} , \\gamma H - \\zeta H)\n\nFrom the above description, it can be seen that the KZG10 commitment scheme has the following characteristics:\n\nRequires a trusted setup to generate public parameters with a specific algebraic structure.\n\nUses bilinear mapping on elliptic curves to verify opening proofs.\n\nThe opening proof verification requires only one group element, which often makes the proof size constant.","type":"content","url":"/#kzg10","position":9},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Merkle Tree","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#merkle-tree","position":10},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Merkle Tree","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"content":"Merkle Tree-based commitments do not require a Trusted Setup and are based on the properties of Linear Codes. Taking the FRI protocol as an example, to commit to f(X), the Prover sends the Reed-Solomon Code of f(X) to the Verifier in the form of a Merkle Tree. Specifically, let H \\subset \\mathbb{F}_q be a multiplicative group of order 2^k (k \\in \\mathbb{N}), and the rate of the Reed-Solomon Code be \\rho. The Reed-Solomon Code of f(X) is the values of f(X) on H, forming a vector:[f(x)|_{x \\in H}]\n\nThe elements of this vector, or their hash values, serve as the leaf nodes of the Merkle Tree, and the root of this Merkle Tree is the commitment to f(X).\n\nTo prove f(\\zeta) = y (for \\zeta \\notin H), taking the FRI protocol to construct a PCS as an example [H22], one proves:f^{(0)}(X) =\\frac{f(X) - y}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{f(X) - y}{X - \\zeta}\n\nwith degree less than N, where \\lambda \\leftarrow \\mathbb{F}_q is a random number sent by the Verifier. The Prover first encodes f^{(0)}(X) on H using Reed-Solomon, commits to the encoded vector using a Merkle Tree, and then the Prover and Verifier proceed with the FRI protocol. In the Query phase of the protocol, if leaf nodes on the Merkle Tree need to be opened, the Prover must send the corresponding Merkle Paths as proof.\n\nProtocols using Merkle Trees as commitment schemes have the following characteristics:\n\nDo not require trusted setup.\n\nCommitment computation mainly relies on hash, which has less computational overhead compared to KZG10 since KZG10 requires operations on elliptic curves.\n\nProver needs to send Merkle Paths during the proof process, making the proof size larger than KZG10 in most cases.","type":"content","url":"/#merkle-tree","position":11},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Pedersen Commitment","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#pedersen-commitment","position":12},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Pedersen Commitment","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"content":"Pedersen Commitment is another commitment scheme based on elliptic curves. Unlike KZG10, Pedersen Commitment doesn’t require a Trusted Setup to generate vectors with specific algebraic structures. Instead, it uses a Hash-to-point algorithm to generate a set of random elliptic curve group elements:G_0, G_1, \\ldots, G_{M-1}, H_0 \\leftarrow \\mathbb{G}\n\nWith these elements, we can commit to a vector \\vec{a} of length N<M:\\mathsf{cm}(\\vec{a}) = a_0 G_0 + a_1 G_1 + \\ldots + a_{N-1} G_{N-1}\n\nIf the Prover can also generate a random factor \\rho\\leftarrow \\mathbb{F}, then the Pedersen commitment is Perfectly Hiding:\\mathsf{cm}(\\vec{a}) = a_0 G_0 + a_1 G_1 + \\ldots + a_{N-1} G_{N-1} + \\rho H_0\n\nAlthough these random group points have no internal structure, we can still prove that the vector behind the commitment satisfies certain properties. One of the most typical is the inner product proof:\\langle \\vec{a}, \\vec{b} \\rangle = v\n\nWe can typically use either the Bulletproof approach or the ∑-Check approach to prove the inner product. Based on inner product proofs, we can also prove Hadamard products of vectors, or even more complex matrix multiplications. The biggest issue with this commitment scheme is that the Verifier’s computational complexity is O(N), but we can still use amortization or recursive proof techniques to mitigate the Verifier’s computational burden.\n\nProtocols that use Pedersen Commitment as their commitment scheme have the following characteristics:\n\nNo Trusted Setup required.\n\nCommitment computation primarily relies on elliptic curve multiplication.\n\nUsing Bulletproof-based inner product proofs, the proof size is O(\\log(N)), but the Verifier’s computational complexity is O(N).","type":"content","url":"/#pedersen-commitment","position":13},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Ajtai Commitment","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#ajtai-commitment","position":14},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Ajtai Commitment","lvl3":"Classification by Commitment Protocols","lvl2":"MLE-PCS Overview"},"content":"Ajtai commitment is a lattice-based commitment method with post-quantum security. Assuming the vector to be committed is \\vec{a}, Ajtai commitment first selects an n \\times m-size matrix G (similar to the group element in Pedersen commitment), and computes\\mathsf{cm}(\\vec{a}) = G \\vec{a}\n\nto get the commitment result \\vec{t}.\n\nThe key differences between Ajtai commitments and Pedersen commitments are:\n\nAjtai commitment requires that the committed content \\vec{a} must be “small enough”, meaning there is an upper bound B such that for all a_i, |a_i| < B. This is due to the hardness requirements of the SIS/LWE problem, and only under this condition can the binding/hiding properties of Ajtai commitment be reduced to the SIS/LWE problem. To commit to polynomials with arbitrary coefficients, a common method is to split each coefficient into smaller but longer arrays (such as binary representation), and then commit to the split result. This satisfies the < B requirement. Similarly, during opening, an additional step is needed to recover the original coefficients by computing the inner product of the binary vector with (1, 2, 2^2, \\cdots).\n\nSince the result of Ajtai commitment is itself a vector, implementing “commitment-of-commitment” becomes very easy. That is, after splitting, multiple commitments can again undergo Ajtai commitment. This technique is widely used in lattice designs to further reduce proof volume.\n\nTo improve efficiency, many implementations use polynomial rings to implement Ajtai commitment (note that the polynomial ring here is unrelated to the polynomials in polynomial commitments), where the elements of vectors/matrices are ring elements. This more general case allows the security of Ajtai commitment to be reduced to the M-SIS/M-LWE problem.\n\nProtocols that use Ajtai Commitment as their commitment scheme have the following characteristics:\n\nNo Trusted Setup required.\n\nCommitment computation primarily relies on matrix multiplication.\n\nAn additional norm check for the opening is required.\n\nUsing LaBRADOR-based inner product proofs, the proof size is O(\\log(N)), but the Verifier’s computational complexity is O(N).","type":"content","url":"/#ajtai-commitment","position":15},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"type":"lvl3","url":"/#classification-by-evaluation-proof-principles","position":16},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"content":"Based on different implementation methods, MLE-PCS can be categorized as follows:\n\nPrinciple\n\nSchemes\n\nQuotienting\n\nPST13(Libra-PCS), Zeromorph, Zeromorph-FRI\n\nSumcheck\n\nBasefold, Deepfold, WHIR, Ligerito, FRI-Binius\n\nSplit-and-fold\n\nGemini, HyperKZG, Gemini-fri, Hyperwolf\n\nInner-product\n\nLigero, Hyrax, Σ-Check, PH23-kzg, Virgo-PCS, Mercury, Samaritan,  GreyHound","type":"content","url":"/#classification-by-evaluation-proof-principles","position":17},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Quotienting","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#quotienting","position":18},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Quotienting","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"content":"MLE-PCS aims to prove that a multilinear polynomial \\tilde{f}(X_0, \\ldots, X_{n - 1}) at a public point (u_0, \\ldots, u_{n - 1}) has a value of v, i.e., proving that \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v.\n\nAccording to the division decomposition theorem for MLE polynomials given in paper [PST13], we have:\\begin{split}\n\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) -  {\\color{red}\\tilde{f}(u_0, u_1, \\ldots, u_{n-1})} & = \\tilde{q}_{n-1}(X_0, X_1, \\ldots, X_{n-2}) \\cdot (X_{n-1} - u_{n-1}) \\\\\n& + \\tilde{q}_{n-2}(X_0, X_1, \\ldots, X_{n-3}) \\cdot (X_{n-2} - u_{n-2}) \\\\\n& + \\cdots \\\\\n& + \\tilde{q}_{1}(X_0) \\cdot (X_{1} - u_{1}) \\\\\n& + \\tilde{q}_{0} \\cdot (X_{0} - u_{0}) \\\\\n\\end{split}\n\nIf \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v, then we have:\\begin{split}\n\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) - {\\color{red}v} & = \\tilde{q}_{n-1}(X_0, X_1, \\ldots, X_{n-2}) \\cdot (X_{n-1} - u_{n-1}) \\\\\n& + \\tilde{q}_{n-2}(X_0, X_1, \\ldots, X_{n-3}) \\cdot (X_{n-2} - u_{n-2}) \\\\\n& + \\cdots \\\\\n& + \\tilde{q}_{1}(X_0) \\cdot (X_{1} - u_{1}) \\\\\n& + \\tilde{q}_{0} \\cdot (X_{0} - u_{0}) \\\\\n\\end{split}\n\nAt this point, proving \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v can be converted into proving the existence of \\{\\tilde{q}_i\\}_{0\\leq i<n} that satisfy the above division equation.\n\nPST13 [PST13, XZZPS19] introduces structured SRS to commit to quotient polynomials \\tilde{q}_0, \\tilde{q}_1, \\ldots, \\tilde{q}_{n-1}, and the Verifier verifies the correctness of the above division decomposition through ECC-Pairing operations.\n\nThe core of the Zeromorph [KT23] protocol is to provide a mapping from multilinear polynomials to univariate polynomials, where the Evaluations of the MLE polynomial on the Boolean Hypercube directly serve as coefficients of the univariate polynomial. By transforming the multilinear polynomials in the above division decomposition into univariate polynomials through this mapping method, we can derive a key equation that the Zeromorph protocol aims to prove:\\begin{split}\n[[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n - v\\cdot\\Phi_n(X) &=\\sum_{k=0}^{n-1}\\Big(X^{2^k}\\cdot \\Phi_{n-k-1}(X^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(X^{2^k})\\Big)\\cdot [[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_k\n\\end{split}\n\nHere [[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n represents the direct correspondence of the values of \\tilde{f}(X_0, \\ldots, X_{n - 1}) on the boolean hypercube B_n = \\{0,1\\}^n to the coefficients of a univariate polynomial, i.e.,[[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n = \\sum_{i = 0}^{2^n - 1} \\tilde{f}(\\mathsf{bits}(i)) \\cdot X^{i}\n\n[[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_k follows the same mapping method, transforming a multilinear polynomial into a univariate polynomial, i.e.,[[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_k = \\sum_{i = 0}^{2^k - 1} \\tilde{q}_{k}(\\mathsf{bits}(i)) \\cdot X^i\n\n\\Phi_n(X) and \\Phi_{n-k}(X^{2^k}) are also univariate polynomials. Generally, \\Phi_k(X^h) denotes the following polynomial:\\Phi_k(X^h) = 1 + X^h + X^{2h} + \\ldots + X^{(2^{k}-1)h}\n\nTherefore, both sides of the key equation in the Zeromorph protocol are univariate polynomials. The Verifier can randomly select a point, and the Prover only needs to prove that these univariate polynomials satisfy the above equation at that random point, which can be done using univariate-PCS. Thus, the Zeromorph protocol can choose to interface with different univariate-PCS, such as KZG10 or FRI-PCS.","type":"content","url":"/#quotienting","position":19},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Inner-product","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#inner-product","position":20},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Inner-product","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"content":"The proof of \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v for Multilinear Polynomials can be transformed into the following Inner-product form:\\langle \\vec{f}, \\otimes_{j=0}^{n - 1}(1, u_i)\\rangle = v\n\nThat is, proving that the inner product of vector \\vec{f} and vector \\otimes_{j=0}^{n - 1}(1, u_i) is v. There are many protocols that construct MLE-PCS through this inner product proof method, including Virgo[ZXZS19], Hyrax[WTSTW16], PH23-PCS[PH23], Mercury[EG25], and Samaritan[GPS25].\n\nVirgo-PCS[ZXZS19] is described in the Coefficients Form of MLE polynomials, where proving \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v is equivalent to the inner product relation:\\langle \\vec{f}, \\otimes_{j=0}^{n - 1}(1, u_i)\\rangle = v\n\nVirgo-PCS uses Univariate Sumcheck to prove this inner product. In Univariate Sumcheck, besides proving that a univariate polynomial constraint holds, it’s also necessary to prove that a univariate polynomial’s degree is less than a certain value, which is accomplished using the FRI protocol.\n\nIn Univariate Sumcheck, for the Verifier to verify that a constraint on a univariate polynomial holds, they need to calculate the value of a polynomial u(X) constructed from the vector \\otimes_{j=0}^{n - 1}(1, u_i) at a point, but this requires O(N) computation. Virgo uses the GKR protocol to delegate this computation to the Prover, allowing the Verifier to complete verification with only O(\\log^2(N)) computation.\n\nThe PH23-PCS[PH23] protocol describes the Evaluations Form of multilinear polynomials, where proving \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v means proving:\\sum_{i = 0}^{2^n - 1} \\tilde{f}(\\mathsf{bits}(i)) \\cdot \\tilde{eq}(\\mathsf{bits}(i), (u_0,\\ldots,u_{n-1})) = v\n\nLet vector \\vec{a} = (\\tilde{f}(\\mathsf{bits}(0)), \\ldots, \\tilde{f}(\\mathsf{bits}(2^n - 1))), and the i-th component of vector \\vec{c} be \\tilde{eq}(\\mathsf{bits}(i), (u_0,\\ldots,u_{n-1})). Then the above summation can be viewed from an inner product perspective:\\langle \\vec{a}, \\vec{c} \\rangle  = v\n\nThe PH23-PCS proof protocol is divided into two parts:\n\n(1) Proving that the components of vector \\vec{c} are indeed \\tilde{eq}(\\mathsf{bits}(i), (u_0,\\ldots,u_{n-1}))\n\n(2) Proving that the inner product \\langle \\vec{a}, \\vec{c} \\rangle  = v\n\nFor part (1), using the structure of \\tilde{eq}(\\mathsf{bits}(i), (u_0,\\ldots,u_{n-1})), a univariate polynomial c(X) is constructed, and proof is done using constraints of n + 1 univariate polynomials.\n\nFor part (2), this is an inner product proof, which can be done using methods like Grand Sum or Univariate Sumcheck. The article \n\nThe Missing Protocol PH23-PCS (Part 1) provides a complete protocol for PH23-PCS using Grand Sum for inner product proofs. Using the Grand Sum method, proving an inner product can be converted to proving that 3 univariate polynomial constraints hold.\n\nThus, both parts of the PH23-PCS proof can be transformed into proving that n + 4 univariate polynomial constraints hold, which can then be proven using univariate polynomial PCS, so PH23-PCS can interface with KZG10 and FRI-PCS.\n\nThe Hyrax [WTSTW16] protocol directly views the MLE polynomial \\tilde{f}(u_0, u_1, u_2, u_3) as a vector-matrix multiplication equation:\\tilde{f}(u_0, u_1, u_2, u_3) = \n\\begin{bmatrix}\n1 & u_2 & u_3 & u_2u_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 & c_1 & c_2 & c_3 \\\\\nc_4 & c_5 & c_6 & c_7 \\\\\nc_8 & c_9 & c_{10} & c_{11} \\\\\nc_{12} & c_{13} & c_{14} & c_{15}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nu_0 \\\\\nu_1 \\\\\nu_0u_1 \\\\\n\\end{bmatrix}\n\nThen the matrix is committed row by row. Hyrax uses Pedersen Commitment, which has additive homomorphism, so the commitment vectors can first be inner-producted with (1, u_2, u_3, u_2u_3) to get a single commitment, and then prove that the inner product of the vector corresponding to this commitment and (1, u_0, u_1, u_0u_1) equals v. This final inner product proof uses the Bulletproofs-IPA protocol, with the Verifier’s computational complexity at O(\\sqrt{N}) and the Proof size at O(\\log(N)).\n\nThe Mercury [EG25] and Samaritan [GPS25] protocols have very similar approaches, both improving on the matrix multiplication equation described in Hyrax. Unlike Hyrax, Mercury and Samaritan only need to compute commitments for the vector as a whole, rather than row by row, generating \\sqrt{N} commitments. Then they transform the overall Evaluation proof into two inner product proofs. Additionally, Mercury batches the two inner product proofs into one, further optimizing the protocol.","type":"content","url":"/#inner-product","position":21},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Sumcheck Method","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#sumcheck-method","position":22},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Sumcheck Method","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"content":"Substituting \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v into the point-value form of the multilinear polynomial, we can transform it into proving the summation on B_n = \\{0,1\\}^n:\\sum_{\\vec{b} = \\{0,1\\}^n}\\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, \\vec{u}) = v\n\nSo we can prove that the above summation holds, thereby proving the correctness of the Multilinear Polynomial Evaluation. The challenge with using Sumcheck is that in the last step of the Sumcheck protocol, the Verifier needs to obtain the value of \\tilde{f} at a random point for final verification, completing the entire Sumcheck protocol. The significant contribution of the Basefold [ZCF23] protocol is discovering that if we synchronously use the FRI protocol on the univariate polynomial corresponding to \\tilde{f}, and use the same random number as the Sumcheck protocol in each round of folding, then when folded to the last step, the resulting constant is exactly the value of \\tilde{f} at the random point that the Sumcheck protocol wants in the last step.h(r_{n-1}) \\overset{?}{=} \\tilde{f}(r_0, r_1, \\ldots, r_{n-1})\\cdot \\tilde{eq}((r_0, r_1, \\ldots, r_{n-1}), \\vec{u})\n\nThe Deepfold protocol and WHIR protocol also continue with the Sumcheck approach, with the difference being that Deepfold adopts the idea from DEEP-FRI, where the Prover predetermined the evaluation of a polynomial at some Out-of-domain random point as a form of Commitment, ensuring that the Prover always commits to the same polynomial even in the List-decoding Regime. This random Evaluation can also be proven using the Sumcheck protocol. Specifically, in each round of the Basefold protocol interaction, the Verifier additionally randomly selects a z_i\\leftarrow \\mathbb{F}, and then the Prover sends y_i and proves that it is the value of \\hat{f}^{(i)}(z_i). Since \\hat{f}^{(i)} and \\tilde{f}^{(i)} have an isomorphic mapping, y_i is also the evaluation value of the following MLE polynomial:y_i = \\tilde{f}^{(i)}(z_i, z_i^2, \\cdots, z_i^{2^{n-i-1}})\n\nTherefore, in subsequent protocol interactions, the Prover similarly uses a new Sumcheck protocol to prove the correctness of \\tilde{f}^{(i)}(z_i, z_i^2, \\cdots, z_i^{2^{n-i-1}}).\n\nWHIR improves on the Deepfold protocol by merging both Out-of-domain and In-domain random queries into Sumcheck, leading to an optimized protocol state. Similarly, Ligerito, based on Basefold, also utilizes the Sumcheck protocol. In each Round, the Verifier samples some points from the Oracle, and the correctness of the encoding of these points should be calculated by the Verifier. Since this calculation is an inner product, the Verifier can use the Sumcheck protocol to delegate it to the Prover, and this Sumcheck can be merged with the Sumcheck protocol part of the current round of the Basefold protocol, greatly optimizing the subsequent flow of the protocol.","type":"content","url":"/#sumcheck-method","position":23},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Split-and-fold (Recursive)","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"type":"lvl4","url":"/#split-and-fold-recursive","position":24},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Split-and-fold (Recursive)","lvl3":"Classification by Evaluation Proof Principles","lvl2":"MLE-PCS Overview"},"content":"This proof approach is very similar to the FRI protocol, repeatedly splitting and folding a larger polynomial until a constant is reached.\nBoth the Gemini [BCH+22] protocol and HyperKZG use the split-and-fold idea for proof, with the difference being that the multilinear polynomial in Gemini is in Coefficients Form, while in hyperKZG it uses Evaluations Form. In the protocol, only the folding method needs to be changed, without requiring FFT conversion from point-value form to coefficient form.\n\nTaking the Gemini protocol as an example, view the coefficient form of the MLE polynomial:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{2^n-1} f_i \\cdot X_0^{i_0}X_1^{i_1} \\cdots X_{n - 1}^{i_{n - 1}}\n\nas an inner product between a vector and a tensor product structure:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\langle \\vec{f}, \\otimes_{i=0}^{n - 1}(1, X_i)\\rangle\n\nFor example, with n = 3, the coefficient vector:\\vec{f} = (f_0,f_1, f_2, \\ldots, f_7)\n\nThe tensor product structure can also be viewed as a vector:\\otimes_{i=0}^{2}(1, X_i) = (1, X_0, X_1, X_0X_1, X_2, X_0X_2, X_1X_2, X_0X_1X_2)\n\nTherefore, to prove \\tilde{f}(u_0, \\ldots, u_{n - 1}) = v, we prove:\\langle \\vec{f}, \\otimes_{j=0}^{n - 1}(1, u_i)\\rangle = v\n\nThis inner product form can be split-and-folded:\\langle \\vec{f}, \\otimes_{j=0}^{n - 1}(1, u_i)\\rangle = \\langle \\vec{f}_{even}, \\otimes_{j=1}^{n - 1}(1, u_i)\\rangle  + u_0 \\langle \\vec{f}_{odd}, \\otimes_{j=1}^{n - 1}(1, u_i)\\rangle\n\nwhere \\vec{f}_{even} represents the vector composed of even-indexed terms from coefficient vector \\vec{f}, and \\vec{f}_{odd} represents the vector of odd-indexed terms from \\vec{f}. Using n = 3 as an example again:\\begin{align}\n\\langle \\vec{f}, \\otimes_{j=0}^{n - 1}(1, u_i)\\rangle  & =  f_0 + f_1 u_0 + f_2 u_1 + f_3 u_0 u_1 + f_4 u_2 + f_5 u_0 u_2 + f_6 u_1 u_2 + f_7 u_0u_1u_2 \\\\\n & = (f_0 + f_2 u_1 + f_4 u_2 + f_6 u_1u_2) + u_0 \\cdot (f_1 + f_3 u_1 + f_5 u_2 + f_7 u_1u_2) \\\\\n & =  \\langle \\vec{f}_{even}, \\otimes_{j=1}^{n - 1}(1, u_i)\\rangle  + u_0 \\langle \\vec{f}_{odd}, \\otimes_{j=1}^{n - 1}(1, u_i)\\rangle \n\\end{align}\n\nHere, split-and-fold means first splitting the 8-term sum into two parts—even terms f_0 + f_2 u_1 + f_4 u_2 + f_6 u_1u_2 and odd terms f_1 + f_3 u_1 + f_5 u_2 + f_7 u_1u_2—then folding these two parts into one using u_0. This folded part can continue the split-and-fold process. The split-and-fold idea here is the same as in FRI and sumcheck.\n\nWe can directly convert the coefficient form of the MLE polynomial into the coefficients of a univariate polynomial. For instance, the univariate polynomial corresponding to \\tilde{f}(X_0, \\ldots, X_{n - 1}) is:f(X) = \\sum_{i=0}^{2^n-1} f_i \\cdot X^i\n\nSo the split-and-fold technique can be applied directly to f(X), with a process identical to the FRI protocol, except that the folding coefficient is not a random number but u_i. This eventually transforms into a constant polynomial, and the result should equal the value of \\tilde{f} at point (u_0, \\ldots, u_{n - 1}), i.e., equal to v. In this process, the verifier needs to verify the correctness of each fold, which can be done by randomly challenging some points. This can be implemented through univariate-PCS, which can interface with KZG10 or FRI.","type":"content","url":"/#split-and-fold-recursive","position":25},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by MLE Representation Form","lvl2":"MLE-PCS Overview"},"type":"lvl3","url":"/#classification-by-mle-representation-form","position":26},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Classification by MLE Representation Form","lvl2":"MLE-PCS Overview"},"content":"MLE-PCS protocols sit above Multilinear PIOP protocols. These common PIOPs are usually Sumcheck or GKR protocols. In typical implementations, Multilinear Polynomials are represented in Evaluations Form. Not all MLE-PCS protocols directly prove the Evaluations Form. If an MLE-PCS protocol can only prove or commit to the Coefficients Form of Multilinear Polynomial protocols, then the Prover needs to additionally calculate the Coefficients Form of the Multilinear Polynomial using the Algebraic FFT (NTT) algorithm, which requires O(N\\log{N}) computational time complexity. This might prevent the Prover from achieving linear-time workload.\n\nAlthough some MLE-PCS papers only describe one form, such as the Coefficients Form, the protocol itself can also support the Evaluations Form. In engineering practice, one can choose the appropriate protocol variant based on more detailed performance analysis. Below we list the support for the two representation forms of Multilinear Polynomials by the MLE-PCS covered in this project:\n\nScheme\n\nCoefficients\n\nEvaluations\n\nPST13\n\n[PST13] ✅\n\n✅  [XZZPS19]\n\nZeromorph\n\n❓\n\n✅ [KT23]\n\nGemini\n\n[BCH+22]\n\n\n\nhyperKZG\n\n\n\n✅ HyperKZG\n\nPH23-KZG\n\n\n\n✅ [PH23]\n\nMercury\n\n✔️\n\n✅ [EG25]\n\nSamaritan\n\n✔️\n\n✅ [GPS25]\n\nVirgo\n\n✅ [ZXZS19]\n\n❓\n\nHyrax\n\n✅\n\n✅ [WTSTW16]\n\nBasefold\n\n✅ [ZCF23]\n\n✅ [H24]\n\nDeepfold\n\n✅ [GLHQTZ24]\n\n❓\n\nLigerito\n\n✅\n\n✅ [NA25]\n\nWHIR\n\n✅ [ACFY24b]\n\n❓\n\nFRI-Binius\n\n✔️\n\n✅ [DP24]\n\nΣ-Check\n\n✅ [GQZGX24]\n\n✅\n\nGreyhound\n\n✅ [NS24]\n\n✅\n\nHyperwolf\n\n✅ [ZGX25]\n\n✅\n\n✅: Supported\n\n✔️: Supported, but needs further analysis\n\n❓: May not be supported, but not further proven","type":"content","url":"/#classification-by-mle-representation-form","position":27},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Security Analysis of MLE-PCS"},"type":"lvl2","url":"/#security-analysis-of-mle-pcs","position":28},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Security Analysis of MLE-PCS"},"content":"For an MLE-PCS protocol, we’re not only concerned with how the protocol is constructed, but also with its security proofs, including properties like Completeness, Soundness, Knowledge soundness, and Zero-knowledge. There are significant differences in security assumptions between KZG10-based and FRI-based MLE-PCS.\n\nAssumption\n\nAlgebra\n\nSchemes\n\nKZG10(BSDH, AGM, )\n\nECC based\n\nPST13, Zeromorph, Gemini, HyperKZG, PH23-KZG, Mercury, Samaritan\n\nRandom Oracle (Hash)\n\nLinear code based\n\nVirgo, PH23-fri, zeromorph-fri, Gemini-fri, Basefold, Deepfold, WHIR, FRI-Binius\n\nEC Discrete Log\n\nECC based\n\nHyrax，∑-check\n\nM-SIS\n\nLattice based\n\nGreyhound, Hyperwolf","type":"content","url":"/#security-analysis-of-mle-pcs","position":29},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on KZG10","lvl2":"Security Analysis of MLE-PCS"},"type":"lvl3","url":"/#security-based-on-kzg10","position":30},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on KZG10","lvl2":"Security Analysis of MLE-PCS"},"content":"For MLE-PCS based on [KZG10], we focus on their Knowledge Soundness proof (also called Extractability).\n\nIn the [KZG10] paper, the authors required the protocol to satisfy the Evaluation Binding property, which only guarantees that the prover cannot forge a proof making the polynomial f(X) open to two different values v_1 \\neq v_2 at the same point a.\n\nHowever, when [KZG10] is used in SNARK design, satisfying only the Evaluation Binding property is insufficient for the security requirements of Knowledge Soundness in the proof system.\n\nTherefore, researchers have proposed stronger security requirements for [KZG10], namely “Extractability”: For any algebraic adversary \\mathcal{A}_{alg}, if it can output a valid polynomial evaluation proof, then there must exist another efficient algorithm \\mathcal{B}_{alg} that can extract the secret value f(X) of the polynomial commitment C, satisfying f(z) = v.\n\nFor the extractability proof of [KZG10], we typically care about two points:\n\nSecurity model: Using standard model or idealized model, such as Random Oracle model, or Algebraic Group model\n\nDifficulty assumption: Mainly considering whether the type of assumption used is Falsifiable or Non-falsifiable\n\nSeveral works based on [KZG10], including [MBKM19], [GWC19], and [CHM+20], have discussed the extractability proof problem. We summarize as follows:\n\npaper\n\nsecurity model\n\nassumption separation\n\nassumption\n\n[MBKM19], [GWC19]\n\nAGM+ROM\n\nFalsifiable\n\nq-DLOG\n\n[CHM+20]\n\nROM\n\nNon-Falsifiable\n\nPKE\n\n[HPS23]\n\nAGMOS+ROM\n\nFalsifiable\n\nFPR+TOFR\n\n[LPS24]\n\nROM\n\nFalsifiable\n\nARSDH\n\nThis project deeply studied the security proofs of the KZG protocol in blog post format, including:\n\nKZG-soundness-1: Introduces the concept of KZG extractability and analyzes the KZG security proof method in the AGM+ROM model from the [MBKM19] paper\n\nKZG-soundness-2: Introduces and analyzes in detail the KZG security proof method in the ROM model from the [LPS24] paper\n\nIn addition, [HPS23] proposed an improved security model called AGMOS (Algebraic Group Model with Oblivious Sampling). As a more realistic variant of AGM, AGMOS gives an adversary the additional ability to blindly sample group elements without knowing the discrete logarithm.\n\nFurthermore, [HPS23] points out that there are two different KZG extractability definitions in actual protocol design:\n\nThe extractor algorithm extracts the polynomial after the Commit and Open phases, as in [MBKM19], [CHM+20]\n\nThe extractor algorithm extracts the polynomial only after the Commit phase, as in [GWC19]\n\nAmong them, although the latter can be proven secure in the AGM model, it would reduce to a spurious knowledge assumption that is insecure in the standard model.\n\nIn addition to extractability, we usually also require [KZG10] to satisfy the hiding property, as an important component for constructing zkSNARK or other secure protocols with the Zero-knowledge property.\nThis project also discusses this aspect, including:\n\nUnderstanding Hiding KZG10: This article details two methods for implementing the Hiding property for KZG10. One scheme is from [KT23], with the main technique being a simplified version of multivariate polynomial commitment from [PST13]. The second scheme is from [CHM+20], with the main technique being an improvement on the original KZG protocol paper [KZG10].","type":"content","url":"/#security-based-on-kzg10","position":31},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on Linear Code","lvl2":"Security Analysis of MLE-PCS"},"type":"lvl3","url":"/#security-based-on-linear-code","position":32},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on Linear Code","lvl2":"Security Analysis of MLE-PCS"},"content":"For MLE-PCS based on Linear Code, we focus on their Soundness proof. The FRI protocol [BBHR18] itself is an IOPP (Interactive Oracle Proof of Proximity) protocol for Reed-Solomon (RS) encoding, and its security is closely related to the properties of RS encoding and some coding theory. We conducted an in-depth study of the soundness proof for the FRI series of protocols.\n\nFor a set of evaluations S in a finite field \\mathbb{F}, assuming the number of elements in S is N, given a rate parameter \\rho \\in (0,1], the encoding \\text{RS}[\\mathbb{F},S,\\rho] represents the set of all functions f: S \\rightarrow \\mathbb{F}, where f is the evaluations of a polynomial of degree d < \\rho N, i.e., there exists a polynomial \\hat{f} of degree d < \\rho N such that f and \\hat{f} have consistent values on S.\n\nThe FRI protocol solves the RS proximity problem: Assuming we can obtain an oracle about the function f: S \\rightarrow \\mathbb{F}, the Verifier needs to use fewer query complexities and have a high probability of distinguishing whether f belongs to one of the following cases:\n\nf \\in \\text{RS}[\\mathbb{F},S,\\rho]\n\n\\Delta(f, \\text{RS}[\\mathbb{F},S,\\rho]) > \\delta\n\nThat is, either f is a codeword in the RS encoding \\text{RS}[\\mathbb{F},S,\\rho], or the relative Hamming distance from f to all codewords in \\text{RS}[\\mathbb{F},S,\\rho] is greater than the proximity parameter \\delta.\n\nThe [BBHR18] paper provides the soundness proof for the FRI protocol. For \\delta < \\delta_0, where \\delta_0 \\approx \\frac{1 - 3 \\rho}{4}, for any malicious Prover P^*, the probability that the Verifier rejects P^* is approximately \\delta - \\frac{O(1)}{|\\mathbb{F}|}.\n\nThrough analysis, we know that under the same security parameter, the larger the value of \\delta_0 can be, the fewer queries the Verifier needs to make in the Query phase, thus reducing the proof size and computational complexity of the FRI protocol. After the [BBHR18] paper, many theoretical research works have appeared to increase \\delta_0 in the FRI protocol.\n\npaper\n\n\\delta_0\n\n[BBHR18]FRI\n\n\\delta_0 \\approx \\frac{1 - 3\\rho}{4}\n\n[BKS18]Worst-case ...\n\n\\delta_0 \\approx 1 - \\rho^{\\frac{1}{4}}\n\n[BGKS20]DEEP-FRI\n\n\\delta_0 \\approx 1 - \\rho^{\\frac{1}{3}} , tight(!)\n\n[BCIKS20]Proximity Gaps\n\n\\delta_0 \\approx 1 - \\rho^{\\frac{1}{2}}\n\nThis project studied the security proofs of the FRI protocol in blog post format, including:\n\nDive into BBHR18-FRI Soundness: Detailed analysis of the security proof in the FRI paper [BBHR18]\n\nDive into BCIKS20-FRI Soundness: Introduction to how the Proximity Gaps theorem can improve the security parameters in the FRI protocol\n\nProximity Gaps and Correlated Agreement: The Core of FRI Security Proof: In-depth exploration of core concepts in FRI security proof\n\nThe [ACFY24a] STIR paper proposed an improvement to the FRI protocol, with the idea of reducing the code rate in each k-fold of the FRI protocol to achieve smaller query complexity. In the blog post \n\nSTIR: Improving Rate to Reduce Query Complexity, we detail the differences between the FRI and STIR protocols, introduce the protocol flow for one iteration, and analyze the soundness of one iteration.\n\nAlthough the Basefold protocol [ZCF23] is applicable to the Random Foldable Code mentioned in the paper, it still applies to Reed Solomon encoding, so it can be understood that the Basefold protocol combines sumcheck and the FRI protocol. In the original Basefold paper [ZCF23], its soundness is only proven to have \\delta_0 at most (1 - \\rho)/2. Subsequently, Ulrich Haböck proved in [H24] that the Basefold protocol for Reed Solomon can reach the Johnson bound 1 - \\sqrt{\\rho}, and Hadas Zeilberger proved in the Khatam [Z24] paper that the Basefold protocol for general linear codes can reach 1 - \\rho^{\\frac{1}{3}}.\n\nRelated note articles on the Basefold protocol include:\n\nBasefold protocol introduction:\n\nNotes on Basefold (Part I): Foldable Linear Codes\n\nNotes on Basefold (Part II): IOPP\n\nNotes on Basefold (Part III): MLE Evaluation Argument\n\nSoundness proof from the original Basefold paper [ZCF23]:\n\nNotes on BaseFold (Part IV): Random Foldable Codes\n\nNotes on Basefold (Part V): IOPP Soundness\n\nSoundness proof of the Basefold protocol given in [H24]:\n\nNote on Basefold’s Soundness Proof under List Decoding\n\nNote on Soundness Proof of Basefold under List Decoding\n\nThe Deepfold protocol and WHIR protocol adopt the same idea as the Basefold protocol, combining the sumcheck protocol to construct MLE-PCS. The Deepfold protocol combines the sumcheck protocol with DEEP-FRI. For a detailed introduction to this protocol, see the blog post \n\nNote on DeepFold: Protocol Overview. The WHIR protocol combines the sumcheck protocol and STIR protocol, and the blog post \n\nNote on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification introduces the WHIR protocol in detail.\n\nThe Basefold protocol, Deepfold protocol, and WHIR protocol have similar approaches. In the blog post \n\nBaseFold vs DeepFold vs WHIR, we compare the construction of these three protocols and, through analysis of their soundness proofs, compare the number of queries by the Verifier in these three protocols.","type":"content","url":"/#security-based-on-linear-code","position":33},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on M-SIS","lvl2":"Security Analysis of MLE-PCS"},"type":"lvl3","url":"/#security-based-on-m-sis","position":34},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Security Based on M-SIS","lvl2":"Security Analysis of MLE-PCS"},"content":"For lattice-based polynomial commitment schemes, we focus on the proof of knowledge soundness.\n\nUnlike discrete logarithm-based schemes, relations in lattice-based cryptography typically include additional norm constraints to meet the security requirements of lattice assumptions. Therefore, in the proof of knowledge soundness, we not only need to prove that the extracted witness satisfies conventional constraints such as IPA, but also must further prove that the norm of this witness is small enough, ensuring it can be bound to the Ajtai commitment.\n\nThe security of the Greyhound protocol is built on the M-SIS problem variant of infinite norm. In the proof of knowledge soundness, Greyhound proves that the extracted witness pair (\\bar{c}, \\bar{w}) satisfies a relaxed relation. This relaxed relation is the same as the original relation in other conventional constraints, differing only in the norm constraint and binding constraint.\n\nFor the binding constraint, the relaxed relation requires \\bar{c} A \\bar{w} = \\bar{c} \\mathsf{cm}, where \\mathsf{cm} is the commitment to the witness and a public parameter. For the norm constraint, the relaxed relation requires | \\bar{c} \\bar{w} | \\le 2\\bar{\\beta}, while the original relation requires | w | \\le \\beta.\n\nBy constraining that the M-SIS problem remains hard at the norm \\min\\{2\\bar{\\beta},\\ 8T\\bar{\\beta}\\}, the proof can guarantee that the extracted witness is both bound to the original commitment \\mathsf{cm} and satisfies all other conventional constraints. Here, T represents the operation norm of \\bar{c}.\n\nThe security of Hyperwolf is built on the \\ell_2-norm version of the M-SIS problem. In proving the norm constraint, the protocol adopts a technical approach similar to Labrador: by proving that the norm of the witness under a certain random projection is small, it can be inferred with high probability that the norm of the original vector is also within an acceptable range.\n\nTo prove that the \\ell_2 norm of a vector \\vec{a} \\in \\mathbb{Z}^n is small, while avoiding leaking its complete information, we can use the Johnson–Lindenstrauss (JL) lemma. The core idea of this lemma is that the \\ell_2 norm of a high-dimensional vector can be approximately preserved with high probability after random linear projection.\n\nThe specific method is as follows: The Verifier randomly generates a projection matrix \\Pi \\in \\mathbb{Z}^{256 \\times n}, where each element is independently sampled from the set {-1, 0, 1} with probabilities \\Pr[-1] = \\Pr[1] = 1/4, \\Pr[0] = 1/2. The Prover calculates and sends the projected vector \\vec{p} = \\Pi \\vec{a}. The Verifier then checks the norm of \\vec{p} to estimate whether the norm of the original vector \\vec{a} satisfies the constraint. The specific content of the JL Lemma is as follows:\n\nModular Johnson–Lindenstrauss Variant:\nLet q \\in \\mathbb{N}, and let \\mathcal{D} be a distribution defined on {0, \\pm 1}, satisfying \\mathcal{D}(1) = \\mathcal{D}(-1) = 1/4, \\mathcal{D}(0) = 1/2. For any \\vec{a} \\in \\mathbb{Z}_q^n, if it satisfies |\\vec{a}| \\le b and b \\le q/125, then:\\begin{split}\n\\Pr_{\\Pi \\leftarrow \\mathcal{D}^{256\\times n}}[\\|\\Pi\\vec{a}\\mod q\\|^2< 30b^2] \\lessapprox  2^{-128}.\n\\nonumber\n\\end{split}\n\nAccording to this theorem, proving the short norm problem can be reduced to proving that a projected vector \\vec{p} is well-formed (i.e., satisfies the IPA relation). This method can guarantee with high probability that the extracted witness satisfies the norm constraint, while only introducing a constant-level slack, achieving a good balance between efficiency and security.","type":"content","url":"/#security-based-on-m-sis","position":35},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Discoveries"},"type":"lvl2","url":"/#discoveries","position":36},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Discoveries"},"content":"In the process of deeply studying the underlying principles of these MLE-PCS, we found that some protocols still have room for optimization, and we proposed some new implementation methods. Below are our main innovations.","type":"content","url":"/#discoveries","position":37},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"∑-Check Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#id-check-protocol","position":38},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"∑-Check Protocol","lvl2":"Discoveries"},"content":"The compressed Σ-protocol theory \n\nAC20, \n\nACF21 offers a general approach for efficiently proving polynomial relations. In the context of PCS, it can be either (1) used directly to prove PCS evaluations by expressing them as a relation \\{(\\vec{f}; F, \\vec{z}, v): \\mathsf{Com}(\\vec{f}) = F, f(\\vec{z}) = v\\}, where \\vec{f}​ represents the coefficients of the polynomial f; or (2) applied as a drop-in replacement for IPA in Bulletproofs-based PCS systems like \n\nHyrax. \n\nAC20 and \n\nACF21 demonstrate that this can be achieved through linearization based on arithmetic circuits, followed by the application of \n\nBulletproofs compression, resulting in a Bulletproofs-based PCS with a transparent setup.\n\nOur recent contribution, \n\nΣ-Check, advances this research field by introducing an efficient sumcheck-based method for proving k distinct polynomial evaluations, each with n variables, at a cost of O(n+\\log k)-size proofs. This approach eliminates the need for circuit-based linearization and proves to be more efficient when handling k polynomials, which previously required O(n+k) cost in \n\nAC20 and \n\nACF21. A prototype implementation is available at \n\nGitHub.","type":"content","url":"/#id-check-protocol","position":39},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyperwolf Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#hyperwolf-protocol","position":40},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyperwolf Protocol","lvl2":"Discoveries"},"content":"In the \n\nGreyhound protocol, the polynomial evaluation process can be expressed as the inner product of a coefficient vector \\vec{f} of length N with the tensor product of two vectors \\vec{a} and \\vec{b} of length n = \\sqrt{N}, i.e., v = \\langle \\vec{f}, \\vec{a} \\otimes \\vec{b} \\rangle. Alternatively, this process can be understood as reconstructing \\vec{f} into a matrix F of size n \\times n, and then performing matrix multiplication successively with \\vec{a} and \\vec{b}. Specifically, \\vec{f} can be seen as concatenated from each row of F in row-major order.\n\nBy rewriting the polynomial evaluation process into the above structure, the Greyhound protocol achieves a reduction in proof size and verification time to sublinear levels, while maintaining the prover’s computational cost as linear. This structural optimization allows the protocol to balance security with efficiency and practicality.\n\nThe \n\nHyperwolf protocol is an optimization of the \n\nGreyhound protocol, with the core idea being to generalize the original two-dimensional structure to k dimensions (k \\ge 2).\n\nSpecifically, it interprets the one-dimensional coefficient vector \\vec{f} as a k-dimensional hypercube [F]^{(k)} with dimensions b \\times b \\times \\cdots \\times b (a total of k dimensions), satisfying b^k = N. The polynomial evaluation process can be viewed as successive tensor-directional matrix multiplication of this hypercube with k auxiliary vectors. Based on this structure, we designed a proof system with k rounds of interaction, where the proof size and verification time for each round is O(b), resulting in an overall proof size and verification time of O(kb) = O(kN^{1/k}). When k = \\log N, the system’s total complexity can be optimized to O(\\log N), significantly enhancing performance.","type":"content","url":"/#hyperwolf-protocol","position":41},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Zeromorph Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#optimizing-the-zeromorph-protocol","position":42},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Zeromorph Protocol","lvl2":"Discoveries"},"content":"In the zeromorph protocol, we need to prove that n quotient polynomials q_i(X) = [[\\tilde{q}_i]]_i (0 \\le i < n) have degrees less than 2^i. Section 6 of the zeromorph paper [KT23] aggregates multiple Degree Bound proofs together, as detailed in \n\nOptimized Protocol. However, in this protocol, the Verifier needs to perform two operations on the elliptic curve \\mathbb{G}_2, which is very expensive.\n\nWe used another method to prove Degree Bound, avoiding operations by the Verifier on the elliptic curve \\mathbb{G}_2. The cost is a slight increase in the Verifier’s calculations on the elliptic curve \\mathbb{G}_1 and adding a point on the elliptic curve \\mathbb{G}_1 and a value on the finite field to the proof, which is acceptable in scenarios with high requirements on Verifier Cost. The detailed description of this protocol is in \n\nZeromorph-PCS (Part II).","type":"content","url":"/#optimizing-the-zeromorph-protocol","position":43},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Gemini Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#optimizing-the-gemini-protocol","position":44},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Gemini Protocol","lvl2":"Discoveries"},"content":"In the gemini protocol, the Prover needs to calculate the values of n polynomials h_0(X), \\ldots, h_{n - 1}(X) at random points \\beta, -\\beta, \\beta^2 and send them to the Verifier, letting the Verifier verify that there is a folding relationship between h_{i + 1}(X^2) and h_{i}(X) and h_i(-X). For this part of the protocol description, see \n\nGemini-PCS (Part I).\n\nWe discovered two ways to optimize the gemini protocol.\n\nOptimization Method 1: The Prover only needs to send h_0(\\beta^2) and the values of h_0(X), \\ldots, h_{n - 1}(X) at random points \\beta, -\\beta, using random numbers to aggregate h_0(X) and h_1(X), \\ldots, h_{n - 1}(X) into one polynomial, proving at once that these polynomials are correct at \\beta, -\\beta, \\beta^2. This can reduce the proof size, eliminating n - 1 values on the finite field. For the specific protocol description, see \n\nGemini-PCS (Part III).\n\nOptimization Method 2: Another optimization method adopts the idea of selecting points in the Query phase of the FRI protocol. It challenges h_0(X) at X = \\beta, then challenges the folded polynomial h_1(X) at X = \\beta^2, and so on, until h_{n - 1}(\\beta^{2^{n-1}}). The benefit is that each opening point of h_{i}(X) can be reused when verifying the folding of h_{i+1}(X), saving n more opening points beyond Optimization Method 1. Compared to Optimization Method 1, the cost of this approach is an increase in computation for both Prover and Verifier, but it reduces proof size. For the specific protocol description, see \n\nGemini-PCS (Part IV).","type":"content","url":"/#optimizing-the-gemini-protocol","position":45},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the PH23 Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#optimizing-the-ph23-protocol","position":46},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the PH23 Protocol","lvl2":"Discoveries"},"content":"In the original paper [PH23], the authors provided an inner product-based MLE-PCS protocol. Following the ideas provided in the original paper, the protocol we designed has a proof size of O(\\log{N}). The core idea of the PH23 protocol is to prove the following inner product:\\tilde{f}(X_0,X_1,\\ldots,X_{n-1}) = \\sum_{\\vec{b}\\in\\{0,1\\}^n} a(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, \\vec{u})\n\nIf we denote \\tilde{eq}(\\vec{b}, \\vec{u}) as \\vec{c}, then the Evaluation proof can be transformed into an inner product proof, i.e., proving\\langle \\vec{a}, \\vec{c} \\rangle \\overset{?}{=} \\tilde{f}(X_0,X_1,\\ldots,X_{n-1})\n\nProving the inner product alone is not enough; the Prover also needs to commit to \\vec{c} and prove the correctness of \\vec{c} to the Verifier. Paper [PH23] provided a scheme that uses \\log{N} polynomial constraints to prove the correctness of \\vec{c}. This can optimize the Proof size to O(\\log{N}) Field elements plus O(1) Group Elements. However, this is not the most optimized scheme for Proof size.\n\nWe can define three column vectors, namely \\vec{c}, \\vec{c}', and \\vec{u}', where \\vec{c}' is the Lookup vector in \\vec{c}, meaning that for any c_i\\in\\vec{c'}, we have c_i\\in\\vec{c}. Note that this is not an Unindexed Lookup relation, but an Indexed Lookup relation. Then we define that each element in \\vec{u}' also comes from \\vec{u}. In this way, we can use a polynomial constraint relation to prove the correctness of \\vec{c}.(1-u'_i) \\cdot c_i - u'_i \\cdot c'_i = 0, \\quad \\forall i \\in [N]\n\nWe can prove the Indexed Lookup relation using a Copy Constraint Argument, or we can use an Indexed Logup Argument protocol, thus achieving O(1) Proof size.","type":"content","url":"/#optimizing-the-ph23-protocol","position":47},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Adding FRI Interface Protocols for PH23 and Gemini","lvl2":"Discoveries"},"type":"lvl3","url":"/#adding-fri-interface-protocols-for-ph23-and-gemini","position":48},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Adding FRI Interface Protocols for PH23 and Gemini","lvl2":"Discoveries"},"content":"For PH23-PCS, zeromorph, and gemini protocols, they all transform MLE-PCS into univariate-PCS. In the original protocols, the interfacing univariate-PCS is KZG10. We tried to interface these protocols with FRI-PCS protocols and provided complete protocol descriptions.\n\nPH23-FRI\n\nWe provided two different protocols for interfacing PH23 with FRI.\n\nProtocol 1 description is in \n\nThe Missing Protocol PH23-PCS (Part 4), where the inner product proof is implemented through Grand Sum.\n\nProtocol 2 description is in \n\nThe Missing Protocol PH23-PCS (Part 5), where the inner product proof is implemented through the Univariate Sumcheck method.\n\nBy comparing these two different implementation methods, we found that Protocol 2 deals with more polynomials, resulting in higher overall proof size and Verifier computational complexity compared to Protocol 1.\n\nGemini-FRI\n\nProtocol description is in \n\nGemini: Interfacing with FRI. One advantage of FRI-PCS is that for opening polynomials of different degrees at multiple points, random numbers can be used to merge them into one polynomial, requiring only one call to FRI’s low degree test to complete all these proofs. Therefore, when combining the Gemini protocol with FRI-PCS, only one call to the FRI protocol is needed to prove the correct opening of multiple polynomials at different points in the Gemini protocol.","type":"content","url":"/#adding-fri-interface-protocols-for-ph23-and-gemini","position":49},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Basefold Protocol","lvl2":"Discoveries"},"type":"lvl3","url":"/#optimizing-the-basefold-protocol","position":50},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Optimizing the Basefold Protocol","lvl2":"Discoveries"},"content":"In the Sumcheck sub-protocol of Basefold, the h^{(i)}(X) sent by the Prover in each step is a univariate quadratic polynomial, but after Sumcheck optimization [Gru24], the Prover can send just a linear polynomial, reducing the communication in the Sumcheck sub-protocol. This point is mentioned in paper [H24]. Further research into the Deepfold protocol revealed that it uses a different Sumcheck protocol from [H24], as detailed in \n\nThe Connection between Deepfold and sumcheck.\n\nBriefly, according to the definition of \\tilde{eq}(\\vec{X}, \\vec{Y}), it can be decomposed as:\\tilde{eq}(\\vec{X}_0\\parallel\\vec{X}_1, \\vec{Y}_0\\parallel\\vec{Y}_1) = eq(\\vec{X}_0, \\vec{Y}_0) \\cdot eq(\\vec{X}_1, \\vec{Y}_1)\n\nFirst, observe the definition of h^{(0)}(X):h^{(0)}(X) = \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot \\tilde{eq}((u_0, u_1, u_2), (X, b_1, b_2))\n\nThe right side of the equation can be rewritten as:\\begin{aligned}\nh^{(0)}(X) &= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot \\tilde{eq}((u_0, u_1, u_2), (X, b_1, b_2)) \\\\\n&= \\sum_{b_1,b_2\\in\\{0,1\\}^{2}} \\tilde{f}(X, b_1, b_2) \\cdot eq(u_0, X) \\cdot eq((u_1, u_2), (b_1, b_2)) \\\\\n&= eq(u_0, X) \\cdot \\sum_{(b_1,b_2)\\in\\{0,1\\}^2}  \\Big( \\tilde{f}(X, b_1, b_2) \\cdot eq((u_1,u_2), (b_1, b_2)) \\Big) \\\\\n\\end{aligned}\n\nThis way, the Prover only needs to send g(X)=\\sum_{(b_1,b_2)\\in\\{0,1\\}^2}  \\Big( \\tilde{f}(X, b_1, b_2) \\cdot eq((u_1,u_2), (b_1, b_2)) \\Big) to the Verifier, who can compute eq(u_0, X) and multiply it. This calculation is just a Linear Combination involving only one multiplication.\n\nAnother article \n\nBasefold Optimization applies Deepfold’s optimization techniques to Basefold, effectively reducing the computation for both Prover and Verifier, while also reducing proof length. After rough estimation, the Sumcheck Prover’s operations are reduced by half, while the Sumcheck Verifier’s operations are reduced to one-sixth of [H24]. For Basefold, the main component of the Verifier’s overall operations is still the FRI-Query operations, so this optimization may not be that significant, but from a protocol design perspective, the optimized protocol is more concise. Whether this technique can be applied to other protocols is worth further study. Interested readers can refer to the optimized code prototype implementation \n\nbasefold​_rs​_opt​_pcs​.py.","type":"content","url":"/#optimizing-the-basefold-protocol","position":51},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Code Implementation (Python)"},"type":"lvl2","url":"/#code-implementation-python","position":52},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Code Implementation (Python)"},"content":"This project has implemented many MLE-PCS protocols in Python code, with Jupyter Notebook versions also available for some protocols, helping users understand the protocols through interactive code.\n\nScheme\n\nPython Code\n\nJupyter Notebook\n\nGemini\n\nbcho_pcs.py\n\nbcho_pcs.ipynb\n\nHyperKZG\n\nhyperkzg_pcs.py\n\n\n\nZeromorph\n\nzeromorph.py, \n\nzeromorph_zk.py, \n\nzerofri.py\n\nzeromorph.ipynb, \n\nzeromorph​_mapping​_tutorial​.ipynb\n\nPH23\n\nph23_pcs.py\n\n\n\nMercury\n\nmercury_pcs.py\n\n\n\nSamaritan\n\nsamaritan_pcs.py\n\n\n\nBasefold\n\nBasefold.py,\n\nbasefold​_rs​_opt​_pcs​.py, \n\nbasefold_rs_pcs.py\n\nBasefold.ipynb\n\nDeepfold\n\ndeepfold_pcs.py\n\ndeepfold.ipynb\n\nWHIR\n\nwhir_pcs.py\n\n\n\nHyrax\n\nhyrax_pcs.py\n\n\n\nPST13(Libra-PCS)\n\nlibra_pcs.py\n\n\n\nIn addition to implementing these protocols, some subprotocols used by MLE-PCS protocols have also been implemented.\n\nSubprotocol\n\nPython Code\n\nJupyter Notebook\n\nFRI\n\nfri.py\n\n\n\nSTIR\n\n\n\nstir.ipynb\n\nKZG10\n\nkzg10.py,\n\nkzg10_hiding_m.py, \n\nkzg10_hiding_z.py, \n\nkzg10_non_hiding.py,\n\nkzg_hiding.py\n\nkzg10.ipynb\n\nIPA\n\nipa.py\n\n\n\nunivariate polynomial\n\nunipolynomial.py, \n\nunipoly.py, \n\nunipoly2.py\n\n\n\nmultilinear polynomial\n\nmle2.py\n\n","type":"content","url":"/#code-implementation-python","position":53},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Algebraic Operation Optimization","lvl2":"Code Implementation (Python)"},"type":"lvl3","url":"/#algebraic-operation-optimization","position":54},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Algebraic Operation Optimization","lvl2":"Code Implementation (Python)"},"content":"When implementing MLE-PCS, polynomial operations are ubiquitous, and different implementation methods have different impacts on the complexity of polynomial operations. In this project, we have researched some optimization methods for polynomial operations, including polynomial division optimization.\n\nOptimization of polynomial division\n\nAssume a finite field \\mathbb{F}. For polynomials f(X) and g(X) in \\mathbb{F}[X], they satisfy the division with remainder equation:f(X) = g(X) \\cdot q(X) + r(X)\n\nand \\deg(r) < \\deg(g). Let n = \\deg(f), m = \\deg(g). Traditional division requires O(n^2) computational complexity to calculate the quotient polynomial q(X) and remainder polynomial r(X) after dividing f(X) by g(X). This project introduces a fast division algorithm using Newton Iteration, with algorithmic complexity consistent with polynomial multiplication, i.e., O(M(n)), where M(n) represents the complexity of polynomial multiplication. For a detailed description of this algorithm, see the blog post \n\nFast Polynomial Division Based on Newton Iteration, with the corresponding Python code implementation in \n\nunipolynomial.py.","type":"content","url":"/#algebraic-operation-optimization","position":55},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl2","url":"/#comparison-of-kzg10-based-mle-pcs","position":56},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"KZG-based MLE-PCS include Libra-PCS, PH23-PCS, zeromorph, gemini, mercury, and samaritan.\n\nThis project theoretically detailed the complexity of PH23-PCS, zeromorph, and gemini protocols, including finite field multiplication, finite field division, addition and multiplication on elliptic curves, etc. All three protocols transform MLE polynomial commitments into univariate polynomial commitments, which can be interfaced with KZG10 or FRI, but their transformation methods differ, resulting in differences in efficiency.\n\nFirst, let’s briefly summarize the approaches of these three protocols. PH23 and zeromorph both consider the point-value form of MLE polynomials on the Hypercube, while gemini considers the coefficient form of MLE polynomials.\n\n\n\nMLE\n\nApproach\n\nph23\n\nevaluation on hypercube\n\nTransformed into proving inner product, needing to prove correct construction of \\vec{c} and inner product proof (sum product scheme), transforming into proving n + 4 univariate polynomials are 0 on \\mathbb{H}.\n\nzeromorph\n\nevaluation on hypercube\n\nUsing remainder theorem to decompose multivariate polynomials, then directly mapping the values of decomposed polynomials on hypercube to univariate polynomials, proving equations about univariate polynomials hold and degree bounds of quotient polynomials.\n\ngemini\n\ncoefficients form\n\nDirectly corresponding to coefficients of univariate polynomials, using split-and-fold to fold univariate polynomials until finally folded into a constant polynomial.\n\nFor zeromorph and gemini protocols, we provide some optimization ideas, resulting in multiple versions of these two protocols. The links to the protocol description documents and complexity analysis documents are shown in the table below.\n\nProtocol\n\nVersion\n\nProtocol Description Document\n\nProtocol Analysis Document\n\nph23\n\n\n\nPH23+KZG10 Protocol (Optimized Version)\n\nph23-analysis\n\ngemini\n\nOptimization 1\n\ngemini-pcs-02\n\ngemini-analysis\n\ngemini\n\nOptimization 2: Similar to FRI query optimization\n\ngemini-pcs-03\n\ngemini-analysis\n\nzeromorph\n\nv1: batched degree bound\n\nOptimized Protocol\n\nzeromorph-anlysis\n\nzeromorph\n\nv2: optimized degree bound proof\n\nZeromorph-PCS (Part II)\n\nzeromorph-anlysis\n\nBelow is the complexity analysis result for these three protocols interfacing with KZG10, where the notation is as follows:\n\nn: Number of variables in the MLE polynomial.\n\nN: N = 2^n.\n\n\\mathbb{F}_{\\mathsf{mul}}: Multiplication operation on finite field \\mathbb{F}. Addition operations on the finite field are not counted in complexity analysis.\n\n\\mathbb{F}_{\\mathsf{inv}}: Division operation on finite field \\mathbb{F}.\n\n\\mathsf{msm}(m, \\mathbb{G}): Complexity of multi-scalar multiplication, where m represents the number of scalars, and \\mathbb{G} represents the elliptic curve group.\n\n\\mathsf{EccMul}^{\\mathbb{G}}: Multiplication operation on elliptic curve group \\mathbb{G}.\n\n\\mathsf{EccAdd}^{\\mathbb{G}}: Addition operation on elliptic curve group \\mathbb{G}.\n\nP: Complexity of pairing operation between two elliptic curves.\n\nD_{max}: Maximum power of system parameters [\\tau^{D_{max}}]_1 and [\\tau^{D_{max}}]_2 generated in the setup phase of the KZG protocol.\n\n\\mathbb{G}_1: First elliptic curve group.\n\n\\mathbb{G}_2: Second elliptic curve group.","type":"content","url":"/#comparison-of-kzg10-based-mle-pcs","position":57},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"PH23","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl3","url":"/#ph23","position":58},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"PH23","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"In step 10 of \n\nRound 3 of the PH23 protocol, the Prover needs to construct the Quotient polynomial q_{\\omega\\zeta}(X):q_{\\omega\\zeta}(X) = \\frac{z(X) - z(\\omega^{-1}\\cdot\\zeta)}{X - \\omega^{-1}\\cdot\\zeta}\n\nWe considered two implementation methods for this calculation.\n\nMethod 1: The numerator and denominator polynomials are represented in coefficient form. When calculating the quotient polynomial, since the denominator is a linear polynomial, linear polynomial division can be used, with complexity (N - 1) ~ \\mathbb{F}_{\\mathsf{mul}}. This method yields the quotient polynomial in coefficient form. In subsequent steps of the protocol, the quotient polynomial needs to be committed and sent to the Verifier. Since q_{\\omega\\zeta}(X) has degree N - 2, assuming the calculated coefficient form is q_{\\omega\\zeta}^{(0)}, q_{\\omega\\zeta}^{(1)}, \\ldots, q_{\\omega\\zeta}^{(N - 2)}, the commitment to the quotient polynomial is:Q_{\\omega\\zeta} = q_{\\omega\\zeta}^{(0)} \\cdot G + q_{\\omega\\zeta}^{(1)} \\cdot (\\tau \\cdot G) + \\cdots + q_{\\omega\\zeta}^{(N - 2)} \\cdot (\\tau^{N - 2} \\cdot G)\n\nwhere G is the generator of elliptic curve \\mathbb{G}_1, and (G, \\tau G, \\ldots, \\tau^{N - 2}G) is the SRS of KZG10. This requires storing these SRS in memory.\n\nMethod 2: Calculate using point-value form. Calculate [q_{\\omega\\zeta}(x)|_{x \\in H}],\n\nFirst calculate [(x - \\omega^{-1} \\cdot \\zeta)^{-1}|_{x \\in H}] using an efficient inversion algorithm, with complexity \\mathbb{F}_{\\mathsf{inv}} + (3N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nCalculate [q_{\\omega\\zeta}(x)|_{x \\in H}] with complexity N ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nThe total complexity of this method for calculating the quotient polynomial is:\\mathbb{F}_{\\mathsf{inv}} + (4N - 3) ~ \\mathbb{F}_{\\mathsf{mul}}\n\nWe can see that since the denominator is only a linear polynomial, method 1 is more efficient, at the cost of needing to store more SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory.\n\nConsidering these two different implementation methods, the complexity of the PH23 protocol is:\n\nProver’s cost:\n\nUsing coefficient form in Round 3-10, complexity is:\\begin{align}\n (17nN + 36N + 9n - 2) ~ \\mathbb{F}_{\\mathsf{mul}} + {(n + 1) \\log^2(n + 1)  ~ \\mathbb{F}_{\\mathsf{mul}} } + 2~ \\mathbb{F}_{\\mathsf{inv}} + 5 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}\n\nThis method requires storing SRS (G, \\tau G, \\ldots, \\tau^{N - 2}G) in memory for polynomial commitment in coefficient form.\n\nUsing method 2, point-value form, in Round 3-10, complexity is:\\begin{align}\n  (17nN + 39N + 9n - 4) ~ \\mathbb{F}_{\\mathsf{mul}} + { (n + 1) \\log^2(n + 1)  ~ \\mathbb{F}_{\\mathsf{mul}} } + 3~ \\mathbb{F}_{\\mathsf{inv}} + 6 ~ \\mathsf{msm}(N, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\\end{align}\n\nVerifier’s cost:(11n + 11) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 4) ~ \\mathbb{F}_{\\mathsf{inv}} + 19 ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + 16 ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2 ~ P\n\nProof size:(n + 1) \\cdot \\mathbb{F}_p + 7 ~ \\mathbb{G}_1","type":"content","url":"/#ph23","position":59},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl3","url":"/#gemini","position":60},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"","type":"content","url":"/#gemini","position":61},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"gemini Optimization 1","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl4","url":"/#gemini-optimization-1","position":62},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"gemini Optimization 1","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"Prover’s cost:(10 N + n + 5) ~ \\mathbb{F}_{\\mathsf{mul}} + N ~ \\mathbb{F}_{\\mathsf{inv}} \n+ \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + \\mathsf{msm}(N - 3, \\mathbb{G}_1) + \\mathsf{msm}(N - 1, \\mathbb{G}_1)\n\nVerifier’s cost:\\begin{aligned}\n    & (4n + 8) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathbb{F}_{\\mathsf{inv}} + (n + 1) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\\end{aligned}\n\nProof size:(2n + 1)  \\mathbb{F}_p + (n + 1) \\cdot \\mathbb{G}_1","type":"content","url":"/#gemini-optimization-1","position":63},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"gemini Optimization 2","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl4","url":"/#gemini-optimization-2","position":64},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"gemini Optimization 2","lvl3":"gemini","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"Prover’s cost:\\begin{aligned}\n     & (14 N + 6n - 11) ~ \\mathbb{F}_{\\mathsf{mul}} + (n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 1} \\mathsf{msm}(2^{i}, \\mathbb{G}_1) + 2 ~ \\mathsf{msm}(N - 1, \\mathbb{G}_1)   \\\\\n\\end{aligned}\n\nVerifier’s cost:8n ~ \\mathbb{F}_{\\mathsf{mul}} + (3n + 1) ~ \\mathbb{F}_{\\mathsf{inv}} + (2n + 4) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 3) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + 2~P\n\nProof size:(n + 1) \\cdot \\mathbb{F}_p + (n + 1) \\cdot \\mathbb{G}_1\n\nComparing these two protocols, we can see that the gemini Optimization 2 protocol reduces proof size by n ~ \\mathbb{F}_p at the cost of increasing the workload of both Prover and Verifier.","type":"content","url":"/#gemini-optimization-2","position":65},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl3","url":"/#zeromorph","position":66},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"We performed detailed complexity analysis on three versions of the zeromorph protocol.","type":"content","url":"/#zeromorph","position":67},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"zeromorph-v1","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl4","url":"/#zeromorph-v1","position":68},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"zeromorph-v1","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"We performed a detailed complexity analysis of two optimized versions of the Zeromorph protocol.\n\nProver’s cost:\\begin{align}\n& (7N + 5n - 9) ~ \\mathbb{F}_{\\mathsf{mul}} + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(D_{max} + 1, {\\mathbb{G}_1})  \\\\\n\\end{align}\n\nVerifier’s cost:(5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + (2n + 2) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 2) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1} + \\mathsf{EccMul}^{\\mathbb{G}_2} + \\mathsf{EccAdd}^{\\mathbb{G}_2} + 2~P\n\nProof size:(n + 2) ~ \\mathbb{G}_1","type":"content","url":"/#zeromorph-v1","position":69},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"zeromorph-v2","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl4","url":"/#zeromorph-v2","position":70},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"zeromorph-v2","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"Prover’s cost:\\begin{aligned}\n    & (7N + 5n - 10) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} \\\\\n    & + \\sum_{k=0}^{n} \\mathsf{msm}(2^k,\\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} , \\mathbb{G}_1) + \\mathsf{msm}(2^{n - 1} - 1, \\mathbb{G}_1) + \\mathsf{msm}(2^n - 1, \\mathbb{G}_1)\n\\end{aligned}\n\nVerifier’s cost:\\begin{aligned}\n    (5n - 1) ~ \\mathbb{F}_{\\mathsf{mul}} + \\mathbb{F}_{\\mathsf{inv}} + (2n + 6) ~ \\mathsf{EccMul}^{\\mathbb{G}_1} + (2n + 7) ~ \\mathsf{EccAdd}^{\\mathbb{G}_1}  + 2~P\n\\end{aligned}\n\nProof size:\\begin{aligned}\n    (n + 3) \\cdot \\mathbb{G}_1 + \\mathbb{F}_q\n\\end{aligned}","type":"content","url":"/#zeromorph-v2","position":71},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Summary","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl4","url":"/#summary","position":72},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Summary","lvl3":"Zeromorph","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"Comparing the complexity analysis results of these protocols, we can see that the unoptimized zeromorph protocol has the largest msm operation length, n ~ \\mathsf{msm}(D_{max} + 1,\\mathbb{G}_1), and the largest proof size, (2n + 1) \\mathbb{G}_1. The detailed complexity analysis can be found in \n\nzeromorph-anlysis. Both zeromorph-v1 and zeromorph-v2 adopt different methods to optimize the degree bound proof, reducing the Prover’s msm operations and decreasing the proof size by about n ~ \\mathbb{G}_1. The main difference between zeromorph-v1 and zeromorph-v2 is that zeromorph-v2 avoids Verifier operations on the elliptic curve \\mathbb{G}_2, at the cost of increasing constant-level computation for the Verifier on the elliptic curve \\mathbb{G}_1 and a proof size of \\mathbb{G}_1 + \\mathbb{F}_q.","type":"content","url":"/#summary","position":73},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Comparison","lvl2":"Comparison of KZG10-based MLE-PCS"},"type":"lvl3","url":"/#comparison","position":74},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Comparison","lvl2":"Comparison of KZG10-based MLE-PCS"},"content":"Referring to the theoretical analysis in the mercury paper [EG25], and combining the above analysis results, we compare the complexity of KZG10-based protocols.\n\nProtocol\n\nProver’s cost\n\nVerifier’s cost\n\nProof size\n\nLibra-PCS\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{G}, O(\\log N) ~ \\mathbb{P}\n\nO(\\log N)~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nPH23-KZG\n\nO(N\\log N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nO(\\log N)~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\ngemini\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(n) ~ \\mathbb{G}\n\nO(\\log N)~ \\mathbb{F}, O(\\log N) ~ \\mathbb{G}\n\nhyperKZG\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(n) ~ \\mathbb{G}\n\nO(\\log N)~ \\mathbb{F}, O(\\log N) ~ \\mathbb{G}\n\nzeromorph-v0\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(\\log N) ~ \\mathbb{G}\n\n(2\\log N + 1) ~ \\mathbb{G}\n\nzeromorph-v1\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(\\log N) ~ \\mathbb{G}\n\n(\\log N + 2) ~ \\mathbb{G}\n\nzeromorph-v2\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(\\log N) ~ \\mathbb{G}\n\n\\mathbb{F}, (\\log N + 3) ~ \\mathbb{G}\n\nmercury [EG25]\n\nO(N) ~ \\mathbb{F}, 2N + O(\\sqrt{N}) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nO(1)~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nsamaritan [GPS25]\n\nO(N) ~ \\mathbb{F}, O(N) \\mathbb{G}\n\nO(\\log N) ~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nO(1)~ \\mathbb{F}, O(1) ~ \\mathbb{G}\n\nThrough comparison, we find:\n\nIn terms of Prover computational complexity, PH23 has the highest complexity, requiring O(N\\log N) level finite field calculations, while other protocols only need O(N) ~ \\mathbb{F} calculations.\n\nIn terms of Verifier computational complexity, all protocols require O(\\log N) finite field operations. PH23, mercury, and samaritan protocols only need constant-level calculations on elliptic curves, while other protocols require O(\\log N) level calculations on elliptic curves.\n\nIn terms of Proof size, mercury and samaritan protocols can achieve constant-level proof sizes. We found that the PH23 protocol, when using schemes similar to Plonk, can also achieve constant-size proofs, which we plan to describe in detail in future work.\nCurrently, it appears that mercury and SamaritanPCS are the most efficient protocols, achieving constant proof sizes without sacrificing the Prover’s linear O(N) finite field operations, rather than logarithmic level O(\\log N).","type":"content","url":"/#comparison","position":75},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl2","url":"/#comparison-of-fri-based-mle-pcs","position":76},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"We have detailed the protocol descriptions for PH23, gemini, and zeromorph interfacing with FRI. For the zeromorph-fri protocol using \n\nmmcs structure and optimized with rolling batch [ZLGSCLD24] techniques, we have analyzed its complexity in detail. Comparing with the Basefold protocol, we found that the Basefold protocol is superior to the zeromorph-fri protocol. Additionally, we compared the Basefold, Deepfold, and WHIR protocols from the perspective of Verifier query complexity.\n\nProtocol\n\nVersion\n\nProtocol Description Document\n\nProtocol Analysis Document\n\nbasefold\n\n\n\nbasefold paper [ZCF23]\n\nbasefold-analysis\n\nph23-fri\n\ninner product using grand sum\n\nMissing Protocol PH23-PCS (Part 4)\n\n\n\nph23-fri\n\ninner product using univariate sumcheck\n\nMissing Protocol PH23-PCS (Part 5)\n\n\n\ngemini-fri\n\n\n\nGemini: Interfacing with FRI\n\n\n\nzeromorph-fri\n\ndirectly interfacing with fri protocol\n\nZeromorph-PCS: Integration with FRI\n\n\n\nzeromorph-fri\n\noptimized: using \n\nmmcs structure to commit quotient polynomials and rolling batch [ZLGSCLD24] technique\n\nZeromorph-PCS: Integration with FRI\n\nzeromorph​-fri​-analysis","type":"content","url":"/#comparison-of-fri-based-mle-pcs","position":77},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl3","url":"/#basefold-v-s-zeromorph-fri","position":78},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"Below are the complexity analysis results for the basefold protocol and zeromorph-fri (optimized version), where the notation is as follows:\n\nn: Number of variables in the MLE polynomial.\n\nN: N = 2^n.\n\n\\mathcal{R}: Blowup factor parameter in the FRI protocol, with the relationship to code rate being \\mathcal{R} = \\rho^{-1}.\n\nl: Number of queries made by the Verifier in the Query phase of FRI.\n\n\\mathbb{F}_{\\mathsf{mul}}: Multiplication operation on finite field \\mathbb{F}.\n\n\\mathbb{F}_{\\mathsf{inv}}: Division operation on finite field \\mathbb{F}.\n\n\\mathsf{MT.commit}(k): Computational cost for committing to a vector of length k using a Merkle Tree.\n\nH: Hash computation.\n\n\\mathsf{MMCS.commit}(k_{n-1}, k_{n-2}, \\ldots, k_{1}): Computational cost for committing to n - 1 vectors of lengths k_{n-1}, \\ldots, k_1 using the MMCS structure, which requires that k_{i + 1}/k_i = 2, i.e., the length of adjacent vectors differs by exactly a factor of 2.\n\nC: Compression calculation in the MMCS structure.","type":"content","url":"/#basefold-v-s-zeromorph-fri","position":79},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Basefold","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl4","url":"/#basefold","position":80},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Basefold","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"After analysis, the complexity of the basefold protocol is:\n\nProver’s cost:\\begin{aligned}\n\\left((\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3n - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \n\\end{aligned}\n\nAdding the algorithmic complexity for the Prover to compute encoding \\pi_n, the total complexity is:\\left(\\frac{\\mathcal{R}}{2} \\cdot nN + (\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3n - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\n\nProof size:\\begin{align}\n((2l + 3)n + \\mathcal{R}) ~ \\mathbb{F} + \\left( \\frac{l}{2} \\cdot n^2 + \\left(\\log \\mathcal{R} \\cdot l +\\frac{1}{2} \\cdot l + 1\\right) \\cdot n \\right) ~ H \n\\end{align}\n\nVerifier’s cost:\\begin{align}\n\\left( \\frac{l}{2} \\cdot n^2 + (l\\log \\mathcal{R} + \\frac{l}{2})n \\right)  ~ H + (5l + 12)n ~ \\mathbb{F}_{\\mathsf{mul}} + ((2l + 5)n + 1) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{align}","type":"content","url":"/#basefold","position":81},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Zeromorph-fri","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl4","url":"/#zeromorph-fri","position":82},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Zeromorph-fri","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"The complexity of the Zeromorph-fri protocol is:\n\nProver’s Cost:\n$$\\begin{align}\n& (2\\mathcal{R}\\cdot nN + (2\\mathcal{R} \\log \\mathcal{R} + 7 \\mathcal{R} + 3) \\cdot  N + n -  \\mathcal{R}\\log \\mathcal{R} - 4 \\mathcal{R} - 3) ~\\mathbb{F}_{\\mathsf{mul}} + (3 \\mathcal{R} \\cdot N - 2 \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) + \\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})\n\\end{align}\n\n$$\n\nProof size:\n$$\\begin{align}\n((2l + 1) \\cdot n + 3l) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l + 1)  ~ H\n\\end{align}\n\n$$\n\nVerifier’s Cost:\\begin{aligned}\n  & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H  \\\\\n & + ((7l + 5)n + 5l + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((3l + 1)n + 2l) ~ \\mathbb{F}_{\\mathsf{inv}}\n\\end{aligned}","type":"content","url":"/#zeromorph-fri","position":83},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Comparison Results","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl4","url":"/#comparison-results","position":84},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl4":"Comparison Results","lvl3":"Basefold v.s. Zeromorph-fri","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"Below is a comparison of the complexity of the Basefold protocol and the zeromorph-fri protocol.\n\nProver’s cost\n\nSubtracting the Prover cost of basefold (including encoding complexity) from zeromorph-fri’s Prover cost:\\begin{align}\n& (2\\mathcal{R}\\cdot nN + (2\\mathcal{R} \\log \\mathcal{R} + 7 \\mathcal{R} + 3) \\cdot  N + n -  \\mathcal{R}\\log \\mathcal{R} - 4 \\mathcal{R} - 3) ~\\mathbb{F}_{\\mathsf{mul}} + (3 \\mathcal{R} \\cdot N - 2 \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) + \\sum_{i = 1}^{n - 1}\\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R}) \\\\\n & - \\left(\\left(\\frac{\\mathcal{R}}{2} \\cdot nN + (\\frac{5}{2} \\mathcal{R} + 9) \\cdot N + 3n - \\frac{5}{2} \\mathcal{R} - 13 \\right) ~ \\mathbb{F}_{\\mathsf{mul}} + (\\mathcal{R} \\cdot N - \\mathcal{R}) ~ \\mathbb{F}_{\\mathsf{inv}} + \\sum_{i = 1}^{n - 1} \\mathsf{MT.commit}(2^{i} \\cdot \\mathcal{R})  \\right) \\\\\n=  & ( \\frac{3}{2}\\mathcal{R}\\cdot nN + (2\\mathcal{R} \\log \\mathcal{R} + \\frac{9}{2} \\mathcal{R} - 6) \\cdot  N - 2 \\cdot n -  \\mathcal{R}\\log \\mathcal{R} - \\frac{3}{2}\\mathcal{R} + 10) ~\\mathbb{F}_{\\mathsf{mul}} + (2 \\mathcal{R} \\cdot N - \\mathcal{R} + 1) ~\\mathbb{F}_{\\mathsf{inv}} \\\\\n& + \\mathsf{MMCS.commit}(2^{n-1} \\cdot \\mathcal{R}, \\ldots, \\mathcal{R}) \\\\\n\\end{align}\n\nIt can be seen that basefold has much less computational cost than zeromorph-fri, manifested in finite field multiplication calculations, inversion operations, and hash calculations for Merkle Tree commitments.\n\nFinite field multiplication: zeromorph-fri produces 2 \\mathcal{R} \\cdot nN finite field multiplications, mainly from calculating \\{[\\hat{q}_k(x)|_{x \\in D^{(k)}}]\\}_{k = 0}^{n - 1} and [f(x)|_{x \\in D}], involving FFT operations, while basefold only has \\frac{\\mathcal{R}}{2} \\cdot nN ~ \\mathbb{F}_{\\mathsf{mul}} computational complexity during the encoding process.\n\nHash calculation: zeromorph-fri not only needs to commit to the original polynomial f(X), but also to n quotient polynomials, using the MMCS structure for commitment, naturally incurring more hash calculations than the Basefold protocol.\n\nProof size\n\nSubtracting basefold’s proof size from zeromorph-fri’s proof size:\\begin{align}\n & ((2l + 1) \\cdot n + 3l) ~ \\mathbb{F} + (\\frac{3}{2} l \\cdot n^2  + (3\\log \\mathcal{R}l  - \\frac{1}{2}l + 1) n  - l + 1)  ~ H \\\\\n & - \\left(((2l + 3)n + \\mathcal{R}) ~ \\mathbb{F} + \\left( \\frac{l}{2} \\cdot n^2 + \\left(\\log \\mathcal{R} \\cdot l +\\frac{1}{2} \\cdot l + 1\\right) \\cdot n \\right) ~ H \\right) \\\\\n=  & (-2n + 3l - \\mathcal{R}) ~ \\mathbb{F} + \\left(l \\cdot n^2 + (2\\log \\mathcal{R} \\cdot l - l) n - l + 1\\right) ~H\n\\end{align}\n\nIt can be seen that basefold has a smaller proof size than zeromorph-fri, sending about ln^2 fewer hash values.\n\nVerifier’s Cost\n\nSubtracting basefold’s verifier cost from zeromorph-fri’s verifier cost:\\begin{align} \\\\\n  & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + (\\frac{l}{2} n^2 + (\\frac{3l}{2} + \\log \\mathcal{R}l)n - l) ~H  \\\\\n & + ((7l + 5)n + 5l + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((3l + 1)n + 2l) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n & - \\left(\\left( \\frac{l}{2} \\cdot n^2 + (l\\log \\mathcal{R} + \\frac{l}{2})n \\right)  ~ H + (5l + 12)n ~ \\mathbb{F}_{\\mathsf{mul}} + ((2l + 5)n + 1) ~ \\mathbb{F}_{\\mathsf{inv}}\\right) \\\\\n=  & (ln^2 + (2 \\log \\mathcal{R}l - l) n + l - 2 \\log \\mathcal{R}l) ~C + \\left(ln - l \\right) ~ H\\\\\n  & + ((2l - 7)n + 5l + 1) ~ \\mathbb{F}_{\\mathsf{mul}} + ((l - 4)n + 2l - 1) ~ \\mathbb{F}_{\\mathsf{inv}} \\\\\n\\end{align}\n\nIt can be seen that basefold has a smaller verifier cost than zeromorph-fri.\n\nConsidering the computational amount in these three aspects, we can conclude that the Basefold protocol is superior to the Zeromorph-fri protocol.","type":"content","url":"/#comparison-results","position":85},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Comparing Basefold, Deepfold, and WHIR","lvl2":"Comparison of FRI-based MLE-PCS"},"type":"lvl3","url":"/#comparing-basefold-deepfold-and-whir","position":86},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Comparing Basefold, Deepfold, and WHIR","lvl2":"Comparison of FRI-based MLE-PCS"},"content":"The comparison between Basefold, Deepfold, and WHIR protocols is detailed in the blog post [BaseFold vs DeepFold vs WHIR](\n\nmle-pcs/basefold-deepfold-whir/basefold-deepfold-whir.md at main · sec-bit/mle-pcs · GitHub), which mainly describes the efficiency comparison results of these three protocols.\n\nBasefold, Deepfold, and WHIR protocols are very similar in protocol framework, all following the BaseFold protocol framework, synchronously performing the sumcheck protocol and FRI/DEEP-FRI/STIR protocol with the same random numbers. The main differences between them come from the differences between the FRI protocol, DEEP-FRI protocol, and STIR protocol.\n\nComparing the efficiency of these three protocols, the Prover’s computational amount doesn’t differ significantly, mainly depending on the number of Verifier queries. More queries lead to larger Verifier computational cost and proof size. Since the STIR protocol theoretically has better query complexity than the FRI and DEEP-FRI protocols, the WHIR protocol has fewer queries compared to the BaseFold and DeepFold protocols.\n\nOn the other hand, the number of Verifier queries is related to the bound that can be achieved in the soundness proof of the protocol:\n\nThe DeepFold protocol based on the DEEP-FRI protocol can achieve the optimal bound 1 - \\rho based on a simple conjecture. For the FRI protocol to reach the 1 - \\rho bound, it would require a stronger conjecture (see [BCIKS20] Conjecture 8.4).\n\nThe BaseFold protocol can reach the Johnson bound 1 - \\sqrt{\\rho} for Reed Solomon encoding.\n\nThe WHIR protocol is only proven in the original paper to reach (1 - \\rho)/2, but based on the method in [H24], it is promising to prove it reaches the Johnson bound 1 - \\sqrt{\\rho}.","type":"content","url":"/#comparing-basefold-deepfold-and-whir","position":87},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Bulletproofs-based MLE-PCS"},"type":"lvl2","url":"/#bulletproofs-based-mle-pcs","position":88},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Bulletproofs-based MLE-PCS"},"content":"Bulletproofs-based MLE-PCS include \n\nHyrax and \n\nΣ-Check.","type":"content","url":"/#bulletproofs-based-mle-pcs","position":89},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyrax","lvl2":"Bulletproofs-based MLE-PCS"},"type":"lvl3","url":"/#hyrax","position":90},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyrax","lvl2":"Bulletproofs-based MLE-PCS"},"content":"Multilinear polynomial evaluations can be viewed as inner-product relations and thus can be proven directly using inner-product arguments (IPAs), such as Bulletproofs. However, a major drawback of Bulletproofs is their linear verification time: for an n-variate multilinear polynomial, verification requires O(2^n) time.\n\nTo address this inefficiency, the \n\nHyrax PCS observes that polynomial evaluation can be reformulated as a matrix product. For example, consider f(z_0, z_1, z_2, z_3) = v. We can write:\n$$\n\\tilde{f}(z_0, z_1, z_2, z_3) =\\begin{bmatrix}\n1 & z_2 & z_3 & z_2z_3 \\\\\n\\end{bmatrix}\\begin{bmatrix}\nf_0 & f_1 & f_2 & f_3 \\\\\nf_4 & f_5 & f_6 & f_7 \\\\\nf_8 & f_9 & f_{10} & f_{11} \\\\\nf_{12} & f_{13} & f_{14} & f_{15}\n\\end{bmatrix}\\begin{bmatrix}\n1 \\\\\nz_0 \\\\\nz_1 \\\\\nz_0z_1 \\\\\n\\end{bmatrix}\n\n$\nLet F denote the inner matrix, \\vec{z}_1 := (1, z_2, z_3, z_2z_3), and \\vec{z}_0 := (1, z_0, z_1, z_0z_1). In the Hyrax protocol, the prover first sends \\vec{w} = \\vec{z}_1 \\cdot F, enabling the verifier to check that \\vec{w} \\cdot \\vec{z}_0^\\top = v. To ensure that \\vec{w} is computed correctly, the verifier issues a random challenge vector \\vec{r}, and the prover responds with \\vec{t}^\\top := F \\cdot \\vec{r}^\\top. This leads to the relation \\vec{z}_1 \\cdot \\vec{t}^\\top = \\vec{z}_1 \\cdot F \\cdot \\vec{r}^\\top = \\vec{w} \\cdot \\vec{r}^\\top, which holds if and only if \\vec{w}$ is correctly computed.\n\nAs a result, the verifier only needs to compute two inner products of length \\sqrt{N} (N = 2^n), achieving sublinear verification cost. Furthermore, the prover can use an IPA to prove these inner-product relations, reducing the proof size to O(\\log n).","type":"content","url":"/#hyrax","position":91},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Σ-Check","lvl2":"Bulletproofs-based MLE-PCS"},"type":"lvl3","url":"/#id-check","position":92},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Σ-Check","lvl2":"Bulletproofs-based MLE-PCS"},"content":"To prove a single evaluation relation f(\\vec{z}) = v, Σ-Check employs an improved sumcheck protocol. In each round, the prover sends a linear polynomial f_i(X) := f(r_1, \\cdots, r_{i-1}, X, z_{i+1}, \\cdots, z_n), while the verifier checks the following condition:f_i(0) \\cdot \\mathsf{eq}(0, z_i) + f_i(1) \\cdot \\mathsf{eq}(1, z_i) \\overset{?}{=} f_{i-1}(r_{i-1}).\n\nThis approach reduces the proof size by n compared to directly applying sumcheck, as in BaseFold.\n\nWhen proving k evaluation relations (f_i(\\vec{z}_i) = v_i)_{i = 1}^k,  Σ-Check interpolates all f_i(\\vec{X}) as a single function f(\\vec{y}, \\vec{X}) over a hypercube \\vec{y} \\in \\{0, 1\\}^{\\log k}. Similarly, \\vec{z}(\\vec{y}) and v(\\vec{y}) are defined. Given a challenge \\vec{\\alpha} \\in \\mathbb{F}^{\\log k}, the prover can then use the improved sumcheck protocol to prove the following relation:\\sum_{\\vec{y} \\in \\{0, 1\\}^{\\log k}} \\mathsf{eq}(\\vec{\\alpha}, \\vec{y}) \\cdot \\big( f(\\vec{y}, \\vec{z}(\\vec{y})) - v(\\vec{y}) \\big).\n\nAfter the sumcheck, this reduces to the relation f(\\vec{r}, \\vec{z}(\\vec{r})) - v(\\vec{r}) = s.\n\nSince \\vec{z}(\\vec{Y}) and v(\\vec{Y}) are public, the verifier can compute them directly. Let f_r(\\vec{X}) := f(\\vec{r}, \\vec{X}), \\vec{z}_r := \\vec{z}(\\vec{r}), and v_r := v(\\vec{r}). The output relation then becomes f_r(\\vec{z}_r) = s + v_r, which is itself an evaluation proof and can be handled by another sumcheck protocol.\n\nIt is worth noting that Σ-Check can also serve as an inner product argument (IPA), as described in Section 4.1 of \n\nGZQ+24). This enables sublinear verification when combined with the Hyrax IOP. We summarize the performance as follows.\n\nScheme\n\nProver Time\n\nVerifier Time\n\nProof Size\n\n\\Sigma-Check (as PCS)\n\nO(k+N) \\mathbb{F}, O(N) \\mathbb{G}\n\nO(k+N) \\mathbb{F}, O(k+N) \\mathbb{G}\n\n2(\\log k + \\log N) \\mathbb{G}\n\n\\Sigma-Check (as IPA)\n\nO(k+\\sqrt{N}) \\mathbb{F}, O(\\sqrt{N}) \\mathbb{G}\n\nO(k+\\sqrt{N}) \\mathbb{F}, O(k+\\sqrt{N}) \\mathbb{G}\n\n2(\\log k + \\log N) \\mathbb{G}","type":"content","url":"/#id-check","position":93},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Lattice-based PCS"},"type":"lvl2","url":"/#lattice-based-pcs","position":94},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Lattice-based PCS"},"content":"","type":"content","url":"/#lattice-based-pcs","position":95},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Greyhound","lvl2":"Lattice-based PCS"},"type":"lvl3","url":"/#greyhound","position":96},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Greyhound","lvl2":"Lattice-based PCS"},"content":"Greyhound is a lattice-based PCS that relies on Labrador—a lattice-based interactive proof for proving inner product relations.\n\nStructurally similar to Brakedown, Greyhound first abstracts the polynomial evaluation process as the inner product of two long vectors:v = \\langle \\vec{f}, \\vec{u} \\rangle\n\nwhere \\vec{f} is the coefficient vector of polynomial f(X) = \\sum_{i=0}^{N-1} f_i X^i, and \\vec{u} = (1, u, u^2, \\dots, u^{N-1}) is a power vector. Then, it parses the coefficient vector \\vec{f} as an n \\times n matrix F, where n = \\sqrt{N}, and writes \\vec{u} as the tensor product of two vectors:\\vec{a} = (1, u, u^2, \\dots, u^{n-1}), \\quad \\vec{b} = (1, u^n, u^{2n}, \\dots, u^{(n-1)n}), \\quad \\vec{b} \\otimes \\vec{a} = \\vec{u}.\n\nIn the proof process, the Prover first calculates F\\cdot \\vec{a} to obtain an intermediate result vector \\vec{w}, which is sent to the Verifier. The Verifier checks if the inner product of \\vec{w} and \\vec{b} equals v, verifying:v ?= \\langle \\vec{w}, \\vec{b} \\rangle.\n\nNext, the Verifier generates a random challenge vector \\vec{c} of length n, the Prover calculates \\vec{z} = \\vec{c}^TF and sends it to the Verifier. The Verifier verifies the correctness of \\vec{w} through the following relation:\\langle \\vec{a}, \\vec{z} \\rangle ?= \\langle \\vec{w}, \\vec{c} \\rangle.\n\nObviously, by expanding the one-dimensional coefficient vector into a two-dimensional matrix, the protocol’s proof size and verification time are significantly optimized, reduced to sublinear levels.\n\nThe security of Greyhound is based on the MSIS (Modular Short Integer Solution) problem on lattices. By decomposing each column of the coefficient matrix with \\delta as the base, n short vectors of length nl can be obtained, where l = \\log_\\delta q. Subsequently, Ajtai Commit is performed on each short vector to generate corresponding commitment values.","type":"content","url":"/#greyhound","position":97},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyperwolf","lvl2":"Lattice-based PCS"},"type":"lvl3","url":"/#hyperwolf","position":98},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl3":"Hyperwolf","lvl2":"Lattice-based PCS"},"content":"Inspired by Greyhound, our latest research achievement Hyperwolf further optimizes the structure, generalizing it to k dimensions, with overall efficiency reaching O(kN^{1/k}). By setting k = \\log N, the scheme successfully achieves log-level proof size and verification time, significantly enhancing performance.\n\nSpecifically, we interpret the one-dimensional coefficient vector of length N as a k-dimensional hypercube [F]^{(k)} with dimensions b \\times b \\times \\cdots \\times b (a total of k dimensions), satisfying b^k = N, and construct k auxiliary vectors (\\vec{a}_i)_{i \\in [k]}, where:\\vec{a}_i = (1, u^{b^{i}}, u^{2b^{i}}, \\cdots, u^{(b-1)b^{i}})\n\nsatisfying:\\vec{a}_{k-1} \\otimes \\vec{a}_{k-2} \\otimes \\cdots \\otimes \\vec{a}_0 = \\vec{u}.\n\nBased on this, the PCS evaluation process can be described as:v = \\left( \\mathcal{F}\\left( \\cdots \\left( \\mathcal{F}\\left( \\mathcal{F}([F]^{(k)}) \\cdot \\vec{a}_0 \\right) \\cdot \\vec{a}_1 \\right) \\cdot \\vec{a}_2 \\right) \\cdots \\right) \\cdot \\vec{a}_{k-1}.\n\nwhere function \\mathcal{F} maps a (k-i)-dimensional hypercube of size b_{k-1} \\times b_{k-2} \\times \\cdots \\times b_i to a (k-i-1)-dimensional hypercube matrix of size (b_{k-1} \\cdots b_{i+1}) \\times b_i, where i \\in [k].\n\nWhen k=2, this evaluation process is identical to that of Greyhound.\n\nThe figure below illustrates the evaluation process for k=3:\n\nOur proof protocol consists of k rounds of interaction, each round can be viewed as reducing a higher-dimensional relation by one dimension.\n\nIn the 0th round, the Prover first sends the intermediate result to the Verifier:\\mathsf{fold}^{(k)} = \\mathcal{F}(\\cdots(\\mathcal{F}(\\mathcal{F}([F]^{(k)}) \\cdot \\vec{a}_0) \\cdot \\vec{a}_1) \\cdots \\vec{a}_{k-2}),\n\nThe Verifier then verifies if it satisfies the inner product relation \\langle \\mathsf{fold}^{(k)}, \\vec{a}_{k-1} \\rangle = v. To further verify the correctness of \\mathsf{fold}^{(k)}, the Verifier randomly generates a challenge vector \\vec{c}^{(k)} of length b. The Prover uses \\vec{c}^{(k)} to fold the k-dimensional hypercube [F]^{(k)}, reducing its dimension to a (k-1)-dimensional hypercube [F]^{(k-1)}, i.e.:[F]^{(k-1)} =(\\vec{c}^{(k)})^T\\cdot([F]^{(k)}) .\n\nThis is equivalent to a 1 \\times b vector multiplying a b \\times (b\\times b\\times … \\times b) matrix.\n\nSimultaneously, the Verifier updates the verification value:y = \\langle \\mathsf{fold}^{(k)}, \\vec{c}^{(k)} \\rangle.\n\nIn each subsequent round, the Prover and Verifier repeat similar interactions:\n\nFor any round i, the Prover sends the intermediate result \\mathsf{fold}^{(k-i)} to the Verifier, the Verifier checks if \\langle \\mathsf{fold}^{(k-i)}, \\vec{a}_{k-i-1} \\rangle = y holds, and generates a new challenge vector \\vec{c}^{(k-i)}. The Prover uses this challenge to fold the witness, reducing its dimension, while the Verifier simultaneously updates the verification value y. After each round, the witness’s dimension is reduced from k-i to k-i-1.\n\nAfter k-2 rounds, the witness is compressed into a one-dimensional vector \\vec{f}^{(1)}. At this point, the Prover sends this one-dimensional vector \\vec{f}^{(1)} directly to the Verifier, who finally verifies if it satisfies the following relation:\\langle \\vec{f}^{(1)}, \\vec{a}_0 \\rangle = \\langle \\mathsf{fold}^{(2)}, \\vec{c}^{(2)} \\rangle.\n\nIn the above interaction process, the proof size sent in each round is O(b) = O(N^{1/k}), and the cost for the Verifier to perform checks and update parameters is also O(N^{1/k}). Therefore, the total proof size and verification cost for k rounds are both O(kN^{1/k}). When k = \\log N, this value can be optimized to the O(\\log N) level.","type":"content","url":"/#hyperwolf","position":99},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Future work"},"type":"lvl2","url":"/#future-work","position":100},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"Future work"},"content":"This project has not yet deeply compared PCS using Small Fields, which would significantly enhance Prover performance. This will be our next area of focus. Additionally, besides RS Code, linear codes with better encoding performance, such as Spelman Code, are often used to construct PCS, like brakedown and orion. Furthermore, there are some new Binary Field-based PCS protocols, and some of the protocols analyzed above can also be used for Binary Fields, like FRI and Ligerito.","type":"content","url":"/#future-work","position":101},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"References"},"type":"lvl2","url":"/#references","position":102},{"hierarchy":{"lvl1":"Comparison of MLE-PCS (Final Report)","lvl2":"References"},"content":"[ACFY24a] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[ACFY24b] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[AHIV17] Scott Ames, Carmit Hazay, Yuval Ishai, and Muthuramakrishnan Venkitasubramaniam. Ligero: lightweight sublinear arguments without a trusted setup”. 2022. \n\nhttps://​eprint​.iacr​.org​/2022​/1608​.pdf\n\n[BBB+18] Bünz, Benedikt, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille, and Greg Maxwell. “Bulletproofs: Short proofs for confidential transactions and more.” In 2018 IEEE symposium on security and privacy (SP), pp. 315-334. IEEE, 2018. \n\nhttps://​eprint​.iacr​.org​/2017​/1066\n\n[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[BCC+16] Jonathan Bootle, Andrea Cerulli, Pyrros Chaidos, Jens Groth, and Christophe Petit. “Efficient Zero-Knowledge Arguments for Arithmetic Circuits in the Discrete Log Setting.”  In Advances in Cryptology–EUROCRYPT 2016: 35th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Vienna, Austria, May 8-12, 2016, Proceedings, Part II 35, pp. 327-357. Springer Berlin Heidelberg, 2016.  \n\nhttps://​eprint​.iacr​.org​/2016​/263\n\n[BCH+22] Bootle, Jonathan, Alessandro Chiesa, Yuncong Hu, et al. “Gemini: Elastic SNARKs for Diverse Environments.” Cryptology ePrint Archive (2022). \n\nhttps://​eprint​.iacr​.org​/2022​/420\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[BGKS20] Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. DEEP-FRI: sampling outside the box improves soundness. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA, volume 151 of LIPIcs, pages 5:1–5:32. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020.\n\n[BSK18] Eli Ben-Sasson, Swastik Kopparty, and Shubhangi Saraf. Worst-case to average case reductions for the distance to a code. In 33rd Computational Complexity Conference, CCC 2018, June 22-24, 2018, San Diego, CA, USA, pages 24:1–24:23, 2018.\n\n[CBBZ22] Chen, Binyi, Benedikt Bünz, Dan Boneh, and Zhenfei Zhang. “Hyperplonk: Plonk with linear-time prover and high-degree custom gates.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 499-530. Cham: Springer Nature Switzerland, 2023.\n\n[CHM+20] Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Psi Vesely, and Nicholas Ward. “Marlin: Preprocessing zkSNARKs with universal and updatable SRS.” Advances in Cryptology–EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020.\n\n[DP23a] Benjamin Diamond and Jim Posen. Proximity Testing with Logarithmic Randomness. 2023. \n\nhttps://​eprint​.iacr​.org​/2023​/630​.pdf\n\n[DP23b] Diamond, Benjamin E., and Jim Posen. “Succinct arguments over towers of binary fields.” Cryptology ePrint Archive (2023).\n\n[DP24] Diamond, Benjamin E., and Jim Posen. “Polylogarithmic Proofs for Multilinears over Binary Towers.” Cryptology ePrint Archive (2024).\n\n[EG25] Eagen, Liam, and Ariel Gabizon. “MERCURY: A multilinear Polynomial Commitment Scheme with constant proof size and no prover FFTs.” Cryptology ePrint Archive (2025). \n\nhttps://​eprint​.iacr​.org​/2025​/385\n\n[GLHQTZ24] Yanpei Guo, Xuanming Liu, Kexi Huang, Wenjie Qu, Tianyang Tao, and Jiaheng Zhang. “DeepFold: Efficient Multilinear Polynomial Commitment from Reed-Solomon Code and Its Application to Zero-knowledge Proofs.” Cryptology ePrint Archive (2024).\n\n[GPS25] Ganesh, Chaya, Sikhar Patranabis, and Nitin Singh. “Samaritan: Linear-time Prover SNARK from New Multilinear Polynomial Commitments.” Cryptology ePrint Archive (2025). \n\nhttps://​eprint​.iacr​.org​/2025​/419\n\n[GQZGX24] Shang Gao, Chen Qian, Tianyu Zheng, Yu Guo, and Bin Xiao. “\\Sigma-Check: Compressed \\Sigma-protocol Theory from Sum-check.” (2024). \n\nhttps://​eprint​.iacr​.org​/2024​/1654\n\n[Gru24] Angus Gruen. “Some Improvements for the PIOP for ZeroCheck”. (2024). \n\nhttps://​eprint​.iacr​.org​/2024​/108.\n\n[GWC19] Ariel Gabizon, Zachary J. Williamson, and Oana Ciobotaru. “Plonk: Permutations over lagrange-bases for oecumenical noninteractive arguments of knowledge.” Cryptology ePrint Archive (2019).\n\n[H22] Ulrich Haböck. “A summary on the FRI low degree test.” Cryptology ePrint Archive (2022).\n\n[H24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).\n\n[HPS23] Lipmaa, Helger, Roberto Parisella, and Janno Siim. “Algebraic group model with oblivious sampling.” Theory of Cryptography Conference. Cham: Springer Nature Switzerland, 2023.\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[KZG10] Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. “Constant-size commitments to polynomials and their applications.” In International conference on the theory and application of cryptology and information security, pp. 177-194. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010.\n\n[LPS24] Lipmaa, Helger, Roberto Parisella, and Janno Siim. “Constant-size zk-SNARKs in ROM from falsifiable assumptions.” Annual International Conference on the Theory and Applications of Cryptographic Techniques. Cham: Springer Nature Switzerland, 2024.\n\n[MBKM19] Mary Maller, Sean Bowe, Markulf Kohlweiss, and Sarah Meiklejohn, et al. “Sonic: Zero-knowledge SNARKs from linear-size universal and updatable structured reference strings”. Proceedings of the 2019 ACM SIGSAC conference on computer and communications security, 2019.\n\n[NA25] Andrija Novakovic and Guillermo Angeris. Ligerito: A Small and Concretely Fast Polynomial Commitment Scheme. 2025. \n\nhttps://​angeris​.github​.io​/papers​/ligerito​.pdf.\n\n[NS24] Ngoc Khanh Nguyen and Gregor Seiler. Greyhound: Fast Polynomial Commitments from Lattices. 2024.  Cryptology ePrint Archive (2024).\n\nhttps://​eprint​.iacr​.org​/2024​/1293\n\n[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284\n\nPlonky3. \n\nhttps://​github​.com​/Plonky3​/Plonky3\n\n[PST13] Papamanthou, Charalampos, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” Theory of Cryptography Conference. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. \n\nhttps://​eprint​.iacr​.org​/2011​/587\n\n[WTSTW16] Riad S. Wahby, Ioanna Tzialla, abhi shelat, Justin Thaler, and Michael Walfish. “Doubly-efficient zkSNARKs without trusted setup.”  In 2018 IEEE Symposium on Security and Privacy (SP), pp. 926-943. IEEE, 2018.  \n\nhttps://​eprint​.iacr​.org​/2016​/263\n\n[XZZPS19] Tiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song. “Libra: Succinct Zero-Knowledge Proofs with Optimal Prover Computation.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/317\n\n[ZCF23] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\n[ZLGSCLD24] Zhang, Zongyang, Weihan Li, Yanpei Guo, Kexin Shi, Sherman SM Chow, Ximeng Liu, and Jin Dong. “Fast {RS-IOP} Multivariate Polynomial Commitments and Verifiable Secret Sharing.” In 33rd USENIX Security Symposium (USENIX Security 24), pp. 3187-3204. 2024.\n\n[ZGX25] Lizhen Zhang, Shang Gao, and Bin Xiao.  HyperWolf: Efficient Polynomial Commitment Schemes from Lattices. Cryptology ePrint Archive (2025).\n\nhttps://​eprint​.iacr​.org​/2025​/922 .\n\n[ZSCZ24] Zhao, Jiaxing, Srinath Setty, Weidong Cui, and Greg Zaverucha. “MicroNova: Folding-based arguments with efficient (on-chain) verification.” Cryptology ePrint Archive (2024).\n\n[ZXZS19] Jiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent Polynomial Delegation and Its Applications to Zero Knowledge Proof”. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 859-876. IEEE, 2020. \n\nhttps://​eprint​.iacr​.org​/2019​/1482.","type":"content","url":"/#references","position":103},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers"},"type":"lvl1","url":"/fri-binius/binius-01","position":0},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nJade Xie \n\njade@secbit.io\n\nBinary fields possess elegant internal structures, and Binius aims to fully utilize these internal structures to construct efficient SNARK proof systems. This article mainly discusses the Binary Fields that Binius relies on at its core and the construction methods of Extension Towers based on Binary Fields. Binary Fields provide smaller fields and are compatible with various tool constructions in traditional cryptography while also being able to fully utilize optimization of special instructions on hardware. There are two main advantages to choosing Extension Towers: one is that the recursive Extension construction provides a consistent and incremental Basis selection, allowing Small Fields to be embedded into Large Fields in a very natural way; the other advantage is that multiplication and inversion operations have efficient recursive algorithms.","type":"content","url":"/fri-binius/binius-01","position":1},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Extension Fields"},"type":"lvl2","url":"/fri-binius/binius-01#extension-fields","position":2},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Extension Fields"},"content":"Let’s try to describe the concept of Extension Fields in simple language to lay the groundwork for our study of Binary Towers. For in-depth learning, please refer to the strict definitions and proofs in finite field textbooks.\n\nThe prime field \\mathbb{F}_{p} is a finite field with p elements, where p must be a prime number. It is isomorphic to \\mathbb{Z}/p\\mathbb{Z}, which means we can use the set of integers \\{0, 1, \\ldots, p-1\\} to represent all elements of \\mathbb{F}_p.\n\nWe can form a Tuple with any two elements from the prime field, i.e., (a, b)\\in\\mathbb{F}_{p}^2, and this Tuple also forms a field with p^2 elements. We can verify this: a+b\\in\\mathbb{F}_p, so we define the addition of Tuples as follows:(a_1, b_1) + (a_2, b_2) = (a_1 + a_2, b_1 + b_2)\n\nWe can verify that \\mathbb{F}_{p}^2 forms a vector space, thus it is an additive group with the zero element being (0, 0). The next question is how to define multiplication. We want multiplication to be closed, i.e.:(a_1, b_1)\\cdot (a_2, b_2) = (c, d)\n\nThe simplest approach would be to use Entry-wise Multiplication to define multiplication, i.e., (a_1, b_1)\\cdot (a_2, b_2) = (a_1a_2, b_1b_2), with the multiplicative identity being (1, 1). This seems to make multiplication closed. However, this doesn’t ensure that every element has an inverse. For example, (1, 0) multiplied by any Tuple can never result in (1, 1), because the second part of the Tuple will always be 0. Therefore, this kind of multiplication cannot form a “field”.\n\nIn finite field theory, the multiplication operation of Tuples is implemented through polynomial modular multiplication. That is, we view (a_1, b_1) as the coefficients of a degree 1 polynomial, and similarly (a_2, b_2) can be viewed as the coefficients of another degree 1 polynomial. By multiplying these two, we get a degree 2 polynomial:(a_1 + b_1\\cdot X) \\cdot (a_2 + b_2\\cdot X) = a_1a_2 + (a_1b_2 + a_2b_1)X + b_1b_2X^2\n\nThen we take the modulus of the resulting polynomial with an irreducible degree 2 polynomial f(X), obtaining a remainder polynomial. The coefficients of this remainder polynomial are (c, d). So we define the new Tuple multiplication as follows:(a_1 + b_1\\cdot X) \\cdot (a_2 + b_2\\cdot X) = c + d\\cdot X \\mod f(X)\n\nAnd we define (1, 0) as the multiplicative identity. Here we emphasize that f(X) must be an irreducible polynomial. What would happen if f(X) were a reducible polynomial? For instance, if f(X)=(u_1+u_2X)(v_1+v_2X), then the product of two non-zero elements (u_1, u_2) and (v_1, v_2) would equal (0, 0), breaking out of the multiplicative group. Strictly speaking, the appearance of Zero Divisors disrupts the structure of the multiplicative group, thus failing to form a “field”.\n\nThe next question is whether there exists an irreducible degree 2 polynomial f(X). If f(X) doesn’t exist, then constructing a field \\mathbb{F}_{p^2} would be out of the question. For the prime field \\mathbb{F}_p, take any w\\in\\mathbb{F}_p that is not a square of any element, which in number theory belongs to the non-quadratic residue class, i.e., w\\in QNR(p). If w exists, then f(X)=X^2-w is an irreducible polynomial. Furthermore, how is the existence of w guaranteed? If p is an odd number, then w must exist. If p=2, although w doesn’t exist, we can specify f(X)=X^2+X+1\\in\\mathbb{F}_2[X] as an irreducible polynomial.\n\nWe now denote the field formed by the set of Tuples \\mathbb{F}_{p}^2, along with the defined addition and multiplication operations, as \\mathbb{F}_{p^2}, which has p^2 elements. According to finite field theory, we can expand the binary Tuple to an n-ary Tuple, thus constructing larger finite fields \\mathbb{F}_{p^n}.\n\nFor an irreducible polynomial f(X) = c_0 + c_1X + X^2 over \\mathbb{F}_p, it can always be factored in \\mathbb{F}_{p^2}. f(X) = (X-\\alpha)(X-\\alpha'), where \\alpha and \\alpha' are conjugates of each other, and they both belong to \\mathbb{F}_{p^2} but not to \\mathbb{F}_p. According to the definition of extension fields, \\mathbb{F}_p(\\alpha) is a degree 2 algebraic extension, which is isomorphic to the finite field we constructed earlier through modular multiplication with an irreducible polynomial. Therefore, we can also use a_1 + a_2\\cdot\\alpha to represent any element in \\mathbb{F}_{p^2}. Or further, we can view (1, \\alpha) as a Basis of the \\mathbb{F}_{p^2} vector space, any a\\in \\mathbb{F}_{p^2} can be represented as a linear combination of the Basis:a = a_0 \\cdot 1 + a_1 \\cdot \\alpha, \\quad a_0, a_1\\in\\mathbb{F}_p\n\nThis way, we can use the symbol a_0 + a_1\\cdot \\alpha to represent elements in \\mathbb{F}_{p^2}, rather than the polynomial representation a_0 + a_1\\cdot X. The “polynomial representation” of elements doesn’t specify which irreducible polynomial we used to construct the extension field, while using \\alpha, the root of the irreducible polynomial, as a way to construct the extension field eliminates any ambiguity.\n\nGeneralizing this concept to \\mathbb{F}_{p^n}, for any element a\\in\\mathbb{F}_{p^n}, it can be represented as:a = a_0 + a_1\\cdot\\alpha + a_2\\cdot\\alpha^2 + \\cdots + a_{n-1}\\cdot\\alpha^{n-1}\n\nHere \\alpha is the root of the degree n irreducible polynomial f(X). Therefore, (1, \\alpha, \\alpha^2, \\cdots, \\alpha^{n-1}) can be viewed as a Basis of \\mathbb{F}_{p^n}, and this Basis is called the Polynomial Basis of the finite field. Note that \\mathbb{F}_{p^n} as an n-dimensional vector space has many different Bases. We will see later that the choice of Basis is a very important step, and an appropriate Basis can greatly optimize or simplify some representations or operations.\n\nTODO: Fp* multiplicative cyclic group","type":"content","url":"/fri-binius/binius-01#extension-fields","position":3},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Binary Field"},"type":"lvl2","url":"/fri-binius/binius-01#binary-field","position":4},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Binary Field"},"content":"For \\mathbb{F}_{2^n}, we call it a binary field because its elements can all be expressed as vectors of length n composed of 0 and 1. \\mathbb{F}_{2^n} can be constructed through two types of methods: one is through an irreducible polynomial of degree n; the other is by repeatedly using quadratic extensions, known as an Extension Tower. There are many paths for field extension, and for 2^n, it has multiple factors of 2, so there exist multiple construction methods between these two methods. For example, for \\mathbb{F}_{2^8}, we can first construct \\mathbb{F}_{2^4}, then obtain \\mathbb{F}_{2^8} through a quadratic extension, or we can first construct \\mathbb{F}_{2^2}, then construct \\mathbb{F}_{2^8} through a quartic irreducible polynomial extension.\n\nLet’s warm up by constructing \\mathbb{F}_{2^2} using the quadratic extension method. We discussed earlier that f(X)=X^2+X+1 is an irreducible polynomial in \\mathbb{F}_2[X]. Suppose \\eta is a root of f(X), then \\mathbb{F}_{2^2} can be represented as a_0 + b_0\\cdot\\eta. Considering that \\mathbb{F}_{2^2} only has four elements, they can be listed below:\\mathbb{F}_{2^2} = \\{0, 1, \\eta, \\eta+1\\}\n\nAnd \\eta as a generator can produce the multiplicative group \\mathbb{F}_{2^2}^*=\\langle \\eta \\rangle, which has an Order of 3:\\begin{split}\n\\eta^0 &= 1 \\\\\n\\eta^1 &= \\eta \\\\\n\\eta^2 &= \\eta+1 \\\\\n\\end{split}\n\nWe’ll demonstrate two construction methods for \\mathbb{F}_{2^4}. The first is to directly use a degree 4 irreducible polynomial over \\mathbb{F}_2. In fact, there are 3 different degree 4 irreducible polynomials, so there are 3 different construction methods in total.\\begin{split}\nf_1(X) &= X^4 + X + 1 \\\\\nf_2(X) &= X^4 + X^3 + 1 \\\\\nf_3(X) &= X^4 + X^3 + X^2 + X + 1 \\\\\n\\end{split}\n\nSince we only need to choose one irreducible polynomial, let’s choose f_1(X) to define \\mathbb{F}_{2^4}:\\mathbb{F}_{2^4} = \\mathbb{F}_2[X]/\\langle f_1(X)\\rangle\n\nLet’s denote the root of f_1(X) in \\mathbb{F}_{2^4} as \\theta, then any element a\\in\\mathbb{F}_{2^4} can be uniquely represented as:a = a_0 + a_1\\cdot\\theta + a_2\\cdot\\theta^2 + a_3\\cdot\\theta^3\n\nTo add here, f_1(X) is also a Primitive polynomial, and its root \\theta is also a Primitive Element of \\mathbb{F}_{2^4}. Note that not all irreducible polynomials are Primitive polynomials, for example, f_3(X) listed above is not a Primitive polynomial.\n\nNow we can list all the elements in \\mathbb{F}_{2^4}, each element corresponding to a 4-bit binary vector:\\begin{array}{ccccccc}\n0000 & 0001 & 0010 & 0011 & 0100 & 0101 & 0110 & 0111 \\\\\n0 & 1 & \\theta & \\theta+1 & \\theta^2 & \\theta^2+1 & \\theta^2+\\theta & \\theta^2+\\theta+1 \\\\\n\\hline\n1000 & 1001 & 1010 & 1011 & 1100 & 1101 & 1110 & 1111 \\\\\n\\theta^3 & \\theta^3+1 & \\theta^3+\\theta & \\theta^3+\\theta+1 & \\theta^3+\\theta^2 & \\theta^3+\\theta^2+1 & \\theta^3+\\theta^2+\\theta & \\theta^3+\\theta^2+\\theta+1 \\\\\n\\end{array}\n\nFor the addition of two elements in \\mathbb{F}_{2^4}, we only need to add their binary representations bit by bit, for example:(0101) + (1111) = (1010)\n\nThis operation is actually the XOR bitwise exclusive OR operation. As for multiplication, for example a\\cdot\\theta, it corresponds to a shift operation on the binary:\\begin{split}\n(0101) << 1 &= (1010)\\\\\n(\\theta^2 + 1)\\cdot\\theta &= \\theta^3 + \\theta \\\\\n\\end{split}\n\nIf we continue to multiply by \\theta, a shift overflow situation will occur:\\begin{split}\n(\\theta^3 + \\theta)\\cdot\\theta &= {\\color{blue}\\theta^4} + \\theta^2 = \\theta^2 + \\theta + 1 \\\\\n(1010) << 1  &= (0100) + (0011) = (0111)\\\\\n\\end{split}\n\nFor the overflow bit, it needs to be added with 0011, which is determined by the definition of the irreducible polynomial f_1(X), \\theta^4=\\theta+1. So once the high bit overflows during shifting, an XOR operation with 0011 needs to be performed. From this, we can see that the multiplication rules of \\mathbb{F}_{2^4} actually depend on the choice of the irreducible polynomial. Therefore, how to choose an appropriate irreducible polynomial is also a key step in optimizing binary field multiplication.","type":"content","url":"/fri-binius/binius-01#binary-field","position":5},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Field Embedding"},"type":"lvl2","url":"/fri-binius/binius-01#field-embedding","position":6},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Field Embedding"},"content":"If we want to construct a SNARK proof system based on binary fields, we would use smaller bit counts to represent smaller numbers, but in any case, in the challenge rounds of the protocol, the Verifier needs to give a random number in a relatively large extension field, in hopes of achieving sufficient cryptographic security strength. This requires us to encode witness information with polynomials in a small field, but perform evaluation operations on these polynomials in a larger field. Therefore, we need to find a way to “embed” the small field K into the large field L.\n\nThe so-called embedding refers to mapping elements from one field K to another field L, denoted as \\phi: K\\to L. This mapping is Injective and this homomorphism mapping preserves the structure of addition and multiplication operations:\\begin{split}\n\\phi(a+b) &= \\phi(a) + \\phi(b) \\\\\n\\phi(a\\cdot b) &= \\phi(a)\\cdot\\phi(b)\n\\end{split}\n\nThat is, if a\\in K, then a also has a unique representation in L. To maintain the structure of multiplication operations, we actually only need to find a Primitive Element \\alpha in K corresponding to an element \\beta in L, then this homomorphism mapping is uniquely determined, because any element in K can be represented as a power of \\alpha. However, usually this embedding homomorphism mapping is not easy to find. Let’s take \\mathbb{F}_{2^2}\\subset\\mathbb{F}_{2^4} as an example to see how to find the mapping of the former embedded into the latter.\n\nBecause \\eta is a Primitive Element in \\mathbb{F}_{2^2}, we only need to consider the representation of \\eta in \\mathbb{F}_{2^4}.\n\nLet’s first look at the Primitive Element \\theta in \\mathbb{F}_{2^4}, is \\eta\\mapsto\\theta an embedding mapping?\\eta^2 = \\eta+1 \\quad \\text{but} \\quad \\theta^2 \\neq \\theta+1\n\nObviously, \\eta^2 \\neq \\theta^2, so \\eta\\mapsto\\theta is not an embedding mapping. Thinking about how irreducible polynomials determine the multiplication relationship between elements, and because \\eta is a root of X^2+X+1 while \\theta is a root of X^4+X+1, the multiplication relationship between \\eta and \\theta must be different. In \\mathbb{F}_{2^4}, there also exist two roots of X^2+X+1, which are \\theta^2+\\theta and \\theta^2+\\theta+1 respectively. Readers can verify the following equation:(\\theta^2+\\theta)^2 + (\\theta^2+\\theta) + 1 = \\theta^4 + \\theta^2 + \\theta^2 + \\theta + 1 = 0\n\nSo, we define the embedding mapping:\\begin{split}\n\\phi &: \\mathbb{F}_{2^2} \\to \\mathbb{F}_{2^4},\\quad \\eta \\mapsto \\theta^2+\\theta\n\\end{split}\n\nThis means that the binary representation (10) corresponds to (0110) in L=\\mathbb{F}_{2^4}; and the binary representation (11) (which is \\eta+1) corresponds to (\\theta^2+\\theta+1) in L, i.e., (0111). Note here that we can also use \\phi': \\eta \\mapsto \\theta^2+\\theta+1 as another different embedding mapping. The underlying principle is that \\theta^2+\\theta and \\theta^2+\\theta+1 are conjugates of each other, they are perfectly symmetric, so both of these mappings can serve as embedding mappings. Although they map to different elements, there is no significant difference from the overall structure.\n\nFor any [L:K]=n, when we embed K into L, a direct method is to find the roots of f(X) in L, although this calculation is not simple. Moreover, both embedding and de-embedding require additional calculations, which undoubtedly increases the complexity of the system.\n\nThe recursive Extension Tower construction method mentioned in the Binius paper, by selecting appropriate irreducible polynomials and Bases, we can obtain very direct (called Zero-cost) embedding and de-embedding mappings.","type":"content","url":"/fri-binius/binius-01#field-embedding","position":7},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Extension Tower"},"type":"lvl2","url":"/fri-binius/binius-01#extension-tower","position":8},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Extension Tower"},"content":"We can construct \\mathbb{F}_{2^4} through two quadratic extensions. First, we choose a quadratic irreducible polynomial f(X)=X^2+X+1, then we can construct \\mathbb{F}_{2^2}, then based on \\mathbb{F}_{2^2} we find another quadratic irreducible polynomial to construct \\mathbb{F}_{2^4}.\\mathbb{F}_{2^2} = \\mathbb{F}_2[X]/\\langle X^2+X+1 \\rangle \\cong\\mathbb{F}_2(\\eta)\n\nNext, we need to find a quadratic irreducible polynomial in \\mathbb{F}_{2^2}[X]. First note that X^2+X+1 can no longer be used, according to the definition of \\mathbb{F}_{2^2}, it can already be factored. Consider X^2+1, it can also be factored (X+1)(X+1). In fact, all quadratic polynomials in \\mathbb{F}_2[X] can be factored. A quadratic irreducible polynomial in \\mathbb{F}_{2^2}[X] must include a term with the new element \\eta in its coefficients.\n\nFor example, X^2+X+\\eta is a quadratic irreducible polynomial over \\mathbb{F}_{2^2}. Then we can construct \\mathbb{F}_{2^4}:\\mathbb{F}_{2^4} = \\mathbb{F}_{2^2}[X]/\\langle X^2+X+\\eta \\rangle\n\nLet’s denote the root of X^2+X+\\eta in \\mathbb{F}_{2^4} as \\zeta, then \\mathbb{F}_{2^4} can be represented as:\\mathbb{F}_{2^4} = \\cong \\mathbb{F}_{2^2}(\\zeta)  \\cong \\mathbb{F}_2(\\eta)(\\zeta) \\cong \\mathbb{F}_2(\\eta, \\zeta)\n\nThen all elements of \\mathbb{F}_{2^4} can be represented using \\eta, \\zeta:\\begin{array}{ccccccc}\n\\hline\n0000 & 0001 & 0010 & 0011 & 0100 & 0101 & 0110 & 0111 \\\\\n0 & 1 & \\eta & \\eta+1 & \\zeta & \\zeta+1 & \\zeta+\\eta & \\zeta+\\eta+1 \\\\\n\\hline\n1000 & 1001 & 1010 & 1011 & 1100 & 1101 & 1110 & 1111 \\\\\n\\zeta\\eta & \\zeta\\eta + 1 & \\zeta\\eta + \\eta & \\zeta\\eta + \\eta + 1 & \\zeta\\eta + \\zeta & \\zeta\\eta + \\zeta +1 & \\zeta\\eta+\\zeta+\\eta & \\zeta\\eta+\\zeta+\\eta+1 \\\\\n\\hline\n\\end{array}\n\nAt this point, each bit in the 4-bit binary corresponds to an element in \\mathbb{F}_{2^4}, (1000) corresponds to \\zeta\\eta, (0100) corresponds to \\zeta, (0010) corresponds to \\eta, (0001) corresponds to 1. Therefore, we can use the following Basis to represent all elements in \\mathbb{F}_{2^4}:\\mathcal{B} = (1,\\ \\eta,\\ \\zeta,\\ \\eta\\zeta)\n\nNow, the binary representation of \\mathbb{F}_{2^2} directly corresponds to the “lower two bits” of the binary representation of \\mathbb{F}_{2^4}, for example:\\begin{split}\n(1010) &= (10) || (10) = \\zeta\\eta + \\eta \\\\\n\\end{split}\n\nTherefore, we can directly pad zeros to the higher two bits of the binary representation of \\mathbb{F}_{2^2} to get the corresponding element in \\mathbb{F}_{2^4}. Conversely, by removing the two high-order zeros, an element in \\mathbb{F}_{2^4} directly maps back to an element in \\mathbb{F}_{2^2}.\n\nAs shown in the figure above, (1011) is the binary representation of \\zeta\\eta+\\eta+1, its lower two bits (11) directly correspond to (\\eta+1) in \\mathbb{F}_{2^2}. This kind of embedding is a “natural embedding”, so the Binus paper calls it Zero-cost Embedding.\n\nHowever, \\mathbb{F}_{2^4} is still a very small field, not enough. If we continue to perform quadratic extensions upwards, how can we find suitable irreducible polynomials? The solution is not unique. Let’s first look at a scheme given in the Binius paper [DP23] — Wiedemann Tower [Wie88].","type":"content","url":"/fri-binius/binius-01#extension-tower","position":9},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Wiedemann Tower"},"type":"lvl2","url":"/fri-binius/binius-01#wiedemann-tower","position":10},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Wiedemann Tower"},"content":"The Wiedemann Tower is a recursive extension tower based on \\mathbb{F}_2. The bottom Base Field is denoted as \\mathcal{T}_0, with elements only 0 and 1:\\mathcal{T}_0 = \\mathbb{F}_2 \\cong \\{0,1\\}\n\nThen we introduce an unknown X_0, constructing a univariate polynomial ring \\mathbb{F}_2[X_0]. As discussed earlier, X^2 + X + 1 is an irreducible polynomial over \\mathcal{T}_0, so we can use it to construct \\mathcal{T}_1.\\mathcal{T}_1 = \\mathbb{F}_2[X_0]/\\langle  X_0^2+X_0+1 \\rangle = \\{0, 1, X_0, X_0+1\\} \\cong \\mathbb{F}_{2^2} \\cong \\mathbb{F}_2(\\alpha_0)\n\nNext, we find a quadratic irreducible polynomial X_1^2+\\alpha_0\\cdot X_1+1 in \\mathcal{T}_1[X_1], then we can construct \\mathcal{T}_2:\\mathcal{T}_2 = \\mathcal{T}_1[X_1]/\\langle  X_1^2+ \\alpha_0\\cdot X_1+1\\rangle \\cong \\mathbb{F}_{2^4} \\cong \\mathbb{F}_2(\\alpha_0, \\alpha_1)\n\nAnd so on, we can construct \\mathcal{T}_3, \\mathcal{T}_4, \\cdots, \\mathcal{T}_n:\\mathcal{T}_{i+1} = \\mathcal{T}_i[X_i]/\\langle  X_i^2+\\alpha_{i-1}\\cdot X_i+1\\rangle \\cong \\mathbb{F}_{2^{2^i}} \\cong \\mathbb{F}_2(\\alpha_0, \\alpha_1, \\ldots, \\alpha_i),\\quad i\\geq 1\n\nHere, \\alpha_0, \\alpha_1, \\ldots, \\alpha_{n-1} are the roots of the successively introduced quadratic irreducible polynomials, such that:\\mathcal{T}_{n} = \\mathbb{F}_2(\\alpha_0, \\alpha_1, \\ldots, \\alpha_{n-1})\n\nAnd |\\mathcal{T}_{n}| = 2^{2^{n}}. The relationship between these introduced roots satisfies the following equation:\\alpha_{i+1} + \\alpha^{-1}_{i+1} = \\alpha_i\n\nIt’s not hard to verify that \\alpha_0+\\alpha^{-1}_0=1. And the polynomial X_i^2+X_{i-1}X_i+1 in the multivariate polynomial ring \\mathcal{T}_0[X_0, X_1, \\ldots, X_n] has two roots \\alpha_i and \\alpha^{-1}_i:(\\alpha^{-1}_i)^2 + \\alpha_{i-1}\\alpha^{-1}_i + 1 = \\alpha^{-1}_i + \\alpha_{i-1} + \\alpha_i = \\alpha_{i-1} + \\alpha_{i-1} = 0\n\nMoreover, \\alpha_i and \\alpha_{i+1} satisfy the following recursive relationship:\\alpha_{i+1} + \\alpha^{-1}_{i+1} = \\alpha_i\n\nThis is because if we multiply both sides of the equation by \\alpha_{i+1}, we get: \\alpha_{i+1}^2 + \\alpha_i\\alpha_{i+1} + 1 = 0, which is exactly the irreducible polynomial we use for recursive quadratic extension construction.","type":"content","url":"/fri-binius/binius-01#wiedemann-tower","position":11},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl3":"Multilinear Basis","lvl2":"Wiedemann Tower"},"type":"lvl3","url":"/fri-binius/binius-01#multilinear-basis","position":12},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl3":"Multilinear Basis","lvl2":"Wiedemann Tower"},"content":"For \\mathcal{T}_{i+1} over \\mathbb{F}_2, it forms an n+1 dimensional vector space over \\mathbb{F}_2. We can use the roots of these irreducible polynomials to construct a Multilinear Basis:\\begin{split}\n\\mathcal{B}_{i+1} &= (1, \\alpha_0)\\otimes (1, \\alpha_1) \\otimes \\cdots \\otimes (1,\\alpha_i)  \\\\\n& = (1, \\alpha_0, \\alpha_1, \\alpha_0\\alpha_1, \\alpha_2,\\ \\ldots,\\ \\alpha_0\\alpha_1\\cdots \\alpha_i)\n\\end{split}\n\nThis is consistent with what we discussed earlier, using (1, \\eta, \\zeta, \\zeta\\eta) as the Basis for \\mathbb{F}_{2}(\\eta, \\zeta). We can quickly verify this. First, (1, \\alpha_0) is the Basis of \\mathcal{T}_1, because every element of \\mathcal{T}_1 can be expressed asa_0 + b_0\\cdot \\alpha_0,  \\quad a_0, b_0\\in\\mathcal{T}_0\n\nAfter \\mathcal{T}_1 is extended to \\mathcal{T}_2 through \\alpha_1, elements of \\mathcal{T}_2 can all be expressed as:a_1 + b_1\\cdot \\alpha_1,  \\quad a_1, b_1\\in\\mathcal{T}_1\n\nSubstituting a_1=a_0+b_0\\cdot \\alpha_0, b_1=a_0'+b_0'\\cdot \\alpha_1, we have:\\begin{split}\na_1 + b_1\\cdot \\alpha_1 &= (a_0+b_0\\cdot \\alpha_0) + (a'_0+b'_0\\cdot \\alpha_0)\\cdot \\alpha_1 \\\\\n &= a_0 + b_0\\alpha_0 + a'_0\\cdot\\alpha_1+ b_0'\\cdot \\alpha_0\\alpha_1\n\\end{split}\n\nThus, (1, \\alpha_0, \\alpha_1, \\alpha_0\\alpha_1) forms the Basis of \\mathcal{T}_2. By extension, (1, \\alpha_0, \\alpha_1, \\alpha_0\\alpha_1, \\alpha_2, \\alpha_0\\alpha_2, \\alpha_1\\alpha_2, \\alpha_0\\alpha_1\\alpha_2) is the Basis of \\mathcal{T}_3. Finally, \\mathcal{B}_{n} is the Basis of \\mathcal{T}_{n}.","type":"content","url":"/fri-binius/binius-01#multilinear-basis","position":13},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl3":"Finding Primitive element","lvl2":"Wiedemann Tower"},"type":"lvl3","url":"/fri-binius/binius-01#finding-primitive-element","position":14},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl3":"Finding Primitive element","lvl2":"Wiedemann Tower"},"content":"We discussed earlier that \\alpha_i and \\alpha^{-1}_{i} are conjugate roots. By Galois theory,\\alpha_{i}^{2^{2^n}} = \\alpha^{-1}_{i}\n\nThen all \\alpha_i satisfy the following property:\\alpha_i^{F_i} = 1\n\nHere F_n represents the Fermat Number, F_n=2^{2^n}+1. A famous theorem is that \\mathsf{gcd}(F_i, F_j) = 1,  i\\neq j, that is, any two different Fermat numbers are coprime, so\\mathsf{ord}(\\alpha_0\\alpha_1\\cdots \\alpha_i) = \\mathsf{ord}(\\alpha_0)\\mathsf{ord}(\\alpha_1)\\cdots\\mathsf{ord}(\\alpha_i)\n\nTherefore, if the Fermat number F_i is prime, then obviously \\mathsf{ord}(\\alpha_i)=F_i. Currently, we know that when i\\leq 4, F_i are all prime, so\\begin{split}\n\\mathsf{ord}(\\alpha_0\\cdot \\alpha_1\\cdot \\cdots \\cdot \\alpha_i) &= \\mathsf{ord}(\\alpha_0)\\cdot \\mathsf{ord}(\\alpha_1)\\cdot \\cdots \\cdot \\mathsf{ord}(\\alpha_n) \\\\\n& = F_0\\cdot F_1\\cdot \\cdots \\cdot F_i = 2^{2^{i+1}} - 1 \\\\\n& = |\\mathcal{T}_{n+1}| -1\n\\end{split}\n\nIf \\alpha_0\\cdots \\alpha_i, i\\leq 4, then according to the properties of finite fields, it is a Primitive Element of \\mathcal{T}_{n+1}.\n\nAdditionally, through computer program verification, for the cases of 5\\leq i \\leq 8, the Order of \\alpha_i is still equal to F_i. The \\alpha_0\\cdots \\alpha_8 is of the size of the finite field \\mathbb{F}_{2^{512}} which can already meet the needs of proof systems like Binius. But mathematically, do all \\alpha_i satisfy this property? This seems to still be an unsolved problem [Wie88].","type":"content","url":"/fri-binius/binius-01#finding-primitive-element","position":15},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Multiplication Optimization"},"type":"lvl2","url":"/fri-binius/binius-01#multiplication-optimization","position":16},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Multiplication Optimization"},"content":"Another significant advantage of adopting the Extension Tower is the optimization of multiplication operations.\n\nThe first optimization is “Small-by-large Multiplication”, that is, the multiplication operation of two numbers a\\in\\mathcal{T}_\\iota and b\\in\\mathcal{T}_{\\iota+\\kappa}. Since b can be decomposed into 2^\\kappa elements of \\mathcal{T}_\\iota, this multiplication operation is equivalent to 2^\\kappa multiplication operations on \\mathcal{T}_\\iota.a \\cdot (b_0, b_1, \\cdots, b_{2^\\kappa-1}) = (a\\cdot b_0, a\\cdot b_1, \\cdots, a\\cdot b_{\\kappa-1})\n\nEven for the multiplication of two elements in the same field, there are still optimization techniques. Suppose a, b\\in\\mathcal{T}_{i+1}, then according to the definition of Tower construction, they can be represented as a_0 + a_1\\cdot\\alpha_i and b_0 + b_1\\cdot \\alpha_i respectively, then their multiplication can be derived as follows:\\begin{split}\na\\cdot b  & = (a_0 + a_1\\cdot\\alpha_i)\\cdot (b_0 + b_1\\cdot\\alpha_i) \\\\\n &= a_0b_0 + (a_0b_1+a_1b_0)\\cdot\\alpha_i + a_1b_1\\cdot\\alpha_i^2 \\\\\n& = a_0b_0 + (a_0b_1+a_1b_0)\\cdot\\alpha_i + a_1b_1\\cdot(\\alpha_{i-1}\\alpha_i+1) \\\\\n& = a_0b_0  + a_1b_1 + (a_0b_1 + a_1b_0 + a_1b_1\\cdot\\alpha_{i-1})\\cdot\\alpha_i \\\\\n& = a_0b_0  + a_1b_1 + \\big((a_0+a_1)(b_0+b_1) - a_0b_0 - a_1b_1) + a_1b_1\\cdot\\alpha_{i-1}\\big)\\cdot\\alpha_i\n\\end{split}\n\nNote on the right side of the above equation, we only need to calculate three multiplications on \\mathcal{T}_{i}, namely A=a_0b_0, B=(a_0+a_1)(b_0+b_1) and C=a_1b_1, then the above formula can be converted to:a\\cdot b = (A + C) + (B-A-C+C\\cdot \\alpha_{i-1})\\cdot\\alpha_i\n\nThere’s still one C\\cdot \\alpha_{i-1} missing, which is a constant multiplication, because \\alpha_{i-1}\\in\\mathcal{T}_{i} is a constant. This constant multiplication can be reduced to a constant multiplication operation on \\mathcal{T}_{i-1}, as shown below:\\begin{split}\nC\\cdot \\alpha_{i-1} &= (c_0 + c_1\\alpha_{i-1})\\cdot \\alpha_{i-1} \\\\\n& = c_0\\cdot \\alpha_{i-1} + c_1\\cdot \\alpha_{i-1}^2 \\\\\n& = c_0\\cdot \\alpha_{i-1} + c_1\\cdot (\\alpha_{i-2}\\cdot \\alpha_{i-1} + 1) \\\\\n& = c_1 + (c_0 + {\\color{blue}c_1\\cdot \\alpha_{i-2}})\\cdot \\alpha_{i-1}\n\\end{split}\n\nThe blue part expression, {\\color{blue}c_1\\cdot \\alpha_{i-2}} is a constant multiplication operation on \\mathcal{T}_{i-2} that needs to be calculated recursively. The entire recursive process only needs to perform several additions to complete.\n\nLooking back at the a\\cdot b operation, we can also construct a Karatsuba-style recursive algorithm, where each layer of recursion only needs to complete three multiplication operations, one less than the four multiplication operations without optimization. Overall, the optimization effect will be very significant.\n\nFurthermore, the multiplication inverse operation on \\mathcal{T}_{i} can also be greatly optimized [FP97]. Consider a, b\\in\\mathcal{T}_{i+1}, satisfying a\\cdot b=1, expand the expressions of a and b:\\begin{split}\na\\cdot b &= (a_0 + a_1\\cdot\\alpha_i)\\cdot (b_0 + b_1\\cdot\\alpha_i) \\\\\n& = a_0b_0  + a_1b_1 + \\big((a_0+a_1)(b_0+b_1) - a_0b_0 - a_1b_1) + a_1b_1\\cdot\\alpha_{i-1}\\big)\\cdot\\alpha_i\\\\\n&= 1\\\\\n\\end{split}\n\nWe can calculate to get the expressions for b_0, b_1:\\begin{split}\nb_0 &= \\frac{a_0 + a_1\\alpha_{i-1}}{a_0(a_0 + a_1\\alpha_{i-1}) + a_1^2}  \\\\[2ex]\nb_1 &= \\frac{a_1}{a_0(a_0 + a_1\\alpha_{i-1}) + a_1^2} \\\\\n\\end{split}\n\nSo, the calculation of b_0 and b_1 includes: one inversion operation, three multiplications, two additions, one constant multiplication, and one squaring operation.\\begin{split}\nd_0 &= \\alpha_{i-1}a_1\\\\\nd_1 &= a_0 + d_0 \\\\\nd_2 &= a_0 \\cdot d_1 \\\\\nd_3 &= a_1^2   \\\\\nd_4 &= d_2 + d_3 \\\\\nd_5 &= 1/d_4 \\\\\nb_0 & = d_1\\cdot d_5\\\\\nb_1 & = a_1 \\cdot d_5\\\\\n\\end{split}\n\nThe inversion operation of d_5 can be recursively calculated layer by layer along the Extension Tower. The main computational cost in the recursive process is three multiplication operations. There’s also the squaring operation of d_3, which can also be calculated recursively:\\begin{split}\na_1^2 &= (e_0 + e_1\\cdot\\alpha_{i-1})^2 \\\\\n& = e_0^2 + e_1^2\\cdot\\alpha_{i-1}^2 \\\\\n& = e_0^2 + e_1^2\\cdot(\\alpha_{i-2}\\alpha_{i-1} + 1) \\\\\n& = (e_0^2 + e_1^2) + (e_1^2\\alpha_{i-2})\\cdot\\alpha_{i-1}  \\\\\n\\end{split}\n\nFor detailed recursive efficiency analysis, please refer to [FP97]. Overall, the computational complexity is comparable to the Karatsuba algorithm, thus greatly reducing the algorithmic complexity of inversion.","type":"content","url":"/fri-binius/binius-01#multiplication-optimization","position":17},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Artin-Schreier Tower (Conway Tower)"},"type":"lvl2","url":"/fri-binius/binius-01#artin-schreier-tower-conway-tower","position":18},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"Artin-Schreier Tower (Conway Tower)"},"content":"There’s another method to construct Binary Towers, originating from a paper published by Amil Artin and Otto Schreier in 1927, which also appears in Conway’s book “On Numbers and Games”. For the historical origins and related theories, please refer to [CHS24].\n\nFor any \\mathbb{F}_{p^n}, we choose h(X_{i+1}) = X_{i+1}^p - X_{i+1} - \\alpha_0\\alpha_1\\cdots \\alpha_i as the irreducible polynomial for each layer of the Tower. And \\alpha_{i+1} is the root of h(X_{i+1})=0 on the previous layer of the Tower. This way we can get an Extension Tower:\\mathbb{F}_2 \\subset \\mathbb{F}_{2^2}\\cong\\mathbb{F}_2(\\alpha_0) \\subset \\mathbb{F}_{2^4}\\cong\\mathbb{F}_{2^2}(\\alpha_1) \\subset \\mathbb{F}_{2^8}\\cong\\mathbb{F}_{2^4}(\\alpha_2)\n\nMoreover, (1, \\alpha_0)\\otimes(1, \\alpha_1)\\otimes\\cdots \\otimes (1, \\alpha_n) forms a Basis for the vector space \\mathbb{F}_{2^{2^{i+1}}}. According to our previous discussion, this set of Bases also supports Zero-cost subfield embedding. This type of Multilinear Basis is also known as Cantor Basis [Can89].","type":"content","url":"/fri-binius/binius-01#artin-schreier-tower-conway-tower","position":19},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"References"},"type":"lvl2","url":"/fri-binius/binius-01#references","position":20},{"hierarchy":{"lvl1":"Notes on FRI-Binius (Part I): Binary Towers","lvl2":"References"},"content":"[Wie88] Wiedemann, Doug. “An iterated quadratic extension of GF (2).” Fibonacci Quart 26.4 (1988): 290-295.\n\n[DP23] Diamond, Benjamin E., and Jim Posen. “Succinct arguments over towers of binary fields.” Cryptology ePrint Archive (2023).\n\n[DP24] Diamond, Benjamin E., and Jim Posen. “Polylogarithmic Proofs for Multilinears over Binary Towers.” Cryptology ePrint Archive (2024).\n\n[LN97] Lidl, Rudolf, and Harald Niederreiter. Finite fields. No. 20. Cambridge university press, 1997.\n\n[FP97] Fan, John L., and Christof Paar. “On efficient inversion in tower fields of characteristic two.” Proceedings of IEEE International Symposium on Information Theory. IEEE, 1997.\n\n[CHS24] Cagliero, Leandro, Allen Herman, and Fernando Szechtman. “Artin-Schreier towers of finite fields.” arXiv preprint arXiv:2405.10159 (2024).\n\n[Can89] David G. Cantor. On arithmetical algorithms over finite fields. J. Comb. Theory Ser. A, 50(2):285–300, March 1989.","type":"content","url":"/fri-binius/binius-01#references","position":21},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial"},"type":"lvl1","url":"/fri-binius/binius-02","position":0},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nJade Xie \n\njade@secbit.io\n\nThe FRI-Binus paper [DP24] discusses the Additive FFT algorithm based on Subspace Polynomial and provides a perspective to understand the Additive FFT algorithm based on Novel Polynomial Basis in [LCH14] using odd-even decomposition. This article directly introduces the Subspace Polynomial, and then introduces the Additive FFT algorithm from the perspective of odd-even decomposition. This article omits the definition of Normalized Subspace Polynomial for easier understanding. Normalization only affects the performance of the FFT algorithm and has no essential difference from the simplified algorithm introduced in this article.\n\nSince the algebraic structure on which Additive FFT relies is very similar to Multiplicative FFT on prime fields, knowledge of Multiplicative FFT will help understand the content of this article.","type":"content","url":"/fri-binius/binius-02","position":1},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Linear Subspace Polynomial"},"type":"lvl2","url":"/fri-binius/binius-02#linear-subspace-polynomial","position":2},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Linear Subspace Polynomial"},"content":"We continue to explore the Extension Field \\mathbb{F}_{2^m} based on \\mathbb{F}_2. Regardless of how \\mathbb{F}_{2^m} is constructed, all elements form a vector space, denoted as V_m, and there exists a Basis (\\beta_0, \\beta_1, \\ldots, \\beta_{m-1}) that spans this vector space, denoted as V_m=\\mathsf{Span}(\\beta_0, \\beta_1, \\ldots, \\beta_{m-1}), or represented by the symbol \\langle \\cdots \\rangle:V_m = \\langle \\beta_0, \\beta_1, \\ldots, \\beta_{m-1} \\rangle\n\nThus, any element  \\theta\\in\\mathbb{F}_{2^m} can be written as a linear combination of Basis components:\\theta = c_0\\cdot \\beta_0 + c_1\\cdot \\beta_1 + \\ldots + c_{m-1}\\cdot \\beta_{m-1}, \\text{ where $c_i\\in \\mathbb{F}_2$}\n\nAt the same time, V_m is also an additive group with the identity element V_0=\\{0\\}. If V_k is a linear subspace of V_m, then V_k is also an additive subgroup of V_m. For V_k, we can use a polynomial to encode all its elements, i.e., the root set of this polynomial exactly corresponds to the set of all elements of V_k. We denote this polynomial as s_k(X). This polynomial is also called the “Subspace Polynomial”:s_k(X) = \\prod_{i=0}^{2^k-1}(X-\\theta_i), \\text{ where $\\theta_i\\in V_k$ }\n\nThe polynomial s_k(X) can also be seen as the Vanishing Polynomial on the Domain V_k, because for any \\theta\\in V_k, it satisfies:s_k(\\theta) = 0","type":"content","url":"/fri-binius/binius-02#linear-subspace-polynomial","position":3},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Linearized Polynomial","lvl2":"Linear Subspace Polynomial"},"type":"lvl3","url":"/fri-binius/binius-02#linearized-polynomial","position":4},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Linearized Polynomial","lvl2":"Linear Subspace Polynomial"},"content":"The Subspace polynomial introduced above is a so-called Linearized Polynomial because its definition satisfies the following form:L(X) = \\sum_{i=0}^{n-1} c_i\\cdot X^{q^i}, \\quad c_i\\in \\mathbb{F}_{q}\n\nThe polynomial L(X) is called a Linearized Polynomial because each L(X) corresponds to a linear operator on the extension field K of \\mathbb{F}_{q}. If all roots of L(X) are in the extension field K=\\mathbb{F}_{q^s}, then for all \\theta\\in K, we have L(\\theta)\\in K. Moreover, if \\theta\\neq\\theta', then L(\\theta)\\neq L(\\theta'). Each L(X) can be viewed as a matrix B\\in \\mathbb{F}_q^{s\\times s}, completing a linear transformation on the vector space \\mathbb{F}^s_q, such that:(c_0, c_1, \\ldots, c_{s-1}) B = (d_0, d_1, \\ldots, d_{s-1})\n\nFor Subspace Polynomials, each s_k(X) is a Linearized Polynomial. Conversely, for any Linearized polynomial L(X)\\in \\mathbb{F}_{q^m}[X], all its roots form a linear subspace V_n\\subset V_m. For detailed proof, please refer to [LN97].","type":"content","url":"/fri-binius/binius-02#linearized-polynomial","position":5},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Linear Properties","lvl2":"Linear Subspace Polynomial"},"type":"lvl3","url":"/fri-binius/binius-02#linear-properties","position":6},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Linear Properties","lvl2":"Linear Subspace Polynomial"},"content":"Since each term of s_k(X) is of the form a_i\\cdot X^{2^i}, it has additive homomorphism:\\begin{align*}\ns_k(x + y) &= s_k(x) + s_k(y), &\\quad \\forall x, y \\in \\mathbb{F}_{2^m}\\\\\ns_k(c\\cdot x) &= c\\cdot s_k(x), &\\quad \\forall x\\in \\mathbb{F}_{2^m}, \\forall c\\in \\mathbb{F}_2\n\\end{align*}\n\nLet’s try to prove the first equation simply. According to a common theorem in finite field theory (Freshman’s dream):(x + y)^{2} = x^2 + 2xy + y^2 = x^{2} + y^{2}, \\quad\\text{where $x, y\\in \\mathbb{F}_{2^m}$}\n\nObviously, 2xy=0, because in binary fields, 2=0. So the following equation also holds:(x + y)^{2^i} = x^{2^i} + y^{2^i}\n\nNext, let’s verify the additive homomorphism of s_k(X):s_k(x + y) = \\sum_{i=0}^{k} a_i\\cdot (x+y)^{2^i}  = \\sum_{i=0}^{k} a_i\\cdot \\big(x^{2^i} + y^{2^i}\\big) = s_k(x) + s_k(y)","type":"content","url":"/fri-binius/binius-02#linear-properties","position":7},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Recursive Formula of Subspace Polynomial","lvl2":"Linear Subspace Polynomial"},"type":"lvl3","url":"/fri-binius/binius-02#recursive-formula-of-subspace-polynomial","position":8},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Recursive Formula of Subspace Polynomial","lvl2":"Linear Subspace Polynomial"},"content":"For the subspace V_k, it can be split into two disjoint sets:V_k = V_{k-1} \\cup (\\beta_{k-1}+V_{k-1})\n\nHere V_k=\\langle \\beta_0, \\beta_1, \\ldots, \\beta_{k-1} \\rangle , V_{k-1}=\\langle \\beta_0, \\beta_1, \\ldots, \\beta_{k-2} \\rangle  ,\nthen the subspace polynomials corresponding to V_k, V_{k-1}, \\beta_{k-1}+V_{k-1} satisfy the following relationship:s_k(X) = s_{k-1}(X) \\cdot s_{k-1}(X+\\beta_{k-1})\n\nLet’s take a simple example, assuming k=3, V_3=\\langle \\beta_0,\\beta_1,\\beta_2\\rangle  consists of two parts, one part is V_2=\\langle \\beta_0,\\beta_1\\rangle , the other part is each element in V_2 plus {\\color{blue}\\beta_2}. Therefore, the number of elements in V_3 is 2^2 + 2^2 = 8. Here are all the elements of V_3:V_3 = \\{0, \\beta_0, \\beta_1, \\beta_0+\\beta_1\\} \\cup \\{{\\color{blue}\\beta_2}, \\beta_0+{\\color{blue}\\beta_2}, \\beta_1+{\\color{blue}\\beta_2}, (\\beta_0+\\beta_1)+{\\color{blue}\\beta_2}\\}\n\nWe can easily verify: s_3(X) = s_{2}(X) \\cdot s_{2}(X+\\beta_{k-1}). Of course, s_{2}(X) can also be split into the product of s_1(X) and s_1(X+\\beta_1). Let’s try to break it down to the bottom:\\begin{split}\ns_3(X) & = s_2(X)\\cdot s_2(X + \\beta_2) \\\\\n& = s_2(X)^2 + \\beta_2\\cdot s_2(X) \\\\\n& = s_1(X)\\cdot s_1(X + \\beta_1)\\cdot s_1(X)\\cdot s_1(X + \\beta_1) + \\beta_2\\cdot s_1(X)\\cdot s_1(X + \\beta_1) \\\\\n& = (s_1(X)^2 + \\beta_1\\cdot s_1(X))^2 + \\beta_2\\cdot s_1(X)^2 + \\beta_1\\beta_2\\cdot s_1(X) \\\\\n& = s_1(X)^4 + \\beta_1^2\\cdot s_1(X)^2 + \\beta_2\\cdot s_1(X)^2 + \\beta_1\\beta_2\\cdot s_1(X) \\\\\n& = s_1(X)^4 + (\\beta_1^2+\\beta_2)\\cdot s_1(X)^2 + \\beta_1\\beta_2\\cdot s_1(X) \\\\\n& = (X\\cdot(X+\\beta_0))^4 + (\\beta_1^2+\\beta_2)\\cdot (X\\cdot(X + \\beta_0))^2 + \\beta_1\\beta_2\\cdot (X\\cdot(X + \\beta_0)) \\\\\n& = (X^2 + \\beta_0\\cdot X)^4 + (\\beta_1^2+\\beta_2)\\cdot (X^2 + \\beta_0\\cdot X)^2 + \\beta_1\\beta_2\\cdot (X^2 + \\beta_0\\cdot X) \\\\\n& = X^8  + \\beta_0^4X^4 + (\\beta_1^2+\\beta_2)X^4 + \\beta_0^2(\\beta_1^2+\\beta_2)X^2 + \\beta_1\\beta_2X^2 + \\beta_0\\beta_1\\beta_2X \\\\\n\\end{split}\n\nFinally, the expansion of s_3(X) satisfies the pattern of \\sum_{i=0}^{k} a_i\\cdot X^{2^i}, which is consistent with our conclusion above.","type":"content","url":"/fri-binius/binius-02#recursive-formula-of-subspace-polynomial","position":9},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Homomorphic Mapping on Subspace"},"type":"lvl2","url":"/fri-binius/binius-02#homomorphic-mapping-on-subspace","position":10},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Homomorphic Mapping on Subspace"},"content":"Because Subspace Polynomial is actually a kind of Vanishing Polynomial, and it also has additive homomorphism, we can use Subspace Polynomial to define homomorphic mapping between subspaces.\n\nFor example, for V_3=\\langle \\beta_0, \\beta_1, \\beta_2\\rangle, we define the subspace V_1=\\{0, \\beta_0\\} of V_3 and its Subspace Polynomial s_1(X)s_1(X) = X\\cdot (X+\\beta_0)\n\nObviously, s_1(V_1)=\\{0, 0\\}. If we apply s_1(X) to V_3, we will get the following result:\\begin{split}\ns_1(0) &= 0\\\\\ns_1(\\beta_0) & = 0 \\\\\ns_1(\\beta_1) & = \\beta_0\\beta_1 + \\beta_1^2\\\\\ns_1(\\beta_0+\\beta_1) & = \\beta_0\\beta_1 + \\beta_1^2\\\\\ns_1(\\beta_2) &= \\beta_0\\beta_2 + \\beta_2^2\\\\\ns_1(\\beta_0+\\beta_2) &= \\beta_0\\beta_2 + \\beta_2^2\\\\\ns_1(\\beta_1+\\beta_2) &= \\beta_0\\beta_1 + \\beta_1^2 + \\beta_0\\beta_2 + \\beta_2^2\\\\\ns_1(\\beta_0+\\beta_1+\\beta_2) &= \\beta_0\\beta_1 + \\beta_1^2 + \\beta_0\\beta_2 + \\beta_2^2\\\\\n\\end{split}\n\nThe above equations show that s_1(V_3) is mapped to a set that is only half the size of V_3, denoted as V_2. This set is also a subspace, V_2= \\langle \\beta'_0, \\beta'_1\\rangle = \\langle \\beta_0\\beta_1 + \\beta_1^2, \\beta_0\\beta_2 + \\beta_2^2 \\rangle , with dimension 2.\n\nThis is not a coincidence. According to the group isomorphism theorem, the Image G of the homomorphic mapping \\phi: H \\to G satisfies G\\cong H/Ker(\\phi), where G is a quotient group, and |G| = |H|/|Ker(\\phi)|. In the above example, s_1: V_3\\to V_2 is a homomorphic mapping, V_1=Ker(s_1).","type":"content","url":"/fri-binius/binius-02#homomorphic-mapping-on-subspace","position":11},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Chain of Mappings","lvl2":"Homomorphic Mapping on Subspace"},"type":"lvl3","url":"/fri-binius/binius-02#chain-of-mappings","position":12},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl3":"Chain of Mappings","lvl2":"Homomorphic Mapping on Subspace"},"content":"For V_2 = s_1(V_3), we can still construct a Subspace Polynomial of Degree 2,s_1'(X) = X\\cdot (X+\\beta_0\\beta_1 + \\beta_1^2)\n\nWe can continue to map V_2 to a one-dimensional subspace V_1=\\langle \\beta''\\rangle. We only need to calculate s_1(\\beta'_0) and s_1(\\beta'_1), these Basis components constitute V_2:\\begin{split}\ns_1'(\\beta_0\\beta_1 + \\beta_1^2) &= 0 \\\\\ns_1'(\\beta_0\\beta_1 + \\beta_1^2 + \\beta_0\\beta_2 + \\beta_2^2)& = \\beta_0^2\\beta_1\\beta_2 + \\beta_0\\beta_1^2\\beta_2 + \\beta_0^2\\beta_2^2 + \\beta_0\\beta_1\\beta_2^2 + \\beta_1^2\\beta_2^2 + \\beta_2^4 \\\\\n & = \\beta''\n\\end{split}\n\nWhere the first component of the Basis (\\beta'_0, \\beta'_1) of V_2 will be mapped to 0, and the second component is mapped to \\beta''.\n\nSo far, we have obtained a chain of mappings:V_3 \\overset{s_1}{\\longrightarrow} V_2 \\overset{s'_1}{\\longrightarrow} V_1\n\nOr it can be written as:\\langle \\beta_0, \\beta_1, \\beta_2 \\rangle \\overset{s_1}{\\longrightarrow} \\langle s_1(\\beta_1), s_1(\\beta_2) \\rangle\n\\overset{s'_1}{\\longrightarrow} \\langle s'_1(s_1(\\beta_2)) \\rangle\n\nAnd each mapping reduces the dimension of the linear subspace by one, i.e., halves the size of the set. This algebraic structure is key to our subsequent construction of FFT and FRI protocols.\n\nIt’s not hard to prove that for any linear subspace, as long as we choose a Basis, we can construct Subspace Polynomials of Degree 2 as mapping functions in sequence, then obtain a subspace with dimension reduced by 1 through mapping, and repeat this process until the subspace is reduced to 1 dimension. Of course, different choices of Basis and different choices of Subspace Polynomial will lead to different mapping chains. Choosing the appropriate mapping chain can significantly improve the efficiency of computation.","type":"content","url":"/fri-binius/binius-02#chain-of-mappings","position":13},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Composition of {s}_1 Mappings"},"type":"lvl2","url":"/fri-binius/binius-02#composition-of-s-1-mappings","position":14},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Composition of {s}_1 Mappings"},"content":"We define the initial subspace of the mapping chain as S^{(0)}, the subspace after mapping as S^{(1)}, and the subspace after i mappings as S^{(i)}:S^{(0)} \\overset{s_1}{\\longrightarrow} S^{(1)} \\overset{s^{(1)}_1}{\\longrightarrow} \\cdots \\overset{s^{(n-1)}_1}{\\longrightarrow} S^{(n)}\n\nGiven a set of Basis for S^{(i)}, assumed to be B^{(i)}=(\\beta^{(i)}_0, \\beta^{(i)}_1,\\ldots, \\beta^{(i)}_s), define Subspace Polynomial {s}^{(i)}_1 on the Basis, and use it as the group homomorphism mapping function to reduce S^{(i)} to S^{(i+1)}. The Basis of the reduced linear subspace S^{(i+1)} needs to transform the Basis of S^{(i)} along with {s}^{(i)}_1 to a new Basis. After switching to the new Basis, we can define a new set of Subspace Polynomials {s}^{(i+1)}_i(X).\n\nLet’s assume we start with S^{(0)}=\\langle \\beta_0, \\beta_1, \\ldots, \\beta_{k-1}\\rangle , given a set of Basis B_k, after mapping by {s}_1, we get S^{(1)}, and its Basis B^{(1)}:B^{(1)} = \\langle {\\color{blue}{s}_1(\\beta_1)}, {\\color{blue}{s}_1(\\beta_2)}, \\ldots, {s}_1(\\beta_{k-1}) \\rangle\n\nDefine {s}^{(1)}_1(X) on S^{(1)} again:{s}^{(1)}_1(X) = {X(X+{s}_1(\\beta_1))}\n\nThen, what is the relationship between S^{(2)} produced by mapping S^{(1)} and S^{(0)}? For any element a \\in S^{(0)}, it is first mapped to S^{(1)} by {s}_1, and then mapped to an element in S^{(2)} by {s}^{(1)}_1, so the value after two mappings can be written as the composition of two mapping functions, {s}^{(1)}_1({s}_1(X)). Let’s simplify this composite function:\\begin{align*}\n{s}^{(1)}_1({s}_1(X)) &= {s}_1(X)({s}_1(X)+{s}_1(\\beta_1))\n&  \\\\[3ex]\n&= {s}_1(X)({s}_1(X+\\beta_1)) & (\\text{additive homomorphism })\\\\[3ex]\n&= {s}_2(X) & (\\text{recurrency } )\n\\end{align*}\n\nSo we derived {s}^{(1)}_1({s}_1(X))={s}_2(X). This means that after two 2-to-1 mappings, it is equivalent to doing one 4-to-1 mapping, and the corresponding homomorphic mapping function is {s}_2:\\begin{split}\n{s}_2: &S^{(0)} \\to S^{(2)} \\\\[3ex]\n& X \\mapsto {X(X+\\beta_0)(X+\\beta_1)(X+\\beta_1+\\beta_0)} \\\\\n\\end{split}\n\nAs shown in the figure below, both left and right mapping methods will result in S^{(2)}:\n\nSimilarly, we can get the following conclusion, for the linear subspace S^{(j)} after j foldsS^{(j)}  = \\langle {s}_{j}(\\beta_j), {s}_{j}(\\beta_{j+1}), \\ldots, {s}_{j}(\\beta_{k-1}) \\rangle\n\nAnd {s}_{j} satisfies the following composite equation:s_{j}(X) = s^{(1)}_{j-1}(s_1(X)) =  s^{(1)}_{j-1}\\circ s_1\n\nThis composite mapping equation can be interpreted as: first do a s_1 mapping to get S^{(1)}, then do a j-1 dimensional mapping s^{(1)}_{j-1}, which is equivalent to directly doing a j dimensional mapping s_{j}, both mapping to the same subspace S^{(j)}.\n\nSimilarly, we can also prove: if we first do a j-1 dimensional mapping s_{j-1}, and then do a one-dimensional mapping on the mapped subspace S^{(j-1)}, we can also get the subspace S^{(j)}:s_{j}(X) = s^{(1)}_{1}(s_{j-1}(X)) =  s^{(1)}_{1}\\circ s_{j-1}\n\nMore generally, we can prove the following important property, that is, doing a j dimensional mapping on any subspace S^{(i)} is equivalent to doing j consecutive 1 dimensional mappings on it:s^{(i)}_{j}(X) = s^{(i+j-1)}_1\\circ s^{(i+j-2)}_1\\circ \\cdots \\circ s^{(i)}_1","type":"content","url":"/fri-binius/binius-02#composition-of-s-1-mappings","position":15},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Polynomial Basis"},"type":"lvl2","url":"/fri-binius/binius-02#polynomial-basis","position":16},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Polynomial Basis"},"content":"For a univariate polynomial f(X) \\in \\mathbb{F}[X]^{<N} of degree less than N=2^n, it has two common forms of expression, “coefficient form” and “point value form”. The coefficient form is the most common form we see:f(X) = c_0 + c_1X + c_2X^2 + \\ldots + c_{N-1}X^{N-1}\n\nwhere \\vec{c}=(c_0, c_1, \\ldots, c_{N-1}) is the coefficient vector of the polynomial. In addition, the vector of unknowns (1, X, X^2, \\ldots, X^{N-1}) forms a basis of polynomials, conventionally called the Monomial Basis, denoted as \\mathcal{B}^{mono}:\\mathcal{B}^{mono}=(1, X, X^2, \\ldots, X^{N-1})\n\nThis basis vector can also be expressed in the form of Tensor Product:\\mathcal{B}^{mono}=(1, X) \\otimes (1, X^2) \\otimes \\ldots \\otimes (1, X^{2^{n-1}})\n\nThe “point value form” of a univariate polynomial is called the Lagrange Basis representation. That is, we can uniquely determine a polynomial of Degree less than N using N “coefficients” (please note that the concept of coefficients here is broader than just the coefficients in the “coefficient form” representation).\n\nThrough polynomial division, we can obtain the coefficients of the polynomial on \\mathcal{B}^{mono}. For example, for a polynomial t(X) of degree 7, we can first calculate the coefficient of X^4\\cdot X^2\\cdot X, that is, calculate the polynomial division: t(X)/(X^4\\cdot X^2\\cdot X), obtaining a coefficient c_7 and a remainder polynomial t'(X); then calculate t'(X)/(X^4\\cdot X^2), obtaining the coefficient c_6 of \\mathcal{B}^{mono}_6 = X^6, and so on. Finally, we can obtain the coefficient vector \\vec{c}=(c_0, c_1, \\ldots, c_7) of t(X) with respect to \\mathcal{B}^{mono}, such that:t(X) = c_0 + c_1X + c_2X^2 + \\ldots + c_7X^7\n\nUsing the Subspace Polynomial s_k(X) discussed earlier, we can define a new set of Basis. According to its definition, the degree of s_k(X) is exactly 2^k, similar to (1, X, X^2, X^4), so (s_0(X), s_1(X), s_2(X)) can also be used as basic materials for constructing polynomial Basis. Following the definition of \\mathcal{B}^{mono}, we define the (Novel) Polynomial Basis \\mathcal{B}^{novel}:\\mathcal{B}^{novel} = (1, s_0(X)) \\otimes (1, s_1(X)) \\otimes \\ldots \\otimes (1, s_{n-1}(X))\n\nPlease note that unlike the papers [LCH14] and [DP24], we haven’t introduced Normalized Subspace Polynomial here for easier understanding. Returning to the above definition, we abbreviate each component \\mathcal{B}^{novel}_i as \\mathcal{X}_i(X), defined as follows:\\mathcal{X}_i(X) = \\prod_{j=0}^{n-1}(s_j(X))^{i_j}, \\text{ where $\\mathsf{bits}(i)=(i_0, i_1, \\ldots, i_{n-1})$}\n\nHere \\mathsf{bits}(i) means expanding the integer i in binary, for example, if i=5, then \\mathsf{bits}(5)=(1, 0, 1), \\mathsf{bits}(6) = (0, 1, 1). For example, when n=3, N=8, according to the above definition, we can calculate a set of polynomial basis \\big(\\mathcal{X}_0(X), \\mathcal{X}_1(X),\\ldots, \\mathcal{X}_7(X)\\big)\\begin{split}\n\\mathcal{X}_0(X) &= 1 \\\\\n\\mathcal{X}_1(X) &= s_0(X) = X\\\\\n\\mathcal{X}_2(X) &= s_1(X) \\\\\n\\mathcal{X}_3(X) &= s_0(X)\\cdot s_1(X)\\\\\n\\mathcal{X}_4(X) &= s_2(X)\\\\\n\\mathcal{X}_5(X) &= s_0(X)\\cdot s_2(X)\\\\\n\\mathcal{X}_6(X) &= s_1(X)\\cdot s_2(X) \\\\\n\\mathcal{X}_7(X) &= s_0(X)\\cdot s_1(X)\\cdot s_2(X) \\\\\n\\end{split}\n\nIt’s easy to verify that the Degree of each Basis component \\mathcal{X}_i(X) is exactly i, so \\mathcal{B}^{novel} forms a set of linearly independent polynomial Basis. For any polynomial f(X)\\in \\mathbb{F}_{2^m}[X] of Degree less than 8:\\begin{split}\nf(X) &= a_0\\mathcal{X}_0(X) + a_1\\mathcal{X}_1(X) + \\ldots + a_7\\mathcal{X}_7(X)\\\\\n&= a_0 + a_1s_0(X) + a_2s_1(X) + a_3s_0(X)\\cdot {s}_1(X) \\\\\n& + a_4s_2(X) + a_5{s}_0(X)\\cdot {s}_2(X) + a_6{s}_1(X)\\cdot {s}_2(X) + a_7{s}_0(X)\\cdot {s}_1(X)\\cdot s_2(X) \\\\\n\\end{split}\n\nSimilarly, we can use polynomial division to convert a polynomial between \\mathcal{B}^{novel} and \\mathcal{B}^{mono}.","type":"content","url":"/fri-binius/binius-02#polynomial-basis","position":17},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Additive FFT"},"type":"lvl2","url":"/fri-binius/binius-02#additive-fft","position":18},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Additive FFT"},"content":"Similar to Multiplicative FFT, to construct Additive FFT, we need to define a mapping chain of additive subgroups in \\mathbb{F}_{2^m}. As mentioned earlier, Subspace Polynomials can be used to construct this mapping chain. At the same time, Subspace Polynomials can also construct a set of polynomial Basis.S^{(0)} \\overset{s_1}{\\longrightarrow} S^{(1)} \\overset{s^{(1)}_1}{\\longrightarrow} \\cdots \\overset{s^{(n-1)}_1}{\\longrightarrow} S^{(n)}\n\nFor convenience of demonstration, specify n=3, S^{(0)}=\\langle \\beta_0, \\beta_1, \\beta_2\\rangle. Following the idea of Multiplicative FFT, we split the polynomial f(X) (of Degree 7) represented by \\mathcal{B}^{novel} into two polynomials with halved degrees:\\begin{split}\nf(X) &= a_0\\mathcal{X}_0(X) + a_1\\mathcal{X}_1(X) + \\ldots + a_7\\mathcal{X}_7(X)\\\\\n&= a_0 + a_1{s}_0(X) + a_2{s}_1(X) + a_3{s}_0(X)\\cdot {s}_1(X) \\\\\n& \\qquad + a_4{s}_2(X) + a_5{s}_0(X)\\cdot {s}_2(X) + a_6{s}_1(X)\\cdot {s}_2(X) + a_7{s}_0(X)\\cdot {s}_1(X)\\cdot {s}_2(X) \\\\\n& = \\big(a_0 + a_2 {s}_1(X) + a_4{s}_2(X) + a_6{s}_1(X)\\cdot {s}_2(X)\\big) \\\\\n& \\qquad + \\big(a_1 + a_3{s}_0(X)\\cdot {s}_1(X) + a_5{s}_0(X)\\cdot{s}_2(X) + a_7{s}_0(X)\\cdot{s}_1(X)\\cdot {s}_2(X)\\big) \\\\\n& = \\big(a_0 + a_2 {s}_1(X) + a_4{s}_2(X) +a_6{s}_1(X)\\cdot {s}_2(X)\\big) \\\\\n& \\qquad + {\\color{red}{s}_0(X)}\\cdot \\big(a_1 + a_3\\cdot{s}_1(X) + a_5\\cdot{s}_2(X) + a_7\\cdot{s}_1(X)\\cdot {s}_2(X)\\big) \\\\\n\\end{split}\n\nThen we introduce two auxiliary polynomials f_{even}(X), f_{odd}(X), they are\\begin{split}\nf_{even}(X) &= a_0 + a_2\\cdot {s}_1(X) + a_4\\cdot {s}_2(X) + a_6\\cdot {s}_1(X)\\cdot {s}_2(X) \\\\\nf_{odd}(X) &= a_1 + a_3\\cdot{s}_1(X) + a_5\\cdot{s}_2(X) + a_7\\cdot{s}_1(X)\\cdot {s}_2(X) \\\\\n\\end{split}\n\nAccording to the composition property of mappings we derived earlier, s_1(X)=s^{(1)}_0\\circ s_0(X), s_2(X)=s^{(1)}_1\\circ s_1(X), so we can get:\\begin{split}\nf_{even}(X) &= a_0 + a_2 \\cdot s^{(1)}_0(s_1(X)) + a_4 \\cdot s^{(1)}_1(s_1(X)) + a_6 \\cdot s^{(1)}_0(s_1(X))\\cdot s^{(1)}_1(s_1(X)) \\\\\n & = a_0 + a_2 \\cdot s^{(1)}_0({\\color{blue}s_1(X)}) + a_4 \\cdot s^{(1)}_1({\\color{blue}s_1(X)}) + a_6 \\cdot s^{(1)}_0({\\color{blue}s_1(X)})\\cdot s^{(1)}_1({\\color{blue}s_1(X)}) \\\\\nf_{odd}(X) &= a_1 + a_3 \\cdot s^{(1)}_0(s_1(X)) + a_5 \\cdot s^{(1)}_1(s_1(X)) + a_7 \\cdot s^{(1)}_0(s_1(X))\\cdot s^{(1)}_1(s_1(X)) \\\\\n & = a_1 + a_3 \\cdot s^{(1)}_0({\\color{blue}s_1(X)}) + a_5 \\cdot s^{(1)}_1({\\color{blue}s_1(X)}) + a_7 \\cdot s^{(1)}_0({\\color{blue}s_1(X)})\\cdot s^{(1)}_1({\\color{blue}s_1(X)}) \\\\\n\\end{split}\n\nAfter substituting {\\color{blue}Y=s_1(X)}, we can split f(X) into an equation about f_{even}({\\color{blue}Y}) and f_{odd}({\\color{blue}Y}):\\begin{split}\nf(X) & = f_{even}({\\color{blue}Y}) +  {s}_0(X)\\cdot f_{odd}({\\color{blue}Y})\n\\end{split}\n\nAnd the polynomials f_{even}({\\color{blue}Y}) and f_{odd}({\\color{blue}Y}) are exactly defined on \\mathcal{X}^{(1)}:\\begin{array}{llll}\n\\mathcal{X}^{(1)}_0(X) &= 1 \\\\\n\\mathcal{X}^{(1)}_1(X) &= {s}^{(1)}_0(X) & = {s}_0({s}_1(X)) & = {s}_1(X) \\\\\n\\mathcal{X}^{(1)}_2(X) &= {s}^{(1)}_1(X) & =  {s}_1({s}_1(X)) & = {s}_2(X) \\\\\n\\mathcal{X}^{(1)}_3(X) &= {s}^{(1)}_0(X)\\cdot {s}^{(1)}_1(X) & = {s}_0({s}_1(X)) \\cdot {s}_1({s}_1(X)) & = {s}_1(X) \\cdot {s}_2(X)\\\\\n\\end{array}\n\nRewrite the odd and even polynomials:\\begin{split}\nf_{even}(X) &= a_0\\cdot\\mathcal{X}^{(1)}_0(X) + a_2\\cdot \\mathcal{X}^{(1)}_1(X) + a_4\\cdot \\mathcal{X}^{(1)}_2(X) + a_6\\cdot \\mathcal{X}^{(1)}_3(X) \\\\\nf_{odd}(X) &= a_1\\cdot\\mathcal{X}^{(1)}_0(X) + a_3\\cdot \\mathcal{X}^{(1)}_1(X) + a_5\\cdot \\mathcal{X}^{(1)}_2(X) + a_7\\cdot \\mathcal{X}^{(1)}_3(X)\n\\end{split}\n\nStructurally, this equation is very similar to the split f(X)=f_{even}(X^2) + X\\cdot f_{odd}(X^2) in Multiplicative FFT; and the X\\mapsto X^2 mapping also corresponds to the s_1: X \\mapsto X(X+\\beta_0) mapping. And S^{(0)} under the mapping of s_1 produces a subspace S^{(1)} that is only half the size of the original:S^{(1)} = \\langle s_1(\\beta_1), s_1(\\beta_2) \\rangle\n\nSo we can rely on recursive calls to find \\{f_{even}(X) \\mid X\\in S^{(1)}\\} and \\{ f_{odd}(X) \\mid X\\in S^{(1)}\\}, and then use the equation f(X)=f_{even}(X) + s_0(X)\\cdot f_{odd}(X) to get the value of f(X) on S^{(0)}.\n\nLet’s assume the recursive call returns successfully, then we have obtained all the evaluations of f_{even}(X) and f_{odd}(X) on S^{(1)}, denoted as \\vec{u} and \\vec{v}, defined as follows:\\begin{split}\n(u_0, u_1, u_2, u_3) &= \\big(f_{even}(0), f_{even}(1), f_{even}(s_1(\\beta_1)), f_{even}(s_1(\\beta_1) + 1)\\big)\\\\[2ex]\n(v_0, v_1, v_2, v_3) &= \\big(f_{odd}(0), f_{odd}(1), f_{odd}(s_1(\\beta_1)), f_{odd}(s_1(\\beta_1) + 1)\\big)\\\\\n\\end{split}\n\nThen we can calculate all the evaluations of f(X) on S^{(0)}, i.e., f(X)\\mid_{S^{(0)}}:\\begin{array}{rlll}\nf(0) &= f_{even}({s}_1(0)) + 0\\cdot f_{odd}({s}_1(0)) \\\\\n    & = u_0\\\\[1ex]\nf(1) &= f_{even}({s}_1(1)) + 1\\cdot v_1 \\\\\n    & = u_0 + v_1 \\\\[1ex]\nf(\\beta_1) &= f_{even}({s}_1(\\beta_1)) + \\beta_1\\cdot f_{odd}({s}_1(\\beta_1)) \\\\ \n    & = u_1 + \\beta_1 \\cdot v_1\\\\[1ex]\nf(\\beta_1+1) &= f_{even}({s}_1(\\beta_1)+{s}_1(1)) + (\\beta_1 +1)\\cdot f_{odd}({s}_1(\\beta_1)+{s}_1(1)) \\\\\n    & = u_1 + \\beta_1\\cdot v_1 + v_1\\\\[1ex]\nf(\\beta_2) &= f_{even}({s}_1(\\beta_2)) + \\beta_2 \\cdot f_{odd}({s}_1(\\beta_2)) \\\\ \n    & = u_2 + \\beta_2\\cdot v_2 \\\\[1ex]\nf(\\beta_2+1) &= f_{even}({s}_1(\\beta_2)+{s}_1(1)) + (\\beta_2 +1) \\cdot f_{odd}({s}_1(\\beta_2)+{s}_1(1)) \\\\\n    & = u_2 + \\beta_2\\cdot v_2 + v_2\\\\[1ex]\nf(\\beta_2+\\beta_1) &= f_{even}({s}_1(\\beta_2)+{s}_1(\\beta_1)) + (\\beta_2 + \\beta_1) \\cdot f_{odd}({s}_1(\\beta_2)+{s}_1(\\beta_1)) \\\\\n    & = u_3 + \\beta_2\\cdot v_3 + \\beta_1\\cdot v_3 \\\\[1ex]\nf(\\beta_2+\\beta_1+1) &= f_{even}({s}_1(\\beta_2)+{s}_1(\\beta_1)+{s}_1(1)) + (\\beta_2 + \\beta_1 + 1) \\cdot f_{odd}({s}_1(\\beta_2)+{s}_1(\\beta_1)+{s}_1(1)) \\\\\n    & = u_3 + \\beta_2\\cdot v_3  + \\beta_1\\cdot v_3 + v_3\\\\[1ex]\n\\end{array}\n\nWe implement this Additive FFT recursive algorithm in Python code as follows:def afft(f, k, B):\n    \"\"\"\n    Perform the Additive Fast Fourier Transform (AFFT) on a given polynomial.\n\n    Args:\n        f (list): Coefficients of the polynomial to be transformed.\n        k (int): The depth of recursion, where 2^k is the size of the domain.\n        B (list): The basis of the domain over which the polynomial is evaluated.\n\n    Returns:\n        list: The evaluations of the polynomial over the domain.\n    \"\"\"\n    if k == 0:\n        return [f[0]]\n    half = 2**(k-1)\n\n    f_even = f[::2]\n    f_odd = f[1::2]\n\n    V = span(B)                                # the subspace spanned by B\n    q = lambda x: x*(x+B[0])/(B[1]*(B[1] + 1)) # s^(i)_1 map\n    B_half = [q(b) for b in B[1:]]             # the basis of the mapped subspace\n    \n    e_even = afft(f_even, k-1, B_half)  # compute the evaluations of f_even\n    e_odd  = afft(f_odd, k-1, B_half)   # compute the evaluations of f_odd\n\n    e = [0] * (2 * half)                # initialize the list of evaluations\n    for i in range(0, half):\n        e[2*i]   = e_even[i] + V[2*i] * e_odd[i]\n        e[2*i+1] = e_even[i] + V[2*i+1] * e_odd[i]\n\n    return e\n\nThe function afft(f, k, B) has three parameters in total, which are the coefficient vector of the polynomial f(X) on \\mathcal{B}^{novel}, the recursion depth k, and the Basis of the current subspace S^{(0)}.","type":"content","url":"/fri-binius/binius-02#additive-fft","position":19},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Summary"},"type":"lvl2","url":"/fri-binius/binius-02#summary","position":20},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"Summary"},"content":"The Additive FFT algorithm requires a mapping chain of subspaces constructed by Subspace Polynomials. The principles introduced in this article are not limited to recursively constructed binary fields, but a more general algebraic structure.\nThe paper [LCH14] uses another recursive Additive FFT algorithm, we will introduce the differences between the two in the next article, as well as the iterative Additive FFT algorithm (Algorithm 2) in the [DP24] paper.","type":"content","url":"/fri-binius/binius-02#summary","position":21},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"References"},"type":"lvl2","url":"/fri-binius/binius-02#references","position":22},{"hierarchy":{"lvl1":"Notes on Binius (Part II): Subspace Polynomial","lvl2":"References"},"content":"[DP24] Benjamin E. Diamond and Jim Posen. “Polylogarithmic Proofs for Multilinears over Binary Towers”. 2024. \n\nhttps://​eprint​.iacr​.org​/2024​/504\n\n[LCH14] Lin, Sian-Jheng, Wei-Ho Chung, and Yunghsiang S. Han. “Novel polynomial basis and its application to Reed-Solomon erasure codes.” 2014 ieee 55th annual symposium on foundations of computer science. IEEE, 2014. \n\nhttps://​arxiv​.org​/abs​/1404​.3458\n\n[LN97] Lidl, Rudolf, and Harald Niederreiter. Finite fields. No. 20. Cambridge university press, 1997.","type":"content","url":"/fri-binius/binius-02#references","position":23},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness"},"type":"lvl1","url":"/fri/bbhr18-fri","position":0},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness"},"content":"Jade Xie  \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article mainly focuses on explaining the paper [BBHR18b] published by Eli Ben-Sasson et al. in 2018, with emphasis on the completeness and soundness proofs of the FRI protocol. In this paper, they proposed a new IOPP (Interactive Oracle Proof of Proximity) for Reed-Solomon (RS) encoding, called FRI (Fast RS IOPP). Subsequently, in [BBHR18a], they used the FRI protocol to construct a practical ZK system, which is the STARK we are familiar with.","type":"content","url":"/fri/bbhr18-fri","position":1},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Primary Problem"},"type":"lvl2","url":"/fri/bbhr18-fri#primary-problem","position":2},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Primary Problem"},"content":"For a set of evaluations S in a finite field \\mathbb{F}, assuming the number of elements in S is N, given a rate parameter \\rho \\in (0,1], the encoding \\text{RS}[\\mathbb{F},S,\\rho] represents the set of all functions f: S \\rightarrow \\mathbb{F}, where f is the evaluation of a polynomial of degree d < \\rho N, i.e., there exists a polynomial \\hat{f} of degree d < \\rho N such that f and \\hat{f} are consistent on S.\n\nThe main focus of the paper is the RS proximity problem: Assuming we can obtain an oracle about the function f: S \\rightarrow \\mathbb{F}, the Verifier needs to distinguish with high confidence and low query complexity which of the following situations f belongs to:\n\nf \\in \\text{RS}[\\mathbb{F},S,\\rho]\n\n\\Delta(f, \\text{RS}[\\mathbb{F},S,\\rho]) > \\delta\n\nIn other words, either f is a codeword in the RS encoding \\text{RS}[\\mathbb{F},S,\\rho], or the relative Hamming distance between f and all codewords in \\text{RS}[\\mathbb{F},S,\\rho] is greater than the proximity parameter \\delta. A natural idea is that the verifier can query d + 1 times and then determine which of the above situations f belongs to. If it belongs to the first case, accept; if it belongs to the second case, reject. The query complexity at this time is d + 1 = \\rho N. When calculating the complexity of the Testing method, no additional information is provided to the verifier, so it is said that the computational complexity for the prover to try to convince the verifier that f \\in \\text{RS}[\\mathbb{F}, S,\\rho] is 0, the number of interactions is 0, and the length of the proof generated is 0. Comparing the complexity of this method (Testing, [RS92]) with FRI, the table below shows ([BBHR18b]).\n\n\n\nProver Computation Complexity\n\nProof Length\n\nVerifier Computation Complexity\n\nQuery Complexity\n\nRound Complexity\n\nTesting [RS92]\n\n0\n\n0\n\n\\rho N \\cdot \\log^{O(1)}\n\n\\rho N\n\n0\n\nFRI [BBHR18b]\n\n<6 \\cdot N\n\n<\\frac{N}{3}\n\n\\le 21 \\cdot \\log N\n\n2 \\log N\n\n\\frac{\\log N}{2}\n\nAs can be seen, the prover’s computational complexity in FRI is strictly linear, and the verifier’s computational complexity is strictly logarithmic, while the query complexity is logarithmic ([BBHR18b]).","type":"content","url":"/fri/bbhr18-fri#primary-problem","position":3},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"FRI Properties"},"type":"lvl2","url":"/fri/bbhr18-fri#fri-properties","position":4},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"FRI Properties"},"content":"As mentioned above, FRI is a type of IOPP. Below is the definition of IOPP.\n\nDefinition 1 [BBHR18b, Definition 1.1] (Interactive Oracle Proof of Proximity (IOPP)). An \\textbf{r}-round Interactive Oracle Proof of Proximity (IOPP) \\textbf{S} = (\\text{P}, \\text{V}) is a (r + 1)-round IOP. We say \\textbf{S} is an (\\textbf{r}-round) IOPP for the error correcting code C= \\{f:S \\rightarrow \\Sigma\\} with soundness \\textbf{s}^{-}: (0,1] \\rightarrow [0,1] with respect to distance measure \\Delta , if the following conditions hold:\n\nFirst message format: the first prover message, denote f^{(0)} , is a purported codeword of C , i.e., f^{(0)}: S \\rightarrow \\Sigma\n\nCompleteness: \\Pr[\\left \\langle \\text{P} \\leftrightarrow \\text{V} \\right \\rangle = \\text{accept}|\\Delta(f^{(0)}, C) = 0] = 1\n\nSoundness: For any \\text{P}^* , \\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{reject}|\\Delta(f^{(0)}, C) = \\delta] \\ge \\textbf{s}^{-}(\\delta)\n\nThis means that the Prover and Verifier will interact for \\textbf{r} rounds, and need to satisfy three conditions.\n\nThe first message f^{(0)} is the codeword initially claimed by the Prover to be in C.\n\nCompleteness: For an honest Prover, if f^{(0)} is in C, then the Verifier will definitely output accept.\n\nSoundness: This analyzes the probability of rejection after interaction with a malicious Prover. In the definition, soundness \\textbf{s}^{-}: (0,1] \\rightarrow [0,1] is a function with variable \\delta \\in (0,1], which also indicates that when analyzing soundness, we consider a malicious Prover, i.e., initially \\Delta(f^{(0)}, C) = \\delta > 0. In this case, the Prover and Verifier interact to calculate the probability of rejection, and the lower bound of this probability is \\textbf{s}^{-}(\\delta) \\in [0,1]. Since this represents a probability, naturally the function value of \\textbf{s}^{-}(\\delta) is in the closed interval [0,1].","type":"content","url":"/fri/bbhr18-fri#fri-properties","position":5},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"FRI Protocol"},"type":"lvl2","url":"/fri/bbhr18-fri#fri-protocol","position":6},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"FRI Protocol"},"content":"Below is an excerpt of the description of the FRI protocol from the paper [BBHR18b].","type":"content","url":"/fri/bbhr18-fri#fri-protocol","position":7},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Definitions and Notations","lvl2":"FRI Protocol"},"type":"lvl3","url":"/fri/bbhr18-fri#definitions-and-notations","position":8},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Definitions and Notations","lvl2":"FRI Protocol"},"content":"Interpolant For a function f : S \\rightarrow \\mathbb{F} , S \\subset \\mathbb{F} , let \\text{interpolant}^f denote the interpolant of f , defined as the unique polynomial P(X) = \\sum_{i=0}^{|S|-1} a_iX^i of degree less than |S| whose evaluation on S equals f|_S , i.e., \\forall x \\in S, f(x) = P(x) . We assume the interpolant P(X) is represented as a formal sum, i.e., by the sequence of monomial coefficients a_0, \\cdots, a_{|S|-1} .\n\nSubspace polynomials Given a set L_0 \\subset \\mathbb{F} , let \\text{Zero}_{L_0} \\triangleq \\prod_{x \\in L_0} (X - x) be the unique non-zero monic polynomial of degree |L_0| that vanishes on L_0 . When L_0 is an additive coset contained in a binary field, the polynomial \\text{Zero}_{L_0}(X) is an affine subspace polynomial, a special type of a linearized polynomial. We shall use the following properties of such polynomials, referring the interested reader to [LN97, Chapter 3.4] for proofs and additional background:\n\nThe map x \\mapsto \\text{Zero}_{L_0}(x) maps each additive coset S of L_0 to a single field element, which will be denoted by y_S .\n\nIf L \\supset L_0 are additive cosets, then \\text{Zero}_{L_0}(L) \\triangleq \\{ \\text{Zero}_{L_0}(z) | z \\in L \\} is an additive coset and \\dim(\\text{Zero}_{L_0}(L)) = \\dim(L) - \\dim(L_0) .\n\nSubspace specification Henceforth, the letter L always denotes an additive coset in a binary field \\mathbb{F} , we assume all mentioned additive cosets are specified by an additive shift \\alpha \\in \\mathbb{F} and a basis \\beta_1, \\cdots, \\beta_k \\in \\mathbb{F}^k so that L = \\left\\{ \\alpha + \\sum_{i=1}^k b_i\\beta_i | b_1, \\cdots, b_k \\in \\mathbb{F}_2 \\right\\} ; we assume \\alpha and \\vec{\\beta} = (\\beta_1, \\cdots, \\beta_k) are agreed upon by prover and verifier.","type":"content","url":"/fri/bbhr18-fri#definitions-and-notations","position":9},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"COMMIT Phase","lvl2":"FRI Protocol"},"type":"lvl3","url":"/fri/bbhr18-fri#commit-phase","position":10},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"COMMIT Phase","lvl2":"FRI Protocol"},"content":"The number of rounds in the protocol is r \\triangleq \\left \\lfloor \\frac{k^{(0)} - \\mathcal{R}}{\\eta}\\right\\rfloor , where \\mathcal{R} = \\log(1/\\rho) , and \\rho represents the code rate. In the i-th round of the COMMIT phase, i \\in \\{0, \\cdots, r - 1\\} , the Verifier can access an oracle of a function f^{(i)}: L^{(i)} \\rightarrow \\mathbb{F} submitted by the Prover, where \\dim(L^{(i)}) = k^{(i)} = k^{(0)} - \\eta \\cdot i , and the space L^{(i)} is fixed in advance, particularly, they do not depend on the Verifier’s messages.\n\nFRI-COMMIT:\nCommon input:\n\nParameters \\mathcal{R}, \\eta, i , all are positive integers:\n– rate parameter \\mathcal{R} : logarithm of RS code rate (\\rho = 2^{-\\mathcal{R}})\n– localization parameter \\eta : dimension of L_0^{(i)} (i.e., |L_0^{(i)}| = 2^{\\eta}); let r \\triangleq \\left \\lfloor \\frac{k^{(0)} - \\mathcal{R}}{\\eta}\\right\\rfloor denote round complexity\n– i \\in \\{0, \\cdots, r\\}: round counter\n\nA parametrization of \\text{RS}^{(i)} \\triangleq \\text{RS}[\\mathbb{F},L^{(i)},\\rho = 2^{-\\mathcal{R}}] , denote k^{(i)} = \\log_2 |L^{(i)}| (notice k^{(i)} = \\dim (L^{(i)}));\n\nL_0^{(i)} \\subset L^{(i)} , \\dim(L_0^{(i)})=\\eta ; let q^{(i)}(X) = \\text{Zero}_{L_0^{(i)}}(X) and denote L^{(i+1)} = q^{(i)}(L^{(i)})\n\nProver input: f^{(i)}:L^{(i)} \\rightarrow \\mathbb{F}, a purported codeword of \\text{RS}^{(i)}\n\nLoop: While i \\le r :\n\nVerifier sends a uniformly random x^{(i)} \\in \\mathbb{F}\n\nProver defines the function f_{f^{(i)},x^{(i)}}^{(i+1)} with domain L^{(i+1)} thus, for each y \\in L^{(i+1)} :\n\nLet S_y = \\{x \\in L^{(i)} | q^{(i)}(x) = y\\} be the coset of L_0^{(i)} mapped by q^{(i)} to \\{y\\} ;\n\nP_y^{(i)}(X) \\triangleq \\text{interpolant}^{f^{(i)}|_{S_y}} ;\n\nf_{f^{(i)},x^{(i)}}^{(i+1)}(y) \\triangleq P_y^{(i)}(x^{(i)}) ;\n\nIf i = r then:\n\nlet f^{(r)} = f_{f^{(r-1)},x^{(r-1)}}^{(r)} for f^{(r)} = f_{f^{(r-1)},x^{(r-1)}}^{(r)} defined in step 2 above;\n\nlet P^{(r)}(X) = \\sum_{j \\ge 0} a_j^{(r)}X^j \\triangleq \\text{interpolant}^{f^{(r)}}(X) ;\n\nlet d = \\rho \\cdot |L^{(r)}| - 1 ;\n\nprover commits to first d + 1 coefficients of P^{(r)}(X) , namely, to \\langle a_0^{(r)}, \\cdots, a_d^{(r)} \\rangle\n\nCOMMIT phase terminates;\n\nElse (i < r ):\n\nlet f^{(i+1)} = f^{(i+1)}_{f^{(i)}, x^{(i)}} for f^{(i+1)}_{f^{(i)}, x^{(i)}} defined in step 2 above;\n\nprover commits to oracle f^{(i+1)}\n\nboth parties repeat the COMMIT protocol with common input\n\nparameters (\\mathcal{R}, \\eta, i + 1)\n\na parametrization of \\text{RS}^{(i+1)} \\triangleq \\text{RS}[\\mathbb{F},L^{(i+1)},\\rho = 2^{-\\mathcal{R}}] and L_0^{(i+1)} \\subset L^{(i+1)} , \\dim(L_0^{(i+1)})=\\eta\nand prover input f^{(i+1)} defined at the beginning of this step;","type":"content","url":"/fri/bbhr18-fri#commit-phase","position":11},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"QUERY Phase","lvl2":"FRI Protocol"},"type":"lvl3","url":"/fri/bbhr18-fri#query-phase","position":12},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"QUERY Phase","lvl2":"FRI Protocol"},"content":"FRI-QUERY:\nverifier input:\n\nparameters \\mathcal{R}, \\eta as defined in the COMMIT phase\n\nrepetition parameter l\n\nsequence of rate-\\rho RS-codes \\text{RS}^{(0)}, \\cdots, \\text{RS}^{(r)}, where \\text{RS}^{(i)} \\triangleq \\text{RS}[\\mathbb{F},L^{(i)},\\rho] and \\log_2|L^{(i)}| = k^{(i)} = k^{(0)} - \\eta; (notice k^{(i)} = \\dim(L^{(i)}));\n\nsequence of affine spaces L_0^{(0)}, \\cdots, L_0^{(r-1)}, each L_0^{(i)} is of dimension \\eta and contained in L^{(i)} ;\n\ntranscript of verifier messages x^{(0)}, \\cdots, x^{(r-1)} \\in \\mathbb{F}\n\naccess to oracles f^{(0)}, \\cdots, f^{(r-1)}\n\naccess to last oracle P^{(r)}(X) = \\sum_{j \\ge 0} a_j^{(r)}X^j for d = \\rho \\cdot |L^{(r)}| - 1 ;\n\nTerminal function reconstruction:\n\nquery a_0^{(r)}, \\cdots, a_d^{(r)} ;(a total of d + 1 \\le 2^{\\eta} queries)\n\nlet P'(X) \\triangleq \\sum_{j \\ge 0} a_j^{(r)}X^j ;\n\nlet f^{(r)} be the evaluation of P'(X) on L^{(r)} ; (notice f^{(r)} \\in \\text{RS}^{(r)} )\n\nRepeat l times: {\n\nSample uniformly random s^{(0)} \\in L^{(0)} and for i = 0, \\cdots, r - 1 let\n\ns^{i + 1} = q^{(i)}(s^{(i)})\n\nS^{(i)} be the coset of L_0^{(i)} in L^{(i)} that contains s^{(i)}\n\nFor i = 0, \\cdots, r - 1 ,\n\nquery f^{(i)} on all of S^{(i)}; (a total of 2\\eta queries)\n\ncompute P^{(i)}(X) \\triangleq \\text{interpolant}^{f^{(i)}|_{S^{(i)}}}; (notice \\deg(P^{(i)}) < 2^{\\eta})\n\nround consistency: If for some i \\in \\{ 0, \\cdots, r - 1\\} it holds that> \n>  f^{(i+1)}(s^{(i+1)}) \\neq P^{(i)}(x^{(i)})\n> \n>\n\nthen reject and abort;\n\n}\n\nReturn accept","type":"content","url":"/fri/bbhr18-fri#query-phase","position":13},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Main Properties of the FRI Protocol"},"type":"lvl2","url":"/fri/bbhr18-fri#main-properties-of-the-fri-protocol","position":14},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Main Properties of the FRI Protocol"},"content":"The following theorem gives the main properties of the FRI protocol, including Completeness, Soundness, Prover complexity, and Verifier complexity. In fact, the paper also provides a slightly simplified version, see [BBHR18b] Theorem 1.3, which can be proved by setting \\eta = 2 and l = 1 in the theorem below. Here we mainly explain this more complex version.\n\nTheorem 1 [BBHR18b, Theorem3.3] (Main properties of the FRI protocol). The following properties hold when the FRI protocol is invoked on oracle f^{(0)}:L^{(0)} \\rightarrow \\mathbb{F} with localization parameter \\eta and rate parameter \\mathcal{R} (and rate \\rho = 2^{- \\mathcal{R}}) such that \\rho |L^{(0)}| > 16 :\n\nCompleteness If f^{(0)} \\in \\text{RS}^{(0)} \\triangleq \\text{RS}[\\mathbb{F}, L^{(0)}, \\rho = 2^{- \\mathcal{R}}] and f^{(1)},  \\cdots, f^{(r)} are computed by the prover specified in the COMMIT phase, then the FRI verifier outputs accept with probability 1.\n\nSoundness Suppose \\delta^{(0)} \\triangleq \\Delta^{(0)}(f^{0}, \\text{RS}^{(0)}) > 0 . Then with probability at least1 - \\frac{3|L^{(0)}|}{\\mathbb{F}}\n\nover the randomness of the verifier during the COMMIT phase, and for any (adaptively chosen) prover oracles f^{(1)}, \\cdots, f^{(r)} the QUERY protocol with repetition parameter l outputs accept with probability at most\\left (1 - \\min \\left \\{\\delta^{(0)}, \\frac{1-3\\rho-2^{\\eta}/\\sqrt{|L^{(0)}|}}{4} \\right \\}\\right )^{l}\n\nConsequently, the soundness of FRI is at least\\textbf{s}^{-}(\\delta^{(0)}) \\triangleq 1 - \\left ( \\frac{3|L^{(0)}|}{|\\mathbb{F}|} + \\left (1 - \\min \\left \\{\\delta^{(0)}, \\frac{1-3\\rho-2^{\\eta}/\\sqrt{|L^{(0)}|}}{4} \\right \\}\\right )^{l} \\right).\n\nProver complexity The i^{th} step of commit phase can be computed by a parallel random access machine (PRAM) with concurrent read and exclusive write (CREW) in 2\\eta + 3 cycles — each cycle involves a single arithmetic operation in \\mathbb{F} — using 2|L^{(i)}| + \\eta processors and a total of 4|L^{(i)}| arithmetic operations over \\mathbb{F}.\nConsequently, the total prover complexity is at most 6|L^{(0)}| arithmetic operations, which can be carried out in at most 4 |L^{(0)}| cycles on a PRAM-CREW with 2n + 3 processors.\n\nVerifier complexity Verifier communication during the COMMIT phase equals \\textbf{r} field elements; query complexity (during QUERY phase) equals l 2^{\\eta} \\textbf{r} = l 2^{\\eta} \\left ( 1 + \\left \\lfloor \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta} \\right \\rfloor \\right ). On a PRAM with exclusive read and exclusive write (EREW) with l \\textbf{r}\\cdot2 \\eta processors, the verifier’s decision is obtained after 2\\eta + 3 + \\log l cycles and a total of l\\cdot \\textbf{r} \\cdot (6 \\cdot 2\\eta + 6 \\eta + 6) arithmetic operations in \\mathbb{F}.\n\nIn the second item, the Soundness conclusion, a parameter \\delta^{(0)} \\triangleq \\Delta^{(0)}(f^{0}, \\text{RS}^{(0)}) > 0 is given first. Here, \\Delta^{(0)}(f^{0}, \\text{RS}^{(0)}) is not actually the common relative Hamming distance. Below is the definition of this measure, along with an explanation of its relationship with the relative Hamming distance.","type":"content","url":"/fri/bbhr18-fri#main-properties-of-the-fri-protocol","position":15},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Block-wise Distance Measure","lvl2":"Main Properties of the FRI Protocol"},"type":"lvl3","url":"/fri/bbhr18-fri#block-wise-distance-measure","position":16},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Block-wise Distance Measure","lvl2":"Main Properties of the FRI Protocol"},"content":"Definition 2 [BBHR18b, Definition3.2] (Block-wise distance measure). Let \\mathcal{S} = \\{S_1, \\cdots, S_m\\} be a partition of a set S and \\Sigma be an alphabet. The relative \\mathcal{S}-Hamming distance measure on \\Sigma^{S} is defined for f, g \\in \\Sigma^{S} as the relative Hamming distance over \\Sigma^{S_1} \\times \\cdots \\times \\Sigma^{S_m} ,\\Delta^{\\mathcal{S}}(f,g) \\triangleq \\Pr_{i \\in [m]}[f|_{S_i} \\neq g|_{S_i}] = \\frac{|\\{i \\in [m] | f|_{S_i} \\neq g|_{S_i}\\}|}{m}.\n\nThus, for \\mathcal{F} \\subset \\Sigma^{S} let \\Delta^{\\mathcal{S}}(g,\\mathcal{F}) = \\min \\{ \\Delta^{\\mathcal{S}}(g,f) | f \\in \\mathcal{F}\\}.\n\nTo better understand this definition, in the FRI protocol, consider the block-wise distance on \\mathbb{F}^{L^{(i)}}, i.e., replace the set S in the above definition with L^{(i)} in step i of the FRI protocol, and replace the alphabet \\Sigma with \\mathbb{F}. In step i, we can determine the set L_0^{(i)}. L_0^{(i)} can actually be set as the kernel of the mapping q^{(i)}, which is the set of elements in L^{(i)} that are mapped to the identity element e in L^{(i+1)} by q^i, expressed mathematically asL_0^{(i)} = \\{x \\in L^{(i)} | q^{(i)}(x) = e\\}.\n\nThen the set L^{(i)} can be partitioned by the cosets of L_0^{(i)}. Assuming it is divided into m sets, the partition of L^{(i)} can be denoted as \\mathcal{S}^{(i)} = \\{L_0^{(i)}, \\cdots, L_{m-1}^{(i)}\\}. Then we briefly write\\Delta^{(i)}(f,g)  \\triangleq  \\Delta^{\\mathcal{S}^{(i)}}(f,g)\n\nFor two functions f,g : L^{(i)} \\rightarrow \\mathbb{F}, both with domain L^{(i)} and codomain \\mathbb{F}, this Block-wise distance now represents the ratio of the number of cosets in \\mathcal{S}^{(i)} where these two functions are not completely consistent. For example, in \\mathcal{S}^{(i)} = \\{L_0^{(i)}, \\cdots, L_{m-1}^{(i)}\\} (assuming m \\ge 2), if the function values of f and g are not completely identical only on the two sets L_0^{(i)} and L_1^{(i)}, i.e., f|_{L_0^{(i)}} \\neq g|_{L_0^{(i)}} and f|_{L_1^{(i)}} \\neq g|_{L_1^{(i)}}, and the functions f and g are completely consistent on the remaining cosets, then we can calculate \\Delta^{(i)}(f,g) = \\frac{2}{m}.\n\nThe above \\Delta^{(i)}(f, g) refers to the measure between two elements in \\mathbb{F}^{L^{(i)}}. Below, we explain the block-wise distance measure corresponding to an element f^{(i)} \\in \\mathbb{F}^{L^{(i)}} and a subset \\text{RS}^{(i)} \\subset \\mathbb{F}^{L^{(i)}} (\\text{RS}^{(i)} = RS[\\mathbb{F}, L^{(i)}, \\rho] is naturally a subset of \\mathbb{F}^{L^{(i)}}), expressed as\\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}) \\triangleq \\Delta^{\\mathcal{S}^{(i)}}(f^{(i)},\\text{RS}^{(i)}) = \\min \\{ \\Delta^{\\mathcal{S}^{(i)}}(f^{(i)},g^{(i)}) | g^{(i)} \\in \\text{RS}^{(i)}\\},\n\nIts meaning is to traverse all codewords g^{(i)} in the set \\text{RS}^{(i)}, calculate these \\Delta^{\\mathcal{S}^{(i)}}(f^{(i)},g^{(i)}), and the smallest value among them is \\Delta^{\\mathcal{S}^{(i)}}(f^{(i)},\\text{RS}^{(i)}).\nRegarding this Block-wise distance measure, an important inequality is1 - \\rho \\ge  \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)})  \\ge \\Delta_H(f^{(i)},\\text{RS}^{(i)}) \\tag{4}\n\nThis equation will be repeatedly used in the Soundness proof of FRI, which is quite important. Here’s its proof.\n\nProof: First, prove the left half of the inequality, i.e., 1 - \\rho \\ge  \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}). There always exists such a polynomial g^{(i)} \\in \\text{RS}^{(i)}, whose degree deg(g^{(i)}) < \\rho |L^{(i)}|, and at the same time \\Delta^{\\mathcal{(i)}}(f^{(i)},g^{(i)}) = 1 - \\rho. Below we explain the existence of g^{(i)}. We construct as follows:\nIn the partition set \\mathcal{S}^{(i)} = \\{L_0^{(i)}, \\cdots, L_{m-1}^{(i)}\\}, we can obtain the sequence of sets \\{ L_0^{(i)}, \\cdots, L_{m-1}^{(i)}\\} = \\{x_0, x_1, \\cdots, x_{|L^{(i)}| - 1}\\} in order. Continuously select the first \\rho |L^{(i)}| points \\{x_0,x_1, \\cdots, x_{\\rho |L^{(i)}| - 1}\\}, obtain the corresponding values of f^{(i)} for these points \\{f^{(i)}(x_0),f^{(i)}(x_1),\\cdots, f^{(i)}(x_{\\rho |L^{(i)}| - 1})\\}, and perform Lagrange interpolation with these point values to obtain a polynomial g^{(i)} of degree < \\rho |L^{(i)}|. It’s easy to see that g^{(i)} constructed in this way belongs to \\text{RS}^{(i)} = RS[\\mathbb{F}, L^{(i)}, \\rho]. At the same time, according to the previous construction, we find that on the set \\{L_0^{(i)}, \\cdots, L_{\\rho m - 1}^{(i)}\\} = \\{x_0, x_1, \\cdots, x_{\\rho |L^{(i)}| - 1} \\}, the function values of f^{(i)} and g^{(i)} are completely equal (here, \\rho |L^{(i)}| points exactly fully occupy \\rho m sets, without the situation where the last few points only occupy part of the last set, this is because \\rho and |L^{(i)}| are chosen to be powers of 2, which can be evenly divided). Then we can calculate\\Delta^{(i)}(f^{(i)}, g^{(i)}) = \\frac{|\\{j \\in [m] | f^{(i)}|_{L_j^{(i)}} \\neq g^{(i)}|_{L_j^{(i)}}\\}|}{m} = 1 - \\rho.\n\nTherefore, \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}) calculates the minimum value under the measure \\Delta^{(i)} between elements in \\text{RS}^{(i)} and f^{(i)}, which certainly won’t exceed the distance of the found g^{(i)} \\in \\text{RS}^{(i)}, thus proving the left half of the inequality 1 - \\rho \\ge  \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}).\nNext, prove the right half of the inequality \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)})  \\ge \\Delta_H(f^{(i)},\\text{RS}^{(i)}). Assume \\Delta^{(i)}(f^{(i)}, g^{(i)} \\in \\text{RS}^{(i)}) = \\delta, without loss of generality, assume f^{(i)} and g^{(i)} are not completely consistent on the cosets \\{L_0^{(i)}, \\cdots, L_{\\delta m - 1}^{(i)}\\} = \\{x_0, \\cdots, x_{\\delta |L^{(i)}| - 1}\\}, and are completely consistent on the remaining set \\{L_0^{(i)}, \\cdots, L_{m-1}^{(i)}\\} \\backslash \\{L_0^{(i)}, \\cdots, L_{\\delta m - 1}^{(i)}\\}. Then considering all points on L^{(i)}, g^{(i)} is at most inconsistent with f^{(i)} on these \\delta |L^{(i)}| points \\{L_0^{(i)}, \\cdots, L_{\\delta m - 1}^{(i)}\\} = \\{x_0, \\cdots, x_{\\delta |L^{(i)}| - 1}\\}, thus indicating that \\Delta_H(f^{(i)},g^{(i)}) \\le \\delta. Furthermore, if we set \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}) = \\delta^*, we can deduce \\Delta_H(f^{(i)},\\text{RS}^{(i)}) \\le \\delta^* = \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)}). \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#block-wise-distance-measure","position":17},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Theorem 1 Completeness Proof"},"type":"lvl2","url":"/fri/bbhr18-fri#theorem-1-completeness-proof","position":18},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Theorem 1 Completeness Proof"},"content":"Below is an explanation of the completeness proof in Theorem 1. Let’s restate the completeness:\n\nCompleteness If f^{(0)} \\in \\text{RS}^{(0)} \\triangleq \\text{RS}[\\mathbb{F}, L^{(0)}, \\rho = 2^{- \\mathcal{R}}] and f^{(1)},  \\cdots, f^{(r)} are computed by the prover specified in the COMMIT phase, then the FRI verifier outputs accept with probability 1.\n\nCompleteness states that for an honest Prover, if the initial function f^{(0)} is in the \\text{RS}^{(0)} encoding space, then through the FRI’s COMMIT phase, a series of functions f^{(1)},  \\cdots, f^{(r)} will be produced, and the Verifier will definitely output accept after the QUERY phase.\n\nFirst, a recursive lemma is presented, which is then used to prove completeness. The lemma states that if f^{(i)} \\in \\text{RS}^{(i)} at step i, then in the COMMIT phase, the Verifier will randomly select x^{(i)} from \\mathbb{F} and send it to the Prover. The Prover uses this random number to construct the next function f^{(i+1)}_{f^{(i)}, x^{(i)}}. For any x^{(i)} in \\mathbb{F}, the constructed f^{(i+1)}_{f^{(i)}, x^{(i)}} is in the \\text{RS}^{(i+1)} space. The formal statement of the recursive lemma is as follows, and its proof will be explained later.\n\nLemma 1 [BBHR18b, Lemma 4.1] (Inductive argument). If f^{(i)} \\in \\text{RS}^{(i)} then for all x^{(i)} \\in \\mathbb{F} it holds that f^{(i+1)}_{f^{(i)}, x^{(i)}} \\in \\text{RS}^{(i+1)} .\n\nThe idea of the completeness proof is that in the QUERY phase, the Verifier mainly checks if round consistency holds in step 3. If it doesn’t hold for any i \\in \\{0, \\cdots, r - 1\\}, it will immediately output reject. Only when the checks pass for all i will it finally output accept. For i < r - 1, according to the construction process of f^{(i + 1)} in the COMMIT phase, round consistency will pass. For i = r - 1, based on the initial condition of completeness f^{(0)} \\in \\text{RS}^{(0)}, this theorem recursively shows that f^{(r)} \\in RS^{(r)}. Finally, based on this conclusion, it is shown that round consistency will also pass in the QUERY phase, and thus the Verifier will definitely output accept. The specific completeness proof is as follows.\n\nProof of Completeness in Theorem 1: For an honest Prover, for any function f^{(i)}, in step 2 of the COMMIT phase, for any i < r - 1, constructf_{f^{(i)}, x^{(i)}}^{(i + 1)} (y) \\triangleq P_y^{(i)}(x^{(i)}).\n\nBased on this construction, round consistency will definitely pass in step 3 of the QUERY phase, i.e.,f^{(i+1)}(s^{(i+1)}) = P^{(i)}(x^{(i)})\n\nholds.\n\nWe only need to prove that round consistency also passes for i = r - 1. From the completeness assumption, we know f^{(0)} \\in \\text{RS}^{(0)}, and by Lemma 1 recursively, we get f^{(r)} \\in RS^{(r)}. Then there must exist a polynomial P^{(r)}(X) of degree <\\rho |L^{(r)}| such that f^{(r)}(X) and P^{(r)}(X) are completely consistent on L^{(r)}. Therefore, the Prover will send d + 1 = \\rho |L^{(r)}| coefficients \\langle a_0^{(r)}, \\cdots, a_d^{(r)} \\rangle of P^{(r)}(X) in step 3 of the COMMIT phase. The Verifier will construct P'(X) \\triangleq \\sum_{j \\le d} a_j^{(r)}X^j based on the d + 1 coefficients sent in the “Terminal function reconstruction” stage of the QUERY phase, and then obtain the function f'^{(r)} based on P'(X). The function f'^{(r)} is the evaluation of P'(X) on L^{(r)}. We can deduce that f'^{(r)}|_{L^{(r)}} = P'(X) = P^{(r)}(X) = f^{(r)}|_{L^{(r)}}. Naturally, it will pass the round consistency check for the i = r - 1 round, i.e.,f'^{(r)}(s^{(i+1)}) = P^{(r-1)}(x^{i})\n\nThus, it is proved that the Verifier will definitely output accept. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#theorem-1-completeness-proof","position":19},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Introduction of Proposition 1","lvl2":"Theorem 1 Completeness Proof"},"type":"lvl3","url":"/fri/bbhr18-fri#introduction-of-proposition-1","position":20},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Introduction of Proposition 1","lvl2":"Theorem 1 Completeness Proof"},"content":"Before proving Lemma 1, we first present an important proposition, and then use this proposition to prove Lemma 1. In the following proposition, lowercase letters x, y are used to represent elements in the field, and uppercase letters X,Y are used to represent variables.\n\nClaim 1 [BBHR18b, Claim 4.2].  For every f^{(i)}: L^{(i)} \\rightarrow \\mathbb{F} there exists Q^{(i)}(X,Y) \\in \\mathbb{F}[X,Y]  satisfying\n\nf^{(i)}(x) = Q^{(i)}(x,q^{(i)}(x)) for all x \\in L^{(i)}\n\n\\deg_X(Q^{(i)}) < |L_0^{(i)}|\n\nIf f^{(i)} \\in RS[\\mathbb{F},L^{(i)},\\rho] then \\deg_Y(Q^{(i)}) < \\rho |L^{(i+1)}|\n\nThis proposition is quite important for understanding the FRI protocol. Vitalik gave a specific example in the “A First Look at Sublinearity” section of his blog post \n\nSTARKs, Part II: Thank Goodness It’s FRI-day, which already shows the prototype of the FRI protocol. Let’s revisit this example from the perspective of Proposition 1.\n\nAssume the size of the finite field L is N = 10^9, and let the polynomial f(X): L \\rightarrow \\mathbb{F} have a degree < 10^6, so we have f \\in RS[\\mathbb{F}, L, \\rho = 10^{-3}]. According to Proposition 1, there must exist a bivariate polynomial g(X,Y) \\in \\mathbb{F}[X,Y] satisfying:\n\nFor \\forall x \\in L, we have g(x,q(x)) = f(x), where q(x) = x^{1000}\n\n\\deg_X(g) < |L_0| = 10^3\n\nSince f \\in RS[\\mathbb{F}, L, \\rho = 10^{-3}], we have \\deg_Y(g) < \\rho |L^{(1)}| = 10^{-3} \\times 10^6 = 10^3\n\nNow the Prover wants to prove to the Verifier that the degree of f(x) is indeed less than \n\n106. The article uses intuitive geometric figures to illustrate the proof process.\n\nIn the figure, the horizontal direction of the square represents the variable X, with a range of L, totaling \n\n109 elements, while the vertical direction represents the variable Y, with a range of \\{x^{1000} | x \\in L\\}. A point (x,y) in the square corresponds to the calculated value of g(x,y). For points (x, y) on the diagonal of the square, satisfying x = y, we have g(x,y) = g(x, x^{1000}) = f(x).\n\nThe proof process is as follows:\n\nThe Prover commits to all points in the square regarding the evaluation of g(X,Y), for example, using a Merkle tree for commitment.\n\nThe Verifier randomly selects about a few dozen rows and columns. For each selected row or column, the Verifier will request samples of about 1010 points, ensuring that one of the required points in each case is on the diagonal. For example, if the Verifier selects the 5th column, then x = x_4, and 1010 sample points need to be selected. The x-coordinate of these points is already determined, so only the y-coordinate needs to be randomly selected. By selecting y = x_4^{1000} in the y-coordinates, it ensures that the point (x_4,x_4^{1000}) is on the diagonal.\n\nThe Prover replies with the values of g(x,y) corresponding to the points requested by the Verifier, along with the corresponding Merkle branches, proving that they are part of the data originally committed by the Prover.\n\nThe Verifier checks if the Merkle branches match, and for each row or column, the Verifier verifies whether the points provided by the Prover really correspond to a polynomial of degree <1000. The Verifier can verify this by interpolating these points.\n\nThe original text mentions:\n\nThis gives the verifier a statistical proof that (i) most rows are populated mostly by points on degree <1000 polynomials, (ii) most columns are populated mostly by points on degree <1000 polynomials, and (iii) the diagonal line is mostly on these polynomials. This thus convinces the verifier that most points on the diagonal actually do correspond to a degree <1,000,000 polynomial.\n\nThese points and conclusions can be linked to the three items given in Proposition 1:\n\nFor most rows, corresponding to polynomials of degree <1000, which indicates that \\deg_X(g) < 1000.\n\nFor most columns, corresponding to polynomials of degree <1000, which indicates that \\deg_Y(g) < 1000.\n\nThe diagonal is mainly composed of points on these polynomials, which indicates that the values of these points satisfy g(x,x^{1000}).\n\nThis also shows that most points (x, x^{1000}) on the diagonal correspond to a polynomial of degree <10^6, and because f(x) = g(x,x^{1000}), it convinces the Verifier that the degree of the polynomial f(X) is < 10^6.\n\nIn summary, if we want to prove that the degree of a polynomial f(X) is less than a certain value, according to Proposition 1, there must exist a bivariate polynomial g(X,Y) that can be associated with f(X). First, we have f(x) = g(x,q(x)), and the remaining two conclusions are about the degrees \\deg_X(g) and \\deg_Y(g) of g(X,Y), which correspond to the degrees of the polynomials represented by the horizontal and vertical lines in the figure, respectively. In fact, the above steps can be recursively applied, which corresponds to the “And Even More Efficiency” section in the article, describing the process of the FRI protocol.\n\nBelow is the proof of Proposition 1.\n\nProof of Proposition 1: Let P^{(i)} = \\text{interpolant}^{f^{(i)}}, i.e., interpolate the function f^{(i)} on L^{(i)} to obtain the polynomial P^{(i)}. Let \\mathbb{F}[X,Y] denote the bivariate polynomial ring over the finite field \\mathbb{F}; first sort the monomials by their total degree, then by their X-degree. LetQ^{(i)}(X,Y) = P^{(i)}(X) \\qquad \\text{mod} \\; Y - q^{(i)}(X)\n\nbe the remainder of P^{(i)}(X) divided by Y - q^{(i)}(X). From this definition, we can deduce that there must exist a quotient R(X,Y) \\in \\mathbb{F}[X,Y] such thatP^{(i)}(X) = Q^{(i)}(X,Y) + (Y - q^{(i)}(X)) \\cdot R(X,Y).\n\nFor \\forall x \\in L^{(i)} and y = q^{(i)}(x), substituting into the rightmost term of the above equation, we get (Y - q^{(i)}(X)) \\cdot R(X,Y) = (y - q^{(i)}(x)) \\cdot R(x,y) = 0. Therefore, P^{(i)}(x) = Q^{(i)}(x,y) = Q^{(i)}(x,q^{(i)}(x)), and since P^{(i)}(X) is obtained by interpolating f^{(i)}(X) on L^{(i)}, we have f^{(i)}(x) = P^{(i)}(x) = Q^{(i)}(x, q^{i}(x)), which proves the first item in the proposition.\n\nFrom the sorting of monomials, we can deduce that the defined remainder Q satisfies\\deg_X(Q^{(i)}(X,Y)) < \\deg(q^{(i)}) = |L_0^{(i)}|,\n\nthus the second item of Proposition 1 holds.\n\nFinally, we prove the third item of Proposition 1. From the condition f^{(i)} \\in RS[\\mathbb{F},L^{(i)},\\rho], we can deduce \\deg(P^{(i)}) < \\rho |L^{(i)}|. According to the division rule and the monomial sorting rule, we get\\deg_Y(Q^{(i)}) = \\left \\lfloor \\frac{\\deg(P^{(i)})}{\\deg(q^{(i)})}\\right \\rfloor = \\left \\lfloor \\frac{\\deg(P^{(i)})}{|L_0^{(i)}|}\\right \\rfloor < \\left \\lfloor \\frac{\\rho |L^{(i)}|}{|L_0^{(i)}|}\\right \\rfloor = \\left \\lfloor \\rho |L^{(i+1)}|\\right \\rfloor \\le \\rho |L^{(i+1)}|.\n\nThus, the third item of Proposition 1 is proved. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#introduction-of-proposition-1","position":21},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Proof of Lemma 1","lvl2":"Theorem 1 Completeness Proof"},"type":"lvl3","url":"/fri/bbhr18-fri#proof-of-lemma-1","position":22},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Proof of Lemma 1","lvl2":"Theorem 1 Completeness Proof"},"content":"Using the notation from Proposition 1. From the third item of the proposition, we have \\deg_Y(Q^{(i)}) < \\rho \\cdot |L^{(i+1)}| for any x^{(i)}. Next, we prove\\forall y \\in L^{(i+1)} , f^{(i+1)}(y) = Q^{(i)}(x^{(i)}, y)\n\nIf this equation holds, it proves that \\deg(f^{(i+1)}) \\le \\deg_Y(Q^{(i)}) < \\rho \\cdot |L^{(i+1)}|, which demonstrates that \\deg(f^{(i+1)}) \\in \\text{RS}^{(i+1)}.\n\nTo prove this equation, first fix y \\in L^{(i+1)}, let S_y \\in \\mathcal{S}^{(i)} be the set satisfying q^{(i)}(S_y) = \\{y\\}, which is also the coset of L_0^{(i)} in L^{(i)}. From the construction of f^{(i+1)}, we knowf^{(i+1)}(y) = \\text{interpolant}^{f^{(i)}|_{S_y}}(x^{(i)}).\n\nFrom the first item of Proposition 1, we get\\forall x \\in S_y, \\quad f^{(i)}(x) = P^{(i)} = Q^{(i)}(x,y)\n\nFrom the second item of Proposition 1, we know that \\deg_X(Q^{(i)}) < |L_0^{(i)}| = |S_y|, so we can treat X as a formal variable and obtain\\text{interpolant}^{f^{(i)}|_{S_y}}(X) = Q^{(i)}(X,y)\n\nThen let X = x^{(i)}, the evaluations of the polynomials on both sides at x^{(i)} must be the same. Thus, we getf^{(i+1)}(y) = \\text{interpolant}^{f^{(i)}|_{S_y}}(x^{(i)}) = Q^{(i)}(x^{(i)},y)\n\nNaturally, for any y \\in L^{(i+1)}, we have\\forall y \\in L^{(i+1)} , f^{(i+1)}(y) = Q^{(i)}(x^{(i)}, y)\n\nThus, the proof is complete. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#proof-of-lemma-1","position":23},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Analysis of Soundness Proof in Theorem 1"},"type":"lvl2","url":"/fri/bbhr18-fri#analysis-of-soundness-proof-in-theorem-1","position":24},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"Analysis of Soundness Proof in Theorem 1"},"content":"This section mainly explains the proof idea of soundness in Theorem 1. First, we give several definitions used in the proof, then explain two important lemmas, and finally prove soundness based on these two lemmas.","type":"content","url":"/fri/bbhr18-fri#analysis-of-soundness-proof-in-theorem-1","position":25},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Round Consistency and Distortion Set","lvl2":"Analysis of Soundness Proof in Theorem 1"},"type":"lvl3","url":"/fri/bbhr18-fri#round-consistency-and-distortion-set","position":26},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Round Consistency and Distortion Set","lvl2":"Analysis of Soundness Proof in Theorem 1"},"content":"The difficulty in analyzing soundness lies in how to accurately estimate the probability that any malicious prover will pass the protocol through interaction with the verifier. To make an accurate estimation, we need to consider where errors might occur in the protocol process. If we can estimate the probability of error for all these error processes without any loss, and then comprehensively analyze, we can obtain the soundness. In this process, to accurately estimate these possible error situations, we need to accurately describe these estimates, that is, we need to quantify them. Below are some necessary definitions in this process.\n\nAt step i, given oracles for f^{(i)} and f^{(i+1)}, and the random number x^{(i)} given by the Verifier.\n\n❓ Question\n\nIs there a mistake in the paper here? Should it be changed to f^{(i-1)}?\n\nInner-layer distance  The inner-layer distance of the ith step is the \\Delta^{(i)}-distance of f^{(i)} from \\text{RS}^{(i)}.\\delta^{(i)} \\triangleq \\Delta^{(i)}(f^{i},\\text{RS}^{(i)})\n\nThis definition is the block-wise distance of step i mentioned earlier.\n\nRound error For i > 0, the ith round error set is a subset of L^{(i)}, defined as follows:A_{\\text{err}}^{(i)}\\left(f^{(i)},f^{(i-1)},x^{(i-1)}\\right) \\triangleq \\left \\{ y_S^{(i)} \\in L^{(i)} | \\text{interpolant} ^{f^{(i-1)|_S}}\\left(x^{(i-1)}\\right) \\neq f^{(i)}\\left(y_S^{(i)}\\right)\\right \\}\n\nThe round error set describes those elements in L^{(i)} where the Verifier will fail the round consistency test in the ith round. The corresponding probability is the ith round error \\text{err}^{(i)}.\\text{err}^{(i)}\\left(f^{(i)},f^{(i-1)},x^{(i-1)}\\right) \\triangleq \\frac{|A_{\\text{err}}^{(i)}|}{|L^{(i)}|}\n\nClosest codeword Let \\bar{f}^{(i)} denote the closest codeword to f^{(i)} in \\text{RS}^{(i)} under the \\Delta^{(i)}(\\cdot) measure. We know that the \\Delta^{(i)}(\\cdot) measure is a metric in the coset partition set \\mathcal{S}^{(i)} of L^{(i)}. Let \\mathcal{S}_B^{(i)} \\subset \\mathcal{S}^{(i)} denote the “bad” cosets in the partition \\mathcal{S}^{(i)} where f^{(i)} and the codeword \\bar{f}^{(i)} are inconsistent, i.e.,\\mathcal{S}_B^{(i)} = \\left\\{ S \\in \\mathcal{S}^{i} | f^{(i)}|_S \\neq \\bar{f}^{(i)}|_S \\right \\}\n\nPutting these “bad” cosets in \\mathcal{S}^{(i)} together forms the set D^{(i)} = \\cup_{S \\in \\mathcal{S}_B^{(i)}}S, which we can see is a subset of L^{(i)}, where each element is a “bad” coset.\n\nIf \\delta^{(i)} < (1-\\rho) /2, then according to the inequality about the block-wise distance \\Delta^{(i)} mentioned earlier, we can get\\Delta_H^{(i)} \\le \\delta^{(i)} < (1-\\rho) /2,\n\nAccording to the bound of relative Hamming distance, unique decoding is possible at this time. Based on f^{(i)}, a unique \\bar{f}^{(i)} can be decoded, so \\mathcal{S}_B^{(i)} is unique at this time, and thus \\Delta_H^{(i)} can also be uniquely determined.\n\nDistortion set For \\epsilon > 0, the distortion set of f^{(i)} isB \\left[ f^{(i)}; \\epsilon \\right ] \\triangleq \\left\\{ x^{(i)} \\in \\mathbb{F} | \\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) < \\epsilon \\right\\}\n\nNote that the measure used above is the relative Hamming distance. We can understand this distortion set as follows: we know that the Verifier will select a random number x^{(i)} from the finite field \\mathcal{F} and send it to the Prover. The Prover constructs the next f^{(i+1)} based on the x^{(i)} sent by the Verifier and f^{(i)}. Then we look at the relative Hamming distance between the constructed next f^{(i+1)} and \\text{RS}^{(i+1)}. If we give a value \\epsilon, we look at which x^{(i)} in \\mathbb{F} will cause the constructed f_{f^{(i)},x^{(i)}}^{(i+1)} to have a minimum relative Hamming distance from the encoding space \\text{RS}^{(i+1)} less than the given parameter \\epsilon. To understand further, it’s considering all x^{(i)} on the field \\mathbb{F}, looking at which f_{f^{(i)},x^{(i)}}^{(i+1)} will have a certain distance from the entire encoding space \\text{RS}^{(i+1)}, with this distance parameter being at most \\epsilon. According to the condition \\epsilon > 0, we know that f_{f^{(i)},x^{(i)}}^{(i+1)} has at least a positive distance to the encoding space, definitely not in the \\text{RS}^{(i+1)} space.\n\nSo what error situations does the distortion set consider? It starts from the perspective of the Verifier’s behavior, considering the situations where the Verifier’s random number selection process may lead to not being in the encoding space.","type":"content","url":"/fri/bbhr18-fri#round-consistency-and-distortion-set","position":27},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Soundness Proof Idea","lvl2":"Analysis of Soundness Proof in Theorem 1"},"type":"lvl3","url":"/fri/bbhr18-fri#soundness-proof-idea","position":28},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Soundness Proof Idea","lvl2":"Analysis of Soundness Proof in Theorem 1"},"content":"We just mentioned that the distortion set considers possible errors from the process of the Verifier selecting random numbers. Then another perspective is the errors produced by the Prover in the construction process or during the COMMIT phase. So when we want to estimate soundness, we consider the following two situations where errors may occur:\n\nErrors caused by the Verifier selecting random numbers x^{(i)} from \\mathbb{F}.\n\nErrors caused by the Prover in the COMMIT phase.\n\nThis gives us the general idea for analyzing soundness: first estimate the probability of the first situation occurring, then assume the first situation doesn’t occur and estimate the probability of the second situation occurring. Finally, analyze the probability of both situations occurring simultaneously, which gives us the soundness we want.\n\nTo estimate the probability of the first situation, we first present a pair of lemmas about the distortion set, and then use these lemmas to prove soundness. These two lemmas consider different values of \\epsilon. We know that in the process of decoding a code, there will first be a relative Hamming distance parameter \\delta, and we consider two cases for the value of \\delta:\n\nIf \\delta \\le (1 - \\rho) / 2, then the decoding is unique, i.e., unique decoding.\n\nIf \\delta > (1 - \\rho) / 2, then the decoding results in a list, which is List decoding.\n\n📖 Notes\n\nTo better understand List Decoding, here’s its definition:\n\nDefinition 2 [Essential Coding Theory, Definition 7.2.1] Given 0 \\le \\rho \\le 1, L \\ge 1, a code C \\subseteq \\Sigma^n is (\\rho, L)-list decodable if for every received word \\vec{y} \\in \\Sigma^n,> |\\{c \\in C | \\Delta(\\vec{y},c) \\le \\rho n\\}| \\le L.\n>\n\nThis means that given a relative Hamming distance parameter \\delta in advance, as well as an upper limit L on the length of the list, for each received message \\vec{y}, in the encoding space C, as long as the relative Hamming distance between the codeword c and the message \\vec{y} is less than or equal to \\rho n, we consider c to be a valid decoding. At the same time, it requires that the number of valid encodings c that meet this distance condition does not exceed L, then we say this encoding is (\\rho n, L)-list decodable.\n\nAccording to the Hamming distance, there is such a property:\n\nProposition 1 [Essential Coding Theory, Proposition 1.4.2] Given a code C, the following are equivalent:\n\nC has minimum distance d \\ge 2,\n\nIf d is odd, C can correct (d−1)/2 errors.\n\nC can detect d − 1 errors.\n\nC can correct d − 1 erasures.\n\nSuppose the relative Hamming distance of C is \\delta, then \\delta = d / n. According to the above property, we know that for C, the proportion of encodings that can be corrected in the worst case is \\le \\frac{\\delta}{2}. And from the Singleton bound, we know,> \\delta \\le 1 - \\rho\n>\n\nTherefore, when the proportion of erroneous encodings is \\le \\frac{1-\\rho}{2}, these errors can be corrected, that is, unique encoding is possible.\n\nNow let’s formally present this pair of lemmas. Lemma 3 describes the case where the decoding radius exceeds the unique decoding bound (1-\\rho)/2, while Lemma 4 talks about the case where the decoding radius is less than (1-\\rho)/2, i.e., unique decoding.\n\nLemma 3 [BBHR18b, Lemma 4.3] (Soundness above unique decoding radius). For any \\epsilon \\le \\frac{2^{\\eta}}{|\\mathbb{F}|} and f^{(i)} such that \\delta^{(i)}>0\\Pr_{x^{(i)} \\in \\mathbb{F}} \\left[ x^{(i)} \\in B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ] \\right] \\le \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nLemma 4 [BBHR18b, Lemma 4.4] (Soundness within unique decoding radius). If \\delta^{(i)} < (1 - \\rho) / 2 then\\Pr_{x^{(i)} \\in \\mathbb{F}} \\left[ x^{(i)} \\in B \\left[ f^{(i)}; \\delta^{(i)} \\right ] \\right] \\le \\frac{|L^{(i)}|}{|\\mathbb{F}|}.\n\nMoreover, suppose that for i < r the sequences \\vec{f} = (f^{(i)}, \\cdots, f^{(r)}) and \\vec{x} = (x^{(i)}, \\cdots, x^{(r - 1)}) satisfy\n\nfor all j \\in \\{i, \\cdots, r\\} we have \\delta^{(j)} < \\frac{1-\\rho}{2}\n\nfor all j \\in \\{i, \\cdots, r - 1\\} we have \\bar{f}^{(j+1)} = f_{\\bar{f}^{(j)},x^{(j)}}^{(j+1)}\n\nfor all j \\in \\{i, \\cdots, r\\} we have x^{(j)} \\notin B[f^{(i)};\\delta^{(j)}]\n\nthen\\Pr_{s^{(i)} \\in D^{(i)}} \\left[ \\text{QUERY}(\\vec{f}, \\vec{x}) = \\text{reject} \\right] = 1\n\nand consequently\\Pr_{s^{(i)} \\in L^{(i)}} \\left[ \\text{QUERY}(\\vec{f}, \\vec{x}) = \\text{reject} \\right] \\ge \\frac{|D^{(i)}|}{| L^{(i)} |} = \\delta^{(i)}\n\nAccording to the definition of the distortion set, these two lemmas are talking about the probability of the Verifier selecting a random number x^{(i)} entering the distortion set under different decoding radii \\epsilon.\n\nThe conclusion following the “moreover” in Lemma 4 states that if the following conditions are met:\n\nFor all j \\in \\{i, \\cdots, r\\}, unique decoding is satisfied, that is, \\delta^{(j)} < \\frac{1 - \\rho}{2}.\n\nFor all j \\in \\{i, \\cdots, r - 1\\}, in \\text{RS}^{(j)}, select the closest codeword \\bar{f}^{(j)} to f^{(j)}, and the next function constructed with the random number x^{(i)} is f_{\\bar{f}^{(j)},x^{(j)}}^{(j+1)}, assuming it equals the closest codeword to f^{(j+1)} in \\text{RS}^{(j+1)}, i.e., satisfying \\bar{f}^{(j+1)} = f_{\\bar{f}^{(j)},x^{(j)}}^{(j+1)}.\n\nFor all j \\in \\{i, \\cdots, r - 1\\}, the random number x^{(j)} has not entered the distortion set, i.e., x^{(j)} \\notin B[f^{(i)};\\delta^{(j)}].\n\nThen the conclusion is that in the QUERY phase, if s^{i} is selected from the “bad” coset D^{(i)}, then the Verifier will definitely reject in the QUERY phase, i.e.,\\Pr_{s^{(i)} \\in D^{(i)}} \\left[ \\text{QUERY}(\\vec{f}, \\vec{x}) = \\text{reject} \\right] = 1\n\nThus, we can conclude that if s^{i} is selected from the entire L^{(i)}, the probability of the Verifier rejecting in the QUERY phase is at least \\frac{|D^{(i)}|}{| L^{(i)} |}, i.e.,\\Pr_{s^{(i)} \\in L^{(i)}} \\left[ \\text{QUERY}(\\vec{f}, \\vec{x}) = \\text{reject} \\right] \\ge \\frac{|D^{(i)}|}{| L^{(i)} |} = \\delta^{(i)}.\n\nNow that we’ve done the preparation work, let’s start proving the soundness of the protocol. So far, considering the possible error situations mentioned earlier, the soundness proof idea is as follows.\n\nIn the COMMIT phase, the Verifier may choose random numbers from the distortion set.\nNow the conclusions of Lemma 3 and Lemma 4 can help us estimate the probability of this happening. We call it a “bad” event when the Verifier selects a random number x^{(i)} from the distortion set. The Verifier will select a total of r random numbers, denoted as x^{(0)}, \\cdots, x^{(r-1)}. The events of selecting a random number from the distortion set in each round are denoted as E^{(0)}, \\cdots, E^{(r-1)}. We estimate the bound on the probability of some “bad” events occurring, which is at most\\frac{3|L^{(0)}|}{|\\mathbb{F}|}.\n\nIn the QUERY phase, the Verifier may reject.\nAssuming situation 1 does not occur, under this condition, we estimate the bound on the probability of the Verifier rejecting in the QUERY phase. The probability of rejection for a complete single round is at least\\min \\left \\{\\delta^{(0)}, \\frac{1-3\\rho-2^{\\eta}/\\sqrt{|L^{(0)}|}}{4} \\right \\}.\n\nConsidering both situations 1 and 2 occurring, and considering that the Verifier repeats the QUERY phase l times, we can obtain that the soundness of the FRI protocol is at least\\textbf{s}^{-}(\\delta^{(0)}) \\triangleq 1 - \\left ( \\frac{3|L^{(0)}|}{|\\mathbb{F}|} + \\left (1 - \\min \\left \\{\\delta^{(0)}, \\frac{1-3\\rho-2^{\\eta}/\\sqrt{|L^{(0)}|}}{4} \\right \\}\\right )^{l} \\right).\n\n🤔 Thoughts\n\nThere is indeed a situation where the Verifier selects some random numbers x^{(i)} that fall into the distortion set, and then the f_{f^{(i)},x^{(i)}}^{(i+1)} constructed from an f^{(i)} that is quite far from the RS code (assume \\epsilon far) and x^{(i)} does not maintain this distance, becoming smaller than before, i.e., distorted. In this case, if we run the QUERY step, we don’t have the ability to distinguish this situation. In other words, if a polynomial f_{f^{(i)},x^{(i)}}^{(i+1)} itself is not in \\text{RS}^{(i+1)}, and at the same time it is less than \\epsilon away from \\text{RS}^{(i+1)}, the Verifier has the ability to identify a polynomial that is \\epsilon far from the RS code space. Now it’s confused, lost, and believes that the Prover hasn’t cheated, because at this point it’s indeed less than a given parameter \\epsilon, and finally outputs accept.\n\n😎 Thoughts on the overall soundness probability derivation\n\nFirst, let’s consider a simplest ZK protocol (this example and image are from \n\nZero Knowledge Proofs - Introduction and History of ZKP)\n\n\nNow let’s consider the soundness analysis, which is about calculating the probability of the Verifier rejecting when the Prover provides a paper that is not two-colored. Here, assume the Prover uses a single-colored paper to interact with the Verifier, then each time the Prover has at most a 1/2 probability of passing, that is, the Verifier outputting accept. The final probability is shown in the figure below.\n\n\nIf we analyze the soundness, that is the probability of the Verifier rejecting, the probability of accepting is at most 1/2, so the probability of rejecting in one interaction is at least 1/2. If it is repeated k times, then the soundness is, for any P^*,> \\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{reject}|\\text{This page only contains 1 color}] \\ge 1 - \\left(\\frac{1}{2}\\right)^k\n>\n\nSimilar to the process of analyzing the soundness of this simple example, let’s look at the soundness of FRI. The probability in the simple example considers the probability that we can make the Verifier finally accept from the Verifier’s coin toss when inputting incorrect knowledge. For the FRI protocol, it’s about inputting an f^{(0)} \\notin \\text{RS}^{(0)}, it’s not in \\text{RS}^{(0)}, so how to measure it? We measure how far it is from \\text{RS}^{(0)} under the block-wise measure, i.e., \\delta^{(0)} \\triangleq \\Delta^{(0)}(f^{0}, \\text{RS}^{(0)}) > 0. Then we similarly consider that the Verifier’s random number toss gives the Prover a loophole to exploit. Because the Verifier threw some random numbers x^{(i)} that allowed the Prover to pass the protocol with an incorrect f^{(0)} \\notin \\text{RS}^{(0)}. That is, some “bad” events occurred, making the selected random numbers enter the distortion set, then the probability of the Verifier passing is at most \\frac{3|L^{(0)}|}{|\\mathbb{F}|}.\n\nThere’s another probability of the Verifier rejecting in the QUERY phase. In the above example, the Verifier directly judges whether the coin’ sent by the Prover is equal to the coin in the Verifier’s hand, which is direct and doesn’t introduce any randomness. If they are not equal, it will directly reject, without any chance to exploit loopholes. So now let’s examine if there’s anything that includes randomness in the QUERY phase of the FRI protocol? We’ll find that in the QUERY phase, the Verifier will select a random number s^{(0)} from L^{(0)}, and then perform calculations to check if the round consistency can pass. This process of introducing randomness with s^{(0)} is the key to estimating the probability of the Verifier rejecting in the QUERY phase.\n\nTo analyze more clearly, assume that the random numbers x^{(i)} selected by the Verifier in the COMMIT phase did not fall into the distortion set. Then let’s look at the randomness introduced in the QUERY phase, which is the selection of s^{(0)}. We can use the moreover conclusion of Lemma 4 to see that if all three conditions are met, it gives a possibility of rejection, which is at least \\delta^{(0)}, and then consider what is the minimum probability of the Verifier rejecting when these three conditions are not simultaneously satisfied. At this time, the sets A_{\\text{err}}^{(i)} and D^{(i)} will be used in the proof process.\n\nNow let’s formally give the Soundness proof.\n\nTheorem 1 Soundness Proof: Let \\epsilon = \\frac{2^{\\eta}}{|L^{(r/2)}|}; for simplicity, assume r is even (using \\epsilon = \\frac{2^{\\eta}}{|L^{\\left \\lceil r/2 \\right \\rceil }|} would yield the same bound, but its analysis would be a bit more complicated).\n\nPart I - A series of bad events The i-th bad event E^{(i)} is defined as follows:\n\nlarge distance: If \\delta^{(i)} \\ge \\frac{1 - \\rho}{2}, then E^{(i)} is the eventx^{(i)} \\in B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\n\nsmall distance: If \\delta^{(i)} < \\frac{1 - \\rho}{2}, then E^{(i)} is the eventx^{(i)} \\in B \\left[ f^{(i)}; \\delta^{(i)} \\right]\n\nAssuming the event E^{(i)} does not occur,\n\nIf \\delta^{(i)} < \\frac{1 - \\rho}{2}, then according to the definition of event E^{(i)} and the distortion set, we can getx^{(i)} \\notin B \\left[ f^{(i)}; \\delta^{(i)} \\right],\n\ni.e.,x^{(i)} \\notin \\left\\{ x^{(i)} \\in \\mathbb{F} | \\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) < \\delta^{(i)} \\right\\},\n\nTherefore, we can get\\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\delta^{(i)}\n\nAnd according to the Block-wise distance inequality, we get\\Delta^{(i+1)} \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\delta^{(i)}\n\nIf \\delta^{(i)} \\ge \\frac{1 - \\rho}{2}, then according to the definition of event E^{(i)} and the distortion set, we can get\\begin{aligned}\n\t\t\\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) & \\ge \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\\\\n\t\t& \\ge \\frac{1}{2} \\cdot \\left( \\frac{(1 - \\rho)}{2} (1 - \\epsilon) - \\rho \\right) \\\\\n\t\t& =  \\frac{(1 - \\rho)(1 - \\epsilon)}{4}  - \\frac{\\rho}{2} \\\\\n\t\t& = \\frac{1 - 3\\rho - \\epsilon + \\rho \\epsilon}{4} \\\\\n\t\t& \\ge \\frac{1 - 3\\rho - \\epsilon }{4}\n\t\\end{aligned}\n\nAccording to the Block-wise distance inequality, we get\n$$\\Delta^{(i+1)} \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\frac{1 - 3\\rho - \\epsilon }{4}\n\n$$\n\nLet \\delta_0 = \\frac{1 - 3\\rho - \\epsilon }{4}, then summarizing the above two cases, if the event E^{(i)} does not occur, we have\\Delta^{(i+1)} \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\min \\left \\{ \\delta^{(i)}, \\delta_0 \\right \\}\n\nPart II - Bounding the probability of a bad event occurring Through Lemma 3 and Lemma 4, and our choice of parameter \\epsilon, we have\\Pr \\left[E^{(i)}\\right] \\le \\max \\left\\{ \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}, \\frac{|L^{(i)}|}{|\\mathbb{F}|} \\right\\} = \\max \\left\\{ \\frac{|L^{(r/2)}|}{|\\mathbb{F}|}, \\frac{|L^{(i)}|}{|\\mathbb{F}|} \\right\\}\n\nSince |L^{i}| is decreasing, therefore, when i \\le r/2,\\max \\left\\{ \\frac{|L^{(r/2)}|}{|\\mathbb{F}|}, \\frac{|L^{(i)}|}{|\\mathbb{F}|} \\right\\} \\le \\frac{|L^{(i)}|}{|\\mathbb{F}|}\n\nWhen i > r/2,\\max \\left\\{ \\frac{|L^{(r/2)}|}{|\\mathbb{F}|}, \\frac{|L^{(i)}|}{|\\mathbb{F}|} \\right\\} \\le \\frac{|L^{(r/2)}|}{|\\mathbb{F}|}\n\nIn summary, we get\\max \\left\\{ \\frac{|L^{(r/2)}|}{|\\mathbb{F}|}, \\frac{|L^{(i)}|}{|\\mathbb{F}|} \\right\\} \\le \n\\begin{cases}\n\t\\frac{|L^{(i)}|}{|\\mathbb{F}|} & i \\le r/2\\\\\n\t\\frac{|L^{(r/2)}|}{|\\mathbb{F}|} &  i > r/2\n\\end{cases}\n\nTherefore, for events E^{(0)}, \\cdots, E^{(r-1)}, the probability that none of them occur is at least\\begin{aligned}\n\t\\Pr \\left[\\bigwedge_{i=1}^{r-1} \\neg E^{(i)} \\right] & \\ge 1 - \\left(\\sum_{i \\le r/2} \\frac{|L^{(i)}|}{|\\mathbb{F}|} + \\frac{r}{2}\\frac{|L^{(r/2)}|}{|\\mathbb{F}|} \\right) \\\\\n\\end{aligned}\n\nSince \\dim(L^{(i)}) = \\dim(L^{(0)}) - i\\eta, therefore|L^{(i)}| = 2^{\\dim(L^{(i)})} = 2^{\\dim(L^{(0)}) - i\\eta} = 2^{\\dim(L^{(0)})} \\cdot \\left(\\frac{1}{2^{\\eta}}\\right)^{i} = |L^{(0)}| \\left(\\frac{1}{2^{\\eta}}\\right)^{i}\n\nAccording to the definition of rr \\triangleq \\lfloor \\frac{k^{(0)} - \\mathcal{R}}{\\eta}\\rfloor\n\nAnd k^{(0)} = \\log |L^{(0)}|, we can getr = \\lfloor \\frac{k^{(0)} - \\mathcal{R}}{\\eta}\\rfloor \\le \\frac{k^{(0)} - \\mathcal{R}}{\\eta} = \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta}\n\nThen the probability inequality becomes\\begin{aligned}\n\t\\Pr \\left[\\bigwedge_{i=1}^{r-1} \\neg E^{(i)} \\right] & \\ge 1 - \\left(\\sum_{i \\le r/2} \\frac{|L^{(i)}|}{|\\mathbb{F}|} + \\frac{r}{2}\\frac{|L^{(r/2)}|}{|\\mathbb{F}|} \\right) \\\\\n\t& \\ge 1 - \\left(\\sum_{i \\le r/2} |L^{(0)}| \\left(\\frac{1}{2^{\\eta}}\\right)^{i} \\frac{1}{|\\mathbb{F}|} + \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2\\eta}\\frac{|L^{(r/2)}|}{|\\mathbb{F}|} \\right) \\\\\n\t& \\color{blue}{(\\text{Substitute}|L^{(i)}| = |L^{(0)}| \\left(\\frac{1}{2^{\\eta}}\\right)^{i}, r \\le \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta})}\\\\\n\t& \\ge 1 - \\left(\\frac{|L^{(0)}|}{|\\mathbb{F}|} \\sum_{i \\le r/2}  \\left(\\frac{1}{2^{\\eta}}\\right)^{i} + \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2\\eta} \\cdot \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }}\\frac{1}{|\\mathbb{F}|} \\right) \\\\\n\t& \\color{blue}{( \\text{Substitute}|L^{(r/2)}| = |L^{(0)}|\\left(\\frac{1}{2^{\\eta}}\\right)^{r/2} \\le |L^{(0)}|\\left(\\frac{1}{2^{\\eta}}\\right)^{\\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta}} = \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }} )} \\\\\n\t& \\ge 1 - \\left(\\frac{|L^{(0)}|}{|\\mathbb{F}|} \\sum_{i \\le r/2}  \\left(\\frac{1}{2^{\\eta}}\\right)^{i} + \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta} \\cdot \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }}\\frac{1}{|\\mathbb{F}|} \\right) \\\\\n\t& \\color{blue}{(\\text{Because}  \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} \\le \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta},\\text{and there's a negative sign in front, so the overall becomes smaller})} \\\\\n\t&  \\ge 1 - \\left(\\frac{|L^{(0)}|}{|\\mathbb{F}|} \\sum_{i \\le r/2}  \\left(\\frac{1}{2}\\right)^{i} + \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta} \\cdot \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }}\\frac{1}{|\\mathbb{F}|} \\right) \\\\\n\t& \\color{blue}{(\\text{Because} \\eta \\ge 1 \\Rightarrow 2^{\\eta} \\ge 2 \\Rightarrow \\frac{1}{2^{\\eta}} \\le \\frac{1}{2} \\Rightarrow \\sum_{i \\le r/2}  \\left(\\frac{1}{2^{\\eta}}\\right)^{i} \\le \\sum_{i \\le r/2}  \\left(\\frac{1}{2}\\right)^{i} )} \\\\\n\t& \\ge 1 - \\frac{1}{|\\mathbb{F}|}\\left(2|L^{(0)}| + \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta} \\cdot \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }}\\right) \\\\\n\t& \\color{blue}{(\\text{Because using the arithmetic sequence sum formula we get} \\sum_{i \\le r/2}  \\left(\\frac{1}{2}\\right)^{i} = \\frac{1 \\left(1 - (\\frac{1}{2})^{r/2 + 1}\\right)}{1 - \\frac{1}{2}} \\le \\frac{1}{2} )} \\\\\n\t& \\ge 1 - \\frac{1}{|\\mathbb{F}|}\\left(2|L^{(0)}| + \\log(\\rho|L^{(0)}| )  \\cdot \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta} }}\\right) \\\\\n\t& \\color{blue}{(\\text{Because} \\frac{\\log |L^{(0)}| - \\mathcal{R}}{\\eta} \\le \\log |L^{(0)}| - \\mathcal{R} = \\log |L^{(0)}| - \\log (1/\\rho) = \\log(\\rho|L^{(0)}| ))}\\\\\n\t& = 1 - \\frac{1}{|\\mathbb{F}|}\\left(2|L^{(0)}| + \\log(\\rho|L^{(0)}| )  \\cdot \\sqrt{|L^{(0)}|/\\rho} \\right) \\\\\n\t& \\color{blue}{(\\text{Because} \\frac{|L^{(0)}|}{2^{\\eta \\frac{\\log |L^{(0)}| - \\mathcal{R}}{2 \\eta}}} = \\frac{|L^{(0)}|}{2^{\\frac{\\log |L^{(0)}| - \\mathcal{R}}{2}}} = \\frac{|L^{(0)}|}{2^{\\frac{\\log (\\rho |L^{(0)}|)}{2}}} = \\frac{|L^{(0)}|}{2^{\\log(\\sqrt{\\rho |L^{(0)}|})}} =  \\frac{|L^{(0)}|}{\\sqrt{\\rho |L^{(0)}|}} = \\sqrt{|L^{(0)}|/\\rho})}\\\\\n\\end{aligned}\n\nLet f(x) = \\log_2x, g(x) = \\sqrt{x}, then when x > 16, f(x) < g(x). We can use sagemath to plot the images of these two functions for comparison.# Import SageMath's plotting functionality\nfrom sage.plot.plot import plot\n\n# Define functions\nf(x) = log(x,2)\ng(x) = x^(1/2)\n\n# Plot function images\np1 = plot(f, (x, -10, 30), color='blue', legend_label='log(x,2)')\np2 = plot(g, (x, -10, 30), color='red', legend_label='x^(1/2)')\n\n# Combine the two image objects and display\n(p1 + p2).show()\n\n📝 Proof that when x > 16, \\log_2x < \\sqrt{x}\nLet h(x) = f(x) - g(x) = \\log_2x - \\sqrt{x}, taking the derivative of h(x) we get> h'(x) = \\frac{1}{x\\ln2} - \\frac{1}{2\\sqrt{x}} = \\frac{2 \\sqrt{x}- x\\ln2}{2(\\ln2) \\cdot x\\sqrt{x}}\n>\n\nWe can see that when x > 16, h'(x) < 0, so h(x) < h(16) = 0, thus \\log_2x < \\sqrt{x}.\n\nAccording to the theorem condition \\rho |L^{(0)}| > 16, we have\\begin{aligned}\n\t\\rho |L^{(0)}| > 16 & \\Rightarrow \\log(\\rho |L^{(0)}|) < \\sqrt{\\rho |L^{(0)}|} \\\\\n\t& \\Rightarrow \\log(\\rho |L^{(0)}|)  \\cdot \\sqrt{|L^{(0)}| / \\rho}  < \\sqrt{\\rho |L^{(0)}|}  \\cdot \\sqrt{|L^{(0)}|/\\rho} \\\\\n\t& {\\color{blue} (\\text{Since} \\rho < 1, \\text{therefore} \\sqrt{|L^{(0)}| / \\rho} > \\sqrt{|L^{(0)}|} > 1, \\text{multiplying both sides by a number greater than 1 does not change the sign of the inequality} )} \\\\\n\t& \\Rightarrow \\log(\\rho |L^{(0)}|)  \\cdot \\sqrt{|L^{(0)}| \\rho}  < |L^{(0)}| \\\\\n\\end{aligned}\n\nSubstituting the above inequality into the probability inequality, we get\\begin{aligned}\n\t\\Pr \\left[\\bigwedge_{i=1}^{r-1} \\neg E^{(i)} \\right] & \\ge 1 - \\frac{1}{|\\mathbb{F}|}\\left(2|L^{(0)}| + \\log(\\rho|L^{(0)}| )  \\cdot \\sqrt{|L^{(0)}|/\\rho} \\right) \\\\\n\t& > 1 - \\frac{1}{|\\mathbb{F}|}\\left(2|L^{(0)}| + |L^{(0)}| \\right) \\\\\n\t& = 1 - 3\\frac{|L^{(0)}|}{|\\mathbb{F}|}.\n\\end{aligned}\n\nNext, let’s assume that no event E^{(i)} will occur and continue with the soundness proof.\n\nPart III - Bounding soundness when no bad events occur First, let’s recall the three assumptions for the sequences \\vec{f} = (f^{(i)}, \\cdots, f^{(r)}) and \\vec{x} = (x^{(i)}, \\cdots, x^{(r - 1)}) in Lemma 4:\n\nfor all j \\in \\{i, \\cdots, r\\} we have \\delta^{(j)} < \\frac{1-\\rho}{2}\n\nfor all j \\in \\{i, \\cdots, r - 1\\} we have \\bar{f}^{(j+1)} = f_{\\bar{f}^{(j)},x^{(j)}}^{(j+1)}\n\nfor all j \\in \\{i, \\cdots, r\\} we have x^{(j)} \\notin B[f^{(i)};\\delta^{(j)}]\n\nSince we assumed that no bad event E^{(i)} will occur, assumption 3 always holds. Therefore, whether the three assumptions hold or not has the following four situations.\n\nNo.\n\nAssumption 1\n\nAssumption 2\n\nAssumption 3\n\nRemarks\n\n1\n\n✖️\n\n✔️\n\n✔️\n\n\n\n2\n\n✖️\n\n✖️\n\n✔️\n\n\n\n3\n\n✔️\n\n✖️\n\n✔️\n\n\n\n4\n\n✔️\n\n✔️\n\n✔️\n\nRejection probability is at least \\delta^{(0)}\n\nLet’s first analyze the situation of No. 4, which is the simplest because Lemma 4 has already given that when all three assumptions are satisfied, the probability of the Verifier rejecting is at least \\delta^{(0)}.\n\nNext, let’s consider the situations of No. 1 and No. 2 together. In this case, assumption 1 is not satisfied, and assumption 2 may or may not be satisfied. So overall, it’s that assumption 1 is not satisfied, i.e.,\\delta^{(j)} \\ge \\frac{1-\\rho}{2}\n\nFinally, let’s consider the situation of No. 3, where the conditions are\\delta^{(j)} < \\frac{1-\\rho}{2} \\text{ and }\\bar{f}^{(j+1)} \\neq f_{\\bar{f}^{(j)},x^{(j)}}^{(j+1)}\n\nIn summary, there exist some i\\in \\{0, \\cdots, r- 1\\} where one of the following two conditions holds:\n\n\\delta^{(i)} \\ge \\frac{1-\\rho}{2}\n\n\\delta^{(i)} < \\frac{1-\\rho}{2} \\text{ and }\\bar{f}^{(i+1)} \\neq f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}\n\nIgnoring notation, let i < r represent the largest integer satisfying one of the above two conditions. Note that at this point D^{(i+1)} is uniquely determined, because \\delta^{(i)} < \\frac{1-\\rho}{2}, so \\bar{f}^{(i+1)} is also unique. The following proposition states that the (i+1)-th message of an honest Prover is at least \\delta_0 far from \\bar{f}^{(i+1)} in relative Hamming distance.\n\nClaim 5 [BBHR18b, Claim  4.5].\\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}) \\ge \\delta_0\n\nIf the Prover knows f^{(i)} and x^{(i)}, it can honestly execute according to the method in the COMMIT phase to construct f_{f^{(i)},x^{(i)}}^{(i+1)}, while \\bar{f}^{(i+1)} represents the codeword in \\text{RS}^{i+1} closest to f^{(i+1)} under the \\Delta^{(i+1)}(\\cdot)-measure. At this time, the relative Hamming distance between f_{f^{(i)},x^{(i)}}^{(i+1)} and \\bar{f}^{(i+1)} is at least \\delta_0.\n\nProof: According to the previous analysis, let’s discuss two cases.\n\n\\delta^{(i)} \\ge \\frac{1 - \\rho}{2} holds. Since our assumption is that no E^{(i)} event occurs, we can get from the analysis in Part I that\\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\frac{1 - 3\\rho - \\epsilon }{4} = \\delta_0\n\nSince \\bar{f}^{(i+1)} represents the closest codeword to f_{f^{(i)},x^{(i)}}^{(i+1)} in \\text{RS}^{(i+1)}, therefore\\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}) = \\Delta_H \\left( f_{f^{(i)},x^{(i)}}^{(i+1)},\\text{RS}^{(i+1)}\\right) \\ge \\delta_0\n\nThus, the proposition holds.\n\n\\delta^{(i)} < \\frac{1-\\rho}{2} \\text{ and }\\bar{f}^{(i+1)} \\neq f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} holds. To simplify the description, let g = f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}. Because \\bar{f}^{(i)} \\in \\text{RS}^{(i)}, we can get g = f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} \\in \\text{RS}^{(i+1)} from Lemma 1. At the same time, obviously \\bar{f}^{(i+1)} \\in \\text{RS}^{(i+1)}. From \\text{RS}^{(i+1)} = \\text{RS}^{(i+1)}[\\mathbb{F},L^{(i+1)},\\rho], by the MDS property of RS code (relative Hamming distance equals 1 - \\rho), we can get its relative Hamming distance \\Delta_H(\\text{RS}^{(i+1)}[\\mathbb{F},L^{(i+1)},\\rho]) = 1 - \\rho, so for two codes \\bar{f}^{(i+1)} and g in \\text{RS}^{(i+1)}, their relative Hamming distance is at least 1 - \\rho. By the triangle inequality, we get1 - \\rho \\le \\Delta_H(\\bar{f}^{(i+1)}, g) \\le \\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}) + \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)},g)\n\nFrom the assumption \\delta^{(i)} < \\frac{1-\\rho}{2} and the inequality between block-wise measure and relative Hamming distance that we proved earlier, we get\\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)},g) \\le \\Delta^{(i)}(f_{f^{(i)},x^{(i)}}^{(i+1)},g) = \\delta^{(i)} < \\frac{1-\\rho}{2}\n\nRearranging the above triangle inequality, we can get\\begin{aligned}\n\t\t\\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}) & \\ge \\Delta_H(\\bar{f}^{(i+1)}, g) - \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)},g) \\\\\n\t\t& > (1 - \\rho) - \\frac{1-\\rho}{2} \\\\\n\t\t& = \\frac{1-\\rho}{2} \\\\\n\t\t& = \\frac{2-2\\rho}{4} \\\\\n\t\t& > \\frac{2-2\\rho - (1 + \\rho + \\epsilon)}{4} \\\\\n\t\t& = \\frac{1-3\\rho - \\epsilon}{4}  \\\\\n\t\t& = \\delta_0\n\t\\end{aligned}\n\nThus, the proposition holds.\n\nIn conclusion, the proposition is proved. \n\n\\Box\n\nThe next proposition is\n\nClaim 6 [BBHR18b, Claim 4.6].\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|} \\ge \\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}).\n\nProof: From the definition of D^{(i+1)}, we can get that for all x \\notin D^{(i+1)},\\bar{f}^{(i+1)}(x) = f^{(i+1)}(x)\n\nAnd from the definition of A_{\\text{err}}^{(i+1)}, we can get that for all x \\notin A_{\\text{err}}^{(i+1)},f^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x)\n\nTherefore, for all x \\notin A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}, we have\\bar{f}^{(i+1)}(x) = f^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x)\n\nAccording to the definition of relative Hamming distance, we get\\Pr_{x \\in L^{(i+1)}}[\\bar{f}^{(i+1)}(x) \\neq f_{f^{(i)},x^{(i)}}^{(i+1)}(x)] = \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)})\n\nThen\\Pr_{x \\in L^{(i+1)}}[\\bar{f}^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x)] = 1- \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)})\n\nTherefore, for all x \\notin A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}, the following two equations must hold simultaneously:\n\n\\bar{f}^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x)\n\n\\bar{f}^{(i+1)}(x) = f^{(i+1)}(x)\n\nNow we have obtained that the probability of the first equation holding is 1- \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)}), so the probability of both equations holding simultaneously must not exceed the probability of only requiring the first equation to hold, i.e.,\\begin{aligned}\n\t\\Pr_{x \\in L^{(i+1)}}[x \\notin A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}] & = \\Pr_{x \\in L^{(i+1)}}[\\bar{f}^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x) = f^{(i+1)}(x)]\\\\\n\t& \\le \\Pr_{x \\in L^{(i+1)}}[\\bar{f}^{(i+1)}(x) = f_{f^{(i)},x^{(i)}}^{(i+1)}(x)] \\\\\n\t& = 1- \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)})\n\\end{aligned}\n\nTherefore\\begin{aligned}\n\t\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|} & = \\Pr_{x \\in L^{(i+1)}}[x \\in A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}]\\\\\n\t& = 1- \\Pr_{x \\in L^{(i+1)}}[x \\notin A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}] \\\\\n\t& \\ge 1 - (1 - \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)}))\\\\\n\t& = \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}^{(i+1)})\n\\end{aligned}\n\nThus, the proposition is proved. \n\n\\Box\n\nCombining the conclusions of Claim 5 and Claim 6, we get\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|} \\ge \\Delta_H(\\bar{f}^{(i+1)}, f_{f^{(i)},x^{(i)}}^{(i+1)}) \\ge \\delta_0\n\ni.e.,\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|} \\ge \\delta_0.\n\nNow consider the random number s^{(i+1)} used in the QUERY phase. First, according to the definition of A_{\\text{err}}^{(i+1)}, we know that if s^{(i+1)} \\in A_{\\text{err}}^{(i+1)}, then the Verifier will definitely reject in the QUERY phase. Next, we consider the probability of the Verifier rejecting in two cases based on different i.\n\nIf i + 1 = r, then since f^{(r)} \\in RS^{(r)}, according to the definition of D^{(i+1)}, at this time D^{(i+1)} = \\emptyset. In this case, if s^{(i+1)} \\in A_{\\text{err}}^{(i+1)}, the Verifier will definitely reject, and\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|}  = \\frac{|A_{\\text{err}}^{(i+1)}|}{|L^{(i+1)}|}  \\ge \\delta_0.\n\nIn this case, the probability of the Verifier rejecting is at least \\delta_0.\n\nIf i + 1 < r, through our previous selection of i, the selected i represents the largest integer satisfying one of the following two conditions\n\n\\delta^{(i)} \\ge \\frac{1-\\rho}{2}\n\n\\delta^{(i)} < \\frac{1-\\rho}{2} \\text{ and }\\bar{f}^{(i+1)} \\neq f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}\n\nThis indicates that the sequences \\vec{f} = (f^{(i + 1)}, \\cdots, f^{(r)}) and \\vec{x} = (x^{(i + 1)}, \\cdots, x^{(r - 1)}) after i are not empty and all satisfy the three conditions of Lemma 4. According to the conclusion of Lemma 4, if s^{(i+1)} \\in D^{(i+1)}, then it will definitely be rejected in the QUERY phase. If s^{(i+1)} \\in A_{\\text{err}}^{(i+1)}, the Verifier will also definitely reject. So this rejection probability is to see how large the union of these two sets is compared to the size of L^{(i+1)}. It has been proved that\\frac{|A_{\\text{err}}^{(i+1)} \\cup D^{(i+1)}|}{|L^{(i+1)}|}  \\ge \\delta_0.\n\nTherefore, in this case, the probability of rejection is also at least \\delta_0.\n\nCombining the above two cases, the probability of rejection is at least \\delta_0.\n\nCombining with the previous analysis of the probability of rejection when the three conditions of Lemma 4 are satisfied, we can conclude that under the condition that no bad events occur, that is, when the third condition of Lemma 4 always holds:\n\nWhen the first two conditions of Lemma 4 both hold, the probability of the Verifier rejecting is at least \\delta^{(0)}.\n\nWhen the first two conditions of Lemma 4 do not both hold, the probability of the Verifier rejecting is at least \\delta_0.\n\nSince\\begin{aligned}\n    \\frac{|L^{(r/2)}|}{\\sqrt{|L^{(0)}|}} & = \\frac{2^{k^{(0)} - \\eta \\cdot (r/2)}}{\\left(2^{k^{(0)} - \\eta \\cdot 0} \\right)^{\\frac{1}{2}}} \\\\\n    & = 2^{k^{(0)} - \\eta \\cdot (r/2) - \\frac{k^{(0)}}{2}} \\\\\n    & = 2^{\\frac{k^{(0)} - \\eta \\cdot r}{2}} \\\\\n    & {\\color{blue}(\\text{Since} k^{(0)} \\ge \\eta \\cdot r \\text{, then} k^{(0)} - \\eta \\cdot r \\ge 0)} \\\\\n    & \\ge 1\n\\end{aligned}\n\nTherefore|L^{(r/2)}| \\ge \\sqrt{|L^{(0)}|}\n\nThus we have\\epsilon = \\frac{2^{\\eta}}{|L^{(r/2)}|} \\le 2^{\\eta} / \\sqrt{|L^{(0)}|}\n\nNow let’s estimate \\delta_0, we get\\delta_0 = \\frac{1 - 3 \\rho - \\epsilon}{4} \\ge \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4}\n\nTherefore, if no bad events occur, the probability of the Verifier rejecting is at least\\min \\{\\delta^{(0)}, \\delta_0\\} \\ge \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\n\nCombining with the analysis in Part II, the probability of the Verifier selecting random numbers in the COMMIT phase is at least1 - \\frac{3|L^{(0)}|}{\\mathbb{F}},\n\nThen for any Prover’s oracle f^{(1)}, \\cdots , f^{(r)}, with the repetition parameter l in the QUERY protocol, the probability of the Verifier outputting accept is at most\\left(1 -  \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\\right)^l\n\nNow let’s analyze how to obtain the soundness of FRI. According to the definition of soundness:\n\nFor any \\text{P}^*, \\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{reject}|\\Delta^{(0)}(f^{(0)}, RS^{(0)}) = \\delta^{(0)}] \\ge \\textbf{s}^{-}(\\delta^{(0)}).\n\nThe main task of soundness analysis is to obtain the lower bound \\textbf{s}^{-}(\\delta^{(0)}) of the rejection probability. First, let’s consider, for any \\text{P}^*, what is the maximum probability that the Verifier will output accept. Through the above analysis, we can consider two cases:\n\nIf bad events E^{(i)} (i = 1, \\cdots, r - 1) occur, then the probability of the Verifier outputting accept is at most\\frac{3|L^{(0)}|}{\\mathbb{F}}\n\nIf no bad events E^{(i)} (i = 1, \\cdots, r - 1) occur, the probability of the Verifier outputting accept is at most\\left(1 -  \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\\right)^l\n\nTherefore, for any \\text{P}^*, we can obtain the upper bound of the probability of the Verifier outputting accept, that is\\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{accept}|\\Delta^{(0)}(f^{(0)}, RS^{(0)}) = \\delta^{(0)}] \\le \\frac{3|L^{(0)}|}{\\mathbb{F}} + \\left(1 -  \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\\right)^l\n\nThus, for any \\text{P}^*, we have\\begin{aligned}\n\t\\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{reject}|\\Delta^{(0)}(f^{(0)}, RS^{(0)}) = \\delta^{(0)}] & = 1 - \\Pr[\\left \\langle \\text{P}^* \\leftrightarrow \\text{V} \\right \\rangle = \\text{accept}|\\Delta^{(0)}(f^{(0)}, RS^{(0)})= \\delta^{(0)}] \\\\\n\t& \\ge 1 - \\left(\\frac{3|L^{(0)}|}{\\mathbb{F}} + \\left(1 -  \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\\right)^l  \\right)\n\\end{aligned}\n\nTherefore, we obtain that the soundness of FRI is at least\\textbf{s}^{-}(\\delta^{(0)}) \\triangleq 1 - \\left(\\frac{3|L^{(0)}|}{\\mathbb{F}} + \\left(1 -  \\min \\left \\{\\delta^{(0)}, \\frac{1 - 3 \\rho - 2^{\\eta} / \\sqrt{|L^{(0)}|}}{4} \\right \\}\\right)^l  \\right)\n\nThis completes the proof of soundness. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#soundness-proof-idea","position":29},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Unique Decoding Radius — Proof of Lemma 4","lvl2":"Analysis of Soundness Proof in Theorem 1"},"type":"lvl3","url":"/fri/bbhr18-fri#unique-decoding-radius-proof-of-lemma-4","position":30},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Unique Decoding Radius — Proof of Lemma 4","lvl2":"Analysis of Soundness Proof in Theorem 1"},"content":"Proof: Since \\delta^{(i)} < \\frac{1 - \\rho}{2}, the analysis mentioned in the definition of closest codeword has already pointed out that \\bar{f} and \\mathcal{S}_B(f^{(i)}) are unique. For a “bad” coset S in the set \\mathcal{S}_B(f^{(i)}), i.e., S \\in \\mathcal{S}_B(f^{(i)}), letX_S^{(i)} = \\left\\{ x^{(i)} \\in \\mathbb{F} | \\text{interpolant}^{f^{(i)}|_S}(x^{(i)}) = \\text{interpolant}^{\\bar{f}^{(i)}|_S}(x^{(i)}) \\right\\}\n\nThe set X_S^{(i)} represents those “misleading” x^{(i)} in \\mathbb{F}, meaning that the interpolation polynomials \\text{interpolant}^{f^{(i)}|_S}(x^{(i)}) = \\text{interpolant}^{\\bar{f}^{(i)}|_S}(x^{(i)}) are consistent, but since S comes from a “bad” coset, they are actually different low-degree polynomials, i.e., f^{(i)}|_S \\neq \\bar{f}^{(i)}|_S. In other words, these x^{(i)} “mislead” us, even though they are not the same polynomial, the polynomials interpolated using x^{(i)} on S are consistent. In the following, we will proveB\\left[ f^{(i)}, \\delta^{(i)} \\right] = \\bigcup_{S \\in \\mathcal{S}_B(f^{(i)})} X_S^{(i)}\n\nSince \\bar{f}^{(i)} \\in \\text{RS}^{(i)}, we get f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} \\in \\text{RS}^{(i+1)} from Lemma 1. For all S \\notin \\mathcal{S}_B(f^{(i)}), and y_S = q^{(i)}(S), since \\mathcal{S}_B(f^{(i)}) = \\left\\{S \\in \\mathcal{S}^{(i)}|f^{(i)}|_S \\neq \\bar{f}^{(i)}|_S\\right \\}, therefore for \\forall S \\notin \\mathcal{S}_B(f^{(i)}), we have f^{(i)}|_S = \\bar{f}^{(i)}|_S, naturally \\text{interpolant}^{f^{(i)}|_{S}} = \\text{interpolant}^{\\bar{f}^{(i)}|_S}, substituting x^{(i)} into the interpolation polynomial we get \\text{interpolant}^{f^{(i)}|_{S}}(x^{(i)}) = \\text{interpolant}^{\\bar{f}^{(i)}|_S}(x^{(i)}), then f_{f^{(i)},x^{(i)}}^{(i+1)}(y_S) = f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}(y_S). Since \\delta^{(i)} is less than the unique decoding radius \\frac{1 - \\rho}{2}, combining f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} \\in \\text{RS}^{(i+1)} with \\forall S \\notin \\mathcal{S}_B(f^{(i)}), we have f_{f^{(i)},x^{(i)}}^{(i+1)}(y_S) = f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}(y_S), we can conclude that f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} is the closest codeword in {RS}^{(i+1)} to f_{f^{(i)},x^{(i)}}^{(i+1)} under Hamming distance. Therefore \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\text{RS}^{(i+1)}) = \\Delta_H(f_{f^{(i)},x^{(i)}}^{(i+1)}, f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)}).\n\nIs the reasoning here for describing f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} as the closest codeword in {RS}^{(i+1)} to f_{f^{(i)},x^{(i)}}^{(i+1)} under Hamming distance correct? It feels like the explanation is not clear enough yet.\n\nAt the same time, these two functions have the same value at y_S if and only if one of the following two conditions holds:\n\nS \\notin \\mathcal{S}_B(f^{(i)})\n\nS \\in \\mathcal{S}_B(f^{(i)}) and x^{(i)} \\in X_S^{(i)}\n\nFrom this, we can conclude that these two functions have different values at y_S if and only if both of the following conditions hold:\n\nS \\in \\mathcal{S}_B(f^{(i)})\n\nS \\notin \\mathcal{S}_B(f^{(i)}) or x^{(i)} \\notin X_S^{(i)}\n\nWhen condition 1 holds, the first case in condition 2 S \\notin \\mathcal{S}_B(f^{(i)}) obviously doesn’t hold, so naturally x^{(i)} \\notin X_S^{(i)} holds, then we can get that these two functions have different values at y_S if and only ifS \\in \\mathcal{S}_B(f^{(i)}) \\text{ and } x^{(i)} \\notin X_S^{(i)}\n\ni.e.,x^{(i)} \\notin \\cup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}.\n\nThis indicates that f_{f^{(i)},x^{(i)}}^{(i+1)} is inconsistent with the (unique) closest \\text{RS}^{(i+1)}-codeword \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)} on all \\{y_S|S \\in \\mathcal{S}_B(f^{(i)})\\} if and only if x^{(i)} \\notin \\cup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}. ThenB\\left[ f^{(i)}, \\delta^{(i)} \\right] = \\left \\{ x^{(i)} \\in \\mathbb{F} | \\Delta_H\\left(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\text{RS}^{(i+1)}\\right) < \\delta^{(i)}  \\right \\} = \\left \\{ x^{(i)} \\in \\mathbb{F} | \\Delta_H\\left(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)}\\right) < \\delta^{(i)} \\right \\}\n\nWhile \\delta^{(i)} represents the ratio of |\\mathcal{S}_B(f^{(i)})| to the number of cosets of L_0^{(i)}, \\Delta_H\\left(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)}\\right) < \\delta^{(i)} means that f_{f^{(i)},x^{(i)}}^{(i+1)} and \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)} can be consistent on some \\{y_S|S \\in \\mathcal{S}_B(f^{(i)})\\}, which naturally is less than \\delta^{(i)}. And f_{f^{(i)},x^{(i)}}^{(i+1)} is consistent with the (unique) closest \\text{RS}^{(i+1)}-codeword \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)} on some \\{y_S|S \\in \\mathcal{S}_B(f^{(i)})\\} if and only if x^{(i)} \\in \\cup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}. Therefore, we can getB\\left[ f^{(i)}, \\delta^{(i)} \\right]  = \\left \\{ x^{(i)} \\in \\mathbb{F} | \\Delta_H\\left(f_{f^{(i)},x^{(i)}}^{(i+1)}, \\bar{f}_{f^{(i)},x^{(i)}}^{(i+1)}\\right) < \\delta^{(i)} \\right \\} = \\bigcup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}\n\nThis proves the equation we wanted to prove above, i.e.,B\\left[ f^{(i)}, \\delta^{(i)} \\right]  =  \\bigcup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}\n\nWith this equation, now let’s estimate the bound of the right side of the equation. In fact, \\text{interpolant}^{f^{(i)}|_{S}} and \\text{interpolant}^{\\bar{f}^{(i)}|_S} are two different polynomials of degree less than |S|, so |X_S| < |S|, otherwise if |X_S| \\ge |S|, then according to the definition of X_S, \\text{interpolant}^{f^{(i)}|_{S}} and \\text{interpolant}^{\\bar{f}^{(i)}|_S} would be consistent on more than |S| points, at which point the two interpolation polynomials would be the same, which contradicts that \\text{interpolant}^{f^{(i)}|_{S}} and \\text{interpolant}^{\\bar{f}^{(i)}|_S} are two different polynomials. Therefore\\left\\lvert B\\left[ f^{(i)}, \\delta^{(i)} \\right] \\right\\rvert = \\left\\lvert \\bigcup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)} \\right\\rvert < |S| \\cdot \\left\\lvert \\mathcal{S}_B\\left(f^{(i)}\\right) \\right\\rvert \\le |L^{(i)}|,\n\nThis proves the first inequality of Lemma 4\\Pr_{x^{(i)} \\in \\mathbb{F}} \\left[ x^{(i)} \\in B \\left[ f^{(i)}; \\delta^{(i)} \\right ] \\right] = \\frac{\\left\\lvert B\\left[ f^{(i)}, \\delta^{(i)} \\right] \\right\\rvert}{|\\mathbb{F}|}\\le \\frac{|L^{(i)}|}{|\\mathbb{F}|}.\n\nNow let’s consider the sequences \\vec{f} and \\vec{x} assumed in the Lemma. For simplicity, we assume that the evaluation of \\bar{f}^{(i)} on L^{(i)} yields the zero function, denote this function as \\mathbf{0}|_{L^{(i)}}. If this is not the case, we can obtain the zero function through f^{(i)} - \\bar{f}^{(i)}. Thenf_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} = f_{\\mathbf{0}|_{L^{(i)}},x^{(i)}}^{(i+1)} = \\mathbf{0}|_{L^{(i+1)}}\n\nFrom the second assumption of the lemma, we get \\bar{f}^{(i+1)} = f_{\\bar{f}^{(i)},x^{(i)}}^{(i+1)} = \\mathbf{0}|_{L^{(i+1)}}, thenf_{\\bar{f}^{(i+1)},x^{(i)}}^{(i+2)} = f_{\\mathbf{0}|_{L^{(i+1)}},x^{(i)}}^{(i+2)} = \\mathbf{0}|_{L^{(i+2)}}\n\nSimilarly, from the second assumption of the lemma, we get \\bar{f}^{(i+2)} = f_{\\bar{f}^{(i+1)},x^{(i)}}^{(i+2)} = \\mathbf{0}|_{L^{(i+2)}}, and so on, by induction, we can get for all j \\in \\{i, \\cdots, r\\}, \\bar{f}^{(j)} = \\mathbf{0}|_{L^{(j)}}. In particular, f^{(r)} = \\mathbf{0}|_{L^{(r)}}.\n\nConsider the sequence (s^{(i)}, \\cdots, s^{(r)}) in the QUERY phase, where s^{(i)} \\in D^{(i)}. Let j denote the largest integer such that s^{(j)} \\in D^{(j)} holds. Since s^{(i)} \\in D^{(i)}, this defined j can be obtained. By definition f^{(r)} = \\mathbf{0}|_{L^{(r)}}, we can get D^{(r)} = \\emptyset, so j < r. Combining with the third assumption of the lemma that for all j \\in \\{i, \\cdots, r\\}, x^{(j)} \\notin B[f^{(i)};\\delta^{(j)}] and the equation we proved earlierB\\left[ f^{(i)}, \\delta^{(i)} \\right]  =  \\bigcup_{S \\in \\mathcal{S}_B(f^{(i)})}X_S^{(i)}\n\nWe can get x^{(j)} \\notin \\bigcup_{S \\in \\mathcal{S}^{(j)}}X_S^{(j)}, so f_{f^{(j)},x^{(i)}}^{(j+1)}(s^{(j+1)}) \\neq 0. But from the definition of j, we know that j is the largest integer such that s^{(j)} \\in D^{(j)} holds, so for j+1 larger than j, we have s^{(j+ 1)} \\notin D^{(j+1)}. According to the definition of D^{(j+1)}, D^{(j+1)} = \\cup_{S \\in \\mathcal{S}_B^{(j+1)}}S, where S represents those “bad” cosets, i.e.,\\mathcal{S}_B^{(j+1)} = \\left\\{ S \\in \\mathcal{S}^{(j+1)} | f^{(j+1)}|_S \\neq \\bar{f}^{(j+1)}|_S \\right\\}\n\nHere’s the English translation of the provided text, maintaining all formulas and markdown formats:\n\nWhile s^{(j+ 1)} \\notin D^{(j+1)}, we have f^{(j+1)}(s^{(j+1)}) = \\bar{f}^{(j+1)}(s^{(j+1)}) = 0. Thus, we obtain:\n\nf_{f^{(j)},x^{(i)}}^{(j+1)}(s^{(j+1)}) \\neq 0\n\nf^{(j+1)}(s^{(j+1)}) = 0\n\nTherefore,f_{f^{(j)},x^{(i)}}^{(j+1)}(s^{(j+1)}) \\neq f^{(j+1)}(s^{(j+1)})\n\nThis indicates that the round consistency check will not pass in the QUERY phase, meaning the Verifier will definitely reject the sequence (s^{(i)}, \\cdots, s^{(r)}) in the QUERY phase. This proves that\\Pr_{s^{(i)} \\in D^{(i)}} \\left[ \\text{QUERY}\\left(\\vec{f}, \\vec{x}\\right)  = \\text{reject} \\right] = 1\n\nFrom the definitions of \\delta^{(i)} and the set D^{(i)}, we know that\\delta^{(i)} = \\frac{|D^{(i)}|}{|L^{(i)}|}\n\nTherefore,\\Pr_{s^{(i)} \\in L^{(i)}} \\left[ \\text{QUERY}(\\vec{f}, \\vec{x}) = \\text{reject} \\right] \\ge \\frac{|D^{(i)}|}{| L^{(i)} |} = \\delta^{(i)}\n\nThis completes the proof of Lemma 4. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#unique-decoding-radius-proof-of-lemma-4","position":31},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Exceeding the Unique Decoding Radius — Proof of Lemma 3","lvl2":"Analysis of Soundness Proof in Theorem 1"},"type":"lvl3","url":"/fri/bbhr18-fri#exceeding-the-unique-decoding-radius-proof-of-lemma-3","position":32},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl3":"Exceeding the Unique Decoding Radius — Proof of Lemma 3","lvl2":"Analysis of Soundness Proof in Theorem 1"},"content":"To prove Lemma 3, we need the following improved version of Lemma 4.2.18 from [Spi95].\n\nLemma 7 [BBHR18b, Lemma 4.7] Let E(X, Y) be a polynomial of degree (\\alpha m, \\delta n) and P(X, Y) a polynomial of degree ((\\alpha + \\epsilon)m, (\\delta + \\rho)n). If there exist distinct x_1, \\cdots, x_m such that E(x_i, Y) | P(x_i, Y) and y_1, \\cdots, y_n such that E(X, y_i) | P(X, y_i) and1 > \\max \\left\\{ \\delta + \\rho, 2\\alpha + \\epsilon + \\frac{\\rho}{\\delta} \\right\\}\n\nthen E(X, Y) | P(X, Y).\n\nProof of Lemma 3: We will prove the contrapositive of Lemma 3. First, let’s recall Lemma 3, which states that for any \\epsilon \\le \\frac{2^{\\eta}}{|\\mathbb{F}|} and f^{(i)}, if \\delta^{(i)} =  \\Delta^{(i)}(f^{(i)}, \\text{RS}^{(i)})>0, then\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} \\le \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}.\n\nTaking the contrapositive, we get:\n\nProposition 8 If for some \\epsilon \\ge \\frac{2^{\\eta}}{|\\mathbb{F}|}, we have\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nthen\\delta^{(i)} = \\Delta^{(i)}(f^{(i)}, \\text{RS}^{(i)}) \\le 0\n\nThis is equivalent to the following proposition:\n\nProposition 9 For some \\epsilon \\ge \\frac{2^{\\eta}}{|\\mathbb{F}|}, if\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nthen\\delta^{(i)} < \\delta\n\nLet’s prove that Propositions 8 and 9 are equivalent.\n\nProof: \\Rightarrow) Proof by contradiction. Assume the conclusion of Proposition 9 does not hold, then\\delta^{(i)} \\ge \\delta\n\nFrom the condition of Proposition 9, we can obtain\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} \\ge \\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nThis satisfies the condition of Proposition 8, therefore\\delta^{(i)} \\le 0\n\nThis contradicts the assumption \\delta^{(i)} \\ge \\delta. Thus, the conclusion of Proposition 9 holds.\n\n\\Leftarrow) Proof by contradiction. Assume the conclusion of Proposition 8 does not hold, then\\delta^{(i)} > 0\n\nThis means there exists a \\delta > 0 such that\\delta^{(i)} \\ge \\delta > 0\n\nThen\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} \\ge \\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|}\n\nFrom the condition of Proposition 8, we can obtain\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nFrom these two inequalities, we can see that \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|} is already a lower bound for\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|}\n\nSo we can conclude\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nThis satisfies the condition of Proposition 9, therefore we can obtain\\delta^{(i)} < \\delta\n\nThis contradicts the assumption, so the conclusion of Proposition 8 holds. \n\n\\Box\n\n🤔 Question\n\nIs there a more concise way to prove the equivalence of these two propositions?\n\nNow that we have proven that Proposition 9 is equivalent to Lemma 3, let’s prove Proposition 9. First, let’s fix some constants: Let n = |L^{(i+1)}|, \\alpha = \\frac{1}{2}(1 - \\epsilon - \\frac{\\rho}{\\delta}), \\delta' = \\delta \\alpha, B = B[f^{(i)};\\delta'], and m = |B|. From the definition of B, we know that for any x \\in B, \\Delta_H\\left(f_{f^{(i)},x}^{(i+1)}, \\text{RS}^{(i+1)}\\right) < \\delta'. Recalling the definition of the closest codeword, we know that \\bar{f}_{f^{(i)},x}^{(i+1)} \\in \\text{RS}^{(i+1)} is the closest codeword to f_{f^{(i)},x}^{(i+1)}. Since we are considering a decoding radius that exceeds the unique decoding radius, there might be multiple codewords that are equally close to f_{f^{(i)},x}^{(i+1)}. Here, we arbitrarily choose one of them.\n\nLet C(X,Y) denote a polynomial satisfying \\deg_X(C) < m, \\deg_Y(C) < \\rho n, and for each x \\in B, the polynomial C(x,Y) is consistent with \\bar{f}_{f^{(i)},x}^{(i+1)}(Y). The polynomial C(X,Y) exists because, by definition, \\bar{f}_{f^{(i)},x}^{(i+1)} is the evaluation of a polynomial of degree less than \\rho n. According to Proposition 1, let Q^{(i)} represent the polynomial related to f^{(i)}, i.e.,Q^{(i)}(X,Y) = P^{(i)}(X) \\qquad \\text{mod} \\; Y - q^{(i)}(X)\n\nFrom the second item of Proposition 1, we can obtain \\deg_X(Q^{(i)}) < |L_0^{(i)}|, and by definition, we know |L_0^{(i)}| = 2^{\\eta}. Since \\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert = \\left\\lvert B \\left[ f^{(i)}; \\delta' \\right ]\\right\\rvert = m, from the condition of Proposition 9\\frac{\\left\\lvert B \\left[ f^{(i)}; \\frac{1}{2} \\cdot \\left( \\delta^{(i)} (1 - \\epsilon) - \\rho \\right) \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nwe can obtain\\frac{m}{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nTherefore 2^{\\eta} < \\epsilon m, so \\deg_X(Q^{(i)}) < |L_0^{(i)}| = 2^{\\eta} < \\epsilon m. From the first item of Proposition 1, we can obtain that for any x \\in L^{(i)},f^{(i)}(x) = Q^{(i)}(x, q^{(i)}(x))\n\nAccording to the definition of the COMMIT phase, for each y \\in L^{(i+1)},\n\nLet S_y = \\{x \\in L^{(i)} | q^{(i)}(x) = y\\} be the coset of L_0^{(i)}, and map x to y through the mapping q^{(i)};\n\nP_y^{(i)}(X) \\triangleq \\text{interpolant}^{f^{(i)}|_{S_y}}\n\nf_{f^{(i)},x^{(i)}}^{(i+1)}(y) \\triangleq P_y^{(i)}(x^{(i)}).\n\nThen we can obtainP_y^{(i)}(X) \\triangleq \\text{interpolant}^{f^{(i)}|_{S_y}} = Q^{(i)}(X, y)\n\nThereforef_{f^{(i)},x^{(i)}}^{(i+1)}(y) \\triangleq P_y^{(i)}(x^{(i)}) = Q^{(i)}(x^{(i)}, y)\n\nNote that x^{(i)} \\in \\mathbb{F} here. If we denote the random number x^{(i)} as x, we can obtain that for any x \\in \\mathbb{F} and any y \\in L^{(i+1)},f_{f^{(i)},x}^{(i+1)}(y) = Q^{(i)}(x, y).\n\nFrom the definition of the distortion set, we getB \\left[ f^{(i)}; \\delta' \\right ] =  \\left\\{ x \\in \\mathbb{F} | \\Delta_H \\left( f_{f^{(i)},x}^{(i+1)},\\text{RS}^{(i+1)}\\right) < \\delta' \\right\\}\n\nand the condition of Proposition 9\\frac{\\left\\lvert B \\left[ f^{(i)}; \\delta' \\right ]\\right\\rvert }{|\\mathbb{F}|} > \\frac{2^{\\eta}}{\\epsilon |\\mathbb{F}|}\n\nThrough the above analysis, we have obtained\n\nFor all x \\in B, C(x,Y) is consistent with \\bar{f}_{f^{(i)},x}^{(i+1)}(Y).\n\nFor any x \\in \\mathbb{F} and any y \\in L^{(i+1)}, f_{f^{(i)},x}^{(i+1)}(y) = Q^{(i)}(x, y).\n\nThen for any x \\in B and any y \\in L^{(i+1)}, we have\n\nC(x,y) = \\bar{f}_{f^{(i)},x}^{(i+1)}(y)\n\nf_{f^{(i)},x}^{(i+1)}(y) = Q^{(i)}(x, y)\n\nSince \\bar{f}_{f^{(i)},x}^{(i+1)}(y) is the closest codeword to f_{f^{(i)},x}^{(i+1)}(y) in \\text{RS}^{(i+1)}, from the definition of B, we can obtain\\Delta_H \\left( f_{f^{(i)},x}^{(i+1)}(y),\\bar{f}_{f^{(i)},x}^{(i+1)}(y)\\right) < \\delta'\n\nThe relative Hamming distance considers the proportion of inconsistencies between f_{f^{(i)},x}^{(i+1)}(y) and \\bar{f}_{f^{(i)},x}^{(i+1)}(y), so\\Pr_{x \\in B, y \\in L^{(i+1)}} \\left[C(x,y) \\neq Q^{(i)}(x,y)\\right] = \\Pr_{x \\in B, y \\in L^{(i+1)}} \\left[\\bar{f}_{f^{(i)},x}^{(i+1)}(y) \\neq f_{f^{(i)},x}^{(i+1)}(y)\\right] < \\delta'.\n\n🤔 Why?\n\nHow to prove the existence of the following non-zero polynomial? How was it derived?\n\nBy constructing \\alpha \\delta \\ge \\delta', there exists a non-zero polynomialE(X,Y), \\qquad \\deg_X(E) \\le \\alpha m, \\deg_Y(E) \\le \\delta n\n\nsuch that E(x,y) = 0 at all points (x,y) where x \\in B, y \\in L^{(i+1)} and C(x,y) \\neq Q^{(i)}(x,y).\n\n📖 Notes\nRegarding the existence of the non-zero polynomial E(X,Y), this is how I understand it. We have already obtained> \\Pr_{x \\in B, y \\in L^{(i+1)}} \\left[C(x,y) \\neq Q^{(i)}(x,y)\\right]< \\delta'\n>\n\nAs shown in the figure below, since \\alpha \\delta \\ge \\delta', the existence of such a non-zero polynomial E(X,Y) is reasonable, with its value being 0 at these blue points in the figure.\n\nThe polynomial E is also known as the error locator polynomial [Sud92] because its roots cover the set of error locations, where Q is obtained from a low-degree polynomial.\n\nSince \\deg_Y(C) < \\rho |L^{(i+1)}| and \\deg_X(Q^{(i)}) < 2^{\\eta} < \\epsilon m, from [Spi95, Chapter 4], there exists a polynomial P(X,Y) satisfying\\deg_X(P) < (\\epsilon + \\alpha)m \\quad \\text{and} \\quad \\deg_Y(P) < (\\delta + \\rho)n \\tag{19}\n\nsuch that\\forall x \\in B, y \\in L^{(i+1)}, \\quad P(x,y)  = C(x,y) \\cdot E(x,y) = Q^{(i)}(x,y) \\cdot E(x,y) \\tag{20}\n\nholds.\n\n📖 Notes\nRegarding the existence of the polynomial P(X,Y), my current understanding is as follows. Let’s consider the reasonableness of its existence based on the degree of the polynomial:\nFirst, consider the variable X. Since \\deg_X(E) \\le \\alpha m and \\deg_X(Q^{(i)}) < \\epsilon m, it’s reasonable that there exists a P(X,Y) satisfying \\deg_X(P) < (\\epsilon + \\alpha)m, and> \\forall x \\in B, y \\in L^{(i+1)}, \\quad P(x,y) = Q^{(i)}(x,y) \\cdot E(x,y)\n>\n\nSimilarly, for the variable Y, since \\deg_Y(E) \\le \\delta n and \\deg_Y(C) < \\rho n, it’s reasonable that there exists a P(X,Y) satisfying \\deg_Y(P) < (\\delta + \\rho)n, and> \\forall x \\in B, y \\in L^{(i+1)}, \\quad P(x,y) = C(x,y) \\cdot E(x,y)\n>\n\n👩‍💻 TODO\n\nRefer to [Spi95, Chapter 4] to understand why such a polynomial would exist.\n\nWhy does P(x,y)  = C(x,y) \\cdot E(x,y) = Q^{(i)}(x,y) \\cdot E(x,y) hold? Isn’t it x \\in B, y \\in L^{(i+1)} and C(x,y) \\neq Q^{(i)}(x,y)?\n\nLet \\alpha' \\triangleq \\frac{\\deg_X(P)}{m} - \\epsilon and \\rho' \\triangleq \\frac{\\deg_Y(P)}{n} - \\delta, then from equation (19) we get\n\n🐞 Fix\nI believe the \\alpha \\triangleq \\frac{\\deg_X(P)}{m} - \\epsilon in the paper here should be changed to \\alpha'.\\deg_X(P) = (\\epsilon + \\alpha')m < (\\epsilon + \\alpha)m\n\nand\\deg_Y(P) = (\\delta + \\rho')n < (\\delta + \\rho)n\n\nFrom this, we can obtain \\alpha' < \\alpha and \\rho' < \\rho.\n\nFrom (19) and (20), we can obtain that for any row y \\in L^{(i+1)}, E(X,y)|P(X,y), and similarly, for any column x \\in B, E(x,Y)|P(x,Y). In other words, there exist distinct y_1, \\cdots, y_n \\in L^{(i+1)} such that E(X,y_i)|P(x_i,y_i) and distinct x_1, \\cdots, x_m \\in B such that E(x_i,Y)|P(x_i,Y).\n\nFrom equation (5)1 - \\rho \\ge  \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)})  \\ge \\Delta_H(f^{(i)},\\text{RS}^{(i)})\n\nwe can obtain \\delta + \\rho < 1.\n\n🤔 Why?\n\nHow is \\delta + \\rho < 1 obtained here? Is it because 1 - \\rho \\ge  \\Delta^{\\mathcal{(i)}}(f^{(i)},\\text{RS}^{(i)})  \\ge \\Delta_H(f^{(i)},\\text{RS}^{(i)}) > \\delta?\n\nNow we know \\Delta_H\\left(f_{f^{(i)},x}^{(i+1)}, \\text{RS}^{(i+1)}\\right) < \\delta' and \\alpha \\delta = \\delta', can it be derived from here?\n\nFrom the previously derived \\alpha' < \\alpha and \\rho' < \\rho and the definition of \\alpha, we can obtain2 \\alpha' + \\epsilon + \\frac{\\rho'}{\\delta} < 2 \\alpha + \\epsilon + \\frac{\\rho}{\\delta} = 2 \\cdot \\frac{1}{2}(1 - \\epsilon - \\frac{\\rho}{\\delta}) + \\epsilon + \\frac{\\rho}{\\delta} = 1.\n\nCombining the above derivations, we get\n\n\\delta + \\rho' < \\delta + \\rho < 1\n\n2 \\alpha' + \\epsilon + \\frac{\\rho'}{\\delta} < 1\n\nThus,1 > \\max \\left\\{ \\delta + \\rho', 2\\alpha' + \\epsilon + \\frac{\\rho'}{\\delta} \\right\\}\n\nAt this point, combining the above analysis, the degree of polynomial E(X,Y) is (\\alpha' m, \\delta n), the degree of polynomial P(X,Y) is ((\\alpha' + \\epsilon)m, (\\delta + \\rho')m), and there exist distinct x_1, \\cdots, x_m \\in B such that E(x_i,Y)|P(x_i,Y) and distinct y_1, \\cdots, y_n \\in L^{(i+1)} such that E(X,y_i)|P(x_i,y_i), while1 > \\max \\left\\{ \\delta + \\rho', 2\\alpha' + \\epsilon + \\frac{\\rho'}{\\delta} \\right\\}\n\n🤔 Question\n\nHow is the degree of E(X,Y) being (\\alpha' m, \\delta n) obtained? Previously, we obtained \\deg_X(E) \\le \\alpha m, \\deg_Y(E) \\le \\delta n.\n\nTherefore, the conditions and assumptions of Lemma 7 are satisfied. From the conclusion of the lemma, we can obtain E(X,Y)|P(X,Y), which are polynomials in the ring \\mathbb{F}[X,Y]. Let Q\\equiv P/E. We can obtain that for each row y \\in L^{(i+1)} where E(X,y) is non-zero, Q(X,y) = Q^{(i)}(X,y). Since \\deg_Y(E) < \\delta n, E(X,y) is zero in fewer than \\delta n rows, so the proportion of non-zero rows is at least 1 - \\delta. Therefore, the proportion of rows satisfying Q(X,y) = Q^{(i)}(X,y) is at least 1 - \\delta.\n\nAccording to Proposition 1, we know that Q^{(i)}(X,y) isQ^{(i)}(X,y) = P^{(i)}(X) \\qquad \\text{mod} \\; y - q^{(i)}(X)\n\nwhereP^{(i)} = \\text{interpolant}^{f^{(i)}}\n\nTherefore, f^{(i)} is consistent with the polynomial P^{(i)} of degree \\rho |L^{(i)}|. Let S_y = \\{x \\in L^{(i)}|q^{(i)}(x) = y\\} represent the coset of L_0^{(i)}. If Q^{(i)}(X,y) = Q(X,y) = P^{(i)}(X)|_{S_y} is satisfied on the coset S_y, then based on the fact that the proportion of rows satisfying Q(X,y) = Q^{(i)}(X,y) is at least 1 - \\delta, f^{(i)} is consistent with the polynomial P^{(i)} on more than a 1 - \\delta proportion of cosets S_y.\n\n👀 TODO\n\nThe understanding here is not very clear, and the explanation is not clear enough. To be improved. The original text:\n\nIn other words f^{(i)} agrees with some polynomial of degree \\rho |L^{(i)}| on more than a (1 - \\delta)-fraction of cosets of L^{(i)} in L^{(i)}.\n\nf^{(i)} is consistent with some polynomial of degree \\rho |L^{(i)}| on more than a 1 - \\delta proportion of cosets of L_0^{(i)} in L^{(i)}. According to the definition, \\delta^{(i)} represents the proportion of inconsistent cosets, so naturally, we can conclude that \\delta^{(i)} < 1 - (1 - \\delta) = \\delta, which completes the proof of the lemma. \n\n\\Box","type":"content","url":"/fri/bbhr18-fri#exceeding-the-unique-decoding-radius-proof-of-lemma-3","position":33},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"References"},"type":"lvl2","url":"/fri/bbhr18-fri#references","position":34},{"hierarchy":{"lvl1":"Dive into  BBHR18-FRI Soundness","lvl2":"References"},"content":"[BBHR18a] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. Scalable, transparent, and post-quantum secure computational integrity. Cryptology ePrint Archive, Report 2018/046, 2018. Available at \n\nhttps://​eprint​.iacr​.org​/2018​/046.\n\n[BBHR18b] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[RS92] Ronitt Rubinfeld and Madhu Sudan. Self-testing polynomial functions efficiently and over rational domains. In Proceedings of the Third Annual ACM/SIGACT-SIAM Symposium on Discrete Algorithms, 27-29 January 1992, Orlando, Florida., pages 23–32, 1992.\n\n[Spi95] Daniel A. Spielman. Computationally Efficient Error-Correcting Codes and Holographic Proofs. PhD thesis, MIT, 1995.\n\n[Sud92] Madhu Sudan. Efficient Checking of Polynomials and Proofs and the Hardness of Approximation Problems. PhD thesis, UC Berkeley, Berkeley, CA, USA, 1992. UMI Order No. GAX93-30747.\n\nVenkatesan Guruswami, Atri Rudra, and Madhu Sudan. Essential Coding Theory. \n\nhttps://​cse​.buffalo​.edu​/faculty​/atri​/courses​/coding​-theory​/book/, 2023.\n\nVitalik Buterin. STARKs, Part II: Thank Goodness It’s FRI-day. \n\nhttps://​vitalik​.eth​.limo​/general​/2017​/11​/22​/starks​_part​_2​.html, 2017.","type":"content","url":"/fri/bbhr18-fri#references","position":35},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness"},"type":"lvl1","url":"/fri/bciks20-proximity-gaps","position":0},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo  \n\nyu.guo@secbit.io\n\nThe paper [BCIKS20] improves the soundness of the FRI protocol in [BBHR18], mainly analyzing the case of batched FRI. This article will provide a detailed analysis of the content related to batched FRI soundness in the [BCIKS20] paper.","type":"content","url":"/fri/bciks20-proximity-gaps","position":1},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Introduction"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#introduction","position":2},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Introduction"},"content":"In the context of interactive proofs, distributed storage, and cryptography, various protocols have emerged that raise questions about the proximity of a linear code V \\subset \\mathbb{F}_q^n, where \\mathbb{F}_q is a finite field and V has minimum relative distance \\delta_V. These protocols assume access to oracles for a batch of vectors \\textbf{u} = \\{u_0, \\cdots, u_l \\} \\subset \\mathbb{F}_q^n, and their soundness requires each vector u_i to be close to V in relative Hamming distance. Furthermore, soundness deteriorates as a function of the largest distance between some vector u_i and the code V. Therefore, we aim to find protocols that minimize the number of queries to elements of \\mathbf{u} while maximizing the probability of identifying when some vector u_i is far from V.\n\n❓ Questions\n\nHow to understand the following sentence?\nFurthermore, soundness deteriorates as a function of the largest distance between some vector ui and the code V.\nDoes soundness decrease as the maximum distance between some vectors u_i and code V increases, meaning that the probability of the Verifier rejecting decreases?\n\nHow to clearly explain the decrease in soundness?\n\nDue to the linearity of V, a natural approach ([RVW13]) is to randomly sample a vector u' uniformly from span(\\textbf{u}) (i.e., linear combinations of elements in \\textbf{u}), and treat the distance \\Delta(u', V) between u' and V as a proxy for the maximum distance between some elements of \\textbf{u} and V. To prove soundness, we want that even if only one u_i is \\delta-far from all elements in V, the randomly chosen u' is also far from V.\n\nIn the following, \\Delta represents the relative Hamming distance. When \\Delta(u, v) \\le \\delta holds for some v \\in V, we say “u is \\delta-close to V”, denoted as \\Delta(u, V) \\le \\delta; otherwise, we say “u is \\delta-far from V”, denoted as \\Delta(u, V) > \\delta.\n\nRegarding this problem, some research results are:\n\n[AHIV17] If \\delta < \\delta_V /4, almost all u' \\in \\text{span}(\\mathbf{u}) are \\delta-far from V.\n\n[RZ18] Improved the above result to \\delta < \\delta_V /3.\n\n[BKS18] Improved to \\delta < 1 - \\sqrt[4]{1 - \\delta_V}.\n\n[BGKS20] Improved to \\delta < 1 - \\sqrt[3]{1 - \\delta_V}, but this bound is tight for RS codes, as it can be achieved when n = q.\n\n🤔 Thoughts\n\nWhy is the focus of research on increasing the upper bound of this \\delta? Regarding this question, my current thoughts are:\nThe upper bound of \\delta here is related to \\delta_V, and for RS code, \\delta_V = 1 - \\rho, which is essentially related to the code rate. So increasing the upper bound means lowering the code rate, which implies more redundancy. If with the same security or the same high probability of rejecting errors, fewer queries are needed. Or to put it another way, if for the same protocol, the number of queries is fixed, the larger \\delta is, the higher the probability of rejection, thus improving soundness.\n\nThe second point of the above analysis seems to contradict “Furthermore, soundness deteriorates as a function of the largest distance between some vector u_i and the code V.” This sentence says that the larger \\delta is, the smaller the soundness? How should this be understood?\n\nOne question we are currently concerned with is: For which codes and what range of \\delta does the following statement hold?\n\nIf some u^* \\in \\text{span}(\\mathbf{u}) is \\delta-far from V, then for almost all u' \\in \\text{span}(\\mathbf{u}), u' is also \\delta-far from V.\n\nOne of the main conclusions of the [BCIKS20] paper shows that when V is an RS code over a sufficiently large field (the field size is polynomially related to the block length of the code) and \\delta is less than the Johnson/Guruswami-Sudan list decoding bound, the above statement holds. Next, we call this a proximity gap.","type":"content","url":"/fri/bciks20-proximity-gaps#introduction","position":3},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Proximity Gaps"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#proximity-gaps","position":4},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Proximity Gaps"},"content":"First, let’s give the definition of Proximity Gaps.\n\nDefinition 1.1 [BCIKS20, Definition 1.1] (Proximity gap). Let P \\subset \\Sigma^n be a property and C \\subset 2^{\\Sigma^n} be a collection of sets. Let \\Delta be a distance measure on \\Sigma^n. We say that C displays a (\\delta, \\epsilon)-proximity gap with respect to P under \\Delta if every S \\in C satisfies exactly one of the following:\n\n\\Pr_{s \\in S} [\\Delta(s, P) \\le \\delta] = 1.\n\n\\Pr_{s \\in S} [\\Delta(s, P) \\le \\delta] \\le \\epsilon.\n\nWe call \\delta the proximity parameter and \\epsilon is the error parameter. By default, \\Delta denotes the relative Hamming distance measure.\n\nFor RS code, if V \\subset \\mathbb{F}^n is an RS encoding, corresponding to P in the above definition, and A \\subset \\mathbb{F}^n is an affine space, corresponding to S in the above definition, then either all elements in A are \\delta-close to V, or almost all elements in A are \\delta-far from V. In other words, there is no such affine space A where about half of the elements are close to V, but at the same time, the other half are far from V.\n\nAs shown in the figure below, A is an affine space, represented here by a line, and elements in the encoding space V are represented by black dots. Circles are drawn with these points as centers and \\delta as the radius. Then there are only two situations:\n\nAll elements on line A fall within the green circular area.\n\n\nOnly a few elements on the line fall within the green circular area.\n\nThe elements in A cannot be half inside the circular area and half outside, which is also the meaning of gap. It divides all the elements in A into exactly two cases, and these two cases form a huge gap based on the relative Hamming distance.\n\nIn the following, we use \\mathbb{F}_q to represent a finite field of size q, and \\text{RS}[\\mathbb{F}_q,\\mathcal{D},k] to represent an RS code with dimension k+1 and blocklength n = |\\mathcal{D}|, whose codewords are evaluated on \\mathcal{D} and are polynomials of degree \\le k. We use \\rho to represent the code rate, so \\rho = \\frac{k+1}{n}. \\delta represents the relative Hamming distance relative to the RS code, and \\epsilon represents the error parameter, which is the probability of a “bad event” occurring.\n\nBelow is the Proximity gaps theorem for RS code.\n\nTheorem 1.2 [BCIKS20, Theorem 1.2] (Proximity gap for RS codes). The collection C_{\\text{Affine}} of affine spaces in \\mathbb{F}_q^n displays a (\\delta, \\epsilon)-proximity gap with respect to the RS code V := \\text{RS}[\\mathbb{F}_q, \\mathcal{D}, k] of blocklength n and rate \\rho = \\frac{k+1}{n}, for any \\delta \\in (0, 1 - \\sqrt{\\rho}), and \\epsilon = \\epsilon(q, n, \\rho, \\delta) defined as the following piecewise function:\n\nUnique decoding bound: For \\delta \\in (0,\\frac{1 - \\rho}{2}], the error parameter \\epsilon is\\epsilon = \\epsilon_\\text{U} = \\epsilon_\\text{U}(q, n) := \\frac{n}{q} \\tag{1.1}\n\nList decoding bound: For \\delta \\in (\\frac{1 - \\rho}{2}, 1 - \\sqrt{\\rho}), setting \\eta := 1 - \\sqrt{\\rho} - \\delta, the error parameter \\epsilon is\\epsilon = \\epsilon_\\text{J} = \\epsilon_\\text{J}(q, n, \\rho, \\delta) := \\frac{(k+1)^2}{\\left(2 \\min \\left(\\eta, \\frac{\\sqrt{\\rho}}{20}\\right)^7\\right)q} = O \\left(\\frac{1}{(\\eta \\rho)^{O(1)}} \\cdot \\frac{n^2}{q} \\right) \\tag{1.2}\n\n🤔 Question\n\nThe larger \\delta is, the more elements may fall into the circular area, so \\epsilon_{\\text{J}} is larger than \\epsilon_{\\text{U}}. Is this the reason?","type":"content","url":"/fri/bciks20-proximity-gaps#proximity-gaps","position":5},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Correlated agreements"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#correlated-agreements","position":6},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Correlated agreements"},"content":"The main theorem proved in the paper is correlated agreement. For two vectors u_0, u_1 \\in \\mathbb{F}^{\\mathcal{D}} in \\mathbb{F}^{\\mathcal{D}}, we choose a random number z in \\mathbb{F}, and we are concerned with the distance between the space formed by u_0 + zu_1 after linear combination with z and V, that is, the one-dimensional affine space A = \\{u_0 + z u_1 : z \\in \\mathbb{F}\\}. The correlated agreement conclusion states that if there are enough elements in A that are close enough to the RS code space V (\\delta-close), then there must exist a non-trivial subdomain \\mathcal{D}' \\subset \\mathcal{D}, whose size is at least 1 - \\delta times the size of \\mathcal{D}, such that restricting u_0, u_1 to \\mathcal{D}', there are valid RS codes v_0, v_1 that agree with u_0, u_1 respectively on \\mathcal{D}'. We say that such a \\mathcal{D}' has the correlated agreement property, meaning that u_0, u_1 and elements in A not only have a large agreement with RS code respectively, but also share a common large agreement set. This result has two parameter ranges, one is the proximity parameter within the unique decoding range, and the other is the proximity parameter within the list decoding range.\n\nThe following presents correlated agreements for three situations. Combined with other conclusions about correlated agreement in the paper, they are shown in the table below.\n\n\n\nSpace U\n\n\\Delta_u(u,V)\n\n\\Delta_u(u,V) unique decoding\n\n\\Delta_u(u,V) list decoding\n\n\\text{agree}_{\\mu}(u,V)\n\nlines\n\n\\{u_0 + z u_1 : z \\in \\mathbb{F}\\}\n\nTheorem 1.4\n\nTheorem 4.1\n\nTheorem 5.1 & Theorem 5.2\n\n\n\nlow-degree parameterized curves\n\n\\text{curve}(\\mathbf{u}) = \\left\\{u_z: = \\sum_{i = 0}^{l}z^i \\cdot u_i  | z \\in \\mathbb{F}_q   \\right\\}\n\nTheorem 1.5\n\nTheorem 6.1\n\nTheorem 6.2\n\nTheorem 7.1 & Theorem 7.2 (More precise version of the Johnson bound)\n\naffine spaces\n\nu_0 + \\text{span}\\{u_1, \\cdots, u_l\\}\n\nTheorem 1.6\n\n\n\n\n\nTheorem 7.3 & Theorem 7.4 (More precise version of the Johnson bound)\n\nThe following three theorems correspond to the correlated agreement theorems for lines, low-degree parameterized curves, and affine spaces, respectively.\n\nTheorem 1.4  [BSCIK20, Theorem 1.4]  (Main Theorem - Correlated agreement over lines). Let V, q, n, k and \\rho be as defined in Theorem 1.2. For u_0, u_1 \\in \\mathbb{F}_q^{\\mathcal{D}}, if \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{z \\in \\mathbb{F}_q} [\\Delta(u_0 + z \\cdot u_1, V) \\le \\delta] > \\epsilon,\n\nwhere \\epsilon is as defined in Theorem 1.2, then there exist D' \\subset D and v_0, v_1 \\in V satisfying\n\nDensity: |D'|/|D| \\ge 1 - \\delta, and\n\nAgreement: v_0 agrees with u_0 and v_1 agrees with u_1 on all of D'.\n\nLet \\mathbf{u} = \\{u_0, \\ldots, u_l \\} \\subset \\mathbb{F}_q^{\\mathcal{D}}, then a parameterized curve of degree l is the set of vectors in \\mathbb{F}_q^{\\mathcal{D}} generated by \\mathbf{u} as follows,\\text{curve}(\\mathbf{u}) := \\left\\{u_z: = \\sum_{i = 0}^{l}z^i \\cdot u_i \\Bigg| z \\in \\mathbb{F}_q   \\right\\}\n\nTheorem 1.5 [BSCIK20, Theorem 1.5] (Correlated agreement for low-degree parameterized curves). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}}. If \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{u \\in \\text{curve}(u)} [\\Delta(\\mathbf{u}, V) \\le \\delta] > l \\cdot \\epsilon,\n\nwhere \\epsilon is as defined in Theorem 1.2, then there exist \\mathcal{D}' \\subset \\mathcal{D} and v_0, \\cdots, v_l \\in V satisfying\n\nDensity: |\\mathcal{D}'|/|\\mathcal{D}| \\ge 1 - \\delta, and\n\nAgreement: for all i \\in \\{0, \\cdots, l\\}, the functions u_i and v_i agree on all of \\mathcal{D}'.\n\nTheorem 1.6 [BSCIK20, Theorem 1.6] (Correlated agreement over affine spaces). Let V, q, n, k and \\rho be as defined in Theorem 1.2. For u_0, u_1, \\cdots, u_l \\in \\mathbb{F}_q^{\\mathcal{D}} let U = u_0 + \\text{span}\\{u_1, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} be an affine subspace. If \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{u \\in U} [\\Delta(u, V) \\le \\delta] > \\epsilon,\n\nwhere \\epsilon is as defined in Theorem 1.2, then there exist \\mathcal{D}' \\subset \\mathcal{D} and v_0, \\cdots, v_l \\in V satisfying\n\nDensity: |\\mathcal{D}'|/|\\mathcal{D}| \\ge 1 - \\delta, and\n\nAgreement: for all i \\in \\{0, \\cdots, l\\}, the functions u_i and v_i agree on all of \\mathcal{D}'.\n\nFurthermore, in the unique decoding regime \\delta \\in \\left(0, \\frac{1 - \\rho}{2}\\right], there exists a unique maximal \\mathcal{D}' satisfying the above, with unique v_i.","type":"content","url":"/fri/bciks20-proximity-gaps#correlated-agreements","position":7},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Correlated Weighted Agreement"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#correlated-weighted-agreement","position":8},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Correlated Weighted Agreement"},"content":"To analyze the soundness of the FRI protocol, we need a weighted version of Theorem 1.5.\n\nFor a given weight vector \\mu : \\mathcal{D} \\rightarrow [0,1], the (relative) \\mu-agreement between u and v is defined as\\text{agree}_{\\mu}(u,v) := \\frac{1}{|\\mathcal{D}|} \\sum_{x:u(x) = v(x)} \\mu(x).\n\nThat is, it looks at the proportion of agreement between u and v on \\mathcal{D} under the weight \\mu. If we let \\mu \\equiv 1, then\\text{agree}_{\\mu}(u,v) = \\frac{1}{|\\mathcal{D}|} \\sum_{x:u(x) = v(x)} 1 = 1 - \\frac{1}{|\\mathcal{D}|} \\sum_{x:u(x) \\neq v(x)} 1 = 1 - \\Delta(u,v).\n\nThe agreement between a word u and a linear code V is the maximum agreement between u and a codeword in V,\\text{agree}_{\\mu}(u,V) := \\max_{v \\in V} \\text{agree}_{\\mu}(u,v).\n\nThe weighted size of a subdomain \\mathcal{D}' \\subset \\mathcal{D} is defined as\\mu(\\mathcal{D}') := \\frac{1}{|\\mathcal{D}|}\\sum_{x \\in \\mathcal{D}'} \\mu(x).\n\nIf we define \\mathcal{D}' in the above definition as \\{x \\in \\mathcal{D}: u(x) = v(x)\\}, then the agreement satisfies \\text{agree}_{\\mu}(u,v) = \\mu(\\{x \\in \\mathcal{D}: u(x) = v(x)\\}).\n\nFinally, for \\mathbf{u} = \\{u_0, \\cdots, u_l\\}, where u_i \\in \\mathbb{F}_q^{\\mathcal{D}} is a group of words, the \\mu-weighted correlated agreement is the maximum \\mu-weighted size of a subdomain \\mathcal{D}' \\subset \\mathcal{D}, such that the restriction of \\mathbf{u} to \\mathcal{D}' belongs to V|_{\\mathcal{D}'}, i.e., for each i = 0, \\cdots, l, there exists v_i \\in V such that u_i|_{\\mathcal{D}'} = v_i|_{\\mathcal{D}'}. When \\mu is not specified, it is set to the constant weight function 1, which recovers the notion of correlated agreement metric discussed earlier.\n\nNext, we assume that the weight function \\mu has some structure, specifically, all weights \\mu(x) are of the form \\mu(x) = \\frac{a_x}{M}, where a_x are varying integers with a common denominator M. For the special case of FRI soundness (where M equals the blocklength of the RS code to which the FRI protocol is applied), this assumption indeed holds. The following is a weighted generalization of Theorem 1.5.\n\nTheorem 7.1 [BSCIK20, Theorem 7.1] (Weighted correlated agreement over curves – Version I). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}}. Let \\alpha \\in (\\sqrt{\\rho}, 1) and let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M. Suppose\\Pr_{u \\in \\text{curve}(\\mathbf{u})} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] > l \\cdot \\epsilon,\n\nwhere \\epsilon is as defined in Theorem 1.2 (with \\eta = \\min(\\alpha - \\sqrt{\\rho}, \\frac{\\sqrt{\\rho}}{20})), and additionally suppose\\Pr_{u \\in \\text{curve}(\\mathbf{u})} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] \\ge \\frac{l(M|\\mathcal{D}|+1)}{q} \\left(\\frac{1}{\\eta} + \\frac{3}{\\sqrt{\\rho}}\\right) .\n\nThen there exists \\mathcal{D}' \\subset \\mathcal{D} and v_0, \\cdots, v_l \\in V satisfying\n\nDensity: \\mu(\\mathcal{D}') \\ge \\alpha, and\n\nAgreement: for all i \\in \\{0, \\cdots, l\\}, the functions u_i and v_i agree on all of \\mathcal{D}'.\n\nA more precise form that only applies to the Johnson bound range is as follows.\n\nTheorem 7.2 [BSCIK20, Theorem 7.2] (Weighted correlated agreement over curves – Version II). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}}. Let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M. Let m \\ge 3 and let\\alpha \\ge \\alpha_0(\\rho, m) := \\sqrt{\\rho} + \\frac{\\rho}{2m}.\n\nLetS = \\{z \\in \\mathbb{F}_q : \\text{agree}_{\\mu}(u_0 + zu_1 + \\cdots + z^lu_l, V) \\ge \\alpha\\}\n\nand suppose|S| > \\max\\left(\\frac{(1 + \\frac{1}{2m})^7 m^7}{3 \\rho^{3/2}}n^2 l,  \\frac{2m + 1}{\\sqrt{\\rho}}(M \\cdot n + 1)l \\right) . \\tag{7.1}\n\nThen u_0, \\ldots, u_l have at least \\alpha correlated \\mu-agreement with V, i.e. \\exists v_0, \\cdots, v_l \\in V such that\\mu(\\{x \\in \\mathcal{D}: \\forall 0 \\le i \\le l, u_i(x) = v_i(x)\\}) \\ge \\alpha .\n\nTheorem 7.3 [BSCIK20, Theorem 7.3] (Weighted correlated agreement over affine spaces). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} and let U = u_0 + \\text{span}\\{u_1, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} be an affine subspace. Let \\alpha \\in (\\sqrt{\\rho}, 1) and let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M. Suppose\\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] > \\epsilon,\n\nwhere \\epsilon is as defined in Theorem 1.2 (with \\eta = \\min(\\alpha - \\sqrt{\\rho}, \\frac{\\sqrt{\\rho}}{20})), and additionally suppose\\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] \\ge \\frac{M|\\mathcal{D}| + 1}{q} \\left(\\frac{1}{\\eta} + \\frac{3}{\\sqrt{\\rho}}\\right) .\n\nThen there exist \\mathcal{D}' \\subset \\mathcal{D} and v_0, \\cdots, v_l \\in V satisfying\n\n\\mu-Density: \\mu(\\mathcal{D}') \\ge \\alpha, and\n\nAgreement: for all i \\in \\{0, \\cdots, l\\}, the functions u_i and v_i agree on all of \\mathcal{D}'.\n\nSimilarly, there is a more precise form for Theorem 7.3 regarding the Johnson bound.\n\nTheorem 7.4 [BSCIK20, Theorem 7.4] (Weighted correlated agreement over affine spaces – Version II). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} and let U = u_0 + \\text{span}\\{u_1, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} be an affine subspace. Let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M. Let m \\ge 3 and let\\alpha \\ge \\alpha_0(\\rho, m) := \\sqrt{\\rho} +  \\frac{\\sqrt{\\rho}}{2m} .\n\nSuppose\\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] > \\max \\left( \\frac{(1 + \\frac{1}{2m})^7m^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{M \\cdot n + 1}{q} \\right) . \\tag{7.2}\n\nThen u_0, \\ldots, u_l have at least \\alpha correlated \\mu-agreement with V, i.e. \\exists v_0, \\cdots, v_l \\in V such that\\mu \\left(\\{x \\in \\mathcal{D} : \\forall 0 \\le i \\le l, u_i(x) = v_i(x)\\}\\right) \\ge \\alpha .","type":"content","url":"/fri/bciks20-proximity-gaps#correlated-weighted-agreement","position":9},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"FRI Protocol"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#fri-protocol","position":10},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"FRI Protocol"},"content":"The purpose of the FRI protocol is to solve the Reed-Solomon proximity testing problem in the IOP model, that is, for a received word f^{(0)}: \\mathcal{D}^{(0)} \\rightarrow \\mathbb{F}, verify its proximity to V^{(0)} := \\text{RS}[\\mathbb{F}, \\mathcal{D}^{(0)}, k^{(0)}]. If f^{(0)} belongs to V^{(0)}, accept; if it is \\delta-far from V^{(0)}, reject. The FRI protocol applies to any case where the evaluation domain \\mathcal{D}^{(0)} is a coset of a 2-smooth group, i.e., for any \\mathcal{D}^{(0)}, it is a coset of an (additive or multiplicative) group of size 2^s, where s is an integer. Therefore, for simplicity, we assume that the group \\mathcal{D}^{(0)} is multiplicative. The FRI protocol has two phases: the COMMIT phase and the QUERY phase.\n\nIn the COMMIT phase, after a finite number of r rounds of interaction, a series of functions f^{(1)}: \\mathcal{D}^{(1)} \\rightarrow \\mathbb{F}, f^{(2)}: \\mathcal{D}^{(2)} \\rightarrow \\mathbb{F}, \\cdots,f^{(r)}: \\mathcal{D}^{(r)} \\rightarrow \\mathbb{F} will be generated. In each iteration, the size of the domain |\\mathcal{D}^{(i)}| decreases. Assuming that for an honest prover, f^{(0)} is low-degree, then for each f^{(i)}, they will all be low-degree (see Proposition 1). At the beginning of the i-th round, the prover’s message f^{(i)}: \\mathcal{D}^{(i)} \\rightarrow \\mathbb{F} has been generated, and the verifier can access the oracle of this message. The Verifier now sends a uniformly random z^{(i)} \\in \\mathbb{F}, and then the prover replies with a new function f^{(i+1)}: \\mathcal{D}^{(i+1)} \\rightarrow \\mathbb{F}, where \\mathcal{D}^{(i+1)} is a (2-smooth) strict subgroup of \\mathcal{D}^{(i)}, meaning that \\mathcal{D}^{(i)} is not only a subgroup of \\mathcal{D}^{(i+1)}, but also its proper subset.\n\n\\mathcal{D}^{(i+1)} divides \\mathcal{D}^{(i)} into cosets of size l^{(i)} := |\\mathcal{D}^{(i)}|/|\\mathcal{D}^{(i+1)}|. Let C_g^{(i)} represent the coset corresponding to g \\in \\mathcal{D}^{(i+1)}, i.e.,C_g^{(i)} := \\{g' \\in \\mathcal{D}^{(i)} \\mid (g')^{l^{(i)}} = g\\}. \\tag{8.1}\n\nThis means selecting those elements in \\mathcal{D}^{(i)} that can be mapped to g in \\mathcal{D}^{(i+1)} through the mapping q(x) = x^{l^{(i)}}, and these elements form the set C_g^{(i)}, which is also a coset.\n\nFor each coset C_g^{(i)}, the interpolation map M_g^{(i)} is an invertible linear map M_g^{(i)}: \\mathbb{F}^{C_g^{(i)}} \\rightarrow \\mathbb{F}^{l^{(i)}}, which maps f^{(i)}|_{C_g^{(i)}}: C_g^{(i)} \\rightarrow \\mathbb{F} (i.e., restricting f^{(i)} to the domain C_g^{(i)} \\subset \\mathcal{D}^{(i)}) to the coefficient vector \\mathbf{u}^{(i)}(g) = (u_0^{(i)}(g), \\ldots, u_{l^{(i)}-1}^{(i)}(g))^{\\intercal} of the polynomial P_{\\mathbf{u}^{(i)}(g)}^{(i)}(Z) = \\sum_{j<l^{(i)}} u_j^{(i)}(g) Z^j, where P_{\\mathbf{u}^{(i)}(g)}^{(i)}(Z) is the polynomial interpolating f^{(i)}|_{C_g^{(i)}}. In other words, M_g^{(i)} is the inverse of the Vandermonde matrix generated by C_g^{(i)}, which means that \\left(M_g^{(i)}\\right)^{-1} \\cdot {\\color{}(u_0, \\ldots, u_{l^{(i)}-1})^{\\intercal}} is the evaluation of the polynomial P_{\\mathbf{u}}(X) = \\sum_{i<l^{(i)}} u_i X^i on the coset C_g^{(i)}.\n\n👀 Notice\nTo maintain consistency throughout this article, we use (x_0, \\ldots, x_n) to represent a row vector, and (x_0, \\ldots, x_n)^{\\intercal} to represent a column vector, which can also be written as:\n$$\\begin{bmatrix}\n> x_0\\\\\n> x_1\\\\\n> \\vdots\\\\\n> x_n\n> \\end{bmatrix}\n\n$$\n\nLet’s explain the description of the interpolation map above in more detail. According to the definition of C_g^{(i)}, we know that it contains l^{(i)} elements. Let C_g^{(i)} =\\{g'_1, \\cdots, g'_{l^{(i)}}\\}, we can write out the Vandermonde matrix generated by C_g^{(i)}:V_{C_g^{(i)}}=\n    \\begin{bmatrix}\n        1 & g'_1 & (g'_1)^2 & \\cdots & (g'_1)^{l^{(i)}-1} \\\\\n        1 & g'_2 & (g'_2)^2 & \\cdots & (g'_2)^{l^{(i)}-1} \\\\\n        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        1 & g'_{l^{(i)}} & (g'_{l^{(i)}})^2 & \\cdots & (g'_{l^{(i)}})^{l^{(i)}-1}\n    \\end{bmatrix}\n\nThen M_g^{(i)} = V_{C_g^{(i)}}^{-1}, which is the inverse of the Vandermonde matrix generated by C_g^{(i)}, therefore\\begin{aligned}\n    \\left(M_g^{(i)}\\right)^{-1} \\cdot (u_0, \\ldots, u_{l^{(i)}-1})^{\\intercal} & = \\left(V_{C_g^{(i)}}^{-1}\\right)^{-1} \\cdot (u_0, \\ldots, u_{l^{(i)}-1})^{\\intercal} \\\\\n    & = V_{C_g^{(i)}} \\cdot (u_0, \\ldots, u_{l^{(i)}-1})^{\\intercal} \\\\\n    & = \n    \\begin{bmatrix}\n        1 & g'_1 & (g'_1)^2 & \\cdots & (g'_1)^{l^{(i)}-1} \\\\\n        1 & g'_2 & (g'_2)^2 & \\cdots & (g'_2)^{l^{(i)}-1} \\\\\n        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        1 & g'_{l^{(i)}} & (g'_{l^{(i)}})^2 & \\cdots & (g'_{l^{(i)}})^{l^{(i)}-1}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        u_0\\\\\n        u_1\\\\\n        \\vdots\\\\\n        u_{l^{(i)}-1}\n    \\end{bmatrix} \\\\\n    & = \\begin{bmatrix}\n        u_0 + u_1 g'_1 + u_2 (g'_1)^2 + \\cdots + u_{l^{(i)}-1} (g'_1)^{l^{(i)}-1}\\\\\n        u_0 + u_1 g'_2 + u_2 (g'_2)^2 + \\cdots + u_{l^{(i)}-1} (g'_2)^{l^{(i)}-1}\\\\\n        \\vdots \\\\\n        u_0 + u_1 g'_{l^{(i)}} + u_2 (g'_{l^{(i)}})^2 + \\cdots + u_{l^{(i)}-1} (g'_{l^{(i)}})^{l^{(i)}-1}\n    \\end{bmatrix} \\\\\n    & = \\begin{bmatrix}\n        P_{\\mathbf{u}}(g'_1)\\\\\n        P_{\\mathbf{u}}(g'_2)\\\\\n        \\vdots \\\\\n        P_{\\mathbf{u}}(g'_{l^{(i)}})\n    \\end{bmatrix}\n\\end{aligned}\n\nFrom the above derivation, we can see that \\left(P_{\\mathbf{u}}(g'_1),  P_{\\mathbf{u}}(g'_2) \\cdots, P_{\\mathbf{u}}(g'_{l^{(i)}}) \\right)^{\\intercal} is the evaluation of the polynomial P_{\\mathbf{u}}(X) = \\sum_{i<l^{(i)}} u_i X^i on the coset C_g^{(i)}, so \\left(M_g^{(i)}\\right)^{-1} \\cdot (u_0, \\ldots, u_{l^{(i)}-1})^{\\intercal} is the evaluation of the polynomial P_{\\mathbf{u}}(X) = \\sum_{i<l^{(i)}} u_i X^i on the coset C_g^{(i)}.\n\nThe following proposition uses the above notation and restates [BBHR18, Section 4.1], differing from [BBHR18, Section 4.1] in that it is done over a multiplicative group rather than an additive group. This proposition describes the property of maintaining low-degree.\n\nClaim 1 [BCIKS20, Claim 8.1]. Suppose that f^{(i)} \\in \\text{RS}[\\mathbb{F}, \\mathcal{D}^{(i)}, k^{(i)}] where k^{(i)} + 1 is an integral power of 2. Then, for any z^{(i)} \\in \\mathbb{F}, letting \\mathbf{z}^{(i)} = \\left(\\left(z^{(i)}\\right)^0, \\left(z^{(i)}\\right)^1, \\ldots, \\left(z^{(i)}\\right)^{l^{(i)}-1}\\right)^{\\intercal}, the function f_{f^{(i)},z^{(i)}}^{(i+1)}: \\mathcal{D}^{(i+1)} \\rightarrow \\mathbb{F} defined on g \\in \\mathcal{D}^{(i+1)} byf_{f^{(i)},z^{(i)}}^{(i+1)}(g) := {\\color{}\\left(\\mathbf{z}^{(i)}\\right)^{\\intercal}  \\cdot \\mathbf{u}^{(i)}(g) = \\left(\\mathbf{z}^{(i)}\\right)^{\\intercal} \\cdot M_g^{(i)} \\cdot f^{(i)}|_{C_g^{(i)}}} \\tag{2}\n\nis a valid codeword of V^{(i+1)} := \\text{RS}[\\mathbb{F}, \\mathcal{D}^{(i+1)}, k^{(i+1)}] where k^{(i+1)} := \\frac{k^{(i)}+1}{l^{(i)}} - 1.\n\nAccording to [BBHR18] and the above notation, in the COMMIT phase of the FRI protocol, fixing a g \\in \\mathcal{D}^{(i+1)}, the next step constructs f_{f^{(i)},z^{(i)}}^{(i+1)}(g) := P_{\\mathbf{u}^{(i)}(g)}^{(i)}(z^{(i)}). Let’s understand the construction formula above.\\begin{aligned}\n    f_{f^{(i)},z^{(i)}}^{(i+1)}(g) & = P_{\\mathbf{u}^{(i)}(g)}^{(i)}(z^{(i)}) \\\\\n    & = \\sum_{j<l^{(i)}} u_j^{(i)}(g) \\cdot (z^{(i)})^j \\\\\n    & = \\left(z^{(i)}\\right)^0  \\cdot u_0^{(i)}(g) + \\left(z^{(i)}\\right)^1 \\cdot u_1^{(i)}(g) + \\cdots + \\left(z^{(i)}\\right)^{l^{(i)}-1} \\cdot u_{l^{(i)}-1}^{(i)}(g) \\\\\n    & = \\begin{bmatrix}\n        \\left(z^{(i)}\\right)^0 & \\left(z^{(i)}\\right)^1 & \\cdots & \\left(z^{(i)}\\right)^{l^{(i)}-1}\n    \\end{bmatrix} \\cdot\n    \\begin{bmatrix}\n        u_0^{(i)}(g)\\\\\n        u_1^{(i)}(g)\\\\\n        \\vdots\\\\\n        u_{l^{(i)}-1}^{(i)}(g)\n    \\end{bmatrix} \\\\\n    & = \\left(\\mathbf{z}^{(i)}\\right)^{\\intercal}  \\cdot \\mathbf{u}^{(i)}(g)\n\\end{aligned}\n\nLet’s explain the second equation given in Proposition 1, i.e., \\mathbf{u}^{(i)}(g) = M_g^{(i)} \\cdot f^{(i)}|_{C_g^{(i)}}. According to the previous analysis, for the Vandermonde matrix generated by C_g^{(i)}, we haveV_{C_g^{(i)}} \\cdot\n\\begin{bmatrix}\n    u_0^{(i)}\\\\\n    u_1^{(i)}\\\\\n    \\vdots\\\\\n    u_{l^{(i)}-1}^{(i)}\n\\end{bmatrix}\n= \\begin{bmatrix}\n        P_{\\mathbf{u}}(g'_1)\\\\\n        P_{\\mathbf{u}}(g'_2)\\\\\n        \\vdots \\\\\n        P_{\\mathbf{u}}(g'_{l^{(i)}})\n\\end{bmatrix}\n= \\begin{bmatrix}\n        f^{(i)}|_{C_g^{(i)}}(g'_1)\\\\\n        f^{(i)}|_{C_g^{(i)}}(g'_2)\\\\\n        \\vdots \\\\\n        f^{(i)}|_{C_g^{(i)}}(g'_{l^{(i)}})\n\\end{bmatrix}\n\nTherefore\\begin{bmatrix}\n    u_0^{(i)}\\\\\n    u_1^{(i)}\\\\\n    \\vdots\\\\\n    u_{l^{(i)}-1}^{(i)}\n\\end{bmatrix}\n= \n\\left(V_{C_g^{(i)}}\\right)^{-1} \\cdot \n\\begin{bmatrix}\n        f^{(i)}|_{C_g^{(i)}}(g'_1)\\\\\n        f^{(i)}|_{C_g^{(i)}}(g'_2)\\\\\n        \\vdots \\\\\n        f^{(i)}|_{C_g^{(i)}}(g'_{l^{(i)}})\n\\end{bmatrix}\n= M_g^{(i)} \\cdot f^{(i)}|_{C_g^{(i)}}\n\nThis gives us \\left(\\mathbf{u}^{(i)}(g)\\right)^{\\intercal}  = M_g^{(i)} \\cdot f^{(i)}|_{C_g^{(i)}}.","type":"content","url":"/fri/bciks20-proximity-gaps#fri-protocol","position":11},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Batching"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#batching","position":12},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Batching"},"content":"In some cases, the first prover’s oracle f^{(0)} is sampled from functions in an affine space F \\subset \\mathbb{F}^{\\mathcal{D}^{(0)}}, which serves as our input,F = \\left\\{ f_0^{(0)} + \\sum_{i=1}^{t} x_i \\cdot f_i^{(0)} \\mid x_i \\in \\mathbb{F}, f_i : \\mathcal{D}^{(0)} \\to \\mathbb{F} \\right\\} \\tag{3}\n\nWhen using the FRI protocol to “batch” multiple instances of different low degree testing problems, we combine them all together through random linear combinations, i.e., f_0^{(0)} + x_1 f_1^{(0)} + \\cdots +x_t f_t^{(0)} in the above formula. In this batching setting, we assume that the prover has already committed to f_1^{(0)}, \\ldots, f_t^{(0)} (note that in this case we set f_0^{(0)} = 0), and the verifier of the batched FRI uniformly randomly samples x_1, \\ldots, x_t \\in \\mathbb{F}, the prover replies with f^{(0)}, which should equal f_0^{(0)} + \\sum_{i=1}^{t} x_i \\cdot f_i^{(0)}, and now the FRI protocol is applied to f^{(0)}. Correspondingly, the QUERY phase of the batched FRI is also extended, so that each time a query for f^{(0)}(g) is requested, the verifier also queries f_0^{(0)}(g), \\ldots, f^{(0)}_t(g) and verifies f^{(0)}(g) = f_0^{(0)}(g) + \\sum_{i=1}^{t} {\\color{orange}x_i} \\cdot f^{(0)}_i(g).\n\n🐞 Fix\n\nThe formula in the paper here is f^{(0)}(g) = f_0^{(0)}(g) + \\sum_{i=1}^{t} f^{(0)}_i(g). I think it’s missing the coefficient x_i from before, and should be f^{(0)}(g) = f_0^{(0)}(g) + \\sum_{i=1}^{t} x_i \\cdot f^{(0)}_i(g).","type":"content","url":"/fri/bciks20-proximity-gaps#batching","position":13},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"The (batched) FRI QUERY phase","lvl2":"Batching"},"type":"lvl3","url":"/fri/bciks20-proximity-gaps#the-batched-fri-query-phase","position":14},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"The (batched) FRI QUERY phase","lvl2":"Batching"},"content":"Proposition 1 shows that for an honest prover, for any value z^{(i)} chosen by the verifier, for each y \\in D^{(i+1)}, the prover can construct a new codeword f^{(i+1)} \\in V^{(i+1)} from a codeword f^{(i)} \\in V^{(i)} by calculating equation (2). Therefore, we will always assume that f^{(r)} \\in V^{(r)}, for example, by assuming that the verifier always queries the first k^{(r)} elements of f^{(r)} (in some canonical order) and compares f^{(r)} with the interpolation polynomial of this function.\n\nProposition 1 provides a very natural testing method to check the consistency between f^{(i)} and f^{(i+1)}, and the query phase of FRI follows this process by iteratively applying this natural test from the “top” (f^{(r)}) to the “bottom” (f^{(0)}).\n\n🤔 Question\n\nHow to better explain this natural testing method here?","type":"content","url":"/fri/bciks20-proximity-gaps#the-batched-fri-query-phase","position":15},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"A single invocation of the FRI QUERY phase","lvl2":"Batching"},"type":"lvl3","url":"/fri/bciks20-proximity-gaps#a-single-invocation-of-the-fri-query-phase","position":16},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"A single invocation of the FRI QUERY phase","lvl2":"Batching"},"content":"Choose g^{(r)} uniformly randomly from \\mathcal{D}^{(r)}. For i = r, \\cdots, 1, choose g^{(i-1)} uniformly randomly from the coset C_{g^{(i)}}^{(i-1)}.\n\nIf f^{(0)}(g^{(0)}) \\neq f_0^{(0)}(g^{(0)}) + \\sum_{i = 1}^{t}x_i \\cdot f_i^{(0)}(g^{(0)}), then reject.\n\nIf, for any i \\in \\{0, \\cdots, r - 1\\}, f^{(i+1)}(g^{(i+1)}) \\neq \\left(\\mathbf{z}^{(i)}\\right)^{\\intercal} \\cdot M_g^{(i)} \\cdot f^{(i)}|_{C_g^{(i)}}, then reject.\n\nOtherwise — if all equations in the above conditions hold, then accept.\n\nThe above QUERY process differs from the QUERY process of FRI in [BBHR18] in that the random number selection starts from the last \\mathcal{D}^{(r)} instead of from the initial \\mathcal{D}^{(0)}. Compared to the QUERY phase in [BBHR18], here we also want to verify whether the batch is correct at step 0, that is, f^{(0)}(g^{(0)}) \\neq f_0^{(0)}(g^{(0)}) + \\sum_{i = 1}^{t}x_i \\cdot f_i^{(0)}(g^{(0)}).","type":"content","url":"/fri/bciks20-proximity-gaps#a-single-invocation-of-the-fri-query-phase","position":17},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"Summary of the batched FRI protocol","lvl2":"Batching"},"type":"lvl3","url":"/fri/bciks20-proximity-gaps#summary-of-the-batched-fri-protocol","position":18},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl3":"Summary of the batched FRI protocol","lvl2":"Batching"},"content":"Let’s summarize the important properties mentioned so far, which will be used in the following soundness analysis.\n\nAt the end of the COMMIT phase of the protocol, the verifier can access through oracles a series of functions f^{(0)}: \\mathcal{D}^{(0)} \\rightarrow \\mathbb{F}, \\ldots, f^{(r)}: \\mathcal{D}^{(r)} \\rightarrow \\mathbb{F}, where \\mathcal{D}^{(0)} \\supsetneq  \\ldots \\supsetneq \\mathcal{D}^{(r)} is a series of 2-smooth groups, and f^{(i)} arbitrarily depends on z^{(0)}, \\ldots, z^{(i)} (and f^{(0)}, \\ldots, f^{(i-1)}). We assume f^{(r)} \\in V^{(r)}.\n\nThere exists a set of l^{(i)} \\times l^{(i)} invertible matrices \\{M_{g^{(i+1)}}^{(i)} : g^{(i+1)} \\in D^{(i+1)}\\}, so that applying M_{g^{(i+1)}}^{(i)} to f^{(i)}|_{C_{g(i+1)}^{(i)}} maps f^{(i)} to a sequence of vectors \\mathbf{u} = \\mathbf{u}^{(i)} = \\{u_0^{(i)}, \\ldots, u_{l(i)}^{(i)}\\} \\subset \\mathbb{F}^{D^{(i+1)}}, where\\mathbf{u}^{(i)}\\left(g^{(i+1)}\\right)  = \\left(u_0^{(i)}\\left(g^{(i+1)}\\right), \\cdots, u_{l^{(i)}-1}^{(i)}\\left(g^{(i+1)}\\right) \\right) = {\\color{}M_{g^{(i+1)}}^{(i)} \\cdot f^{(i)}|_{C_{g(i+1)}^{(i)}}}. \\tag{4}\n\nCertainly. I’ll translate the text into English while maintaining all formulas and markdown formats. Here’s the translation:\n\nMoreover, if f^{(i)} is a valid RS codeword with rate \\rho on D^{(i)}, then each vector on the parametric curve through \\mathbf{u}^{(i)} is also a valid RS codeword with rate \\rho on D^{(i+1)}.\n\nIn each iteration of the QUERY phase, it checks whether f^{(i+1)} is constructed from f^{(i)} through equation (2), and (in the batched case) checks whether f^{(0)} is correctly calculated through equation (3).","type":"content","url":"/fri/bciks20-proximity-gaps#summary-of-the-batched-fri-protocol","position":19},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Soundness"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#soundness","position":20},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Soundness"},"content":"Lemma 8.2 [BSCIK20, Lemma 8.2] (batched FRI error bound). Let V^{(0)} = \\text{RS}[\\mathbb{F}, \\mathcal{D}^{(0)}, k^{(0)}] where \\mathcal{D}^{(0)} is a coset of a 2-smooth multiplicative group, and k^{(0)} + 1 is a power of 2; set \\rho = (k^{(0)} + 1)/|\\mathcal{D}^{(0)}|.\n\nLet F \\subseteq \\mathbb{F}^{\\mathcal{D}^{(0)}} be a space of functions as defined in Eq. (3) whose correlated agreement density with V^{(0)} is \\alpha. For integer m \\ge 3, let\\alpha^{(0)}(\\rho, m) = \\max\\left\\{ \\alpha, \\sqrt{\\rho}(1 + 1/2m) \\right\\}.\n\nAssume the FRI protocol is used with r rounds, and let l^{(i)} = |\\mathcal{D}^{(i)}|/|\\mathcal{D}^{(i+1)}| denote the ratio between prover messages (oracles) i and i + 1. Let \\epsilon_Q denote the probability that the verifier accepts a single FRI QUERY invocation. Then,\\Pr_{x_1, \\ldots, x_t, z^{(0)}, \\ldots, z^{(r-1)}}\\left[ \\epsilon_Q > \\alpha^{(0)}(\\rho, m) \\right] \\le \\epsilon_{\\text{C}}, \\tag{8.5}\n\nwhere\\epsilon_{\\text{C}} = \\frac{\\left(m + \\frac{1}{2}\\right)^7 \\cdot \\vert \\mathcal{D}^{(0)}\\vert^2}{2 \\rho^{3/2} \\vert \\mathbb{F} \\vert} + \\frac{(2m+1) \\cdot (\\vert \\mathcal{D}^{(0)}\\vert + 1)}{\\sqrt{\\rho}} \\cdot \\frac{\\sum_{i = 0}^{r - 1} l^{(i)}}{\\vert \\mathbb{F} \\vert}.\n\nIn words: For any interactive FRI prover P^*, the probability that the oracles f^{(0)}, \\ldots, f^{(r)} sent by P^* will pass a single invocation of the batched FRI QUERY test with probability greater than \\alpha^{(0)}(\\rho, m), is smaller than \\epsilon_{\\text{C}}. The probability is over the random variables x_1, \\ldots, x_t used to sample f^{(0)} from F and over the random messages z^{(0)}, \\ldots, z^{(r-1)} sent by the verifier during the COMMIT phase.\n\nTheorem 8.3  [BSCIK20, Theorem 8.3]  (Batched FRI Soundness). Let f_0^{(0)}, \\ldots, f_t^{(r)}: \\mathcal{D}^{(0)} \\rightarrow \\mathbb{F} be a sequence of functions and let V^{(0)} = \\text{RS}[\\mathbb{F}, \\mathcal{D}^{(0)}, k^{(0)}] where \\mathcal{D}^{(0)} is a coset of a 2-smooth group of size n^{(0)} = |\\mathcal{D}^{(0)}|, and \\rho = \\frac{k^{(0)} + 1}{n^{(0)}} satisfies \\rho = 2^{-\\text{R}} for positive integer \\text{R}. Let \\alpha = \\sqrt{\\rho}(1 + 1/2m) for integer m \\ge 3 and \\epsilon_{\\text{C}} be as defined in Lemma 8.2.\n\nAssume the FRI protocol is used with r rounds. Let l^{(i)} = |\\mathcal{D}^{(i)}|/|\\mathcal{D}^{(i+1)}| denote the ratio between prover messages (oracles) i and i + 1. Assume furthermore that s is the number of invocations of the FRI QUERY step.\n\nSuppose there exists a batched FRI prover P^* that interacts with the batched FRI verifier and causes it to output “accept” with probability greater than\\epsilon_{\\text{FRI}} := \\epsilon_{\\text{C}} + \\alpha^s = \\frac{\\left(m + \\frac{1}{2}\\right)^7 \\cdot \\vert \\mathcal{D}^{(0)}\\vert^2}{2 \\rho^{3/2} \\vert \\mathbb{F} \\vert} + \\frac{(2m+1) \\cdot (\\vert \\mathcal{D}^{(0)}\\vert + 1)}{\\sqrt{\\rho}} \\cdot \\frac{\\sum_{i = 0}^{r - 1} l^{(i)}}{\\vert \\mathbb{F} \\vert} + \\left(\\sqrt{\\rho} \\cdot \\left( 1 + \\frac{1}{2m} \\right)  \\right)^s .\n\nThen f_0^{(0)}, \\ldots, f_t^{(0)} have correlated agreement with V^{(0)} on a domain \\mathcal{D}' \\subset \\mathcal{D}^{(0)} of density at least \\alpha.\n\nProof of Theorem 8.3: By contradiction, then directly prove through Lemma 8.2. Assume that the maximum correlated agreement of f_0^{(0)}, \\ldots, f_t^{(0)} with V^{(0)} is less than \\alpha^{(0)}(\\rho, m) = \\sqrt{\\rho}(1 + 1/2m), but at the same time, the acceptance probability is greater than \\epsilon_{\\text{C}} + (\\alpha^{(0)}(\\rho, m))^s.\n\nLet E be the event that the acceptance probability in each FRI QUERY phase is greater than \\alpha^{(0)}(\\rho, m). This event depends on x_1, \\ldots, x_t, f^{(0)}, z^{(0)}, \\ldots, z^{(r-1)}, f^{(r)}, where each f^{(i)} is generated by P^* based on the previous messages from the Verifier. According to Lemma 8.2, for any Prover P^*, the probability of event E occurring does not exceed \\epsilon_{\\text{C}}. When event E does not hold, the probability that s independent invocations of FRI QUERY all return “accept” does not exceed (\\alpha^{(0)}(\\rho, m))^s.\n\nTherefore, the probability that the FRI Verifier accepts does not exceed \\epsilon_{\\text{C}} + (\\alpha^{(0)}(\\rho, m))^s, which contradicts the assumption. \n\n\\Box\n\n❓ Question\n\nHere, for the event E where the acceptance probability is greater than \\alpha^{(0)}(\\rho, m) in each FRI QUERY phase, how is it still not exceeding \\epsilon_{\\text{C}} when called s times? Why isn’t it (\\epsilon_{\\text{C}})^s?","type":"content","url":"/fri/bciks20-proximity-gaps#soundness","position":21},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Proof of Lemma 8.2"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#proof-of-lemma-8-2","position":22},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"Proof of Lemma 8.2"},"content":"Before proving Lemma 8.2, let’s introduce a method to track whether the verifier’s consistency check passes. Specifically, the Prover will construct function f^{(i+1)} based on the random number z^{(i)} sent by the Verifier, and then respond to the Verifier with function f^{(i+1)}. In the QUERY phase of FRI, the Verifier will check the consistency between function f^{(i+1)} and function f^{(i)}.\n\nDefine a series of weight functions, \\mu^{(i)}:\\mathcal{D}^{(i)} \\rightarrow [0,1] and \\nu^{(i)}:\\mathcal{D}^{(i)} \\rightarrow [0,1], where i = 0, \\ldots, r. These weight functions are defined by induction. When i = 0, use \\{0,1\\} weights to indicate whether f^{(0)}(g) is calculated correctly:\\mu^{(0)}(g) =\n\\begin{cases}\n 1 & f^{(0)}(g) = f_0^{(0)}(g) + \\sum_{i = 1}^t x_i f_i^{(0)}(g) \\\\\n 0 & \\text{otherwise}\n\\end{cases}\n\nNow, \\mu^{(i)} obtained by induction can be used to define an auxiliary weight function \\nu^{(i+1)}: \\mathcal{D}^{(i+1)} \\rightarrow [0,1]. By taking an element g in \\mathcal{D}^{(i+1)}, we can get a coset C_g^{(i)} \\subset \\mathcal{D}^{(i)}, which is composed of all elements in \\mathcal{D}^{(i)} that can be mapped to g through the mapping q^{(i)}(x) = x^{l^{(i)}}, as shown in (8.1),C_g^{(i)} := \\{g' \\in \\mathcal{D}^{(i)} \\mid (g')^{l^{(i)}} = g\\}.\n\nThen the definition of \\nu^{(i+1)} is\\nu^{(i+1)}(g) = \\mathbb{E}_{g' \\in C_g^{(i)}}\\left[ \\mu^{(i)}(g') \\right]. \\tag{8.6}\n\nIn other words, \\nu^{(i+1)}(g) is the expected value of the \\mu^{(i)} weights of all elements in the coset C_g^{(i)}. Finally, define the function \\mu^{(i+1)}, for each g \\in \\mathcal{D}^{(i+1)}:\\mu^{(i+1)}(g) = \n\\begin{cases}\n    \\nu^{(i+1)}(g) & f^{(i+1)}(g) = f_{f^{(i)}, z^{(i)}}^{(i+1)}(g) \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\nRegarding the definition of \\mu^{(i)}, an important property is that \\mu^{(i)}(g) is a measure of the probability of success in the FRI QUERY phase, conditional on querying g from f^{(i)}, which is an important reason for the following proposition to hold.\n\nClaim 8.5. The probability \\epsilon_{\\text{Q}} that a single invocation of the batched FRI QUERY accepts f^{(0)}, \\ldots, f^{(r)}, where f^{(r)} \\in \\text{RS}[\\mathbb{F},\\mathcal{D}^{(r)},k^{(r)}], satisfies\\epsilon_{\\text{Q}} = \\mathbb{E}_{g^{(r)} \\in \\mathcal{D}^{(r)}}\\left[ \\mu^{(r)}(g^{(r)}) \\right].\n\nProof: Recall the invocation of FRI QUERY, a series of random g^{(r)}, \\ldots, g^{(0)} will be selected, where g^{(i-1)} is uniformly randomly selected from the coset C_{g^{(i)}}^{(i-1)}. We will prove by induction that for i = 0, \\ldots, r\\mathbb{E}_{g^{(i)} \\in \\mathcal{D}^{(i)}}\\left[ \\mu^{(i)}(g^{(i)}) \\right]\n\nequals the probability that when g^{(i)} is uniformly randomly selected, and it is generated from a random sequence g^{(i-1)} \\in C_{g^{(i)}}^{(i-1)}, \\ldots, g^{(0)} \\in C_{g^{(1)}}^{(0)}, all tests related to g^{(i)} and its induced tests pass.\n\nThe idea of the induction proof is as follows:\n\nProve that \\mu^{(0)} holds for the most basic case when i = 0\n\nAssume \\mu^{(i-1)} holds for i - 1, prove that \\mu^{(i)} holds for i\n\nThis will prove the proposition.\n\nWhen i = 0, by the definition of \\mu^{(0)},\\mu^{(0)}(g^{(0)}) =\n\\begin{cases}\n 1 & f^{(0)}(g^{(0)}) = f_0^{(0)}(g^{(0)}) + \\sum_{i = 1}^t x_i f_i^{(0)}(g^{(0)}) \\\\\n 0 & \\text{otherwise}\n\\end{cases}\n\nThe probability of FRI QUERY passing naturally equals \\mathbb{E}_{g^{(0)} \\in \\mathcal{D}^{(0)}}\\left[ \\mu^{(0)}(g^{(0)}) \\right].\n\nAssuming \\mu^{(i-1)} holds for i - 1, now analyze \\mu^{(i)}(g^{(i)}). If f^{(i)}(g^{(i)}) is not calculated correctly according to equation (8.2), then \\mu^{(i)}(g^{(i)}) = 0, otherwise, according to the definition\\mu^{(i)}(g^{(i)}) = \\nu^{(i)}(g^{(i)}) = \\mathbb{E}_{g^{(i-1)} \\in C_{g^{(i)}}^{(i-1)}}\\left[ \\mu^{(i-1)}(g^{(i-1)}) \\right].\n\nThis indicates that \\mu^{(i)}(g^{(i)}) is the average of the values of \\mu^{(i-1)} on the coset C_{g^{(i)}}^{(i-1)} \\subseteq \\mathcal{D}^{(i-1)}. By the induction hypothesis, it is the probability that all tests related to g^{(i-1)}, \\ldots, g^{(0)} pass, therefore \\mu^{(i)} holds for i. \n\n\\Box\n\nLemma 8.2 needs to estimate the probability in the FRI QUERY phase. Recall the protocol of the batched FRI QUERY phase, there are two places involving random numbers:\n\nIn step 2 of the protocol, use t random numbers x_1, \\ldots , x_t to batch f_1^{(0)}, f_2^{(0)}, \\ldots ,f_t^{(0)}, which corresponds to the case of affine space, and will use the conclusion corresponding to Theorem 7.4.\n\nIn step 3 of the protocol, use \\mathbf{z}^{(i)} = \\left(\\left(z^{(i)}\\right)^0, \\left(z^{(i)}\\right)^1, \\ldots, \\left(z^{(i)}\\right)^{l^{(i)}-1}\\right) for batching, corresponding to the case of curves, and will use the conclusion of Theorem 7.2.\n\nProof of Lemma 8.2: Now we need to prove Lemma 8.2. By Proposition 8.5, we only need to prove that in the verifier’s random selection, with probability greater than 1 - \\epsilon_C,\\mathbb{E}_{g \\in \\mathcal{D}^{(r)}}\\left[ \\mu^{(r)}(g) \\right] \\le \\alpha^{(0)}(\\rho, m). \\tag{8.7}\n\nIf we prove the above holds, it means that when selecting random numbers in \\mathbb{F}_q, if \\epsilon_Q > \\alpha^{(0)}(\\rho, m), then its probability is less than or equal to \\epsilon_C, which proves Lemma 8.2.\n\n🤔 Question\n\nWhy isn’t it “with probability greater than 1 - \\epsilon_C” here?\n\nThe proof idea is to first define a series of bad events E^{(0)}, \\ldots, E^{(r)}, where the probability of some events occurring is the sum of the probabilities of each event occurring, proving that this probability is less than or equal to \\epsilon_C. Then assuming that no bad events occur, prove that equation (8.7) holds.\n\nLet E^{(0)} be the event\\text{agree}_{\\mu^{(0)}} \\left(f^{(0)}, V^{(0)}\\right) > \\alpha^{(0)}(\\rho, m).\n\nBy the definition of \\mu^{(0)}, event E^{(0)} is\\text{agree} \\left(f_0^{(0)} + \\sum_{i=1}^{t} x_i f_i^{(0)} , V^{(0)}\\right) > \\alpha^{(0)}(\\rho, m) = \\max\\left\\{ \\alpha, \\sqrt{\\rho}(1 + 1/2m) \\right\\}.\n\n🤔 Question\n\nWhat exactly does \\text{agree} mean here? What’s the difference between it and \\text{agree}_{\\mu^{(0)}}? Does it represent the constant 1?\n\nTherefore, this event E^{(0)} mainly depends on the random numbers x_1, \\ldots, x_t. According to the assumption in the lemma, the maximum correlated agreement density of (f_0^{(0)}, \\ldots, f_t^{(0)}) with V^{(0)} does not exceed \\alpha.\n\nRecall Theorem 7.4:\nTheorem 7.4 (Weighted correlated agreement over affine spaces – Version II). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} and let U = u_0 + \\text{span}\\{u_1, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} be an affine subspace. Let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M . Let m \\ge 3 and let> \\alpha \\ge \\alpha_0(\\rho, m) := \\sqrt{\\rho} +  \\frac{\\sqrt{\\rho}}{2m} .\n>\n\nSuppose> \\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] > \\max \\left( \\frac{(1 + \\frac{1}{2m})^7m^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{M \\cdot n + 1}{q} \\right) . \\tag{7.2}\n>\n\nThen u_0, \\ldots, u_l have at least \\alpha correlated \\mu -agreement with V , i.e. \\exists v_0, \\cdots, v_l \\in V such that> \\mu \\left(\\{x \\in \\mathcal{D} : \\forall 0 \\le i \\le l, u_i(x) = v_i(x)\\}\\right) \\ge \\alpha .\n>\n\nIts contrapositive is: If u_0, \\ldots, u_l have at most \\alpha correlated \\mu -agreement with V, then> \\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) \\ge \\alpha] \\le \\max \\left( \\frac{(1 + \\frac{1}{2m})^7m^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{M \\cdot n + 1}{q} \\right) . \n>\n\nBy the contrapositive of Theorem 7.4, taking \\alpha = \\alpha^{(0)}(\\rho, m), \\mu \\equiv 1 and M = 1, we have\\begin{aligned}\n    \\Pr_{x_1, \\ldots, x_t} [E^{(0)}] & = \\Pr_{u \\in U} [\\text{agree}_{\\mu}(u,V) > \\alpha^{(0)}(\\rho, m)] \\\\\n    & {\\color{orange}\\text{(Why can the parentheses here be directly changed to strictly $>$?)}} \\\\\n    & \\le \\max \\left( \\frac{(1 + \\frac{1}{2m})^7m^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{M \\cdot n + 1}{q} \\right) \\\\\n    & = \\max \\left( \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{n + 1}{q} \\right)\n\\end{aligned}\n\nNote that according to Theorem 7.4 and Theorem 1.2, V = \\mathsf{RS}[\\mathbb{F}_q, \\mathcal{D}^{(0)}, k^{(0)}], n = |\\mathcal{D}^{(0)}|, \\rho = \\frac{k^{(0)} + 1}{n}.\n\nLet’s derive\\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q} > \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{n + 1}{q}\n\nSince\\begin{aligned}\n    & \\frac{(m+\\frac{1}{2})^7}{3} \\ge 2m + 1 \\\\\n    \\Rightarrow & \\frac{(2m + 1)^7}{3 \\times 2^7} \\ge 2m + 1 \\\\\n    \\Rightarrow &  (2m + 1)^6 \\ge 3 \\times 2^7 \\\\\n\\end{aligned}\n\nBy the condition m \\ge 3 in the theorem, (2m + 1)^6 is an increasing function when m \\ge 3, so (2m + 1)^6 \\ge (2 \\times 3 + 1)^6 = 7^6 = 117649, while the right side of the above equation 3 \\times 2^7 = 384, satisfying (2m + 1)^6 \\ge 117649 > 3 \\times 2^7. From this, we get that \\frac{(m+\\frac{1}{2})^7}{3} > 2m + 1 (not equal) holds. Then\\begin{aligned}\n    \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q} & > \\frac{2m + 1}{\\rho^{3/2}} \\cdot \\frac{n^2}{q} \\\\\n    & = \\frac{2m + 1}{\\rho \\cdot \\rho^{1/2}} \\cdot \\frac{n^2}{q} \\\\\n    & {\\color{blue}{\\text{(Since $\\rho < 1$)}}} \\\\\n    & > \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{n^2}{q} \\\\\n    & {\\color{blue}{\\text{(Since $n^2 > n + 1$ when $n \\ge 2$)}}} \\\\\n    & > \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{n + 1}{q}\n\\end{aligned}\n\nThus\\begin{aligned}\n    \\Pr_{x_1, \\ldots, x_t} [E^{(0)}] & \\le \\max \\left( \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}, \\frac{2m + 1}{\\sqrt{\\rho}} \\cdot \\frac{n + 1}{q} \\right) \\\\\n    & = \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q}\n\\end{aligned}\n\nLet\\epsilon = \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{n^2}{q} {\\color{blue}\\quad {\\text{(Note that here $n = |\\mathcal{D}^{(0)}|$)}}}\n\nWe get\\Pr_{x_1, \\ldots, x_t} [E^{(0)}] \\le \\epsilon \\tag{8.8}\n\nNow fix i \\in \\{0, \\ldots, r - 1\\}. Define event E^{(i+1)} as\\text{agree}_{\\nu^{(i+1)}} \\left(f_{f^{(i)},z^{(i)}}^{(i+1)}, V^{(i+1)}\\right) > \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right) . \\tag{8.9}\n\n📖 Notes\nUnderstand event E^{(i+1)}. According to the definition> \\begin{aligned}\n>   \\text{agree}_{\\nu^{(i+1)}} \\left(f_{f^{(i)},z^{(i)}}^{(i+1)}, V^{(i+1)}\\right) & = \\max_{g^{(i+1)} \\in V^{(i+1)}} \\text{agree}_{\\nu^{(i+1)}} \\left(f_{f^{(i)},z^{(i)}}^{(i+1)}, g^{(i+1)} \\right)  \\\\   \n>   & = \\max_{g^{(i+1)} \\in V^{(i+1)}} \\frac{1}{|\\mathcal{D}^{(i+1)}|} \\sum_{x:f_{f^{(i)},z^{(i)}}^{(i+1)}(x) = g^{(i+1)}(x)} \\nu^{(i+1)}(x)\\\\ \n>   & = \\max_{g^{(i+1)} \\in V^{(i+1)}} \\frac{1}{|\\mathcal{D}^{(i+1)}|} \\sum_{x:f_{f^{(i)},z^{(i)}}^{(i+1)}(x) = g^{(i+1)}(x)} \\mathbb{E}_{g' \\in C_x^{(i)}} \\left[ \\mu^{(i)}(g') \\right] \\\\\n> \\end{aligned}\n>\n\nIt measures, after constructing f_{f^{(i)},z^{(i)}}^{(i+1)} from f^{(i)} and random number z^{(i)}, finding x in \\mathcal{D}^{(i+1)} that make f_{f^{(i)},z^{(i)}}^{(i+1)} consistent with a polynomial g^{(i+1)} in V^{(i+1)}, then calculating the sum of the expected values of the \\mu^{(i)} weights of the elements in the corresponding cosets in \\mathcal{D}^{(i)} for these x.\n\nOn the right side of equation (8.9)> \\begin{aligned}\n>    \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right) & = \\max_{g^{(i)} \\in V^{(i)}} \\frac{1}{|\\mathcal{D}^{(i)}|} \\sum_{x:f^{(i)}(x) = g^{(i)}(x)} \\mu^{(i)}(x)\n> \\end{aligned}\n>\n\n\\mu^{(i)}(x) measures the probability of passing in the FRI QUERY phase when querying x from f^{(i)}.\n\nEvent E^{(i+1)} aims to define such events: for f_{f^{(i)},z^{(i)}}^{(i+1)} constructed from f^{(i)} and z^{(i)}, for a polynomial g^{(i+1)} in V^{(i+1)}, take out the set of points x that make their values equal, calculate the sum of the expected \\mu^{(i)} weights of the cosets corresponding to these points, and the ratio to the size of \\mathcal{D}^{(i+1)}.\n\nFix f^{(i)} and \\mu^{(i)}, then event E^{(i+1)} is determined by the random number z^{(i)}. According to the definition of \\mu^{(i+1)}, we have\\mu^{(i+1)}(g) = \n\\begin{cases}\n    \\nu^{(i+1)}(g) & f^{(i+1)}(g) = f_{f^{(i)}, z^{(i)}}^{(i+1)}(g) \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\nOnly when the condition f^{(i+1)}(g) = f_{f^{(i)}, z^{(i)}}^{(i+1)}(g) is satisfied, \\mu^{(i+1)}(g) will be equal to \\nu^{(i+1)}(g). Naturally, we can get\\text{agree}_{\\mu^{(i+1)}} \\left(f^{(i+1)}, V^{(i+1)}\\right) \\le \\text{agree}_{\\nu^{(i+1)}} \\left(f_{f^{(i)},z^{(i)}}^{(i+1)}, V^{(i+1)}\\right)\n\nTherefore, if event E^{i+1} does not occur, then according to equation (8.9), we can get\\text{agree}_{\\mu^{(i+1)}} \\left(f^{(i+1)}, V^{(i+1)}\\right) \\le \\text{agree}_{\\nu^{(i+1)}} \\left(f_{f^{(i)},z^{(i)}}^{(i+1)}, V^{(i+1)}\\right) \\le  \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right)\n\nThen\\text{agree}_{\\mu^{(i+1)}} \\left(f^{(i+1)}, V^{(i+1)}\\right) \\le \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right) \\tag{8.10}\n\nLet \\alpha = \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right). According to the definition, expanding f_{f^{(i)},z^{(i)}}^{(i+1)}, we get that event E^{(i+1)} is\\text{agree}_{\\nu^{(i+1)}} \\left(u_0 + z^{(i)}u_1 + \\ldots + (z^{(i)})^{l^{(i)} -1} u_{l^{(i)} -1}, V^{(i+1)}\\right) > \\alpha,\n\nwhere u_0, \\ldots, u_{l^{(i)} - 1}: \\mathcal{D}^{(i+1)} \\rightarrow \\mathbb{F} are the functions obtained from f^{(i)} as defined in the FRI protocol (see Proposition 8.1). This is exactly the case handled by Theorem 7.2.\n\n📖 Recall Theorem 7.2\n\nTheorem 7.2 (Weighted correlated agreement over curves – Version II). Let V, q, n, k and \\rho be as defined in Theorem 1.2. Let \\mathbf{u} = \\{u_0, \\cdots, u_l\\} \\subset \\mathbb{F}_q^{\\mathcal{D}} . Let \\mu : \\mathcal{D} \\rightarrow [0,1] be a vector of weights, whose values all have denominator M . Let m \\ge 3 and let> \\alpha \\ge \\alpha_0(\\rho, m) := \\sqrt{\\rho} + \\frac{\\rho}{2m}.\n>\n\nLet> S = \\{z \\in \\mathbb{F}_q : \\text{agree}_{\\mu}(u_0 + zu_1 + \\cdots + z^lu_l, V) \\ge \\alpha\\}\n>\n\nand suppose> |S| > \\max\\left(\\frac{(1 + \\frac{1}{2m})^7 m^7}{3 \\rho^{3/2}}n^2 l,  \\frac{2m + 1}{\\sqrt{\\rho}}(M \\cdot n + 1)l \\right) . \\tag{7.1}\n>\n\nThen u_0, \\ldots, u_l have at least \\alpha correlated \\mu -agreement with V , i.e. \\exists v_0, \\cdots, v_l \\in V such that> \\mu(\\{x \\in \\mathcal{D}: \\forall 0 \\le i \\le l, u_i(x) = v_i(x)\\}) \\ge \\alpha .\n>\n\nIn Theorem 7.2, take M = |\\mathcal{D}^{(0)}|/|\\mathcal{D}^{(i+1)}|. At this time, we are analyzing the case of i + 1, so n = |\\mathcal{D}^{i+1}| in the theorem, then M \\cdot n = |\\mathcal{D}^{(0)}|. Since we are analyzing \\mathbf{u} = \\{u_0, \\cdots, u_{l^{(i)} - 1}\\}, l = l^{(i)} - 1 in equation (7.1). According to Theorem 7.2, if\\Pr_{z^{(i)}}\\left[E^{(i+1)}\\right] \\ge (l^{(i)} - 1) \\cdot \\left(\\epsilon^{(i)} + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\right)\n\nwhere,\\epsilon^{(i)} = \\frac{|\\mathcal{D}^{(i+1)}|^2}{|\\mathcal{D}^{(0)}|^2} \\epsilon = \\frac{\\epsilon}{(l^{(0)}\\cdots l^{(i)})^2} = \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{|\\mathcal{D}^{(0)}|^2}{q} \\cdot \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2}\n\nIf the above condition is satisfied, referring to Theorem 7.2, we have\\begin{aligned}\n    |S| & \\ge |\\mathbb{F}| \\cdot (l^{(i)} - 1) \\cdot \\left(\\epsilon^{(i)} + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{\\mathbb{F}} \\right) \\\\\n    & = \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{|\\mathcal{D}^{(0)}|^2}{q} \\cdot \\frac{|\\mathcal{D}^{(i+1)}|^2}{|\\mathcal{D}^{(0)}|^2} \\cdot |\\mathbb{F}|  \\cdot (l^{(i)} - 1) + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot (|\\mathcal{D}^{(0)}| + 1) \\cdot (l^{(i)} - 1) \\\\\n    & = \\frac{(1 + \\frac{1}{2m})^7 m^7}{3 \\rho^{3/2}}  \\cdot |\\mathcal{D}^{(i+1)}|^2 \\cdot (l^{(i)} - 1) + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot (M \\cdot n + 1) \\cdot (l^{(i)} - 1) \\\\\n    & = \\frac{(1 + \\frac{1}{2m})^7 m^7}{3 \\rho^{3/2}}  \\cdot n^2 \\cdot (l^{(i)} - 1) + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot (M \\cdot n + 1) \\cdot (l^{(i)} - 1) \\\\\n    & > \\max\\left(\\frac{(1 + \\frac{1}{2m})^7 m^7}{3 \\rho^{3/2}}n^2 l,  \\frac{2m + 1}{\\sqrt{\\rho}}(M \\cdot n + 1)l \\right)\n\\end{aligned}\n\nSatisfying equation (7.1), therefore by Theorem 7.2, there exists a set S \\subseteq \\mathcal{D}^{(i+1)}, and codewords v_0, \\ldots, v_{l^{(i)} - 1} \\in V, such that u_i and v_i are consistent on S, and \\nu^{(i+1)}(S) > \\alpha. Recalling equation (8.4), we know\\mathbf{u}^{(i)}\\left(g^{(i+1)}\\right) = {\\color{}M_{g^{(i+1)}}^{(i)} \\cdot f^{(i)}|_{C_{g(i+1)}^{(i)}}}\n\nThe invertible interpolation mapping M_{g^{(i+1)}}^{(i)} maps f^{(i)}|_{C_{g(i+1)}^{(i)}} to \\mathbf{u}^{(i)}\\left(g^{(i+1)}\\right). Using its inverse mapping, i.e., the evaluation mapping, for each g^{(i+1)} \\in \\mathcal{D}^{(i+1)}, apply this inverse mapping to v_0(g^{(i+1)}), \\ldots, v_{l^{(i)}-1}(g^{(i+1)}). Let C_{g^{(i+1)}}^{(i)} =\\{g'_0, \\cdots, g'_{l^{(i)}-1}\\}, then the result after application is\\begin{aligned}\n    \\left(M_{g^{(i+1)}}^{(i)}\\right)^{-1}  \\cdot \\begin{bmatrix}\n        v_0(g^{(i+1)}) \\\\\n        v_1(g^{(i+1)}) \\\\\n        \\vdots \\\\\n        v_{l^{(i)}-1}(g^{(i+1)})\n    \\end{bmatrix} & = V_{C_{g^{(i+1)}}^{(i)}} \\cdot \\begin{bmatrix}\n        v_0(g^{(i+1)}) \\\\\n        v_1(g^{(i+1)}) \\\\\n        \\vdots \\\\\n        v_{l^{(i)}-1}(g^{(i+1)})\n    \\end{bmatrix} \\\\\n    & =  \n    \\begin{bmatrix}\n        1 & g'_0 & (g'_0)^2 & \\cdots & (g'_0)^{l^{(i)}-1} \\\\\n        1 & g'_1 & (g'_1)^2 & \\cdots & (g'_1)^{l^{(i)}-1} \\\\\n        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        1 & g'_{l^{(i)-1}} & (g'_{l^{(i)}-1})^2 & \\cdots & (g'_{l^{(i)}-1})^{l^{(i)}-1}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        v_0(g^{(i+1)}) \\\\\n        v_1(g^{(i+1)}) \\\\\n        \\vdots \\\\\n        v_{l^{(i)}-1}(g^{(i+1)})\n    \\end{bmatrix} \\\\\n    & = \\begin{bmatrix}\n        v_0(g^{(i+1)}) + v_1(g^{(i+1)}) g'_0 + u_2 (g'_0)^2 + \\cdots + v_{l^{(i)}-1}(g^{(i+1)}) (g'_0)^{l^{(i)}-1}\\\\\n        v_0(g^{(i+1)}) + v_1(g^{(i+1)}) g'_1 + u_2 (g'_1)^2 + \\cdots + v_{l^{(i)}-1}(g^{(i+1)}) (g'_1)^{l^{(i)}-1}\\\\\n        \\vdots \\\\\n        v_0(g^{(i+1)}) + v_1(g^{(i+1)}) g'_{l^{(i)-1}} + u_2 (g'_{l^{(i)}-1})^2 + \\cdots + v_{l^{(i)}-1}(g^{(i+1)}) (g'_{l^{(i)}-1})^{l^{(i)}-1}\n    \\end{bmatrix} \\\\\n    & = \\begin{bmatrix}\n        h^{(i)}(g'_0)\\\\\n        h^{(i)}(g'_1)\\\\\n        \\vdots \\\\\n        h^{(i)}(g'_{l^{(i)}-1})\n    \\end{bmatrix}\n\\end{aligned}\n\nWe can get function h^{(i)}: \\mathcal{D}^{(i)} \\rightarrow \\mathbb{F}, for each g^{(i)} \\in C_{g^{(i+1)}}^{(i)} we haveh^{(i)}(g^{(i)}) = \\sum_{j = 0}^{l^{(i)}-1} \\left(g^{(i)}\\right)^j \\cdot v_j\\left({\\color{blue}{g^{(i+1)}}}\\right) =  \\sum_{j = 0}^{l^{(i)}-1} \\left(g^{(i)}\\right)^j \\cdot v_j   \\left( {\\color{blue}{\\left(g^{(i)}\\right)^{l^{(i)}}}}   \\right).\n\nTherefore, since v_j \\in V^{(i+1)}, we have h^{(i)} \\in V^{(i)}. Moreover, according to the definition\\begin{aligned}\n    \\text{agree}_{\\mu^{(i)}}\\left( f^{(i)}, V^{(i)} \\right) & = \\max_{v \\in V^{(i)}} \\text{agree}_{\\mu^{(i)}}\\left( f^{(i)}, v \\right)\\\\\n    & \\ge \\text{agree}_{\\mu^{(i)}}\\left( f^{(i)}, h^{(i)} \\right) \\\\\n    & = \\frac{1}{|\\mathcal{D}^{(i)}|} \\sum_{x: f^{(i)}(x) = h^{(i)}(x)} \\mu^{(i)}(x) \\\\\n    & = \\nu^{(i+1)}(S) \\\\\n    & > \\alpha,\n\\end{aligned}\n\nThis contradicts the definition of \\alpha: \\alpha = \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right). This means that the assumption we made when applying Theorem 7.2 does not hold, that is, the following equation holds:\\Pr_{z^{(i)}}\\left[E^{(i+1)}\\right] < (l^{(i)} - 1) \\cdot \\left(\\epsilon^{(i)} + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\right) .\n\nTherefore, if event E^{(i+1)} does not occur, according to equation (8.10), for all i \\in 0, 1, \\ldots, r - 1 we have:\\text{agree}_{\\mu^{(i+1)}} \\left(f^{(i+1)}, V^{(i+1)}\\right) \\le \\max \\left( \\text{agree}_{\\mu^{(i)}} \\left(f^{(i)}, V^{(i)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right)\n\nAccording to equation (8.8), we get\\Pr_{x_1, \\ldots, x_t} [E^{(0)}] \\le \\epsilon , \\quad \\text{where} \\epsilon = \\frac{(m + \\frac{1}{2})^7}{3 \\rho^{3/2}} \\cdot \\frac{{|\\mathcal{D}^{(0)}|}^2}{q} {\\color{blue}}.\n\nIf the probability of event E^{(0)} or some E^{(i+1)} occurring is estimated as\\begin{aligned}\n    \\Pr_{x_1,\\ldots, x_t} \\left[E^{(0)}\\right] + \\sum_{i=0}^{r-1} \\Pr_{z^{(i)}} \\left[E^{(i+1)}\\right] & \\le \\epsilon + \\sum_{i=0}^{r-1} \\left( (l^{(i)} - 1) \\cdot \\left(\\epsilon^{(i)} + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\right) \\right) \\\\\n    & = \\epsilon + \\sum_{i=0}^{r-1} (l^{(i)} - 1) \\cdot \\epsilon^{(i)} + \\sum_{i=0}^{r-1} \\left((l^{(i)} - 1) \\cdot \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|}\\right)  \\\\\n    & = \\left(1 + \\sum_{i=0}^{r-1} \\frac{l^{(i)} - 1}{(l^{(0)}\\cdots l^{(i)})^2} \\right) \\epsilon + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\cdot \\sum_{i=0}^{r-1} (l^{(i)} - 1)\n\\end{aligned}\n\nLet’s estimate \\sum_{i=0}^{r-1} \\frac{l^{(i)} - 1}{(l^{(0)}\\cdots l^{(i)})^2}. Since for i \\in \\{0, \\ldots, r - 1\\} we have l^{(i)} \\ge 2, therefore\\begin{aligned}\n    \\sum_{i=0}^{r-1} \\frac{l^{(i)} - 1}{(l^{(0)}\\cdots l^{(i)})^2} & = \\sum_{i=0}^{r-1} \\left(\\frac{l^{(i)}}{(l^{(0)}\\cdots l^{(i)})^2} - \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2}\\right) \\\\\n    & = \\sum_{i=0}^{r-1} \\left(\\frac{1}{(l^{(0)}\\cdots l^{(i-1)})^2 l^{(i)}} - \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2}\\right) \\\\\n    & = \\frac{1}{l^{(0)}} + \\left(- \\frac{1}{(l^{(0)})^2} + \\frac{1}{(l^{(0)})^2l^{(1)}}\\right)  + \\left(- \\frac{1}{(l^{(0)}l^{(1)})^2} + \\frac{1}{(l^{(0)}l^{(1)})^2l^{(2)}} \\right)\\\\\n    & \\quad + \\ldots + \\left(- \\frac{1}{(l^{(0)}\\cdots l^{(r-2)})^2} + \\frac{1}{(l^{(0)}\\cdots l^{(r-2)})^2l^{(r-1)}} \\right) - \\frac{1}{(l^{(0)}\\cdots l^{(r-1)})^2} \\\\\n    & \\le \\frac{1}{l^{(0)}} + \\left(- \\frac{1}{(l^{(0)})^2} + \\frac{1}{(l^{(0)})^2 \\cdot 2}\\right)  + \\left(- \\frac{1}{(l^{(0)}l^{(1)})^2} + \\frac{1}{(l^{(0)}l^{(1)})^2 \\cdot 2} \\right)\\\\\n    & \\quad + \\ldots + \\left(- \\frac{1}{(l^{(0)}\\cdots l^{(r-2)})^2} + \\frac{1}{(l^{(0)}\\cdots l^{(r-2)})^2 \\cdot 2} \\right) - \\frac{1}{(l^{(0)}\\cdots l^{(r-1)})^2} \\\\\n    & = \\frac{1}{l^{(0)}} - \\frac{1}{(l^{(0)})^2 \\cdot 2}  - \\frac{1}{(l^{(0)}l^{(1)})^2 \\cdot 2} - \\ldots - \\frac{1}{(l^{(0)}\\cdots l^{(r-2)})^2 \\cdot 2}\\\\\n    & \\quad  - \\frac{1}{(l^{(0)}\\cdots l^{(r-1)})^2} \\\\\n    & < \\frac{1}{l^{(0)}} \\\\\n    & < \\frac{1}{2}\n\\end{aligned}\n\n📖 Another proof method: using geometric series summation\\begin{aligned}\n   \\sum_{i=0}^{r-1} \\frac{l^{(i)} - 1}{(l^{(0)}\\cdots l^{(i)})^2} & = \\sum_{i=0}^{r-1} \\left(\\frac{l^{(i)}}{(l^{(0)}\\cdots l^{(i)})^2} - \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2}\\right) \\\\\n   & = \\sum_{i=0}^{r-1} \\left(\\frac{1}{(l^{(0)}\\cdots l^{(i-1)})^2 l^{(i)}} - \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2}\\right) \\\\\n   & < \\sum_{i=0}^{r-1} \\frac{1}{(l^{(0)}\\cdots l^{(i-1)})^2 l^{(i)}} \\\\\n   & {\\color{blue}(\\text{Because } \\frac{1}{(l^{(0)}\\cdots l^{(i)})^2} > 0)} \\\\\n   & < \\sum_{i=0}^{r-1} \\frac{1}{l^{(0)}\\cdots l^{(i-1)} l^{(i)}} \\\\\n   & {\\color{blue}(\\text{Because } l^{(i)} \\ge 2, \\text{so } {l^{(i)}}^2 > l^{(i)} )} \\\\\n   & < \\sum_{i=0}^{r-1} \\left(\\frac{1}{2}\\right) ^{i+1} \\\\\n   & = \\frac{1}{2}\\sum_{i=0}^{r-1} \\left(\\frac{1}{2}\\right) ^{i} \\\\\n   & < \\frac{1}{2} \\cdot \\frac{1}{2} \\\\\n   & < \\frac{1}{2}\n\\end{aligned}\n\n🤔 Question\n\n Is there a more concise way for the above proof?\n\nTherefore\\begin{aligned}\n    \\Pr_{x_1,\\ldots, x_t} \\left[E^{(0)}\\right] + \\sum_{i=0}^{r-1} \\Pr_{z^{(i)}} \\left[E^{(i+1)}\\right] & \\le \\left(1 + \\sum_{i=0}^{r-1} \\frac{l^{(i)} - 1}{(l^{(0)}\\cdots l^{(i)})^2} \\right) \\epsilon + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\cdot \\sum_{i=0}^{r-1} (l^{(i)} - 1) \\\\\n    & < \\left(1 + \\frac{1}{2} \\right) \\epsilon + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\cdot \\sum_{i=0}^{r-1} (l^{(i)} - 1) \\\\\n    & < \\frac{3}{2} \\epsilon + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\cdot \\sum_{i=0}^{r-1} l^{(i)}.\n\\end{aligned}\n\nIn summary, we have obtained that when some bad events E^{(i)} occur, their probability is strictly less than\\frac{3}{2} \\epsilon + \\frac{2m + 1}{\\sqrt{\\rho}}\\cdot \\frac{|\\mathcal{D}^{(0)}| + 1}{|\\mathbb{F}|} \\cdot \\sum_{i=0}^{r-1} l^{(i)} = \\epsilon_C,\n\nWhen no bad events occur, the following equation holds\\begin{aligned}\n  \\text{agree}_{\\mu^{(r)}} \\left(f^{(r)}, V^{(r)}\\right) & =  \\mathbb{E}_{g^{(r)} \\in \\mathcal{D}^{(r)}}\\left[ \\mu^{(r)}(g^{(r)}) \\right] \\\\\n  & \\color{blue}{(\\text{Because } f^{(r)} \\in V^{(r)})} \\\\\n  & \\le \\max \\left( \\text{agree}_{\\mu^{(r - 1)}} \\left(f^{(r - 1)}, V^{(r - 1)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right) \\\\\n  & \\color{blue}{(\\text{According to the definition of event } E^{(i+1)}, \\text{see equation } (8.9))} \\\\\n  & \\le \\max \\left( \\text{agree}_{\\mu^{(r - 2)}} \\left(f^{(r - 2)}, V^{(r - 2)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right) \\\\\n  & \\le \\ldots \\\\\n  & \\le \\max \\left( \\text{agree}_{\\mu^{(0)}} \\left(f^{(0)}, V^{(0)}\\right), \\sqrt{\\rho}(1 + 1/2m) \\right) \\\\\n  & = \\max \\left( \\alpha, \\sqrt{\\rho}(1 + 1/2m) \\right) \\\\\n  & = \\alpha^{(0)}(\\rho, m)\n\\end{aligned}.\n\nAt this point, we have proved that equation (8.7) holds, thus proving Lemma 8.2. \n\n\\Box","type":"content","url":"/fri/bciks20-proximity-gaps#proof-of-lemma-8-2","position":23},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"References"},"type":"lvl2","url":"/fri/bciks20-proximity-gaps#references","position":24},{"hierarchy":{"lvl1":"Dive into BCIKS20-FRI Soundness","lvl2":"References"},"content":"[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[RVW13] Guy N. Rothblum, Salil Vadhan, and Avi Wigderson. Interactive proofs of proximity: delegating computation in sublinear time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 793–802. ACM, 2013.","type":"content","url":"/fri/bciks20-proximity-gaps#references","position":25},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview"},"type":"lvl1","url":"/fri/deepfold","position":0},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article mainly introduces the key ideas of the DeepFold protocol [GLHQTZ24]. The DeepFold protocol is a polynomial commitment scheme (PCS) for multilinear polynomials, combining ideas from DEEP-FRI [BGKS20] and BaseFold [ZCF24]. The BaseFold protocol [ZCF24] is also a PCS for multilinear polynomials, combining the FRI protocol and the sumcheck protocol. However, in its original paper, it is limited to unique decoding. If it could be optimized to work under list decoding, the number of queries made by the verifier to achieve the same security parameter \\lambda could be reduced, thus also reducing the size of the proof. The DeepFold protocol adopts the DEEP method from DEEP-FRI to achieve this. However, in [H24], Haböck proved the security of the BaseFold protocol for Reed-Solomon codes under list decoding. On the other hand, the STIR protocol [ACFY24a] has fewer queries compared to the DEEP-FRI protocol. The WHIR protocol [ACFY24b], which combines the STIR protocol and the BaseFold protocol, can achieve fewer queries compared to the DeepFold protocol, although its security under list decoding has not been rigorously proven yet.","type":"content","url":"/fri/deepfold","position":1},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"DEEP Method: From Unique Decoding to List Decoding"},"type":"lvl2","url":"/fri/deepfold#deep-method-from-unique-decoding-to-list-decoding","position":2},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"DEEP Method: From Unique Decoding to List Decoding"},"content":"First, let’s review the BaseFold protocol. Taking a trivariate (let \\mu = 3) linear polynomial as an example, let\\tilde{f}(X_1, X_2, X_3) = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1 X_2 + a_4 X_3 + a_5 X_1X_3 + a_6 X_2 X_3 + a_7 X_1 X_2 X_3\n\nThe corresponding univariate polynomial isf(X) = a_0 + a_1 X + a_2 X^2 + a_3 X^3 + a_4 X^4 + a_5 X^5 + a_6 X^6 + a_7 X^7\n\nf and \\tilde{f} are referred to as “twin polynomials” in the [GLHQTZ24] paper, sharing the same coefficients \\vec{a} = (a_0, a_1, \\cdots, a_7). Suppose the query point is \\vec{z} = \\{z_1, z_2, z_3\\}, and the prover wants to commit to the value of \\tilde{f} at this point as \\tilde{f}(\\vec{z}). The BaseFold protocol first converts the committed value \\tilde{f}(\\vec{z}) into a sum form over a hypercube \\{0,1\\}^3, i.e.,\\tilde{f}(\\vec{z}) = \\sum_{\\vec{b} \\in \\{0,1\\}^3} \\tilde{f}(\\vec{b}) \\cdot \\tilde{eq}(\\vec{b}, \\vec{z}) \\tag{1}\n\nwhere \\tilde{eq}(\\vec{b}, \\vec{z}) = \\prod_{i = 1}^3((1 - \\vec{b}[i])(1 - \\vec{z}[i]) + \\vec{b}[i] \\cdot \\vec{z}[i]). To prove that equation (1) is correct, the sumcheck protocol can be used. However, in the last step of the sumcheck protocol, it will require obtaining the value of \\tilde{f} at a random point \\tilde{f}(r_1, r_2, r_3). This value can be obtained through the FRI protocol for f. For an honest prover, a Merkle tree can be used to commit to a vector \\vec{v} = f^{(0)}(X)|_{L_0} \\in \\mathrm{RS}[\\mathbb{F}, L_0, \\rho], where f^{(0)}(X) = f(X), rate \\rho = 2^{3} / |L_0|, and evaluation domain L_{i + 1} = \\{x^2: x \\in L_i\\}. Express f^{(0)}(X) as even and odd term polynomials\\begin{aligned}\n    f^{(0)}(X) & = f_E^{(1)}(X^2) + X \\cdot f_O^{(1)}(X^2) \\\\\n    & = (a_0 + a_2 X^2 + a_4 X^4 + a_6 X^6) + X \\cdot (a_1 + a_3 X^2 + a_5 X^4 + a_7 X^6)\n\\end{aligned}\n\nThen use the same random number r_1 \\in \\mathbb{F} as in sumcheck to fold f_E^{(1)} and f_O^{(1)} to get a new polynomial f^{(1)}(X)\\begin{aligned}\n    f^{(1)}(X) & = f_E^{(1)}(X) + r_1 \\cdot f_O^{(1)}(X) \\\\\n    & = (a_0 + a_2 X + a_4 X^2 + a_6 X^3) + r_1 \\cdot (a_1 + a_3 X + a_5 X^2 + a_7 X^3)\n\\end{aligned}\n\nIt can be found that the multilinear polynomial corresponding to f^{(1)}(X) is\\begin{aligned}\n    \\tilde{f}(r_1, X_2, X_3) & = a_0 + a_1 r_1 + a_2 X_2 + a_3 \\cdot r_1 X_2 + a_4 X_3 + a_5 \\cdot  r_1X_3 + a_6 X_2 X_3 + a_7 \\cdot r_1 X_2 X_3 \\\\\n    & = (a_0 + a_2 X_2 + a_4 X_3 + a_6 X_2X_3) + r_1 \\cdot (a_1 + a_3 X_2 + a_5 X_3 + a_7 X_2X_3)\n\\end{aligned}\n\nThe prover sends the Merkle commitment \\vec{v}^{(1)} = f^{(1)}|_{L_1} to the verifier. Generally, continuing the above steps, divide f^{(i - 1)}(X) into odd and even terms,f^{(i - 1)}(X) = f_E^{(i)}(X^2) + X \\cdot f_O^{(i)}(X^2) \\tag{2}\n\nThen fold using the random number r_i,f^{(i)}(X) = f_E^{(i)}(X) + r_i \\cdot f_O^{(i)}(X) \\tag{3}\n\nThe prover sends the Merkle commitment \\vec{v}^{(i)} = f^{(i)}|_{L_i} to the verifier. In the last step of the FRI protocol, we can obtain f^{(3)}(X) = \\tilde{f}(r_1, r_2, r_3) as a constant, which is exactly the value that the last step of sumcheck wants to obtain. This way, performing sumcheck protocol and FRI protocol simultaneously completes the commitment of the multilinear polynomial, which is the idea of the BaseFold protocol.\n\nIt can be found that in the BaseFold protocol, the role of the FRI protocol, in addition to its own purpose of ensuring that \\vec{v} is \\Delta close to the corresponding RS code space \\mathrm{RS}[\\mathbb{F}, L_0, \\rho], also provides the value of f^{(3)} to ensure the correctness of \\tilde{f}(r_1, r_2, r_3). It is mentioned in [GLHQTZ24] that the original FRI protocol only requires the provided vector \\vec{v} to be close to some RS codes, but in the i-th round, it does not specifically require which codes \\vec{v}^{(i)} should be close to. Under unique decoding, there is at most one code f^{(i)} close to the corresponding \\vec{v}^{(i)} in the i-th round. If it is list decoding, it means that there can be multiple codes f^{(i)} close to \\vec{v}^{(i)}, and a malicious prover can choose f^{(i)^{'}} to proceed with the protocol, which can also pass subsequent checks, and in the last round, f^{(3)^{'}} is obtained, which is not a correct value.\n\nTherefore, we now need a method to ensure the correctness of f^{(\\mu)} = f^{(3)} under list decoding, that is, in the i-th round, we need to ensure that only f^{(i)} can be \\Delta close to \\vec{v}^{(i)}, and f^{(i)} corresponds to the correct multivariate polynomial \\tilde{f}(r_1, \\ldots, r_i, X_{i + 1}, \\ldots, X_{\\mu}). The DeepFold protocol uses the DEEP (Domain Extending for Eliminating Pretenders) technique from the DEEP-FRI protocol [BGKS20] to solve this problem. In the i-th round, a random number \\alpha_i is selected from \\mathbb{F} instead of from L_i, and the verifier additionally queries two values f^{(i - 1)}(\\pm \\alpha_i), from which the verifier can calculate the value of f^{(i)}(\\alpha_i^2) by themselves. Sincef^{(i)}_E(X^2) = \\frac{f^{(i - 1)}(X) + f^{(i - 1)}(- X)}{2}, \\quad f^{(i)}_O(X^2) = \\frac{f^{(i - 1)}(X) - f^{(i - 1)}(- X)}{2X}\n\nTherefore\\begin{aligned}\n    f^{(i)}(X^2) & = f^{(i)}_E(X^2) + r_i \\cdot f^{(i)}_O(X^2) \\\\\n    & = \\frac{f^{(i - 1)}(X) + f^{(i - 1)}(- X)}{2} + r_i \\cdot \\frac{f^{(i - 1)}(X) - f^{(i - 1)}(- X)}{2X}\n\\end{aligned}\n\nSubstituting X = \\alpha_i, we can getf^{(i)}(\\alpha_i^2) = \\frac{f^{(i - 1)}(\\alpha_i) + f^{(i - 1)}(- \\alpha_i)}{2} + r_i \\cdot \\frac{f^{(i - 1)}(\\alpha_i) - f^{(i - 1)}(- \\alpha_i)}{2 \\cdot \\alpha_i}\n\nThe verifier can calculate the value of f^{(i)}(\\alpha_i^2) based on the above equation. Since \\alpha_i is a random number selected from the entire \\mathbb{F}, with high probability, there will not be two different polynomials f^{(i)} within the \\Delta range of \\vec{v}^{(i)} that satisfy the same value at f^{(i)}(\\alpha_i^2) under list decoding. This way, through the selection of \\alpha_i, the list decoding is restricted to only select the unique polynomial f^{(i)}.\n\nLet’s explain why, with high probability, there can only be a unique polynomial f^{(i)} that satisfies the same value at f^{(i)}(\\alpha_i^2). Suppose there are two different polynomials f_1^{(i)} and f_2^{(i)} that have the same value at a random point \\alpha \\in \\mathbb{F}, i.e., f_1^{(i)}(\\alpha) = f_2^{(i)}(\\alpha), and they are both within the \\Delta range of \\vec{v}^{(i)}. Let |\\vec{v}^{(i)}| = n, \\Delta = 1 - \\rho - \\varepsilon, and there are no more than \\mathcal{L} codewords within the \\Delta range of \\vec{v}^{(i)}. According to the conjecture in [BGKS20], we know that |\\mathcal{L}| \\le \\mathrm{poly}(n). Since f_1^{(i)}(\\alpha) = f_2^{(i)}(\\alpha), the polynomial f_1^{(i)} - f_2^{(i)} has a value of 0 at \\alpha, and the polynomial degrees of f_1^{(i)} and f_2^{(i)} will not exceed n, so the degree of f_1^{(i)} - f_2^{(i)} will also not exceed n, and there are at most n zeros in \\mathbb{F}. Since \\alpha \\in \\mathbb{F}, the probability of such f_1^{(i)} - f_2^{(i)} being 0 at point \\alpha will not exceed n / |\\mathbb{F}|. There are \\binom{|\\mathcal{L}|}{2} ways to choose different f_1^{(i)} and f_2^{(i)} within the \\Delta range of \\vec{v}^{(i)}, so the overall probability will not exceed n \\cdot \\binom{|\\mathcal{L}|}{2} / |\\mathbb{F}|. If |\\mathbb{F}| is large enough, this probability is very small. Therefore, it’s the same for \\alpha_i^2, with high probability, there is only one polynomial f^{(i)} that satisfies the same value at f^{(i)}(\\alpha_i^2).\n\nNow through the DEEP technique, we can convert list decoding to unique decoding, solving the problem that under list decoding, there may be multiple polynomials within the \\Delta range of \\vec{v}^{(i)}, and the prover can choose different polynomials leading to inconsistent f^{(\\mu)}. Now the remaining problem is to let the verifier verify the correctness of the value of f^{(i)}(\\alpha_i^2) in each round.","type":"content","url":"/fri/deepfold#deep-method-from-unique-decoding-to-list-decoding","position":3},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"Ensuring the Correctness of DEEP Method Evaluation"},"type":"lvl2","url":"/fri/deepfold#ensuring-the-correctness-of-deep-method-evaluation","position":4},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"Ensuring the Correctness of DEEP Method Evaluation"},"content":"The [GLHQTZ24] paper mentions that in the DEEP-FRI paper [BGKS20], the quotient method can be used to verify the correctness of f^{(i)}(\\alpha_i^2). According to the folding relation in equation (3),f^{(i)}(X) = f_E^{(i)}(X) + r_i \\cdot f_O^{(i)}(X)\n\nA new form can be constructed, namelyf^{(i)}(X) = \\frac{(f_E^{(i)}(X) + r_i \\cdot f_O^{(i)}(X)) - (f_E^{(i)}(\\alpha_i^2) + r_i \\cdot f_O^{(i)}(\\alpha_i^2))}{X - \\alpha_i^2} \\tag{4}\n\nIf f^{(i)}(\\alpha_i^2) is correct, then the newly constructed f^{(i)}(X) above is a polynomial, thus transforming the problem of verifying the correctness of f^{(i)}(\\alpha_i^2) into an IOPP problem about f^{(i)}. However, this method is not applicable in the current multilinear polynomial PCS scheme, because although equation (4) can ensure the correctness of f^{(i)}(\\alpha_i^2) in each round, the f^{(\\mu)} obtained at the end of the protocol is not equal to \\tilde{f}(\\vec{r}).\n\nThe DeepFold protocol provides a new method to ensure the correctness at these points \\{\\alpha_i\\}. Let’s still use the case of \\mu = 3 to explain this method. Suppose now the verifier selects a random number \\alpha_1 \\leftarrow \\$ \\mathbb{F} in the i = 1 round, and now the verifier wants to ensure the correctness of f^{(1)}(\\alpha_1^2). First, the verifier can query the prover for the values of f^{(0)}(\\pm \\alpha_1), substituting into the expression of f(X), we get\\begin{aligned}\n    f^{(0)}(\\pm \\alpha_1) & = a_0 + a_1 \\cdot (\\pm \\alpha_1) + a_2 \\cdot (\\pm \\alpha_1)^2 + a_3 \\cdot (\\pm \\alpha_1)^3 \\\\\n    & \\quad + a_4 \\cdot (\\pm \\alpha_1)^4 + a_5 \\cdot (\\pm \\alpha_1)^5 + a_6 \\cdot (\\pm \\alpha_1)^6 + a_7 \\cdot (\\pm \\alpha_1)^7 \\\\\n    & = a_0 + a_1 \\cdot (\\pm \\alpha_1) + a_2 \\cdot \\alpha_1^2 + a_3 \\cdot (\\pm \\alpha_1)\\cdot \\alpha_1^2 \\\\\n    & \\quad + a_4 \\cdot \\alpha_1^4 + a_5 \\cdot (\\pm \\alpha_1) \\cdot \\alpha_1^4 + a_6 \\cdot \\alpha_1^2 \\cdot \\alpha_1^4 + a_7 \\cdot (\\pm \\alpha_1) \\cdot \\alpha_1^2 \\cdot \\alpha_1^4\n\\end{aligned}\n\nThis exactly corresponds to the value of the multilinear polynomial \\tilde{f}(X_1, X_2, X_3) at the point (\\pm \\alpha_1, \\alpha_1^2, \\alpha_1^4),\\begin{aligned}\n    \\tilde{f}(\\pm \\alpha_1, \\alpha_1^2, \\alpha_1^4)  & = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1 X_2 + a_4 X_3 + a_5 X_1X_3 + a_6 X_2 X_3 + a_7 X_1 X_2 X_3 \\\\\n    & = a_0 + a_1 \\cdot (\\pm \\alpha_1) + a_2 \\cdot \\alpha_1^2 + a_3 \\cdot (\\pm \\alpha_1)\\cdot \\alpha_1^2 \\\\\n    & \\quad + a_4 \\cdot \\alpha_1^4 + a_5 \\cdot (\\pm \\alpha_1) \\cdot \\alpha_1^4 + a_6 \\cdot \\alpha_1^2 \\cdot \\alpha_1^4 + a_7 \\cdot (\\pm \\alpha_1) \\cdot \\alpha_1^2 \\cdot \\alpha_1^4\n\\end{aligned}\n\nTherefore f^{(0)}(\\pm \\alpha_1) = \\tilde{f}(\\pm \\alpha_1, \\alpha_1^2, \\alpha_1^4). After the verifier receives f^{(0)}(\\pm \\alpha_1), they can calculate f^{(1)}(\\alpha_1^2) themselves, that is, by calculating using the following equationf^{(i)}(\\alpha_i^2) = \\frac{f^{(i - 1)}(\\alpha_i) + f^{(i - 1)}(- \\alpha_i)}{2} + r_i \\cdot \\frac{f^{(i - 1)}(\\alpha_i) - f^{(i - 1)}(- \\alpha_i)}{2 \\cdot \\alpha_i} \\tag{5}\n\nSimilar to the derivation of f^{(0)}(\\pm \\alpha_1) above, the f^{(1)}(\\alpha_1^2) obtained at this time should have the following relationship with the corresponding multilinear polynomial:f^{(1)}(\\alpha_1^2) = \\tilde{f}(r_1, \\alpha_1^2, \\alpha_1^4)\n\nNow, to ensure the correctness of f^{(1)}(\\alpha_1^2), the verifier can query the prover for f^{(1)}(-\\alpha_1^2), and the verifier can calculate f^{(2)}(\\alpha_1^4) themselves using equation (5), at this timef^{(2)}(\\alpha_1^4) = \\tilde{f}(r_1, r_2, \\alpha_1^4)\n\nNow the correctness of f^{(1)}(\\alpha_1^2) has been transformed into proving the correctness of f^{(2)}(\\alpha_1^4). Similarly, the verifier queries the prover for f^{(2)}(-\\alpha_1^4), and the verifier can calculate f^{(3)}(\\alpha_1^8), at this time it should equalf^{(3)}(\\alpha_1^8) = \\tilde{f}(r_1, r_2, r_3)\n\nIn this way, the correctness of f^{(2)}(\\alpha_1^4) is finally transformed into the correctness of the value of f^{(3)}(\\alpha_1^8), which should equal \\tilde{f}(r_1, r_2, r_3), which is exactly the value that will be obtained in the last step of FRI.\n\nThrough the above process, we can also find that if i \\neq 1, generally, in the i-th round, the correctness of the value of f^{(i-1)}(\\pm \\alpha_i) provided is transformed into verifying the correctness of f^{(i)}(\\alpha_i^2), by the prover additionally sending f^{(i)}(-\\alpha_i^2), it is transformed into verifying f^{(i + 1)}(\\alpha_i^4), until finally all are transformed into verifying the correctness of f^{(\\mu)} = \\tilde{f}(r_1, r_2, \\ldots, r_{\\mu}), which is exactly what the FRI protocol provides.","type":"content","url":"/fri/deepfold#ensuring-the-correctness-of-deep-method-evaluation","position":5},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"DeepFold Protocol"},"type":"lvl2","url":"/fri/deepfold#deepfold-protocol","position":6},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"DeepFold Protocol"},"content":"Summarizing the introduction of the DEEP method above, in order to avoid a malicious prover possibly selecting an incorrect polynomial f^{(i)'} within the \\Delta range of \\vec{v}^{(i)} to pass verification under list decoding, the verifier selects \\alpha_i from the range of \\mathbb{F} in each round, forcing the prover to provide only the unique polynomial f^{(i)}, making its value at f^{(i)}(\\alpha_i^2) correct. To verify the correctness of the value at f^{(i)}(\\alpha_i^2), the prover provides f^{(i)}(-\\alpha_i^2), the verifier calculates f^{(i + 1)}(\\alpha_i^4) on their own, until finally it is transformed into verifying the correctness of f^{(\\mu)} = \\tilde{f}(r_1, \\ldots, r_{\\mu}). Below, taking the PCS of a trivariate linear polynomial as an example, we will go through the complete DeepFold protocol [GLHQTZ24]. Although the protocol process has many steps, the core ideas are still the two points mentioned above.\n\nIn the commitment phase of \\tilde{f}, the polynomial commitment sent by the prover to the verifier is \\mathcal{C} = \\langle rt_0, \\alpha, c \\rangle.\n\nThe prover calculates \\vec{v} = f^{(0)}|_{L_0}, and commits to this vector using a Merkle tree, that is, sends \\mathsf{MT.Commit}(\\vec{v}) \\rightarrow rt_0 to the verifier.\n\nThe verifier sends a random point \\alpha \\leftarrow \\$ \\mathbb{F}.\n\nThe prover calculates c := f^{(0)}(\\alpha) and sends c to the verifier.\n\nThe prover wants to prove to the verifier that: at the query point \\vec{z} = \\{z_1, z_2, z_3\\}, \\tilde{f}(z_1, z_2, z_3) = y. At the same time, the verifier has \\mathcal{C} = \\langle rt_0, \\alpha, c \\rangle received from the prover during the polynomial commitment phase. The prover and verifier perform the following protocol process:\nStep 1: Let A_0:= \\{\\vec{z}, \\vec{\\alpha}\\}, where \\vec{\\alpha} = (\\alpha, \\alpha^2, \\alpha^4).\nStep 2: For each round i \\in [3], perform the following steps:\n\n2.1 When i = 1\n\na. The verifier sends \\alpha_1 \\leftarrow \\$ \\mathbb{F} to the prover. Let A_0 := \\{A_0, \\vec{\\alpha_1}\\} = \\{\\vec{z}, \\vec{\\alpha}, \\vec{\\alpha_1}\\}, where \\vec{\\alpha_1} = (\\alpha_1, \\alpha_1^2, \\alpha_1^4).\n\nThe \\alpha_1 sent in this step is the random number outside of L_0 used in the DEEP method to limit the prover to only send the unique polynomial f^{(1)}. The vector \\vec{\\alpha_1} = (\\alpha_1, \\alpha_1^2, \\alpha_1^4) is for subsequent continuous verification of the correctness of f^{(1)}(\\alpha_1^2) = \\tilde{f}(r_1, \\alpha_1^2, \\alpha_1^4).\n\nb. Let A_1 := \\emptyset, for each \\vec{\\omega} \\in A_0 = \\{\\vec{z}, \\vec{\\alpha}, \\vec{\\alpha_1}\\}, the prover sends the following polynomials to the verifier:\\begin{aligned}\n    & g_{\\vec{z}_{[2:]}} = g_{(z_2, z_3)} := \\tilde{f}(X, z_2, z_3) \\\\\n    & g_{\\vec{\\alpha}_{[2:]}} =g_{(\\alpha^2, \\alpha^4)} := \\tilde{f}(X, \\alpha^2, \\alpha^4) \\\\\n    & g_{\\vec{\\alpha_1}_{[2:]}} =g_{(\\alpha_1^2, \\alpha_1^4)} := \\tilde{f}(X, \\alpha_1^2, \\alpha_1^4)\n\\end{aligned}\n\nLet A_1 := \\{A_1, \\vec{w}_{[2:]}\\} = \\{(z_2, z_3), (\\alpha^2, \\alpha^4), (\\alpha_1^2, \\alpha_1^4)\\}.\n\nThe g(X) polynomials in this step are similar to the univariate polynomials constructed in the sumcheck protocol to prove the correctness of the sum.\n\nc. The verifier sends r_1 \\leftarrow \\$ \\mathbb{F} to the prover.\nd. The prover calculates the folded polynomial f^{(1)}(X) = f_E^{(1)}(X) + r_1 \\cdot f_O^{(1)}(X), where f_E^{(1)}(X) and f_O^{(1)}(X) should satisfyf^{(0)}(X) = f_E^{(1)}(X^2) + X \\cdot f_O^{(1)}(X^2)\n\nThe meaning of satisfying this equation is to ensure that f_E^{(1)}(X^2) and f_O^{(1)}(X^2) are the even and odd term functions of f^{(0)}(X).\n\ne. Let \\vec{v}^{(1)} = f^{(1)}|_{L_1}, the prover sends the Merkle tree commitment of vector \\vec{v}^{(1)} to the verifier, i.e., \\mathsf{MT.Commit}(\\vec{v}^{(1)}) \\rightarrow rt_1.\n\n2.2 When i = 2\n\na. The verifier sends \\alpha_2 \\leftarrow \\$ \\mathbb{F} to the prover. Let A_1 := \\{A_1, \\vec{\\alpha_2}\\} = \\{(z_2, z_3), (\\alpha^2, \\alpha^4), (\\alpha_1^2, \\alpha_1^4), (\\alpha_2, \\alpha_2^2)\\}, where \\vec{\\alpha_2} = (\\alpha_2, \\alpha_2^2).\n\nNote that the length of each vector in A_1 has now changed to 2. The \\alpha_2 selected here is to use the DEEP method in the second round to limit the prover to only send the unique polynomial f^{(2)}(X), and ensure that the polynomial f^{(2)}(X) satisfies f^{(2)}(\\alpha_2^2) = \\tilde{f}(r_1, r_2, \\alpha_2^2) at point \\alpha_2^2.\n\nb. Let A_2 := \\emptyset, for each \\vec{\\omega} \\in A_1 = \\{(z_2, z_3), (\\alpha^2, \\alpha^4), (\\alpha_1^2, \\alpha_1^4), (\\alpha_2, \\alpha_2^2)\\}, the prover sends the following polynomials to the verifier:\\begin{aligned}\n    & g_{\\vec{z}_{[2:]}} = g_{(z_3)} := \\tilde{f}(r_1, X, z_3) \\\\\n    & g_{\\vec{\\alpha}_{[2:]}} =g_{(\\alpha^4)} := \\tilde{f}(r_1, X, \\alpha^4) \\\\\n    & g_{\\vec{\\alpha_1}_{[2:]}} =g_{(\\alpha_1^4)} := \\tilde{f}(r_1, X, \\alpha_1^4) \\\\\n    & g_{\\vec{\\alpha_2}_{[2:]}} =g_{(\\alpha_2^2)} := \\tilde{f}(r_1, X, \\alpha_2^2)\n\\end{aligned}\n\nLet A_2 := \\{A_2, \\vec{w}_{[2:]}\\} = \\{(z_3), (\\alpha^4), (\\alpha_1^4), (\\alpha_2^2)\\}.\n\nc. The verifier sends r_2 \\leftarrow \\$ \\mathbb{F} to the prover.\nd. The prover calculates the folded polynomial f^{(2)}(X) = f_E^{(2)}(X) + r_2 \\cdot f_O^{(2)}(X), where f_E^{(2)}(X) and f_O^{(2)}(X) should satisfyf^{(1)}(X) = f_E^{(2)}(X^2) + X \\cdot f_O^{(2)}(X^2)\n\ne. Let \\vec{v}^{(2)} = f^{(2)}|_{L_2}, the prover sends the Merkle tree commitment of vector \\vec{v}^{(2)} to the verifier, i.e., \\mathsf{MT.Commit}(\\vec{v}^{(2)}) \\rightarrow rt_2.\n\n2.3 When i = 3\n\na. The verifier sends \\alpha_3 \\leftarrow \\$ \\mathbb{F} to the prover. Let A_2 := \\{A_2, \\vec{\\alpha_3}\\} = \\{(z_3), (\\alpha^4), (\\alpha_1^4), (\\alpha_2^2), (\\alpha_3)\\}, where \\vec{\\alpha_3} = (\\alpha_3).\n\nb. The prover sends the linear function to the verifierg(X) := \\tilde{f}(r_1, r_2, X)\n\nNow it’s the last round, directly send the function g(X).\n\nc. The verifier sends r_3 \\leftarrow \\$ \\mathbb{F} to the prover.\nd. The prover calculates the folded polynomial f^{(3)}(X) = f_E^{(3)}(X) + r_3 \\cdot f_O^{(3)}(X), where f_E^{(3)}(X) and f_O^{(3)}(X) should satisfyf^{(2)}(X) = f_E^{(3)}(X^2) + X \\cdot f_O^{(3)}(X^2)\n\ne. Let \\vec{v}^{(3)} = f^{(3)}|_{L_3}, the prover sends f^{(3)} \\in \\mathbb{F} to the verifier.\n\nIn the last round, FRI will finally fold into a constant polynomial, so here directly send a value f^{(3)}.\n\nThe following steps are the verification process performed by the verifier.\n\nStep 3: The verifier checks\\begin{aligned}\n    & g_{\\vec{z}_{[2:]}}(z_1)  = y \\\\\n    & g_{\\vec{\\alpha}_{[2:]}}(\\alpha) = c \\\\\n    & g(r_{3}) = f^{(3)}\n\\end{aligned}\n\nAccording to the construction of the g(X) function when i = 1 and i = 3, for an honest prover, the above three equations hold because> \\begin{aligned}\n>     & g_{\\vec{z}_{[2:]}}(z_1) = \\tilde{f}(z_1, z_2, z_3) = y\\\\\n>     & g_{\\vec{\\alpha}_{[2:]}}(\\alpha) = \\tilde{f}(\\alpha, \\alpha^2, \\alpha^4) = c\\\\\n>     & g(r_3) = \\tilde{f}(r_1, r_2, r_3) = f^{(3)}\n> \\end{aligned}\n>\n\nNext, for each round, the verifier also needs to perform the following checks.\n\n3.1 When i = 1\n\na. For each \\vec{w} \\in A_0 = \\{\\vec{z}, \\vec{\\alpha}, \\vec{\\alpha_1}\\}, check g_{\\vec{w}}(r_0) = g_{\\vec{w}_{[2:]}}(w_1), i.e., check\\begin{aligned}\n    & g_{(z_1, z_2, z_3)}(r_0) = g_{(z_2, z_3)}(z_1) \\\\\n    & g_{(\\alpha, \\alpha^2, \\alpha^4)}(r_0) = g_{(\\alpha^2, \\alpha^4)}(\\alpha) \\\\\n    & g_{(\\alpha_1, \\alpha_1^2, \\alpha_1^4)}(r_0) = g_{(\\alpha_1^2, \\alpha_1^4)}(\\alpha_1)\n\\end{aligned}\n\n🐞fix\nI think in the original paper’s Step 3\n\nFor each round i, where i \\in [\\mu] ,\na. For each \\vec{w} \\in A_{i - 1}, if i < \\mu, \\mathcal{V} checks  g_{\\vec{w}}(r_i) = g_{\\vec{w}_{[2:]}}(w_1) ; otherwise, \\mathcal{V} checks g_{\\vec{w}}(r_i) = g(w_1) .\n\nshould be changed to, when i < \\mu, verifier checks g_{\\vec{w}}(r_{i - 1}) = g_{\\vec{w}_{[2:]}}(w_1), otherwise checks g_{\\vec{w}}(r_{i - 1}) = g(w_1). The reason is that, for example, when i = 2, g_{\\vec{w}}(r_1) = g_{\\vec{w}_{[2:]}}(w_1) does not hold when substituted into the function construction sent by the prover earlier.\n\nActually, the last equation above does not need to be checked, i.e., g_{(\\alpha_1, \\alpha_1^2, \\alpha_1^4)}(r_0) = g_{(\\alpha_1^2, \\alpha_1^4)}(\\alpha_1). We can verify that the above equations are correct because substituting into the g(X) equation from Round 1 gives\\begin{aligned}\n    & g_{(z_1, z_2, z_3)}(r_0) = \\tilde{f}(z_1, z_2, z_3) = y & g_{(z_2, z_3)}(z_1) = \\tilde{f}(z_1, z_2, z_3)\\\\\n    & g_{(\\alpha, \\alpha^2, \\alpha^4)}(r_0) = \\tilde{f}(\\alpha, \\alpha^2, \\alpha^4) = c & g_{(\\alpha^2, \\alpha^4)}(\\alpha) = \\tilde{f}(\\alpha, \\alpha^2, \\alpha^4)\n\\end{aligned}\n\n3.2 When i = 2\n\na. For each \\vec{w} \\in A_1 = \\{(z_2, z_3), (\\alpha^2, \\alpha^4), (\\alpha_1^2, \\alpha_1^4), (\\alpha_2, \\alpha_2^2)\\}, check g_{\\vec{w}}(r_1) = g_{\\vec{w}_{[2:]}}(w_1), i.e., check\\begin{aligned}\n    & g_{(z_2, z_3)}(r_1) = g_{(z_3)}(z_2) \\\\\n    & g_{(\\alpha^2, \\alpha^4)}(r_1) = g_{(\\alpha^4)}(\\alpha^2) \\\\\n    & g_{(\\alpha_1^2, \\alpha_1^4)}(r_1) = g_{(\\alpha_1^4)}(\\alpha_1^2) \\\\\n    & g_{(\\alpha_2, \\alpha_2^2)}(r_1) = g_{(\\alpha_2^2)}(\\alpha_2)\n\\end{aligned}\n\nThe last equation g_{(\\alpha_2, \\alpha_2^2)}(r_1) = g_{(\\alpha_2^2)}(\\alpha_2) does not need to be checked. We can verify that the above equations hold because substituting into the g(X) equations from Rounds 1 and 2 gives\\begin{aligned}\n    & g_{(z_2, z_3)}(r_1) = \\tilde{f}(r_1, z_2, z_3) &  g_{(z_3)}(z_2) = \\tilde{f}(r_1, z_2, z_3)\\\\\n    & g_{(\\alpha^2, \\alpha^4)}(r_1) = \\tilde{f}(r_1, \\alpha^2, \\alpha^4) & g_{(\\alpha^4)}(\\alpha^2) = \\tilde{f}(r_1, \\alpha^2, \\alpha^4)\\\\\n    & g_{(\\alpha_1^2, \\alpha_1^4)}(r_1) = \\tilde{f}(r_1, \\alpha_1^2, \\alpha_1^4) & g_{(\\alpha_1^4)}(\\alpha_1^2) = \\tilde{f}(r_1, \\alpha_1^2, \\alpha_1^4) \n\\end{aligned}\n\n3.2 When i = 3\n\na. For each \\vec{w} \\in A_2 = \\{(z_3), (\\alpha^4), (\\alpha_1^4), (\\alpha_2^2), (\\alpha_3)\\}, check g_{\\vec{w}}(r_2) = g(w_1), i.e., check\\begin{aligned}\n    & g_{(z_3)}(r_2) = g(z_3) \\\\\n    & g_{(\\alpha^4)}(r_2) = g(\\alpha^4) \\\\\n    & g_{(\\alpha_1^4)}(r_2) = g(\\alpha_1^4) \\\\\n    & g_{(\\alpha_2^2)}(r_2) = g(\\alpha_2^2) \\\\\n     & g_{(\\alpha_3)}(r_2) = g(\\alpha_3) \n\\end{aligned}\n\nSimilarly, the last equation g_{(\\alpha_3)}(r_2) = g(\\alpha_3) does not need to be checked. We can verify that the above 4 equations hold because substituting into the g(X) equations from Rounds 2 and 3 gives\\begin{aligned}\n    & g_{(z_3)}(r_2) = \\tilde{f}(r_1, r_2, z_3) &  g(z_3) = \\tilde{f}(r_1, r_2, z_3)\\\\\n    & g_{(\\alpha^4)}(r_2) = \\tilde{f}(r_1, r_2, \\alpha^4) & g(\\alpha^4) = \\tilde{f}(r_1, r_2, \\alpha^4)\\\\\n    & g_{(\\alpha_1^4)}(r_2) = \\tilde{f}(r_1, r_2, \\alpha_1^4) & g(\\alpha_1^4) = \\tilde{f}(r_1, r_2, \\alpha_1^4) \\\\\n    & g_{(\\alpha_2^2)}(r_2) = \\tilde{f}(r_1, r_2, \\alpha_2^2)  & g(\\alpha_2^2) = \\tilde{f}(r_1, r_2, \\alpha_2^2)\n\\end{aligned}\n\nStep 4: Repeat query s times:\na. The verifier sends \\beta_0 \\leftarrow \\$ L_0 to the prover. For i \\in [3], define \\beta_i := \\beta_{i - 1}^2.\nb. For each i \\in [3], the prover uses \\mathsf{MT.Open} to open f^{(i - 1)}(\\beta_{i - 1}) and f^{(i - 1)}(-\\beta_{i - 1}).\nc. The verifier checks if the results sent by the prover are correct by calling \\mathsf{MT.Verify}.\nd. For each i \\in [3], the verifier needs to check if the following three points are on a straight line:\\left(\\beta_{i - 1}, f^{(i - 1)}(\\beta_{i - 1})\\right), \\quad \\left(-\\beta_{i - 1}, f^{(i - 1)}(-\\beta_{i - 1})\\right), \\quad \\left(r_i, f^{(i)}(\\beta_{i})\\right)\n\nIn this step, the verifier is performing FRI folding queries, randomly checking if the folding is correct, repeating the query s times.\n\nStep 5: If all the above checks pass, the verifier outputs 1, indicating acceptance; otherwise, the verifier outputs 0, indicating rejection.","type":"content","url":"/fri/deepfold#deepfold-protocol","position":7},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"Connection Between DeepFold and Sumcheck"},"type":"lvl2","url":"/fri/deepfold#connection-between-deepfold-and-sumcheck","position":8},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"Connection Between DeepFold and Sumcheck"},"content":"Due to the introduction of DEEP-FRI, the DeepFold protocol requires proving the correctness of values of the multilinear polynomial \\tilde{f} at specific points in each round. Using n = 3 as an example, as shown in the figure below, the Prover needs to prove the values represented by the green parts in the diagram.\n\nThese value proofs implicitly invoke the sumcheck protocol, continuing until the final step of sumcheck, which requires obtaining the value of \\tilde{f}(r_1, r_2, r_3) to conclude the sumcheck protocol. This value is provided by the final step of the FRI protocol.\n\nBelow, we illustrate the sumcheck-like proof process in the DeepFold protocol using the example of proving \\tilde{f}(z_1, z_2, z_3) = y. This process differs from the sumcheck protocol used in Basefold, as shown in the following diagram.\n\nWe can observe that the two protocols differ in how they construct univariate polynomials and in the equations that the Verifier checks.","type":"content","url":"/fri/deepfold#connection-between-deepfold-and-sumcheck","position":9},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl3":"Sumcheck in Basefold","lvl2":"Connection Between DeepFold and Sumcheck"},"type":"lvl3","url":"/fri/deepfold#sumcheck-in-basefold","position":10},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl3":"Sumcheck in Basefold","lvl2":"Connection Between DeepFold and Sumcheck"},"content":"The sumcheck protocol in the Basefold protocol is more intuitive because its summation form is more evident. First, \\tilde{f}(z_1, z_2, z_3) is converted into a sum over the boolean hypercube \\{0,1\\}^3:\\tilde{f}(z_1, z_2, z_3) = \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}} \\sum_{b_1= \\{0,1\\}} \\tilde{f}(b_1, b_2, b_3) \\cdot \\tilde{eq}((b_1,b_2,b_3), (z_1, z_2, z_3))\n\nProving \\tilde{f}(z_1, z_2, z_3) = y is transformed into proving:\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}} \\sum_{b_1= \\{0,1\\}} \\tilde{f}(b_1, b_2, b_3) \\cdot \\tilde{eq}((b_1,b_2,b_3), (z_1, z_2, z_3)) = y \\tag{6}\n\nThe above equation is a sum of 8 terms. We first convert this 8-term sum problem into two 4-term sum problems, breaking it down into two smaller problems. Then, using a random number r_1, we transform these two 4-term sum problems into one 4-term sum problem. This process is as follows:\n\nSplit: Decompose the sum\\begin{align}\n & \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}} \\sum_{b_1= \\{0,1\\}} \\tilde{f}(b_1, b_2, b_3) \\cdot \\tilde{eq}((b_1,b_2,b_3), (z_1, z_2, z_3))  \\\\\n  =  & \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(0, b_2, b_3) \\cdot \\tilde{eq}((0,b_2,b_3), (z_1, z_2, z_3)) \\\\\n & + \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(1, b_2, b_3) \\cdot \\tilde{eq}((1,b_2,b_3), (z_1, z_2, z_3)) \\\\  \\\\\n := & g_1(0) + g_1(1)\n\\end{align}\n\nTherefore, proving\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}} \\sum_{b_1= \\{0,1\\}} \\tilde{f}(b_1, b_2, b_3) \\cdot \\tilde{eq}((b_1,b_2,b_3), (z_1, z_2, z_3)) = y\n\ncan be transformed into proving\\begin{align}\n\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(0, b_2, b_3) \\cdot \\tilde{eq}((0,b_2,b_3), (z_1, z_2, z_3)) = g_1(0) \\\\\n\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(1, b_2, b_3) \\cdot \\tilde{eq}((1,b_2,b_3), (z_1, z_2, z_3)) = g_1(1)\n\\end{align}\n\nand proving that the split is correct, i.e., provingg_1(0) + g_1(1) = y\n\nTo summarize, after splitting the original summation problem, we need to prove three statements:\n\n(1) The split is correct: g_1(0) + g_1(1) = y\n\n(2) The first part of the sum is correct: \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(0, b_2, b_3) \\cdot \\tilde{eq}((0,b_2,b_3), (z_1, z_2, z_3)) = g_1(0)\n\n(3) The second part of the sum is correct: \\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(1, b_2, b_3) \\cdot \\tilde{eq}((1,b_2,b_3), (z_1, z_2, z_3)) = g_1(1)\n\nWe can see that after decomposition, proving (2) and (3) each involves half the scale compared to proving the original problem (6), and they both have the same form. Proving (2) and (3) separately would not be more efficient than directly proving equation (6). However, if we could merge (2) and (3) and prove a half-sized summation problem, it would be more efficient than directly proving equation (6). How can we accomplish this? We can ask the Verifier for a random number r_1 and use it to fold (2) and (3) into a single half-sized problem.\n\nFold: Use a random number to transform proving (2) and (3) into proving\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(r_1, b_2, b_3) \\cdot \\tilde{eq}((r_1,b_2,b_3), (z_1, z_2, z_3)) = g_1(r_1) \\tag{7}\n\nCompared to proving equation (6), the scale of the problem has been halved. Originally, we needed to prove that the sum of 8 terms equals a certain value, but now we only need to prove that the sum of 4 terms equals a certain value, and equations (6) and (7) have the same summation form.\n\nTo summarize, proving equation (6) is transformed into proving:\n\nThe split is correct: g_1(0) + g_1(1) = y\n\nThe half-sized subproblem:\\sum_{b_3 = \\{0,1\\}}\\sum_{b_2= \\{0,1\\}}  \\tilde{f}(r_1, b_2, b_3) \\cdot \\tilde{eq}((r_1,b_2,b_3), (z_1, z_2, z_3)) = g_1(r_1)\n\nWe can continue to apply the split and fold process to the equation above, continuously reducing the problem size to one that is easier for the Verifier to check. Therefore, the entire sumcheck protocol can be viewed from this split-and-fold perspective, as illustrated in the diagram below.","type":"content","url":"/fri/deepfold#sumcheck-in-basefold","position":11},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl3":"Sumcheck in DeepFold","lvl2":"Connection Between DeepFold and Sumcheck"},"type":"lvl3","url":"/fri/deepfold#sumcheck-in-deepfold","position":12},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl3":"Sumcheck in DeepFold","lvl2":"Connection Between DeepFold and Sumcheck"},"content":"The sumcheck protocol in DeepFold also uses this split-and-fold approach to prove \\tilde{f}(z_1, z_2, z_3) = y, but its decomposition method differs from the sumcheck protocol in Basefold. Let\\tilde{f}(X_1, X_2, X_3) = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1 X_2 + a_4 X_3 + a_5 X_1X_3 + a_6 X_2 X_3 + a_7 X_1 X_2 X_3\n\nThen the summation can be rewritten as\\begin{align}\n\\tilde{f}(X_1, X_2, X_3)  & = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1 X_2 + a_4 X_3 + a_5 X_1X_3 + a_6 X_2 X_3 + a_7 X_1 X_2 X_3 \\\\\n & = (a_0 + a_2 X_2 + a_4 X_3 + a_6 X_2X_3) + X_1 \\cdot (a_1X_1 + a_3 X_2 + a_5  X_3 + a_7 X_2 X_3) \\\\\n& : = \\tilde{f}_{even}(X_2, X_3) + X_1 \\cdot \\tilde{f}_{odd}(X_2, X_3)\n\\end{align} \\tag{8}\n\nSince\\tilde{f}(z_1, z_2, z_3) = a_0 + a_1 z_1 + a_2 z_2 + a_3 z_1 z_2 + a_4 z_3 + a_5 z_1z_3 + a_6 z_2 z_3 + a_7 z_1 z_2 z_3\n\nproving \\tilde{f}(z_1, z_2, z_3) = y means provinga_0 + a_1 z_1 + a_2 z_2 + a_3 z_1 z_2 + a_4 z_3 + a_5 z_1z_3 + a_6 z_2 z_3 + a_7 z_1 z_2 z_3 = y \\tag{9}\n\nSplit: Decompose the summation according to equation (8)\\begin{align}\n & a_0 + a_1 z_1 + a_2 z_2 + a_3 z_1 z_2 + a_4 z_3 + a_5 z_1z_3 + a_6 z_2 z_3 + a_7 z_1 z_2 z_3  \\\\\n = & (a_0 + a_2 z_2 + a_4 z_3 + a_6 z_2z_3) + z_1 \\cdot (a_1  + a_3 z_2 + a_5 z_3 + a_7 z_2 z_3)  \\\\\n=  & \\tilde{f}_{even}(z_2, z_3) + z_1 \\cdot \\tilde{f}_{odd}(z_2, z_3)\n\\end{align}\n\nAfter this decomposition, proving \\tilde{f}(z_1, z_2, z_3) = y is transformed into proving:\n\n(1) The split is correct: \\tilde{f}_{even}(z_2, z_3) + z_1 \\cdot \\tilde{f}_{odd}(z_2, z_3) = y\n\n(2) The first part is correct: a_0 + a_2 z_2 + a_4 z_3 + a_6 z_2z_3 = \\tilde{f}_{even}(z_2, z_3)\n\n(3) The second part is correct: a_1  + a_3 z_2 + a_5 z_3 + a_7 z_2 z_3 = \\tilde{f}_{odd}(z_2, z_3)\n\nFold: Use a random number r_1 to transform proving (2) and (3) into proving(a_0 + r_1 a_1) + (a_2 + r_1 a_3) z_2 + (a_4 + r_1 a_5) z_3 + (a_6 + r_1 a_7) z_2z_3 = \\tilde{f}_{even}(z_2, z_3) + r_1 \\cdot \\tilde{f}_{odd}(z_2, z_3)\n\nIf we define y' = \\tilde{f}_{even}(z_2, z_3) + r_1 \\cdot \\tilde{f}_{odd}(z_2, z_3) and\\begin{align}\na_0' = a_0 + r_1 a_1 \\quad a_1' = a_2 + r_1 a_3 \\quad a_2' = a_4 + r_1 a_5 \\quad a_3' = a_6 + r_1 a_7\n\\end{align}\n\nwe can see that the scale of the proof problem is halved to provinga_0' + a_1' z_2 + a_2' z_3 + a_3' z_2z_3 = y'\n\nand this summation form has the same structure as equation (9).\n\nTo summarize, proving equation (9) is transformed into proving:\n\nThe split is correct: \\tilde{f}_{even}(z_2, z_3) + z_1 \\cdot \\tilde{f}_{odd}(z_2, z_3) = y\n\nThe half-sized subproblem:(a_0 + r_1 a_1) + (a_2 + r_1 a_3) z_2 + (a_4 + r_1 a_5) z_3 + (a_6 + r_1 a_7) z_2z_3 = \\tilde{f}_{even}(z_2, z_3) + r_1 \\cdot \\tilde{f}_{odd}(z_2, z_3)\n\nUsing the representation of \\tilde{f}(X_1, X_2, X_3), we can simplify the above equation.\\begin{align}\n & \\tilde{f}_{even}(z_2, z_3) + r_1 \\cdot \\tilde{f}_{odd}(z_2, z_3)  \\\\\n = & (a_0 + a_2 z_2 + a_4 z_3 + a_6 z_2z_3) + r_1 \\cdot (a_1  + a_3 z_2 + a_5 z_3 + a_7 z_2 z_3)  \\\\\n=  & a_0 + a_1 r_1 + a_2 z_2 + a_3 r_1 z_2 + a_4 z_3 + a_5 r_1z_3 + a_6 z_2 z_3 + a_7 r_1 z_2 z_3 \\\\\n=  & \\tilde{f}(r_1, z_2, z_3)\n\\end{align}\n\nLet g_1(X) = \\tilde{f}(X, z_2, z_3), then \\tilde{f}_{even}(z_2, z_3) + r_1 \\cdot \\tilde{f}_{odd}(z_2, z_3) = g_1(r_1), and we can derive:\\begin{align} \n & \\tilde{f}_{even}(z_2, z_3) = g_1(0) \\\\\n & \\tilde{f}_{odd}(z_2, z_3) = g_1(1) - g_1(0) \\\\\n & \\tilde{f}_{even}(z_2, z_3) + z_1 \\cdot \\tilde{f}_{odd}(z_2, z_3) = g_1(0) + z_1(g_1(1) - g_1(0)) = g_1(z_1)\n\\end{align}\n\nSo this proof can be written as:\n\nSplit proof: g_1(0) + z_1(g_1(1) - g_1(0)) = y\n\nHalf-sized subproblem:(a_0 + r_1 a_1) + (a_2 + r_1 a_3) z_2 + (a_4 + r_1 a_5) z_3 + (a_6 + r_1 a_7) z_2z_3 = g_1(r_1)\n\nWe continue to apply the split-and-fold approach to the equation above. The sumcheck flow in the DeepFold protocol is illustrated in the diagram below.","type":"content","url":"/fri/deepfold#sumcheck-in-deepfold","position":13},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"References"},"type":"lvl2","url":"/fri/deepfold#references","position":14},{"hierarchy":{"lvl1":"Note on DeepFold: Protocol Overview","lvl2":"References"},"content":"[GLHQTZ24] Yanpei Guo, Xuanming Liu, Kexi Huang, Wenjie Qu, Tianyang Tao, and Jiaheng Zhang. “DeepFold: Efficient Multilinear Polynomial Commitment from Reed-Solomon Code and Its Application to Zero-knowledge Proofs.” Cryptology ePrint Archive (2024).\n\n[ACFY24a] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[ACFY24b] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[ZCF24] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\n[BGKS20] Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. “DEEP-FRI: sampling outside the box improves soundness.” arXiv preprint arXiv:1903.12243 (2019).\n\n[H24] Ulrich Haböck. “Basefold in the List Decoding Regime.” Cryptology ePrint Archive(2024).","type":"content","url":"/fri/deepfold#references","position":15},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof"},"type":"lvl1","url":"/fri/fri-proximity-gap","position":0},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article is mainly inspired by the video \n\nProximity Gaps & Applications to Succinct Proofs, combined with the paper [BCIKS20], introducing the concept of Proximity Gaps and the closely related Correlated Agreement theorem, which play a very important role in the security proof of FRI.\n\nIn the FRI protocol, for a polynomial f: \\mathcal{D} \\rightarrow \\mathbb{F}_q, let f(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_{k-1}x^{k-1}, which is a polynomial of degree less than k, evaluated on the domain \\mathcal{D}, where |\\mathcal{D}| = n, then f \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}, k]. The Prover wants to prove to the Verifier that the degree of f(x) is indeed less than k. If f \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}, k], the Verifier outputs accept, if f is \\delta far from the corresponding code space \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}, k], it outputs reject. What the Verifier can obtain is the oracle about a series of functions, and what the FRI protocol wants to achieve is that the Verifier queries the oracle as little as possible and can distinguish which of the above situations f belongs to.\n\nLet’s assume k-1 is even, then\\begin{aligned}\n    f(x) & = a_0 + a_1 x + a_2 x^2 + \\ldots + a_{k-1}x^{k-1} \\\\\n    & = (a_0 + a_2 x^2 + \\cdots + a_{k-1}x ^{k-1}) + x (a_1 + a_3 x^2 + \\cdots + a_{k-2}x^{k-3}) \\\\\n    & := g(x^2) + xh(x^2) \n\\end{aligned}\n\nWe can find that the functions\\begin{aligned}\n    g(x) = a_0 + a_2 x + \\cdots + a_{k-1}x ^{\\frac{k-1}{2}} \\\\\n    h(x) = a_1 + a_3 x + \\cdots + a_{k-2}x^{\\frac{k-3}{2}}\n\\end{aligned}\n\nInitially, the Prover wants to prove to the Verifier that the degree of f(x) is less than k, now it can be decomposed into three sub-problems:\n\nProve that the degree of function g(x) is less than k/2, i.e., g(x) \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2]\n\nProve that the degree of function h(x) is less than k/2, i.e., h(x) \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2]\n\nProve that f(x) = g(x^2) + x \\cdot h(x^2)\n\nwhere |{D}^{(1)}| = n/2. The third item is to prove that the odd-even splitting is correct. Similarly, g(x) and h(x) can be decomposed into odd and even terms like f(x), decomposing them into two polynomials of degree less than k/4, so we need to prove that 4 polynomials are of degree less than k/4, until finally decomposing to prove constant polynomials. This process is shown in the figure below, and we can see that the polynomials to be proved are growing in the form of powers of 2. In this process, in order to prove that the odd-even splitting is not problematic, we need to send oracles about all these polynomials to the Verifier, and we can imagine that there are too many polynomials being sent, which grow explosively as k increases.\n\nSince our purpose is to prove that the degree of the polynomial is less than a certain number, our idea is that we don’t want to split the problem of f(x) like above, splitting it into two polynomials, we want to prove in the next step that a polynomial is of degree less than k/2, which can greatly reduce the number of polynomials sent. How to do this? We can ask the Verifier for a random number r \\in \\mathbb{F}, make a linear combination of g(x) and h(x), get g(x) + r \\cdot h(x), and decompose the problem of f(x) being of degree less than k into:\n\nThe degree of f^{(1)}(x) = g(x) + r \\cdot h(x) is less than k/2, i.e., f^{(1)}(x) \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2]\n\nAt this time, the graph of the polynomials to be sent becomes like the figure below, and you can see that the oracle of the polynomials to be sent is greatly reduced.\n\nNow the remaining question is, is this equivalent to the original method? Of course, if the Prover is honest, according to the linearity of RS encoding, g(x),h(x) \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2], then their linear combination is still in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2]. But what if the Prover cheats? For example, if g(x) is \\delta far from the code space \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2], we hope that after the linear combination with the random number r, g(x) + r \\cdot h(x) is still \\delta far, so that the Verifier can discover the Prover’s cheating. What we don’t want is that the folded g(x) + r \\cdot h(x) becomes closer to the corresponding code space. Proximity Gaps tells us that the probability of this happening is very small, like winning the lottery, so we can boldly use random numbers for folding.","type":"content","url":"/fri/fri-proximity-gap","position":1},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Proximity Gaps"},"type":"lvl2","url":"/fri/fri-proximity-gap#proximity-gaps","position":2},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Proximity Gaps"},"content":"Above we considered the case of folding two polynomials, in practice we will use random numbers to fold multiple times or batch multiple polynomials. Here let’s consider the general case, assuming there are m vectors (u_0, \\ldots, u_{m-1}), for each u_i \\in \\mathbb{F}_q^{\\mathcal{D}}, it can be seen as a polynomial on \\mathcal{D} \\rightarrow \\mathbb{F}, or as a vector of dimension |\\mathcal{D}| = n. Make a linear combination of these m vectors, denoted as A = \\mathrm{span}\\{u_0, \\ldots, u_{m-1}\\}, where A is an affine space in \\mathbb{F}^{\\mathcal{D}}, and let the code space be V := \\mathrm{RS}[\\mathbb{F}, \\mathcal{D},k].\n\nWe are concerned about the distance relationship between elements in A and the code space V. As shown in the figure below, represent all codes in the code space V as points, draw a sphere with these points as the center and \\delta as the radius. The space formed by A is represented by a two-dimensional plane. If the elements in A have a relative Hamming distance less than or equal to \\delta from some codes in V, it means that they intersect with some Hamming balls in the figure, and all these intersections combined form the green shaded area in the figure. In other words, for each element a in the shaded area S \\subset A, there must exist a v \\in V, such that \\Delta(a, v) \\leq \\delta.\n\nLet’s form a set \\mathrm{C}_{\\mathrm{Affine}} consisting of all affine spaces in \\mathbb{F}^{\\mathcal{D}}. The Proximity Gaps conclusion [BCIKS20, Theorem 1.2] tells us that for any A \\in \\mathrm{C}_{\\mathrm{Affine}} (such as A = \\mathrm{span}\\{u_0, \\ldots, u_{m-1}\\}), either all elements in A are in the shaded area, or only a very small part of the elements in A are in the shaded area. It’s impossible to say that half of the elements in A are in the shaded area while the other half are not. Expressed in formula, it can only conform to one of the following two situations:\n\n\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] \\le \\epsilon\n\n\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] = 1\n\nWe call \\delta the proximity parameter, and \\epsilon the error parameter, which is a very small number. Of course, there is a specific expression for \\epsilon, which is related to q,n,\\rho,\\delta, that is, \\epsilon = \\epsilon(q,n,\\rho,\\delta), where \\rho represents the code rate, \\rho = \\frac{k}{n}.\n\nSo what does the shaded area here represent? What is the relationship between this conclusion and the security analysis of FRI? Let’s analyze the application of the Proximity Gaps conclusion for the cases of honest Prover and cheating Prover.","type":"content","url":"/fri/fri-proximity-gap#proximity-gaps","position":3},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl3":"Honest Prover","lvl2":"Proximity Gaps"},"type":"lvl3","url":"/fri/fri-proximity-gap#honest-prover","position":4},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl3":"Honest Prover","lvl2":"Proximity Gaps"},"content":"If it’s an honest Prover, then for each vector in (u_0, \\ldots, u_{m-1}), we have u_i \\in V.\n\nDue to the linearity of RS encoding, we know that after linear combination, it must still be in the code space V, so A \\subset V. At this time, all elements in A are in V, so when the Verifier makes a random linear combination and arbitrarily selects a point a \\in A, they will always get a \\in V, and the Verifier will definitely accept. This situation corresponds to the second case in Proximity Gaps, taking \\delta = 0, at this time\\Pr_{a \\in A}[\\Delta(a, V) = 0] = 1","type":"content","url":"/fri/fri-proximity-gap#honest-prover","position":5},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl3":"Malicious Prover","lvl2":"Proximity Gaps"},"type":"lvl3","url":"/fri/fri-proximity-gap#malicious-prover","position":6},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl3":"Malicious Prover","lvl2":"Proximity Gaps"},"content":"If the Prover cheats, suppose one vector in the m vectors \\vec{u} = (u_0, \\ldots, u_{m-1}) sent by the Prover to the Verifier is \\delta far from V, that is\\exists u_i^* \\in \\vec{u}, \\quad \\Delta(u_i^*, V) > \\delta\n\nThen in A = \\mathrm{span}\\{u_0,u_1, \\ldots,u_{m-1}\\}, take a^* = u_i^* \\in A, we must have\\exists a^* \\in A, \\quad \\Delta(a^*, V) > \\delta\n\nAt this time, according to the Proximity Gaps conclusion, there is already an element in A that is not in the shaded area, so the case \\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] = 1 is excluded, and it can only be \\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] \\le \\epsilon. This also means that even if only one of the m vectors is \\delta far from the corresponding code space, most elements in A are \\delta far from V. In other words, a point a randomly selected from A can represent the farthest distance from V among the m vectors.\n\nNow the Verifier randomly selects a point a \\in A to check whether \\Delta(a,V) is greater than \\delta. Two situations will occur. One is that it falls into the shaded area in the figure, and the other is that it falls outside the shaded area.\n\nCase 1: \\Delta(a, V) \\le \\delta. At this time, the point a selected by the Verifier is in the shaded area. We say that the Prover is very lucky at this time. Although the Prover provided an incorrect witness, which is \\delta far from the code space, after random linear combination, it becomes \\delta close to the code space, and at this time, the Prover can successfully deceive the Verifier. The occurrence of this situation is not good for the Verifier, but fortunately, the Proximity Gaps conclusion tells us that \\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] \\le \\epsilon, which means that the probability of randomly choosing a point that can enter the shaded area is very, very small. The Prover needs to be as lucky as winning the lottery, that is, at this time, the probability that the Prover can successfully deceive the Verifier will not exceed \\epsilon.\n\nCase 2: \\Delta(a, V) > \\delta. At this time, the point a selected by the Verifier is outside the shaded area. Can the Prover still succeed in cheating? There is still a chance, because the Verifier received the oracle about a, but will not check all the values in a, only wants to query some values to see if it is in V. If the Verifier only queries once, since \\Delta(a, V) > \\delta, more than \\delta proportion of the components in a are not equal to the corresponding components of v, at this time the Verifier has a probability greater than \\delta to catch the Prover cheating, which means that at this time the probability that the Prover can cheat successfully does not exceed 1 - \\delta.\n\nIf the Verifier repeats the query \\kappa times, the probability that the Prover can cheat successfully will not exceed (1 - \\delta)^{\\kappa}.\n\nSo, the probability that a cheating Prover can succeed is the joint probability of the above two cases, that is, it will not exceed\\epsilon + (1 - \\delta)^{\\kappa}\n\nThe above analysis is actually the general idea of the soundness analysis of the FRI protocol. In the paper, the occurrence of case 1 is called the occurrence of some “bad” events, and then assuming that the “bad” events did not occur, estimate the probability of case 2, and finally combine the two for analysis.\n\nWe know that the FRI protocol is divided into two stages, one is the Commit stage and the other is the Query stage. We can correspond the above two cases to these two stages:\n\nThe above case 1 occurs in the Commit stage, where the Verifier will select random numbers to let the Prover fold the polynomials.\n\nThe above case 2 corresponds to the Query stage, where the Verifier will randomly select some points for query checks.\n\nIf it’s a batched version of the FRI protocol, to prove multiple polynomials f_0^{(0)}, f_1^{(0)}, \\ldots, f_t^{(0)} are all polynomials of degree less than k, we can first use random numbers \\{x_1,\\ldots, x_t\\} for aggregation, obtainingf^{(0)}(x) = f_0^{(0)} + \\sum_{i = 1}^{t} x_i \\cdot f_i^{(0)}\n\nThen apply the general FRI protocol to f^{(0)}(x) to prove that it is a polynomial of degree less than k. The soundness analysis here also corresponds to the above case 1, that is, there may exist a situation where due to the selection of random numbers, f^{(0)}(x) is no longer \\delta far from the corresponding RS code space.","type":"content","url":"/fri/fri-proximity-gap#malicious-prover","position":7},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Impact of Increasing \\delta"},"type":"lvl2","url":"/fri/fri-proximity-gap#impact-of-increasing-delta","position":8},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Impact of Increasing \\delta"},"content":"Let’s analyze what impact the increase of the proximity parameter \\delta will bring. We have already analyzed that the probability of a cheating Prover successfully deceiving the Verifier does not exceed\\epsilon + (1 - \\delta)^{\\kappa}\n\nThis probability consists of two parts, the increase of \\delta will lead to:\n\n\\epsilon \\uparrow. From a graphical understanding, \\delta controls the radius of each Hamming ball. If \\delta increases, then the Hamming balls become larger, and their intersection with the affine space A should be larger, which means the shaded area increases, which implies that \\epsilon will increase.\n\nThis is good news for the cheating Prover :). Because at this time, the Prover becomes luckier than before, with a greater probability of entering the green shaded area, and can successfully deceive the Verifier.\n\nNaturally, this is bad news for the Verifier :(.\n\n(1 - \\delta)^{\\kappa} \\downarrow. This expression is directly related to \\delta, if \\delta increases, then (1 - \\delta)^{\\kappa} will decrease.\n\nThis is bad news for the cheating Prover :(. Because at this time, the probability of the Prover’s successful cheating will decrease.\n\nThis is good news for the Verifier :). At this time, there is a greater probability of catching the Prover cheating. Under the same security requirements, the Verifier only needs fewer rounds of polling to meet the requirements.\n\nIt can be seen that the increase of \\delta causes \\epsilon to increase and (1 - \\delta)^{\\kappa} to decrease. In practice, \\epsilon is very small, and (1 - \\delta)^{\\kappa} accounts for a larger proportion in the whole sum, so the overall will still decrease, which means that for the entire FRI protocol, the soundness decreases, indicating that it will be more secure.\n\nThe above analysis is from the perspective of soundness. The video \n\nProximity Gaps & Applications to Succinct Proofs also mentions a point that the increase of \\delta will make the corresponding Correlated Agreement related conclusions weaker.  Correlated Agreement is a stronger conclusion than Proximity Gaps (so far, their equivalence has not been proven). Let’s introduce the Correlated Agreement conclusion below.","type":"content","url":"/fri/fri-proximity-gap#impact-of-increasing-delta","position":9},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Correlated Agreement"},"type":"lvl2","url":"/fri/fri-proximity-gap#correlated-agreement","position":10},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Correlated Agreement"},"content":"For the affine space A = \\mathrm{span}\\{u_0,u_1, \\ldots,u_{m-1}\\} mentioned earlier, to maintain consistency with [BCIKS20, Theorem 1.6], we don’t use a random number before the first vector u_0, let A = u_0 + \\mathrm{span}\\{u_1, \\ldots,u_{m-1}\\}.\n\nThe Correlated Agreement theorem ([BCIKS20, Theorem 1.6]) says that if \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] > \\epsilon,\n\nwhere \\epsilon is the \\epsilon given in the Proximity Gaps conclusion, then there exist \\mathcal{D}' \\subset \\mathcal{D} and v_0, \\ldots, v_{m-1} \\in V such that\n\nDensity: \\frac{|\\mathcal{D}'|}{|\\mathcal{D}|} \\ge 1 - \\delta,\n\nAgreement: For any i \\in \\{0, \\ldots, m - 1\\}, we have u_i|_{\\mathcal{D}'} = v_i|_{\\mathcal{D}'}.\n\nThis means that if there are many elements falling into the shaded area, with a proportion larger than \\epsilon in the Proximity Gaps conclusion, then there exist codewords v_0, \\ldots, v_{m-1} in V, and there will be a subset \\mathcal{D}' in the domain \\mathcal{D} with a very large proportion (more than 1 - \\delta), where each u_i is consistent with the corresponding v_i on \\mathcal{D}'.\n\nAccording to the conclusion of Proximity Gaps, the elements in A fall into two categories:\n\n\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] \\le \\epsilon\n\n\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] = 1\n\nNow, if the proportion of elements falling into the shaded area is greater than \\epsilon , we can naturally exclude the first case. This leads to the conclusion that all elements in A fall within the shaded area, i.e.,\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] = 1 .\n\nThe correlated agreement theorem provides a more specific conclusion. It describes the relationship between the elements u_{i} before folding and the codewords v_{i} found in the encoding space V .\n\nFor example, if the Prover wants to prove that a polynomial f \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(0)}, k], let \\mathcal{D}^{(0)} = \\{x_1, \\ldots, x_n\\}, calculate \\{f(x_1), \\ldots, f(x_n)\\}, the Prover will send the oracle of these values to the Verifier. In practice, Merkle trees are used to implement the oracle.\n\nSplit f to obtain two polynomials g(x) and h(x). In the honest case, g,h \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2], where |\\mathcal{D}^{(1)}| = |\\mathcal{D}^{(0)}| / 2 = n/2.\n\nThe Correlated Agreement conclusion tells us that for the affine space A = \\{g + z \\cdot h : z \\in \\mathbb{F}\\} formed by g(x) and h(x), if more than \\epsilon proportion of elements in A fall into the “shaded area”, i.e., satisfying \\Delta(a, V) \\ge \\delta, then there exist \\mathcal{D}' as shown in the figure below, and \\bar{g},\\bar{h} \\in \\mathrm{RS}[\\mathbb{F}_q, \\mathcal{D}^{(1)}, k/2]. Let’s assume \\mathcal{D}' = \\{\\alpha_1, \\alpha_2, \\ldots, \\alpha_i\\}, then according to the conclusion |\\mathcal{D}'| / |\\mathcal{D}^{(1)}| \\ge 1 - \\delta, we have index i \\ge (1 - \\delta)n/2. On all \\mathcal{D}', g is consistent with \\bar{g}, and h is consistent with \\bar{h}, which is represented in green in the figure, meaning that when evaluating at these points in the \\mathcal{D}' set, their values are the same.\n\nBack to the analysis of \\delta increase, we can see that as \\delta increases, the 1 - \\delta in the first condition Density of the Correlated Agreement conclusion will become smaller, which makes the subset \\mathcal{D}' that can be ensured to exist in the conclusion smaller, making the obtained conclusion weaker.\n\nIn the [BCIKS20] paper, it is said that the Proximity Gap theorem ([BCIKS20, Theorem 1.2]) is derived from the Correlated Agreement theorem ([BCIKS20, Theorem 1.6]), but it is not known yet whether the Proximity Gap theorem can derive the Correlated Agreement theorem. If the Proximity Gap cannot derive the Correlated Agreement theorem, it means that the Correlated Agreement theorem is a stronger conclusion than the Proximity Gap theorem. If it can be derived, it means that these two theorems are equivalent.\n\nIn fact, there are many versions of the Correlated Agreement theorem, taking different A can lead to different theorems, A can be:\n\nLines: A = \\{u_0 + z u_1: z \\in \\mathbb{F}\\}\n\nLow-degree parameterized curves: \\mathrm{curve}(\\mathbf{u}) = \\left\\{u_z: = \\sum_{i = 0}^{m-1}z^i \\cdot u_i  | z \\in \\mathbb{F}_q   \\right\\}\n\nAffine space: u_0 + \\mathrm{span}\\{u_1, \\cdots, u_{m-1}\\}\n\nAt the same time, regarding the condition of the Correlated Agreement theorem\\Pr_{a \\in A}[\\Delta(a, V) \\le \\delta] > \\epsilon,\n\nHere we measure the relative Hamming distance between a and V, we can also make this measure more general by adding weights. Give a weight function \\mu: \\mathcal{D} \\rightarrow [0,1], define the relative \\mu -agreement between two vectors u and v as\\mathrm{agree}_{\\mu}(u,v) := \\frac{1}{|\\mathcal{D}|} \\sum_{x: u(x) = v(x)} \\mu(x)\n\nWhen taking \\mu \\equiv 1,\\mathrm{agree}_{\\mu}(u,v) = \\frac{1}{|\\mathcal{D}|} \\sum_{x: u(x) = v(x)} \\mu(x) = \\frac{1}{|\\mathcal{D}|} \\sum_{x: u(x) = v(x)} 1 = 1 - \\Delta(u,v)\n\nThe value of this measure is exactly equal to 1 minus the relative Hamming distance. Similarly, define the maximum agreement between a vector u and the code space V as\\mathrm{agree}_{\\mu}(u,V):= \\max_{v \\in V} \\mathrm{agree}_{\\mu}(u,v)\n\nChanging the condition in the theorem to:\\Pr_{a \\in A}[\\mathrm{agree}_{\\mu} \\le \\alpha] > \\epsilon,\n\nWe will get the corresponding Weighted correlated agreement theorem (see [BCIKS20, Section 7]). It can be seen that the Correlated agreement theorem is very flexible. In the paper [BCIKS20, Theorem 8.3], for the soundness proof of the batched FRI protocol, it first defines the required weight function \\mu, uses the Weighted Correlated Agreement theorem to prove, rather than using the Proximity Gap theorem to prove. And this theorem generally appears in proof by contradiction, it can powerfully help us find the codewords v_{i} of the code space, and satisfy the properties mentioned in the theorem conclusion, which can help us find contradictions through derivation.","type":"content","url":"/fri/fri-proximity-gap#correlated-agreement","position":11},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Application of Correlated Agreement Theorem in Soundness"},"type":"lvl2","url":"/fri/fri-proximity-gap#application-of-correlated-agreement-theorem-in-soundness","position":12},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Application of Correlated Agreement Theorem in Soundness"},"content":"Here’s a brief description of the application of the Correlated Agreement theorem in the soundness proof, which is not so rigorous, and the actual security analysis will be more complex.\n\nAs mentioned before, the soundness analysis of the FRI protocol is divided into two parts:\n\nIn the batch stage or Commit stage, due to the improper selection of random numbers, polynomials that were originally far from the code space become closer to the corresponding code space after folding, that is, entering the “shaded area”.\n\nIn the Query stage, due to random checks, the Prover’s cheating was not caught.\n\nThe Correlated Agreement theorem is mainly applied in the probability analysis of the first part. It will first define the “bad” event E^{(i)}: before folding \\Delta^*(f^{(i)}, \\mathrm{RS}^{(i)}) > \\delta, split f^{(i)} into g^{(i+1)} and h^{(i+1)}, then use a random number r \\in \\mathbb{F} for folding to get \\mathrm{fold}_{r}(f^{(i)}), and the following occurs\\Delta(\\mathrm{fold}_{r}(f^{(i)}), \\mathrm{RS}^{(i+1)}) \\le \\delta\n\nHere \\Delta^* is used, its definition is different from the Hamming distance, it is related to the random query of the Query stage of FRI, which will not be explained in detail here. Assume that the probability of a “bad” event E^{(i)} occurring does not exceed \\epsilon, that is\\Pr[E^{(i)}] = \\Pr_{r \\in \\mathbb{F}}[\\Delta(\\mathrm{fold}_{r}(f^{(i)}), \\mathrm{RS}^{(i+1)}) \\le \\delta] \\le \\epsilon \\tag{1}\n\nIf the FRI protocol folds d times, then the probability of some “bad” events occurring does not exceed d \\cdot \\epsilon, that is\\bigcup_{i = 0}^{d} \\Pr[E^{(i)}] \\le d \\cdot \\epsilon\n\nThis way, the probability analysis of the first part is done, then assume that these “bad” events do not occur, analyze the probability of the second part, and finally combine the two parts of probability to get the conclusion of soundness.\n\nNow the remaining key problem is how to prove equation (1), that is, to prove that if \\Delta^*(f^{(i)}, \\mathrm{RS}^{(i)}) > \\delta, we have\\Pr_{r \\in \\mathbb{F}}[\\Delta(\\mathrm{fold}_{r}(f^{(i)}), \\mathrm{RS}^{(i+1)}) \\le \\delta] \\le \\epsilon \\tag{2}\n\nThe idea is to use proof by contradiction, assuming that equation (2) does not hold, that is\\Pr_{r \\in \\mathbb{F}}[\\Delta(\\mathrm{fold}_{r}(f^{(i)}), \\mathrm{RS}^{(i+1)}) \\le \\delta] > \\epsilon\n\nThis satisfies the condition of the Correlated Agreement theorem, which means that there exist \\mathcal{D}' \\subset \\mathcal{D}^{(i+1)}, and \\bar{g}^{(i+1)}, \\bar{h}^{(i+1)} \\in \\mathrm{RS}^{(i+1)} satisfying\\bar{g}^{(i+1)}|_{\\mathcal{D}'} = {g}^{(i+1)}|_{\\mathcal{D}'} , \\quad \\bar{h}^{(i+1)}|_{\\mathcal{D}'} = {h}^{(i+1)}|_{\\mathcal{D}'}\n\nand |\\mathcal{D}'| \\ge (1-\\delta) |\\mathcal{D}^{(i+1)}|. With these codewords \\bar{g}^{(i+1)} and \\bar{h}^{(i+1)} in the code space, we can get a polynomial \\bar{f}^{(i)},\\bar{f}^{(i)}(x) = \\bar{g}^{(i+1)}(x^2) + x \\cdot \\bar{h}^{(i+1)}(x^2)\n\nDue to the linearity of encoding, \\bar{f}^{(i)} must also be a codeword, and \\bar{f}^{(i)} \\in \\mathrm{RS}^{(i)}, and at the same time we have\\bar{f}^{(i)}|_{\\mathcal{D}'} = {f}^{(i)}|_{\\mathcal{D}'}\n\nSince |\\mathcal{D}'| \\ge (1-\\delta) |\\mathcal{D}^{(i+1)}|, we can get \\Delta^*(f^{(i)}, \\mathrm{RS}^{(i)}) \\le \\Delta^*(f^{(i)}, \\bar{f}^{(i)}) \\le \\delta, which contradicts the assumption, so equation (2) holds.","type":"content","url":"/fri/fri-proximity-gap#application-of-correlated-agreement-theorem-in-soundness","position":13},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Summary"},"type":"lvl2","url":"/fri/fri-proximity-gap#summary","position":14},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"Summary"},"content":"Proximity gap plays a crucial role in the FRI protocol, allowing us to confidently use random numbers to fold polynomials, which greatly reduces the number of oracles sent by the Prover and also reduces the number of queries by the Verifier. In addition, Proximity gap is closely related to the Correlated Agreement theorem and plays a key role in the soundness analysis of FRI.","type":"content","url":"/fri/fri-proximity-gap#summary","position":15},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"References"},"type":"lvl2","url":"/fri/fri-proximity-gap#references","position":16},{"hierarchy":{"lvl1":"Proximity Gaps and Correlated Agreement: The Core of FRI Security Proof","lvl2":"References"},"content":"[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\nVideo: \n\nProximity Gaps & Applications to Succinct Proofs","type":"content","url":"/fri/fri-proximity-gap#references","position":17},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity"},"type":"lvl1","url":"/fri/stir-en","position":0},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity"},"content":"Jade Xie  \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article is mainly inspired by the blog post \n\nSTIR: Reed–Solomon Proximity Testing with Fewer Queries by the authors of the \n\nSTIR paper and the presentation \n\nZK11: STIR: Reed–Solomon Proximity Testing with Fewer Queries - Gal Arnon & Giacomo Fenzi, introducing the STIR protocol.\n\nLike FRI, STIR also solves the Reed-Solomon Proximity Testing problem, but compared to FRI, it has lower query complexity, which reduces the size of the argument and the hash complexity of the Verifier. So how does STIR achieve this? The answer is in the name itself, STIR stands for Shift To Improve Rate, and the core idea of STIR is to improve the rate by shifting the evaluation domain in each round. Intuitively, the rate actually characterizes the proportion of true information contained in the codeword. As the rate decreases, the true information decreases, corresponding to an increase in redundancy in the codeword, making it easier for the Verifier to test the proximity of a received message to the encoding space. In other words, the Verifier’s testing ability becomes stronger. This means that the Verifier only needs fewer queries to achieve the target security. Let’s look at how STIR reduces the rate by comparing FRI and STIR.","type":"content","url":"/fri/stir-en","position":1},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"FRI v.s. STIR"},"type":"lvl2","url":"/fri/stir-en#fri-v-s-stir","position":2},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"FRI v.s. STIR"},"content":"For a finite field \\mathbb{F}, let \\mathcal{L} \\subseteq \\mathbb{F} be the evaluation domain, with size |\\mathcal{L}| = n, and let d denote the degree bound (assume both n and d are powers of 2). Then the Reed-Solomon encoding space \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d] contains all functions f: \\mathcal{L} \\rightarrow \\mathbb{F} such that f is consistent with the evaluation of a polynomial of degree strictly less than d on \\mathcal{L}. The rate is \\rho := d/|\\mathcal{L}|.\n\nThe goal of the protocol is to solve the Reed-Solomon Proximity Testing problem, where the Verifier can obtain a function f: \\mathcal{L} \\rightarrow \\mathbb{F} through queries. The Verifier’s goal is to query the values of f at as few locations as possible to distinguish which of the following cases f belongs to:\n\nf is a Reed-Solomon codeword, i.e., f \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d];\n\nf is \\delta-far from all codewords in the Reed-Solomon encoding space \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d] in relative Hamming distance, i.e., \\Delta(f, \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d]) > \\delta.\n\nWe consider the above Reed-Solomon Proximity Testing problem under the IOPP (Interactive Oracle Proofs of Proximity) model, where the Verifier can interact with the Prover and obtain the Prover’s messages through an oracle, as shown in the following figure.\n\nAfter a series of interactions with the Prover, the Verifier has two situations:\n\nIf f \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], the Verifier accepts :)\n\nIf \\Delta(f, \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d]) > \\delta, the Verifier rejects with high probability :(\n\nWe compare the FRI protocol and the STIR protocol in the case of k-fold, as shown in the following figure.\n\nIn the FRI protocol, assume that g_1 is obtained by k-fold using the random number \\alpha_1, where \\mathcal{L}^{k} = \\{x^k,x\\in \\mathcal{L}\\}. Therefore, testing f \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d] is converted to testing g_1 \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L}^k,d/k], and recursively testing g_i \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L}^{k^i},d/k^i]. Thus, in the i-th round, its rate is\\rho_i = \\frac{\\frac{d}{k^i}}{|\\mathcal{L}_i|} = \\frac{d}{k^i} \\cdot \\frac{k^i}{n} = \\frac{d}{n} = \\rho\n\nWe can see that in each round, the rate \\rho_i remains constant at \\rho.\n\nIn the STIR protocol, note that g_1' is still k-folded, but the size of its evaluation domain \\mathcal{L}' is not reduced by k times, but by 2 times. At this time, testing f \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d] is converted to testing g_1' \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k]. Then in the i-th round, we need to test g_i' \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L}_{i}',d/k^i]. Now\\rho_i = \\frac{\\frac{d}{k^i}}{|\\mathcal{L}'_i|} = \\frac{d}{k^i} \\cdot \\frac{2^i}{n} = \\left( \\frac{2}{k}\\right)^i \\cdot \\frac{d}{n} = \\left( \\frac{2}{k}\\right)^i \\cdot \\rho\n\nIf \\frac{2}{k} < 1, i.e., k > 2, we can see that the rate \\rho_{i} decreases in each round, which is the key to STIR’s reduction in query complexity. In other words, because the Verifier’s testing ability becomes stronger, it only needs fewer queries to achieve the target security.\n\nWhen we compile the above IOPP into a SNARK, we need to use the BCS transformation ([BCS16], BCS transformation), which consists of two steps:\n\nMerkle commit the Prover’s messages, and when the Verifier wants to query, open these commitments. This step transforms the IOPP into a succinct interactive argument.\n\nUse the Fiat-Shamir transform to convert the succinct interactive argument obtained in the first step into a non-interactive one.\n\nIn the BCS transformation, the IOPP needs to have a strong soundness property called round-by-round soundness, which requires the IOPP to have a relatively small soundness error in each round, which is a stronger requirement than requiring the entire IOPP to have a relatively small soundness error. Let’s assume that the bound for the round-by-round soundness error is 2^{-\\lambda}. Each round can be queried t_{i} times repeatedly, and the entire IOPP protocol goes through M rounds, so the total query complexity of the entire proof is \\sum_{i = 0}^M t_i. For \\delta reaching the Johnson bound, i.e., \\delta = 1 - \\sqrt{\\rho}, we can calculate that\n\nThe query complexity of FRI is:O \\left( \\lambda \\cdot \\frac{\\log d}{- \\log \\sqrt{\\rho}} \\right)\n\nThe query complexity of STIR is:O \\left( \\lambda \\cdot \\log \\left( \\frac{\\log d}{- \\log \\sqrt{\\rho}} \\right) + \\log d \\right)\n\nIn the query complexity of STIR, d is usually not large, so the first term \\lambda \\cdot \\log \\left( \\frac{\\log d}{- \\log \\sqrt{\\rho}} \\right) accounts for a larger proportion. We can see that it is at the \\log \\log level, while the original FRI is only at the \\log level.\n\nFigure 2 in Section 6.4 of the paper [ACFY24] gives the experimental results comparing FRI and STIR. We can see that the reduction in query complexity of STIR results in better performance in terms of argument size and the number of hashes computed by the Verifier compared to FRI. This is understandable, as fewer query complexity means:\n\nA reduction in the overall argument size is obvious.\n\nWith fewer queries, the Verifier needs to open fewer Merkle commitments, resulting in fewer hash computations.","type":"content","url":"/fri/stir-en#fri-v-s-stir","position":3},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Powerful Tools for RS Encoding"},"type":"lvl2","url":"/fri/stir-en#powerful-tools-for-rs-encoding","position":4},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Powerful Tools for RS Encoding"},"content":"Here we first introduce several powerful tools for RS encoding, which can help us understand the specific STIR protocol construction.","type":"content","url":"/fri/stir-en#powerful-tools-for-rs-encoding","position":5},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Folding","lvl2":"Powerful Tools for RS Encoding"},"type":"lvl3","url":"/fri/stir-en#folding","position":6},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Folding","lvl2":"Powerful Tools for RS Encoding"},"content":"For a function f: \\mathcal{L} \\rightarrow \\mathbb{F}, given a random number r \\in \\mathbb{F}, its k-fold function is denoted as f_r := \\mathrm{Fold}(f,r) : \\mathcal{L}^{k} \\rightarrow \\mathbb{F}. It is defined as, for each x \\in \\mathcal{L}^{k}, we can find k y in \\mathcal{L} satisfying y^k = x. From k pairs of (y, f(y)), we can obtain a unique polynomial \\hat{p} of degree less than k satisfying \\hat{p}(y) = f(y), then \\hat{p}(r) is the value of the function f_r(x). This definition of the Fold function is completely consistent with the definition of the Fold function in the FRI protocol, and it has two good properties.\n\nThe first property is distance preservation.\n\nIf the function f before folding is in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d], then for any randomly chosen r \\in \\mathbb{F}, the folded function is still an RS code, i.e., f_r \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}^k, d/k].\n\nFor \\delta \\in (0, 1 - \\sqrt{\\rho}), if f is \\delta-far from \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d], then with probability at least 1 - \\mathrm{poly}(|\\mathcal{L}|)/\\mathbb{F} over the choice of random r, f_r is \\delta-far from \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}^k, d/k].\n\nThis property ensures that we can boldly perform folding. If the Prover cheats and provides a function that is \\delta-far from the encoding space, the folded function will still be \\delta-far from the corresponding encoding space with high probability.\n\nThe second property is called Local, which means that to obtain the value of the folded function at any point, we only need to query the values of f at k points to calculate it, because at this time we can obtain a unique polynomial \\hat{p} of degree less than k, and then substitute r to calculate \\hat{p}(r), which is the value at that point. At this time, the Prover does not need to provide an oracle for \\mathrm{Fold}(f,r) separately, the Verifier can obtain it by accessing the oracle of f, which reduces the argument size.","type":"content","url":"/fri/stir-en#folding","position":7},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Quotienting","lvl2":"Powerful Tools for RS Encoding"},"type":"lvl3","url":"/fri/stir-en#quotienting","position":8},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Quotienting","lvl2":"Powerful Tools for RS Encoding"},"content":"For functions f: \\mathcal{L} \\rightarrow \\mathbb{F} and p: S \\rightarrow \\mathbb{F}, where S \\subseteq \\mathbb{F}, the quotient with respect to function f is defined as:\\mathrm{Quotient}(f, S, p)(x) := \\frac{f(x) - \\hat{p}(x)}{\\prod_{a \\in S}(X - a)},\n\nwhere \\hat{p} is the unique polynomial of degree less than |S| satisfying \\hat{p}(a) = p(a) for all a \\in S.\n\nAn important property of this function is Consistency. Assuming S and \\mathcal{L} are disjoint (actually they can intersect, the conclusion will be more complex, see [ACFY24] Lemma 4.4), then\n\nIf f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d], it is an evaluation of a polynomial of degree less than d on \\mathcal{L}, and this polynomial is consistent with p on S, then \\mathrm{Quotient}(f, S, p) \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d - |S|].\n\nIf for any polynomial \\hat{u} of degree less than d that is \\delta-close to f, \\hat{u} is not completely consistent with p on S, i.e., for some a \\in S, \\hat{u}(a) \\neq p(a), then \\mathrm{Quotient}(f, S, p) is \\delta-far from \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d - |S|].\n\nRegarding point 2 above, for codewords \\hat{u} within \\delta range of f, the set of these codewords is denoted as \\mathrm{List}(f,d,\\delta). For any \\hat{u} \\in \\mathrm{List}(f,d,\\delta), as long as there is a point on S where \\hat{u}(a) \\neq p(a), the distance of the quotient polynomial \\mathrm{Quotient}(f, S, p) is amplified and becomes \\delta-far. In other words, if an incorrect value f(a) - p(a) is divided here, the quotient polynomial becomes very far from the RS encoding space of low-degree polynomials.\n\nNote that here we require that for any \\hat{u} \\in \\mathrm{List}(f,d,\\delta), \\hat{u} is inconsistent with p on S. Using the Out of Domain Sampling method, we can limit the codewords within \\delta range of f to at most one with high probability, which makes it easier for the Verifier to detect. We will discuss this method in detail in the next section.\n\nThe \\mathrm{Quotient} function can help us add constraints on the function f. For example, if we want to restrict the value of f at point a to be b, we can achieve this through \\mathrm{Quotient}(f, \\{a\\}, p), where p(a) = b, i.e.,\\mathrm{Quotient}(f, \\{a\\}, p) = \\frac{f(x) - p(x)}{x - a}\n\nThen we just need to prove \\mathrm{Quotient}(f, \\{a\\}, p) \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d - 1]. If the f provided by the Prover does not have a value of b at point a, i.e., f(a) \\neq b, then f(a) \\neq p(a), which will cause \\mathrm{Quotient}(f, \\{a\\}, p) to be \\delta-far from \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d - 1], making it easy for the Verifier to detect. Here we only added one constraint, of course we can add multiple constraints, so we can add constraints to f while converting the test of f to testing whether the \\mathrm{Quotient} function is \\delta-close to the corresponding RS encoding space.\n\nThe \\mathrm{Quotient} function, like the folding function, has the Local property. To calculate the value of the \\mathrm{Quotient} function at a point x \\in \\mathcal{L}\\backslash\\mathcal{S}, it can be calculated by querying the value of function f at point x.","type":"content","url":"/fri/stir-en#quotienting","position":9},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Out of Domain Sampling","lvl2":"Powerful Tools for RS Encoding"},"type":"lvl3","url":"/fri/stir-en#out-of-domain-sampling","position":10},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl3":"Out of Domain Sampling","lvl2":"Powerful Tools for RS Encoding"},"content":"Out of Domain Sampling is a powerful tool that can help us limit the number of codewords within \\delta range of the function f provided by the Prover, thus converting List Decoding to Unique Decoding.\n\nGenerally, for a function f: \\mathcal{L} \\rightarrow \\mathbb{F}, the Verifier randomly selects a number \\alpha \\in \\mathbb{F} \\backslash \\mathcal{L} from outside the domain \\mathcal{L}, and the Prover returns a value \\beta. Then in the list of codewords \\mathrm{List}(f,d,\\delta) within \\delta range of f, with high probability, there is at most one codeword \\hat{u} satisfying \\hat{u}(\\alpha) = \\beta.\n\nThis can be explained using the fundamental theorem of algebra. We only need to prove that the probability of two different codewords \\hat{u}' and \\hat{u} in \\mathrm{List}(f,d,\\delta) having the same value at point \\alpha is relatively small, which implies that with high probability, there is at most one codeword satisfying \\hat{u}(\\alpha) = \\beta.\n\nFirst, fix two different codewords \\hat{u}' and \\hat{u}. Since they are different codewords and both have degree less than d, by the fundamental theorem of algebra, we have\\Pr_{\\alpha \\leftarrow \\mathbb{F} \\backslash \\mathcal{L}} [\\hat{u}'(\\alpha) = \\hat{u}(\\alpha)] \\le \\frac{d - 1}{|\\mathbb{F}| - |\\mathcal{L}|}\n\nSuppose \\mathrm{RS}[\\mathbb{F}, \\mathcal{L},d] is (\\delta, l) list-decodable, meaning that there are at most l codewords within \\delta range. Then there are \\binom{l}{2} ways to choose two different codewords \\hat{u}' and \\hat{u}. Therefore, the probability of any two different codewords \\hat{u}' and \\hat{u} having the same value at point \\alpha does not exceed \\binom{l}{2} \\cdot \\frac{d - 1}{|\\mathbb{F}| - |\\mathcal{L}|}. This probability is very small, thus proving the point.\n\nHow to restrict that the \\beta sent by the Prover is really the value of f at point a? This can be achieved using the Quotient tool introduced in the previous section.","type":"content","url":"/fri/stir-en#out-of-domain-sampling","position":11},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Deep Dive into One Iteration of the STIR Protocol"},"type":"lvl2","url":"/fri/stir-en#deep-dive-into-one-iteration-of-the-stir-protocol","position":12},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Deep Dive into One Iteration of the STIR Protocol"},"content":"In this section, we will apply the three tools mentioned earlier to delve into one iteration of the STIR protocol.\n\nObjective:\n\nInitially given a function f, we want to prove that it is in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], where \\mathcal{L} =\\langle \\omega \\rangle.\n\nAfter one iteration, prove that function f' \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k], where \\mathcal{L}' = \\omega \\cdot \\langle \\omega^2 \\rangle.\n\nThat is, function f is k-folded, its degree is reduced to d/k, but the size of the evaluation domain \\mathcal{L}' of function f' after one iteration is not reduced by k times, but by 2 times. This is the core idea of the STIR protocol mentioned earlier, reducing query complexity by improving the rate.\n\nRegarding the evaluation domains \\mathcal{L} =\\langle \\omega \\rangle and \\mathcal{L}' = \\omega \\cdot \\langle \\omega^2 \\rangle, here’s an example to illustrate. Suppose \\omega^8 = 1.\n\nThe \\mathcal{L}' constructed in this way is half the size of \\mathcal{L}, but actually \\langle \\omega^2 \\rangle can also satisfy the requirement of halving. Why not choose \\mathcal{L}' = \\langle \\omega^2 \\rangle? Suppose we perform k = 4 folds, we can ensure that \\mathcal{L}^4 = \\{\\omega^4, \\omega^8\\} and \\mathcal{L}' = \\{\\omega^1, \\omega^3,\\omega^5, \\omega^7\\} are disjoint. The advantage of doing this is to avoid constructing the function \\mathrm{Fill} defined by the intersection points in \\mathcal{L}^4 \\cap \\mathcal{L}', so the Verifier doesn’t need to additionally check if the function values of \\mathrm{Fill} are correct (as explained in [ACFY24] Remark 5.3).\n\nThe protocol flow for one iteration is shown in the following figure:\n\nSample folding randomness: The Verifier first randomly selects a number r^{\\mathrm{fold}} from \\mathbb{F}, which will be used to fold function f.\n\nSend folded function: The Prover sends the folded function g: \\mathcal{L}' \\rightarrow \\mathbb{F}. If the Prover is honest, then function g is the evaluation of polynomial \\hat{g} on \\mathcal{L}'. Here, evaluation means that g is completely consistent with \\hat{g} on \\mathcal{L}', and polynomial \\hat{g} is obtained through \\mathrm{Fold}(f, r^{\\mathrm{fold}}). First, use the random number r^{\\mathrm{fold}} to perform k-fold on function f, obtaining \\mathrm{Fold}(f, r^{\\mathrm{fold}}) : \\mathcal{L}^k \\rightarrow \\mathbb{F}. At this time, the range of the folded function is \\mathcal{L}^k, but we want it to take values on \\mathcal{L}'. We just need to extend the domain of \\mathrm{Fold}(f, r^{\\mathrm{fold}}) to \\mathcal{L}', obtaining polynomial \\hat{g}: \\mathcal{L}' \\rightarrow \\mathbb{F}, which has degree less than d/k.\n\nOut-of-domain sample: The Verifier takes a random number r^{\\mathrm{out}} from \\mathbb{F}\\backslash \\mathcal{L}' and sends it to the Prover.\n\nOut-of-domain reply: The Prover replies with \\beta \\in \\mathbb{F}. If the Prover is honest, then \\beta := \\hat{g}(r^{\\mathrm{out}}).\n\n📝 Notes\nThe purpose of steps 3 and 4 here is to use Out of Domain Sampling to convert list decoding to unique decoding, that is, the Verifier selects a random number r^{\\mathrm{out}} from \\mathbb{F} \\backslash \\mathcal{L} and requires the Prover to reply with \\beta.\n\nShift queries: The Verifier selects t random numbers from \\langle \\omega^k \\rangle, i.e., \\forall i \\in [t],r_i^{\\mathrm{shift}} \\leftarrow \\langle \\omega^k \\rangle. According to the Local property of the folding function, the Verifier can calculate y_i := f_{\\mathrm{fold}}(r_i^{\\mathrm{shift}}) by querying f, where f_{\\mathrm{fold}} :=\\mathrm{Fold}(f, r^{\\mathrm{fold}}).\n\nIn step 2, the Prover sent g: \\mathcal{L}' \\rightarrow \\mathbb{F} and claims that it is consistent with \\mathrm{Fold}(f, r^{\\mathrm{fold}}) on \\mathcal{L}', but the Verifier cannot directly query the values of the folded function on \\mathcal{L}'. The Verifier can only calculate the values of \\mathrm{Fold}(f, r^{\\mathrm{fold}}) on \\mathcal{L}^k by querying f. Fortunately, we can use the Quotient tool here to ensure consistency.\n\nIn steps 3 and 4, the Out-of-domain Sampling method is first used to limit the number of codewords within \\delta range of g to at most one, denoted as \\hat{u}. Then in step 5, query the values of \\mathrm{Fold}(f, r^{\\mathrm{fold}}) on \\mathcal{L}^k, which is convenient for subsequent verification of whether \\hat{u} is consistent with the folded function on \\mathcal{L}^k. The verification of consistency is left to the Quotient function.\n\nLet’s form a set \\mathcal{G} := \\{r^{\\mathrm{out}},r_1^{\\mathrm{shift}}, \\ldots, r_t^{\\mathrm{shift}}\\} of all these points that need to ensure consistency, and then define a function p: \\mathcal{G}\\rightarrow \\mathbb{F} that satisfies:p(r^{\\mathrm{out}}) = \\beta,p(r_i^{\\mathrm{shift}}) = y_i.\n\nDefine the next function f' asf' := \\mathrm{Quotient}(f, \\mathcal{G}, p) = \\frac{g(x) - \\hat{p}(x)}{\\prod_{a \\in \\mathcal{G}}(X - a)}.\n\nDue to the Local property of the Quotient function, to calculate the values of f' on \\mathcal{L}', we only need to query the values of g on \\mathcal{L}'.\n\nAt this point, we just need to test whether f' is \\delta-close to \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k].\n\nLooking closely at the formula for f', we can see that if the Prover is honest, f' \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}', d/k - |\\mathcal{G}|]. There is actually a reduction in the degree of the polynomial here, and degree correction is needed to correct the degree of f' to d/k. This point will be discussed in the following text.","type":"content","url":"/fri/stir-en#deep-dive-into-one-iteration-of-the-stir-protocol","position":13},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Soundness Analysis"},"type":"lvl2","url":"/fri/stir-en#soundness-analysis","position":14},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Soundness Analysis"},"content":"In this section, we will perform a soundness analysis for one iteration, that is, if the Prover cheats and f is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], we analyze the probability that f' is also relatively far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|]. [ACFY24] Lemma 1 gives the following conclusion:\n\nProposition 1 [ACFY24, Lemma 1] If f is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], then except with probability (1 - \\delta)^t +  \\mathrm{poly}(|\\mathcal{L}|)/|\\mathbb{F}|, f' is (approximately) (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|].\n\nProof idea:\n\nAccording to the distance-preserving property of the folding function, the function f_{r^{\\mathrm{fold}}} := \\mathrm{Fold}(f, r^{\\mathrm{fold}}) obtained after folding f with the random number r^{\\mathrm{fold}} is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}^k,d/k] with probability greater than 1 - \\mathrm{poly}(|\\mathcal{L}|/|\\mathbb{F}|).\n\nAccording to the property of Out-of-domain Sampling, the probability that g has at most one codeword \\hat{u} within 1 - \\sqrt{\\rho'} range satisfying \\hat{u}(r^{\\mathrm{out}}) = \\beta is greater than 1 - \\mathrm{poly}(|\\mathcal{L}|)/|\\mathbb{F}|.\n\nNow let’s analyze point 2. The function g: \\mathcal{L}' \\rightarrow \\mathbb{F}, now consider its distance from the encoding space \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}', d/k]. According to the Johnson bound, \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}', d/k] is (\\gamma, l)-list-decodable, where \\gamma \\approx 1 - \\sqrt{\\rho'}, l = \\mathrm{poly}(|\\mathcal{L}'|) =\\mathrm{poly}(|\\mathcal{L}|), which means there are at most l polynomials of degree less than d/k that are not more than \\gamma away from g. Then for any two different polynomials \\hat{u}' and \\hat{u} chosen from these l polynomials, when randomly selecting r^{\\mathrm{out}} from \\mathbb{F} \\backslash \\mathcal{L}', the probability that their values at point r^{\\mathrm{out}} are both equal to \\beta does not exceed \\frac{d/k - 1}{|\\mathbb{F}| - |\\mathcal{L}'|}. There are \\binom{l}{2} ways to choose these two polynomials, so this probability does not exceed\\binom{l}{2} \\cdot \\frac{d/k - 1}{|\\mathbb{F}| - |\\mathcal{L}'|} = O\\left(\\frac{l^2 \\cdot (d/k - 1)}{|\\mathbb{F}| - |\\mathcal{L}'|}\\right) = \\mathrm{poly}(|\\mathcal{L}|)/|\\mathbb{F}|.\n\nTherefore, the probability that g has at most one codeword \\hat{u} within 1 - \\sqrt{\\rho'} range satisfying \\hat{u}(r^{\\mathrm{out}}) = \\beta is greater than 1 - \\mathrm{poly}(|\\mathcal{L}|)/|\\mathbb{F}|.\n\nIf both item 1 and item 2 hold, this probability is greater than 1 - \\mathrm{poly}(|\\mathcal{L}|)/|\\mathbb{F}|. Now we only need to prove that the probability that f' is (approximately) (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|] is at least 1 - (1 - \\delta)^t.\n\nLet’s discuss two cases:\n\nIf there is no codeword satisfying the requirement in item 2, that is, there is no codeword satisfying \\hat{u}(r^{\\mathrm{out}}) = \\beta within 1 - \\sqrt{\\rho'} range of g, and according to the construction of the protocol, p(r^{\\mathrm{out}}) = \\beta. Therefore, for any codeword within 1 - \\sqrt{\\rho'} range of g, we have \\hat{u}(r^{\\mathrm{out}}) \\neq p(r^{\\mathrm{out}}). Sincef' := \\mathrm{Quotient}(g, \\mathcal{G}, p) = \\frac{g(x) - \\hat{p}(x)}{\\prod_{a \\in \\mathcal{G}}(X - a)}.\n\nAccording to the consistency of the Quotient function, at this time \\hat{u} and p are not completely consistent on \\mathcal{G}, so f' = \\mathrm{Quotient}(f, \\mathcal{G}, p) is (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|].\n\nIf there exists a codeword \\hat{u} satisfying the requirement in item 2, there is already a codeword satisfying \\hat{u}(r^{\\mathrm{out}}) = \\beta within 1 - \\sqrt{\\rho'} range of g. According tof' := \\mathrm{Quotient}(g, \\mathcal{G}, p) = \\frac{g(x) - \\hat{p}(x)}{\\prod_{a \\in \\mathcal{G}}(X - a)}.\n\nNow \\hat{u}(r^{\\mathrm{out}}) = \\beta = p(r^{\\mathrm{out}}) is already satisfied. If for all i \\in [t], we have \\hat{u}(r_i^{\\mathrm{shift}}) = y_i = p(r_i^{\\mathrm{shift}}), then f' = \\mathrm{Quotient}(f, \\mathcal{G}, p) is not more than (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|]. Otherwise, according to the consistency of the Quotient function, as long as for some i we have \\hat{u}(r_i^{\\mathrm{shift}}) \\neq y_i, at this time \\hat{u}(r_i^{\\mathrm{shift}}) \\neq p(r_i^{\\mathrm{shift}}), it will cause f' to be (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|].\n\nSince item 1 holds, we have \\Delta(f_{r^{\\mathrm{fold}}}, \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}^k, d/k]) \\ge \\delta for the folded function, so\\begin{aligned}\n         \\Pr \\left[\\forall i \\in [t], \\hat{u}(r_i^{\\mathrm{shift}}) = y_i \\right] & =  \\Pr \\left[\\forall i \\in [t], \\hat{u}(r_i^{\\mathrm{shift}}) = f_{r^{\\mathrm{fold}}}(r_i^{\\mathrm{shift}}) \\right] \\\\\n         & \\le (1 - \\delta)^t.\n    \\end{aligned}\n\nTherefore, the probability that f' is (approximately) (1 - \\sqrt{\\rho'})-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|] is at least 1 - (1 - \\delta)^t.\n\nThus, Proposition 1 is proved. \n\n\\Box\n\nIn fact, the round-by-round soundness error of the protocol is approximately \\max \\{\\frac{\\mathrm{poly}(|\\mathcal{L}|)}{|\\mathbb{F}|}, (1 - \\delta)^t\\}.","type":"content","url":"/fri/stir-en#soundness-analysis","position":15},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Degree correction"},"type":"lvl2","url":"/fri/stir-en#degree-correction","position":16},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Degree correction"},"content":"Now there’s a small problem left to solve. According to the definition of function f'f' := \\mathrm{Quotient}(g, \\mathcal{G}, p) = \\frac{g(x) - \\hat{p}(x)}{\\prod_{a \\in \\mathcal{G}}(X - a)}.\n\nWe can see that, strictly speaking, this converts the test of f to testing the distance of f' from \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k- |\\mathcal{G}|], rather than \\mathrm{RS}[\\mathbb{F},\\mathcal{L}',d/k], which requires degree correction.\n\nGenerally, let’s assume that the function we want to perform degree correction on is f: \\mathcal{L} \\rightarrow \\mathbb{F}, its initial degree is d, and the target corrected degree is d^* \\ge d. We want to construct an efficient degree correction algorithm that can output a function f^* satisfying:\n\nIf f \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], then f^* \\in \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d^*].\n\nIf f is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d], then with high probability, f^* is also \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d^*].\n\nQueries to f^* can be efficiently computed through queries to f.\n\nThe STIR paper ([ACFY24], Section 2.3) proposes a method that not only satisfies the above three conditions but also uses the method of summing geometric series to make the calculation in item 3 more efficient.\n\nThe method is to randomly sample an element r \\leftarrow \\mathbb{F} from the field and definef^*(x) = \\sum_{i=0}^{e} r^i \\cdot f_i(x) \\tag{1}\n\nwhere f_i(x) := x^i \\cdot f(x), e = d^* - d. Expanding equation (1), we getf^*(x) = r^0 \\cdot x^0 \\cdot f(x) + r^1 \\cdot x^1 \\cdot f(x) + \\cdots + r^e \\cdot x^e \\cdot f(x) \\tag{2}\n\nAccording to the construction of f^*, item 1 naturally holds.\n\nFor \\delta < \\min \\{ 1 - \\sqrt{\\rho}, 1 - (1 + 1/d^*) \\cdot \\rho\\}, item 2 also holds. This can be obtained from the Correlated Agreement theorem in [BCIKS20], which we won’t elaborate on here.\n\nNext, let’s analyze item 3. From equation (2), we can see that to calculate the value of f^* at point x, after querying the value of f(x), we need to sum e + 1 terms, which takes O(e) time. If e = \\Omega(d), this is inefficient, but by using the method of summing geometric series, we can reduce the computational complexity to O(\\log e).\\begin{aligned}\n    f^*(x) & = \\sum_{i=0}^{e} r^i \\cdot f_i(x) \\\\\n    & = \\sum_{i=0}^{e} r^i \\cdot x^i \\cdot f(x)\\\\\n    & = f(x) \\cdot \\sum_{i=0}^{e} (r \\cdot x)^i \\\\\n\\end{aligned}\n\nUsing the geometric series sum formula for \\sum_{i=0}^{e} (r \\cdot x)^i, we can getf^*(x) = \\begin{cases}\n    f(x) \\cdot \\frac{1 - (r \\cdot x)^{e+1}}{1 - r \\cdot x} &  \\text{if} \\quad r \\cdot x \\neq 1 \\\\\n    f(x) \\cdot (e + 1) & \\text{if} \\quad r \\cdot x = 1\n    \\end{cases}\n\nFor the more complex f(x) \\cdot \\frac{1 - (r \\cdot x)^{e+1}}{1 - r \\cdot x}, the term (r \\cdot x)^{e+1} can be calculated using the repeated squaring method, which takes O(\\log e) calculations. Then by querying the value of f at point x to get f(x), it takes O(\\log e) operations in total to calculate f^*(x).\n\nThis method can be extended to multiple functions of different degrees. For m functions f_1, \\ldots, f_m: \\mathcal{L} \\rightarrow \\mathbb{F} and degrees d_1, \\ldots, d_m, we want to perform batch-degree-correction, finally obtaining a function f^* with degree d^*. Randomly sample a random number r \\leftarrow \\mathbb{F}, define e_i = d^* - d_i andf^*(x) = \\sum_{i = 0}^{e_1} r^i \\cdot x^i \\cdot f_1(x) + r^{1 + e_1} \\sum_{i = 0}^{e_2} r^i \\cdot x^i \\cdot f_2(x) + \\cdots + r^{m - 1 + \\sum_{j = 1}^{m - 1}e_j} \\sum_{i = 0}^{e_m} r^i \\cdot x^i \\cdot f_m(x).\n\nSimilar to the degree correction of a single function above, for \\delta < \\min \\{ 1 - \\sqrt{\\rho}, 1 - (1 + 1/d^*) \\cdot \\rho\\}, if any f_i is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d_i], then f^* is \\delta-far from \\mathrm{RS}[\\mathbb{F},\\mathcal{L},d^*]. Similarly, using the method of summing geometric series, by querying f_1, \\ldots, f_m, it takes O(\\sum_i \\log e_i) = O(m \\cdot \\log d^*) operations to calculate the value of f^* at point x.","type":"content","url":"/fri/stir-en#degree-correction","position":17},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Summary"},"type":"lvl2","url":"/fri/stir-en#summary","position":18},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"Summary"},"content":"STIR changes the evaluation domain of the function in each round, changing the original \\mathcal{L}^k in the FRI protocol to \\mathcal{L}'. The function is still k-folded, but \\mathcal{L}' is only half the size of the original. This reduces the rate of the encoding space, which can reduce the number of queries by the Verifier, and this is the core idea of STIR.\n\nIn the construction of the STIR protocol, several powerful tools for RS encoding are used, making the entire protocol efficient and secure.\n\nFirst, consistent with the FRI protocol, the function f is k-folded, but the resulting function needs to extend its evaluation domain from \\mathcal{L}^k to \\mathcal{L}'. According to the distance-preserving property of the folding function, we can confidently perform this folding.\n\nThen, to reduce the Verifier’s work, the Out of Domain Sampling method is used to convert list decoding to unique decoding. This is where the Verifier selects a random number r^{\\mathrm{out}} from \\mathbb{F} \\backslash \\mathcal{L} and requires the Prover to reply with \\beta in the protocol.\n\nAt this point, after changing the evaluation domain to \\mathcal{L}', the problem faced is that the Verifier can only query the values of the k-folded function \\mathrm{f}_{r^{\\mathrm{fold}}} on \\mathcal{L}^k. Fortunately, the powerful Quotient tool can be used to constrain the function sent by the Prover to be consistent with the folded function on \\mathcal{L}^k. At this time, the Verifier selects t random numbers r_{i}^{\\mathrm{shift}} from \\mathcal{L}^k for querying.\n\nFinally, combine r^{\\mathrm{out}} and r_{i}^{\\mathrm{shift}}, and use the Quotient tool to constrain the values sent by the Prover at these points to be correct.\n\nCombining these tools, a soundness analysis of one iteration of the STIR protocol was performed. In fact, we can obtain that the round-by-round soundness error of STIR is \\max \\{\\frac{\\mathrm{poly}(|\\mathcal{L}|)}{|\\mathbb{F}|}, (1 - \\delta)^t\\}.\n\nFinally, to raise the degree of f' after iteration from d/k - |\\mathcal{G}| to d/k, a degree correction method using geometric series summation that can be efficiently calculated was introduced.","type":"content","url":"/fri/stir-en#summary","position":19},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"References"},"type":"lvl2","url":"/fri/stir-en#references","position":20},{"hierarchy":{"lvl1":"STIR: Improving Rate to Reduce Query Complexity","lvl2":"References"},"content":"[ACFY24] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[BCS16] Eli Ben-Sasson, Alessandro Chiesa, and Nicholas Spooner. “Interactive Oracle Proofs”. In: Proceedings of the 14th Theory of Cryptography Conference. TCC '16-B. 2016, pp. 31–60.\n\n[BGKS20] Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. “DEEP-FRI: Sampling Outside the Box Improves Soundness”. In: Proceedings of the 11th Innovations in Theoretical Computer Science Conference. ITCS '20. 2020, 5:1–5:32.\n\nSTIR: Reed–Solomon Proximity Testing with Fewer Queries\n\nVideo: \n\nZK11: STIR: Reed–Solomon Proximity Testing with Fewer Queries - Gal Arnon & Giacomo Fenzi","type":"content","url":"/fri/stir-en#references","position":21},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification"},"type":"lvl1","url":"/fri/whir","position":0},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification"},"content":"Jade Xie  \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article mainly introduces the WHIR (Weights Help Improving Rate) protocol [ACFY24b]. Like the FRI [BBHR18], STIR [ACFY24a], and BaseFold [ZCF24] protocols, WHIR is also an IOPP protocol, but it has a smaller query complexity and a faster verification time. The paper [ACFY24b] mentions that WHIR’s verifier typically runs in hundreds of microseconds (1 microsecond = \n\n10-6 seconds), while other protocols’ verifiers take a few milliseconds (1 millisecond = \n\n10-3 seconds). Additionally, WHIR is an IOPP protocol for constrained Reed-Solomon codes (CRS), which allows WHIR to support queries for both multivariate linear polynomials and univariate polynomials, which is why WHIR can be compared simultaneously with BaseFold, FRI, and STIR [ACFY24b]. Overall, WHIR combines the ideas of BaseFold and STIR, enabling the WHIR protocol to support multivariate linear polynomials without sacrificing Prover efficiency and argument size, while also having a smaller query complexity.","type":"content","url":"/fri/whir","position":1},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"From Univariate Polynomials to Multivariate Linear Polynomials"},"type":"lvl2","url":"/fri/whir#from-univariate-polynomials-to-multivariate-linear-polynomials","position":2},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"From Univariate Polynomials to Multivariate Linear Polynomials"},"content":"For a finite field \\mathbb{F}, evaluation domain \\mathcal{L} \\subseteq \\mathbb{F}, and Reed-Solomon encoding of degree d \\in \\mathbb{N}, it represents the set of evaluations of all univariate polynomials over \\mathbb{F} with degree strictly less than d on \\mathcal{L}, denoted as \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d]. Assuming \\mathcal{L} is a multiplicative coset of \\mathbb{F}^*, and its order is a power of 2 (called “smooth” \\mathcal{L}), and also assuming the degree d = 2^m is in the form of a power of 2, then we can view the univariate polynomial as a multivariate linear polynomial with m variables. (From [ACFY24b, 1.1 Constrained Reed-Solomon codes])\n\nLet’s first give a simple example with d = 2^3, letf(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7\n\nLet X_1 = x, X_2 = x^2 , X_3 = x^4, then f(x) can be represented as:\\begin{aligned}\n    f(x) & = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7 \\\\\n    & = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1X_2 + a_4 X_3 + a_5 X_1 X_2 + a_6 X_1 X_3 + a_7 X_1 X_2 X_3\n\\end{aligned}\n\nDenote the new multivariate linear polynomial as\\hat{f}(X_1, X_2, X_3) = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1X_2 + a_4 X_3 + a_5 X_1 X_2 + a_6 X_1 X_3 + a_7 X_1 X_2 X_3\n\nIn this way, f(x) can be viewed as a univariate polynomial, or as a multivariate linear polynomial after variable substitution X_1 = x, X_2 = x^2 , X_3 = x^4.\n\nFor univariate polynomials in the RS code \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d], it’s similar, they can be viewed from the perspective of multivariate linear polynomials, i.e.,\\begin{aligned}\n    \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d] & := \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{g} \\in \\mathbb{F}^{< 2^m}[X] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{g}(x)\\} \\\\\n    & = \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{f} \\in \\mathbb{F}^{< 2}[X_1, \\ldots, X_m] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{f}(x^{2^0}, x^{2^1},\\ldots, x^{2^{m-1}})\\}\n\\end{aligned}\n\nIn the above equation, \\hat{g}(x) is the univariate polynomial, while \\hat{f}(X_1, \\ldots, X_m) is the multivariate linear polynomial with m variables. The idea used here appears in BaseFold. (From [ACFY24b, 1.1 Constrained Reed-Solomon codes])\n\nFurthermore, consistent with the FRI protocol, folding a univariate polynomial with a random number \\alpha_1 can be equivalently viewed as substituting \\alpha_1 for one of the variables in the multivariate linear polynomial.\n\nFor example, for the above f(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7, first fold with \\alpha_1, then\\begin{aligned}\n    f(x) & = a_0 + a_2 x^2 + a_4 x^4 + a_6 x^6 + x (a_1 + a_3 x^2 + a_5 x^4 + a_7 x^6) \\\\\n    & := f_1(x^2) + x f_2(x^2)\n\\end{aligned}\n\nThe folded polynomial is\\begin{aligned}\n    f^{(1)}(x) & = f_1(x) + \\alpha_1 f_2(x) \\\\\n    & = a_0 + a_2 x + a_4 x^2 + a_6 x^3 + \\alpha_1 (a_1 + a_3 x + a_5 x^2 + a_7 x^3) \\\\\n    & = a_0 + a_2 X_1 + a_4 X_2 + a_6 X_1X_2 + \\alpha_1 (a_1 + a_3 X_1 + a_5 X_2 + a_7 X_1X_2)\n\\end{aligned}\n\nThis is equivalent to directly substituting values and replacing variables in the original multivariate polynomial \\hat{f}(X_1,X_2,X_3), specifically:\n\nFirst, substitute X_1 with \\alpha_1, we get\\begin{aligned}\n    \\hat{f}(\\alpha_1,X_2,X_3) & = a_0 + a_1 \\cdot \\alpha_1 + a_2 X_2 + a_3 \\cdot  \\alpha_1 X_2 + a_4 X_3 + a_5 \\cdot \\alpha_1 X_2 + a_6 X_2 X_3 + a_7 \\cdot \\alpha_1 X_2 X_3 \\\\\n    & = a_0 + a_2 X_2 + a_4 X_3 + a_6 X_2 X_3 + \\alpha_1 (a_1 + a_3 X_2 + a_5 X_2 + a_7 X_2 X_3) \n\\end{aligned}\n\nLet the new variables X_1 = X_2, and X_2 = X_3, we get the folded polynomial as\\begin{aligned}\n    \\hat{f}^{(1)}(X_1, X_2) & = a_0 + a_2 X_1 + a_4 X_2 + a_6 X_1 X_2 + \\alpha_1 (a_1 + a_3 X_1 + a_5 X_2 + a_7 X_1 X_2) \\\\\n    & = f^{(1)}(x)\n\\end{aligned}\n\nWe can see that the polynomials obtained by the two folding methods are equivalent, except that f^{(1)}(x) is in the form of a univariate polynomial, while \\hat{f}^{(1)}(X_1, X_2) is in the form of a multivariate linear polynomial.\n\nIf we want to perform a 4-fold on the original polynomial f(x), from the perspective of univariate polynomials, we can perform a 2-fold on the polynomial f^{(1)}(x) after the 2-fold, i.e.,\\begin{aligned}\n    f^{(1)}(x) & = a_0 + a_2 x + a_4 x^2 + a_6 x^3 + \\alpha_1 (a_1 + a_3 x + a_5 x^2 + a_7 x^3) \\\\\n    & = (a_0 + \\alpha_1 a_1) + (a_2 + \\alpha_1 a_3) \\cdot x + (a_4 + \\alpha_1 a_5) \\cdot x^2 + (a_6 + \\alpha_1 a_7) \\cdot x^3 \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) \\cdot x^2) + x \\cdot ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7) \\cdot x^2) \\\\\n    & := f_1^{(1)}(x^2) + x f_2^{(1)}(x^2)\n\\end{aligned}\n\nFolding with a random number \\alpha_2, we get the folded polynomial as\\begin{aligned}\n    f^{(2)}(x) & = f_1^{(1)}(x) + \\alpha_2 f_2^{(1)}(x) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5)  x) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7) x) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_1) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_1)\n\\end{aligned}\n\nFrom the perspective of multivariate linear polynomials, we can perform a 2-fold on the multivariate linear polynomial \\hat{f}^{(1)}(X_1, X_2) after the 2-fold, i.e.,\n\nSubstitute X_1 with \\alpha_2, we get\\begin{aligned}\n    \\hat{f}^{(1)}(\\alpha_2, X_2) & = a_0 + a_2 \\alpha_2 + a_4 X_2 + a_6 \\alpha_2 X_2 + \\alpha_1 (a_1 + a_3 \\alpha_2 + a_5 X_2 + a_7 \\alpha_2 X_2) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_2) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_2)\n\\end{aligned}\n\nLet the new variable X_1 = X_2, we get the folded polynomial as\\hat{f}^{(2)}(X_1) = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_1) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_1)\n\nWe can find that for multiple folds, folding using univariate polynomials and directly folding using multivariate linear polynomials are equivalent. The process of folding a multivariate linear polynomial with random numbers (\\alpha_1, \\alpha_2) is just the process of direct variable substitution, i.e., we get \\hat{f}^{(2)}(X_1) = \\hat{f}(\\alpha_1, \\alpha_2, X_1).\n\nBelow we introduce the definition of the folding function given in the paper [ACFY24b], which is consistent with the folding method in the FRI protocol.\n\nDefinition 1 [ACFY24b, Definition 4.14] Let f: \\mathcal{L} \\rightarrow \\mathbb{F} be a function, \\alpha \\in \\mathbb{F}. Define \\mathrm{Fold}(f, \\alpha): \\mathcal{L}^2 \\rightarrow \\mathbb{F} as follows:\\forall x \\in \\mathcal{L}^2, \\; \\mathrm{Fold}(f, \\alpha)(x^2) = \\frac{f(x) + f(-x)}{2} + \\alpha \\cdot \\frac{f(x) - f(-x)}{2 \\cdot x}\n\nTo calculate \\mathrm{Fold}(f, \\alpha)(x^2), it’s sufficient to query the values of f at x and -x.\n\nFor k \\le m and \\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_k) \\in \\mathbb{F}^k, define \\mathrm{Fold}(f, \\boldsymbol{\\alpha}) : \\mathcal{L}^{(2^k)} \\rightarrow \\mathbb{F}, denote \\mathrm{Fold}(f, \\boldsymbol{\\alpha}) := f_k, recursively define: f_0 := f and f_i := \\mathrm{Fold}(f_{i-1}, \\alpha_i).\n\nThe following proposition tells us that folding a Reed-Solomon code on any set of points still results in a Reed-Solomon code. ([ACFY24b])\n\nProposition 1 [ACFY24b, Claim 4.15] Let f: \\mathcal{L} \\rightarrow \\mathbb{F} be a function, \\boldsymbol{\\alpha} \\in \\mathbb{F}^k represent folding random numbers, let g:= \\mathrm{Fold}(f, \\boldsymbol{\\alpha}). If f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] and k \\le m, then g \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}^{(2^k)}, m-k], and the multilinear extension of g is \\hat{g}(X_k, \\ldots, X_m) := \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m), where \\hat{f} is the multilinear extension of f.\n\nThe \\hat{g}(X_k, \\ldots, X_m) := \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m) given in the proposition is consistent with the folding of the univariate polynomial f and the direct folding of the multivariate linear polynomial \\hat{f} with random numbers mentioned above. From the perspective of multivariate linear polynomials, it is just direct variable substitution with random numbers \\boldsymbol{\\alpha}, i.e., \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m).\n\nRecalling the FRI protocol, it continuously folds the univariate polynomial f with random numbers (\\alpha_1, \\ldots, \\alpha_m) until finally obtaining a constant polynomial. From the perspective of multivariate linear polynomials, we would eventually get \\hat{f}(\\alpha_1, \\ldots, \\alpha_m) as a constant. Connecting to the Sumcheck protocol, the last step also requires obtaining the value of a multivariate polynomial at a certain random point, and the verifier needs to obtain this value for verification. This step is usually implemented using an oracle. Now the FRI protocol can also provide the value of \\hat{f}(\\alpha_1, \\ldots, \\alpha_m) at a random point at the end. If the Sumcheck protocol and the FRI protocol choose the same random point (\\alpha_1, \\ldots, \\alpha_m), then the FRI protocol can directly provide the value needed for the last step of the Sumcheck protocol when it reaches the end. Combining the FRI protocol and the Sumcheck protocol in this way is the idea of the BaseFold protocol [ZCF24].","type":"content","url":"/fri/whir#from-univariate-polynomials-to-multivariate-linear-polynomials","position":3},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"CRS: Constrained Reed-Solomon codes"},"type":"lvl2","url":"/fri/whir#crs-constrained-reed-solomon-codes","position":4},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"CRS: Constrained Reed-Solomon codes"},"content":"Below is the definition of constrained Reed-Solomon codes given in the WHIR paper [ACFY24b]. It is a subset of Reed-Solomon codes, but with an additional constraint similar to Sumcheck.\n\nDefinition 2 [ACFY24b, Definition 1] For a field \\mathbb{F}, smooth evaluation domain \\mathcal{L} \\subseteq \\mathbb{F}, number of variables m \\in \\mathbb{N}, weight polynomial \\hat{w} \\in \\mathbb{F}[Z, X_1, \\ldots, X_m], and target \\sigma \\in \\mathbb{F}, the constrained Reed-Solomon code is defined as\\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma] := \\left\\{ f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m]: \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma \\right\\}.\n\nFrom the definition, we can see that CRS (constrained Reed-Solomon code) is first a RS code, i.e., f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] in the definition, but on top of this, it needs to satisfy a summation constraint similar to Sumcheck \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma.\n\nThe paper [ACFY24b] mentions that the weight polynomial \\hat{w} in the definition can be defined by oneself and has wide applications. The paper gives such an example: an evaluation constraint \\hat{f}(\\mathbf{z}) = \\sigma, which constrains the value of the multivariate polynomial \\hat{f} at point \\mathbf{z} \\in \\mathbb{F}^m to be the target value \\sigma. First, perform a multilinear extension on f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] to get\\hat{f}(\\mathbf{X}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} f(\\mathbf{b}) \\cdot \\mathrm{eq}(\\mathbf{b}, \\mathbf{X})\n\nwhere \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = \\prod_{i=1}^m (b_i X_i + (1 - b_i) \\cdot (1 - X_i)). Therefore, when \\mathbf{b},\\mathbf{X} \\in \\{0,1\\}^m, if \\mathbf{b} = \\mathbf{X}, then \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = 1, if \\mathbf{b} \\neq \\mathbf{X}, then \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = 0. Thus\\hat{f}(\\mathbf{z}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} f(\\mathbf{b}) \\cdot \\mathrm{eq}(\\mathbf{b}, \\mathbf{z}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b})\n\nThe weight polynomial \\hat{w}(Z, \\mathbf{X}) can be defined as\\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}).\n\nIn this way, an evaluation constraint can be represented using the weight polynomial. Based on this, the corresponding PCS can be constructed (from [ACFY24b, 1.1 Hash-based PCS from CRS codes]), in two cases:\n\nConstrain the value of the multivariate linear polynomial \\hat{f} at \\mathbf{z} \\in \\mathbb{F}^m to be \\sigma, let the weight polynomial be\n\\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}).\n\nConstrain the value of a univariate polynomial f at z \\in \\mathbb{F} to be \\sigma, convert this case to the case of multivariate linear polynomials, consider the evaluation point as \\mathbf{z} = (z^{2^0}, \\ldots, z^{2^{m-1}}), then the weight polynomial is\n\\hat{w}(Z, X) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, (z^{2^0}, \\ldots, z^{2^{m-1}})).","type":"content","url":"/fri/whir#crs-constrained-reed-solomon-codes","position":5},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"One Iteration of WHIR"},"type":"lvl2","url":"/fri/whir#one-iteration-of-whir","position":6},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"One Iteration of WHIR"},"content":"As mentioned earlier, BaseFold combined the Sumcheck and FRI protocols, while the WHIR protocol combines the ideas of BaseFold and STIR, replacing the FRI protocol in BaseFold with the STIR protocol. Compared to the FRI protocol, the STIR protocol has a smaller query complexity. The core idea of the STIR protocol is to reduce the rate of each iteration, increasing the redundancy in the messages sent by the Prover, thereby reducing the Verifier’s query complexity.\n\nLet’s delve into one iteration of the WHIR protocol (from [ACFYb, 2.1.3 WHIR protocol]) to see how WHIR specifically combines BaseFold and the STIR protocol. After one iteration, the problem of testing the proximity of f \\in \\mathcal{C} := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma] is transformed into testing f' \\in \\mathcal{C}' := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'].\n\nSumcheck rounds. Prover and Verifier interact for k rounds of Sumcheck for the constraint in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma]\\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma\n\nwhere \\hat{f} is the multivariate linear polynomial corresponding to f.\n\na. Prover sends a univariate polynomial \\hat{h}_1(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-1}} \\hat{w}(\\hat{f}(X, \\mathbf{b}), X, \\mathbf{b}) to Verifier, Verifier checks \\hat{h}_1(0) + \\hat{h}_1(1) = \\sigma, selects a random number \\alpha_1 \\leftarrow \\mathbb{F} and sends it, the sumcheck claim becomes \\hat{h}_1(\\alpha_1) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-1}} \\hat{w}(\\hat{f}(\\alpha_1, \\mathbf{b}), \\alpha_1, \\mathbf{b}).\nb. For the i-th round, i from 2 to k, Prover sends a univariate polynomial\\hat{h}_i(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-i}} \\hat{w}(\\hat{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b})\n\nVerifier checks \\hat{h}_{i}(0) + \\hat{h}_{i}(1) = \\hat{h}_{i-1}(\\alpha_{i-1}), selects a random number \\alpha_i \\leftarrow \\mathbb{F}, the sumcheck claim becomes\\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-i}} \\hat{w}(\\hat{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}) = \\hat{h}_i(\\alpha_i)\n\nTherefore, after the above k rounds of sumcheck, prover has sent polynomials (\\hat{h}_1, \\ldots, \\hat{h}_k), verifier has selected random numbers \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k) \\in \\mathbb{F}^k. The initial claim becomes the following statement\\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(\\hat{f}(\\boldsymbol{\\alpha}, \\mathbf{b}), \\boldsymbol{\\alpha}, \\mathbf{b}) = \\hat{h}_k(\\alpha_k)\n\nSend folded function. Prover sends function g: \\mathcal{L}^{(2)} \\rightarrow \\mathbb{F}. In the case of an honest Prover, \\hat{g} \\equiv \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), g is defined as the evaluation of \\hat{g} on domain \\mathcal{L}^{(2)}.\n\nThis means first folding \\hat{f} 2^k times with random numbers \\boldsymbol{\\alpha} to get \\hat{g} = \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), at this point \\hat{g} : \\mathcal{L}^{(2^k)} \\rightarrow \\mathbb{F}, with its domain range as \\mathcal{L}^{(2^k)}. Since \\hat{g} is essentially a polynomial, we can change the domain of its variables to \\mathcal{L}^{(2)}, the function g is consistent with the evaluation of \\hat{g} on \\mathcal{L}^{(2)}.\n\nOut-of-domain sample. Verifier selects a random number z_0 \\leftarrow \\mathbb{F} and sends it to Prover. Let \\boldsymbol{z}_0 := (z_0^{2^0}, \\ldots, z_0^{2^{m-k - 1}}).\n\nOut-of-domain answers. Prover sends y_0 \\in \\mathbb{F}. In the honest case, y_0 := \\hat{g}(\\boldsymbol{z}_0).\n\nShift queries and combination randomness. For Verifier, for each i \\in [t], select random numbers z_i \\leftarrow \\mathcal{L}^{(2^k)} and send, obtain y_i := \\mathrm{Fold}(f, \\boldsymbol{\\alpha})(z_i) by querying f. Let \\boldsymbol{z}_i := (z_i^{2^0}, \\ldots, z_i^{2^{m- k - 1}}). Verifier also selects a random number \\gamma \\leftarrow \\mathbb{F} and sends.\n\nRecursive claim. Prover and Verifier define new weight polynomial and target value:\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\\sigma' := \\hat{h}_k(\\alpha_k) + \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i,\n\nThen, recursively test g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'].\n\nFirst, let’s explain that the constraint in g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'] is correct, i.e., prove\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}'(g(\\boldsymbol{b}), \\boldsymbol{b}) = \\sigma'\n\nSubstituting \\hat{w}' and \\sigma', we get\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) + \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = \\hat{h}_k(\\alpha_k) + \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\nWe prove this in two parts:\n\nProve\n\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) = \\hat{h}_k(\\alpha_k)\n\nFrom step 2 of the protocol, we know g(\\boldsymbol{b}) = \\hat{f}(\\boldsymbol{\\alpha}, \\boldsymbol{b}), therefore\\begin{aligned}\n    \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) & = \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(\\hat{f}(\\boldsymbol{\\alpha}, \\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) \\\\\n    & = \\hat{h}_k(\\alpha_k)\n\\end{aligned}\n\nThe last equation is obtained from the final claim of the sumcheck in step 1 of the protocol.\n\nProve\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\nProof:\\begin{aligned}\n    \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) & =  \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} g(\\boldsymbol{b}) \\cdot  \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) \\\\\n    & = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot g(\\boldsymbol{z}_i) \\\\\n    & = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\\end{aligned}\n\nWhere \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} g(\\boldsymbol{b}) \\cdot  \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = g(\\boldsymbol{z}_i) is precisely what we mentioned earlier about using the weight polynomial \\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}) to constrain the value of a multivariate linear polynomial at a certain point.\n\nThis also explains that the constraint definition in g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'] is correct.\n\nThe definition of the new weight polynomial \\hat{w}' is\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\n\nIt consists of two parts:\n\nThe first part \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) constrains the correctness of k rounds of sumcheck in step 1 of the protocol.\n\nThe second part Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X}) constrains that the values of g at \\boldsymbol{z}_i are correct, and uses random number \\gamma to linearly combine these t + 1 constraints.\na. The constraint g(\\boldsymbol{z}_0) = y_0 is actually verifying the correctness of out-of-domain answers.\nb. For i \\in [t], the constraint g(\\boldsymbol{z}_i) = y_i is requiring the correctness of shift queries.\n\nThis also shows the flexibility of the weight polynomial definition, which can implement multiple constraints at once.","type":"content","url":"/fri/whir#one-iteration-of-whir","position":7},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and BaseFold","lvl2":"One Iteration of WHIR"},"type":"lvl3","url":"/fri/whir#connection-between-whir-and-basefold","position":8},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and BaseFold","lvl2":"One Iteration of WHIR"},"content":"WHIR adopts the idea of BaseFold, and the definition of CRS itself introduces a constraint similar to sumcheck. In step 1 of the protocol, it first performs k rounds of sumcheck, where the random numbers \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k) selected for sumcheck are completely consistent with the random numbers used for folding \\hat{f} later, i.e., in step 2 of the protocol \\hat{g} = \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), where \\hat{f} is folded 2^k times.","type":"content","url":"/fri/whir#connection-between-whir-and-basefold","position":9},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and STIR","lvl2":"One Iteration of WHIR"},"type":"lvl3","url":"/fri/whir#connection-between-whir-and-stir","position":10},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and STIR","lvl2":"One Iteration of WHIR"},"content":"After using sumcheck in step 1 of the protocol, the subsequent steps 2-5 are similar to the STIR protocol. The following figure shows one iteration of the STIR protocol.\n\nThe core idea of the STIR protocol is to reduce the rate in each iteration. Specifically, in the next iteration, the folded polynomial \\hat{g} is not evaluated on \\mathcal{L}^{(2^k)}, but instead chooses to evaluate on a domain \\mathcal{L}^{(2)} that is only half the size of the original domain \\mathcal{L}. This corresponds to step 2 of the WHIR protocol. The benefit of doing this is that it greatly increases the redundancy of the sent messages, reducing the query complexity of the verifier.\n\nFor f \\in \\mathcal{C} := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma], its rate is \\rho = \\frac{2^m}{|\\mathcal{L}|}, and after one WHIR iteration f' \\in \\mathcal{C}' := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'], its rate is\\rho' = \\frac{2^{m - k}}{|\\mathcal{L}^{(2)}|} = \\frac{2^{m - k}}{\\frac{|\\mathcal{L}|}{2}} = \\frac{2^{m - k + 1}}{|\\mathcal{L}|} = 2^{1 - k} \\cdot \\rho = \\left(\\frac{1}{2}\\right)^{k - 1} \\cdot \\rho\n\nWhen k \\ge 2, we can see that \\rho' will be smaller than \\rho, the rate decreases.","type":"content","url":"/fri/whir#connection-between-whir-and-stir","position":11},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Mutual correlated agreement"},"type":"lvl2","url":"/fri/whir#mutual-correlated-agreement","position":12},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Mutual correlated agreement"},"content":"The correlated agreement theorem given in the [BCIKS20] paper is a key theorem for proving the security of FRI and STIR protocols, which ensures that the process of folding the original function with random numbers in the FRI protocol or STIR protocol is secure. In the security analysis of WHIR, a new concept of mutual correlated agreement is introduced, which has a stronger conclusion than correlated agreement.\n\n[ACFYb, 1.2 Mutual correlated agreement] gives the related definitions of correlated agreement and mutual correlated agreement. A code \\mathcal{C} := \\text{RS}[\\mathbb{F}, \\mathcal{L}, m] has (\\delta, \\varepsilon)-correlated agreement means: for every f_1, \\ldots, f_\\ell, under the uniform selection of \\alpha \\leftarrow \\mathbb{F}, with probability 1 - \\varepsilon: if there exists a set S \\subseteq \\mathcal{L}, where |S| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|, f^*_\\alpha := \\sum_{i=1}^\\ell \\alpha^{i-1} \\cdot f_i is consistent with \\mathcal{C} on S, then there exists a set T \\subseteq \\mathcal{L}, where |T| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|, each f_i is consistent with \\mathcal{C} on T.\n\nIn the above definition, describing a function f as “consistent” with a code \\mathcal{C} on a set S means that there exists a codeword u \\in \\mathcal{C} in the encoding space such that for any x \\in S, f(x) = u(x).\n\nThe definition of correlated agreement is shown in the following figure (refer to the video \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification).\n\nThe [BCIKS20] paper shows that a Reed-Solomon code with rate \\rho has (\\delta, \\varepsilon)-correlated agreement, where \\delta \\in (0, 1 - \\sqrt{\\rho}), \\varepsilon := \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}. In other words, if \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{\\alpha \\in \\mathbb{F}} \\left[ \\Delta(f^*_\\alpha, \\mathcal{C}) \\le \\delta \\right] > \\varepsilon = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}\n\nthen, there exist sets T \\subseteq \\mathcal{L}, and codes c_0, \\ldots, c_l \\in \\mathcal{C} such that\n\n|T| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|\n\nEach f_i is consistent with c_i on T\n\nIt can be found that the definition of (\\delta, \\varepsilon)-correlated agreement does not require the sets S and T to be the same set, while in WHIR, a concept stronger than correlated agreement is introduced, called mutual correlated agreement, which requires the sets S and T to be the same set. As shown in the following figure (refer to the video \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification):\n\nThe WHIR paper gives the following conjecture about mutual correlated agreement.\n\nConjecture 1 [ACFY24b, Conjecture 1] (informal). For every Reed-Solomon code \\mathcal{C} = \\text{RS}[\\mathbb{F}, \\mathcal{L}, m], if it has (\\delta, \\varepsilon)-correlated agreement, where \\varepsilon = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}, then it has (\\delta, \\varepsilon')-mutual correlated agreement, where \\varepsilon' = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}.\n\nThe WHIR paper proves that in the case of unique decoding, i.e., when \\delta \\in (0, \\frac{1 - \\rho}{2}), Conjecture 1 holds with \\varepsilon' = \\varepsilon. This also connects correlated agreement with mutual correlated agreement.","type":"content","url":"/fri/whir#mutual-correlated-agreement","position":13},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Summary"},"type":"lvl2","url":"/fri/whir#summary","position":14},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Summary"},"content":"The WHIR protocol combines the ideas of BaseFold and STIR. First, for univariate polynomials in RS encoding, they can be viewed as equivalent multivariate linear polynomials through variable substitution, and the folding of univariate polynomials is also equivalent to folding the corresponding multivariate linear polynomials. This allows WHIR to support both univariate polynomials and multivariate linear polynomials.\n\nSecondly, a new CRS encoding definition is given, adding a constraint similar to sumcheck on the basis of RS encoding, which is a constraint similar to sumcheck on the weight polynomial \\hat{w}. The flexibility of the weight polynomial definition allows multiple constraints to be required at once in the protocol, including constraining the correctness of sumcheck, the correctness of out-of-domain answers, and the correctness of shift queries.\n\nThen, by delving into one iteration of the WHIR protocol, we can see its connection with BaseFold and STIR protocols. The key here is still to build a bridge between univariate polynomials and multivariate linear polynomials, allowing free switching between the univariate function f and the multivariate linear polynomial \\hat{f}. Through the introduction of CRS, the goal of the protocol is increased to verify the correctness of a constraint similar to sumcheck,\\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma\n\nTherefore, combining the idea of BaseFold, first perform k rounds of sumcheck, replacing k variables in the multivariate linear polynomial \\hat{f} with the random numbers \\boldsymbol{\\alpha} from the sumcheck protocol. The folding of \\hat{f} still uses the same random numbers \\boldsymbol{\\alpha}. Combining the idea of STIR, to reduce the rate, the folded function is evaluated on a larger domain \\mathcal{L}^{(2)}. The subsequent steps of out-of-domain sample and shift queries in the WHIR protocol are similar to the STIR protocol.\n\nFinally, we introduced the mutual correlated agreement conclusion used in the security proof of the WHIR protocol, which is stronger than the correlated agreement conclusion.","type":"content","url":"/fri/whir#summary","position":15},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"References"},"type":"lvl2","url":"/fri/whir#references","position":16},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"References"},"content":"[ACFY24a] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[ACFY24b] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[ZCF24] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\nBlog: \n\nWHIR: Reed–Solomon Proximity Testing with Super-Fast Verification\n\nvideo: \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","type":"content","url":"/fri/whir#references","position":17},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)"},"type":"lvl1","url":"/gemini/gemini-pcs-1","position":0},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)"},"content":"Tianyu ZHENG \n\ntian​-yu​.zheng@connect​.polyu​.hk\n\nGemini [BCH+22] is an elastic SNARK, where “elastic” means that the prover can balance between proof time and memory by setting parameters to meet the requirements of different usage scenarios.\n\nAs the core algorithm of Gemini, Tensor Product Check provides us with a method to prove the evaluation of multilinear polynomials, such as \\tilde{f}(\\vec{\\rho}) = u. In other words, this method realizes the conversion from multivariate polynomials to univariate polynomials, thus inspiring us to construct a new multivariate polynomial commitment scheme.\n\nIn terms of specific construction, Tensor Product Check adopts the split-and-fold idea similar to previous work (Sumcheck, Bulletproofs, FRI), achieving relatively efficient communication and verifier complexity, while its prover algorithm can achieve elastic properties.","type":"content","url":"/gemini/gemini-pcs-1","position":1},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"MLE and Tensor Product"},"type":"lvl2","url":"/gemini/gemini-pcs-1#mle-and-tensor-product","position":2},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"MLE and Tensor Product"},"content":"In the Zeromorph notes, we mentioned that a Multilinear Extension uniquely corresponds to a function mapping from Boolean vectors to finite fields, in the form of f: \\{0,1\\}^n \\rightarrow \\mathbb{F}_q. The figure below is an example of a three-dimensional MLE polynomial \\tilde{f}(X_0,X_1,X_2), which can be uniquely represented by the “point-value vector” (a_0, a_1,...,a_7).\n\nSimilarly, an MLE polynomial can also be represented using a “coefficient form”. For example, the above figure can be written as\\tilde{f}(X_0,X_1,X_2) = f_0+f_1X_0+f_2X_1+f_3X_2+f_4X_0X_1+f_5X_0X_2 + f_6X_1X_2 + f_7X_0X_1X_2\n\nThe ordering of monomials in this expression is based on Lexicographic Order.\n\nIn addition to the “point-value form” and “coefficient form”, we will now introduce a new form of expression - the expression based on “tensor product”.\n\nSimply put, tensor product is a special “multiplication” between two vectors, denoted as \\vec{a} \\otimes \\vec{b}. Specifically, we can first calculate a b^T (assuming \\vec{a}, \\vec{b} are both column vectors), then concatenate the resulting matrix into a vector column by column, and this vector is the result of the tensor product. For example, for \\vec{a}=(a_1,a_2) and \\vec{b}=(b_1, b_2, b_3):\\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix} \\cdot \\begin{bmatrix}b_1, b_2,b_3\\end{bmatrix} = \\begin{bmatrix}a_1b_1, a_1b_2, a_1b_3 \\\\ a_2b_1, a_2b_2, a_2b_3\\end{bmatrix}\n\nWe get \\vec{a} \\otimes \\vec{b} = (a_1b_1, a_2b_1, a_1b_2, a_2b_2, a_1b_3, a_2b_3).\n\nComparing with the MLE polynomial expressed in “coefficient form” that we mentioned earlier, we will find that all its monomials can be obtained by a continuous tensor product:(1,X_0)\\otimes(1,X_1)\\otimes(1,X_2) = (1, X_0, X_1, X_0X_1, X_2, X_0X_2, X_1X_2, X_0X_1X_2)\n\nWe abbreviate the left-hand side as \\otimes_{j=0}^2 (1,X_j). Then an MLE polynomial can be written in inner product form:\\tilde{f}(X_0,X_1,X_2) = \\langle \\vec{f}, \\otimes_{j=0}^2 (1,X_j) \\rangle\n\nwhere the left element is the coefficient vector \\vec{f}, and the right element is a monomial vector \\otimes_{j=0}^2 (1,X_j).","type":"content","url":"/gemini/gemini-pcs-1#mle-and-tensor-product","position":3},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Split-and-Fold Method"},"type":"lvl2","url":"/gemini/gemini-pcs-1#split-and-fold-method","position":4},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Split-and-Fold Method"},"content":"In Gemini, the authors present a protocol for checking the correctness of tensor products based on a univariate polynomial commitment scheme (such as KZG10). Based on this protocol, we can further construct a conversion from multivariate to univariate polynomials. We will first use the three-dimensional MLE polynomial mentioned earlier as an example to explain the main idea of Tensor Product Check.\n\nSuppose the prover wants to prove the instance: \\vec{f} = (f_0,...,f_7), satisfying the relation \\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle = u, where \\rho_0,\\rho_1, \\rho_2 are in the finite field F.\n\nFor convenience, we rewrite the subscripts of elements in vector \\vec{f} into binary representation in little-endian order, i.e.,f_i = f_{i_0i_1i_2}, i = \\langle (2^0, 2^1, 2^2), (i_0,i_1,i_2) \\rangle\n\nwhere i_0,i_1,i_2 \\in \\{0,1\\}.\n\nAfter expanding the renumbered tensor product, we get the following equation\\begin{matrix} &&\\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle & \\\\ & = & f_{000}\\rho_0^{0}\\rho_1^{0}\\rho_2^{0}& + &f_{100}\\rho_0^{1}\\rho_1^{0}\\rho_2^{0}& + &f_{010}\\rho_0^{0}\\rho_1^{1}\\rho_2^{0}& + &f_{110}\\rho_0^{1}\\rho_1^{1}\\rho_2^{0} \\\\ & + & f_{001}\\rho_0^{0}\\rho_1^{0}\\rho_2^{1}& + &f_{101}\\rho_0^{1}\\rho_1^{0}\\rho_2^{1}& + &f_{011}\\rho_0^{0}\\rho_1^{1}\\rho_2^{1}& + &f_{111}\\rho_0^{1}\\rho_1^{1}\\rho_2^{1} \\end{matrix}\n\nWe will find that the subscripts of each coefficient f_{i_0i_1i_2} correspond one-to-one with the exponents of the multiplied \\rho_0,\\rho_1,\\rho_2, i.e.,f_{i_0i_1i_2} \\cdot \\rho_0^{i_0}\\rho_1^{i_1} \\rho_2^{i_2}, \\text{ for all } i_0,i_1,i_2 \\in \\{ 0,1 \\}\n\nTherefore, we can always divide \\vec{f} into two equal-length parts according to the exponent i_j of \\rho_j, and the two parts satisfy a tensor product subproblem respectively. For example, after dividing \\vec{f} according to \\rho_0, we can obtain two tensor product relations about \\vec{f}_1, \\vec{f}_2:\\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle =  \\langle\\vec{f}_1, \\otimes_{j=1}^{2}(1,\\rho_j) \\rangle + \\rho_0 \\langle\\vec{f}_2, \\otimes_{j=1}^{2}(1,\\rho_j) \\rangle\n\nNote that in these two subproblems, the right elements of the inner product are the same: both are \\otimes_{j=1}^2 (1,\\rho_j), so they can be further combined into one \\langle\\vec{f}_1 + \\rho_0 \\vec{f}_2, \\otimes_{j=1}^{2}(1,\\rho_j) \\rangle.\n\nIt can be seen that for a vector \\vec{f} of length N, we divide it into two vectors of length N/2, and then combine them into one vector. Through this round of operation, we turn a tensor product problem of size N into a problem of size N/2.\n\nBy analogy, this problem can eventually be reduced to size 1.\n\n[Multivariate Polynomial Split-and-fold]\n\nAs mentioned earlier, we can view a tensor product relation as a multivariate polynomial evaluation relation, i.e.,\\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle = u \\quad \\Leftrightarrow \\quad \\tilde{f}(\\rho_0,\\rho_1,\\rho_2) = u\n\nFor the multivariate polynomial \\tilde{f}^{(0)} = \\tilde{f}, its split-and-fold process in the j-th round (j\\in[1,3]) is as follows:\n\nsplit: The prover divides the multivariate polynomial \\tilde{f}^{(j-1)} into two parts: the first part contains any monomial that includes X_j of order 0 (denoted as \\tilde{f}_e^{(j-1)}), the second part contains any monomial that includes X_{j-1} of order 1 (denoted as X_{j-1} \\cdot \\tilde{f}_o^{(j-1)}), satisfying\\tilde{f}^{(j-1)} = \\tilde{f}_{e}^{(j-1)} + X_{j-1} \\cdot \\tilde{f}_{o}^{(j-1)}\n\nfold: The prover linearly combines the two separated polynomials \\tilde{f}_e^{(j-1)}, \\tilde{f}_o^{(j-1)}, using \\rho_{j-1} as the weight for combination, resulting in a new multivariate polynomial denoted as \\tilde{f}^{(j)}(X) = \\tilde{f}_e^{(j-1)}(X) + \\rho_{j-1} \\cdot \\tilde{f}_o^{(j-1)}(X).\n\nThe figure below shows the calculation process when j=1:","type":"content","url":"/gemini/gemini-pcs-1#split-and-fold-method","position":5},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Tensor Product Check Protocol"},"type":"lvl2","url":"/gemini/gemini-pcs-1#tensor-product-check-protocol","position":6},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Tensor Product Check Protocol"},"content":"Through the above recursive algorithm, we reduce the correctness check of a tensor product relation of length N to checking the correctness of n = \\left \\lceil \\log N \\right \\rceil split-and-fold processes.\n\nIn fact, this divide-and-conquer problem-solving idea (split-and-fold) has appeared in many previous protocols, such as Sumcheck, Bulletproofs, and FRI. The difference is that Gemini provides a protocol based on KZG10 to prove the split-and-fold process, which requires n= \\log(|\\vec{f}|) interactions.\n\nWe present the PIOP protocol for proving tensor product relations as follows:\n\n[Tensor-product Check Protocol]\n\nTarget relation: \\langle\\vec{f}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle = u\n\nProver input: Public parameters, instance x = (\\rho_0,...,\\rho_{n-1}, u), secret w = \\vec{f}\n\nVerifier input: Public parameters, instance x = (\\rho_0,...,\\rho_{n-1}, u)\n\nThe prover constructs a univariate polynomial f^{(0)}(X) = f(X).\n\nFor j \\in 1,...,n, the prover calculatesf^{(j)}(X) = f_e^{(j-1)}(X) + \\rho_{j-1} \\cdot f_o^{(j-1)}(X)\n\nwhere f_e^{(j-1)}, f_o^{(j-1)} are polynomials composed of even-order terms and odd-order terms of f^{(j-1)} respectively, satisfying f^{(j-1)}(X) = f_e^{(j-1)}(X^2) + X \\cdot f_o^{(j-1)}(X^2).\n\nThe prover sends Oracles of f^{(0)},f^{(1)},...,f^{(n-1)} to the verifier.\n\nThe verifier randomly selects a challenge value \\beta \\leftarrow \\mathbb{F} and makes the following queries to the Oracles:e^{(j-1)}:= f^{(j-1)}(\\beta), \\bar{e}^{(j-1)} := f^{(j-1)}(-\\beta), \\hat{e}^{(j-1)} := f^{(j)}(\\beta^2)\n\nwhere j=1,...,n, when j=n, ignore the query f^{(n)}(\\beta^2), and directly set \\hat{e}^{(n-1)} := u.\n\nFor j = 0,...,n-1, the verifier checks\\hat{e}^{(j)} = \\frac{e^{(j)} + \\bar{e}^{(j)}}{2} + \\rho_j \\cdot \\frac{e^{(j)}-\\bar{e}^{(j)}}{2\\beta}\n\nIn each round, the prover will generate Oracles for the polynomials before split and after fold respectively. Specifically, a split-and-fold relation can be written as:\n\nGiven f(X), f_e(X), f_o(X),f'(X), weight \\rho, they satisfy the following relations> f(X)=f_e(X^2)+X\\cdot f_o(X^2) \\qquad \\% \\ \\text{split} \\\\ f'(X) = f_e(X)+\\rho \\cdot f_o(X)\\qquad \\% \\ \\text{fold}\n>\n\nSince even and odd polynomials satisfy (1) f_e(X^2)=(f(X)+f(-X))/2, (2) f_o(X^2)=(f(X)-f(-X))/2X respectively, we can further write the above two equations into one, i.e.,f'(X^2)=\\frac{f(X)+f(-X)}{2} + \\rho \\cdot \\frac{f(X)-f(-X)}{2X}\n\nTo check if this equation holds, the verifier only needs to randomly select a challenge value \\beta in the finite field F, and check if the values of f,f' satisfy the relation when X=\\beta.","type":"content","url":"/gemini/gemini-pcs-1#tensor-product-check-protocol","position":7},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Multivariate to Univariate Conversion"},"type":"lvl2","url":"/gemini/gemini-pcs-1#multivariate-to-univariate-conversion","position":8},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Multivariate to Univariate Conversion"},"content":"Before introducing the protocol for multivariate to univariate conversion, let’s delve deeper into some principles hidden in the tensor product protocol. Although the goal of the tensor product protocol is to prove the value of a multivariate polynomial, apart from inputting the coefficient vector of the multivariate polynomial, all polynomials involved in the protocol are univariate.\n\nLet’s write out the Split-and-fold process using univariate polynomials:\n\n[Univariate Polynomial Split-and-fold] In the j-th round:\n\nsplit: The prover divides the univariate polynomial f^{(j-1)} into two parts: the first part contains any monomial that includes X of even order (denoted as f_e^{(j-1)}), the second part contains any monomial that includes X of odd order (denoted as X \\cdot f_o^{(j-1)}), satisfyingf^{(j-1)}(X) = f_e^{(j-1)}(X^2) + X \\cdot f_o^{(j-1)}(X^2)\n\nfold: The prover linearly combines the two separated polynomials f_e^{(j-1)}, f_o^{(j-1)}, using \\rho_{j-1} as the weight for combination, resulting in a new univariate polynomial denoted as f^{(j)}(X) = f_e^{(j-1)}(X) + \\rho_{j-1} \\cdot f_o^{(j-1)}(X). [Note] Here we need to introduce an additional mapping X^2 \\mapsto X to obtain f_e^{(j-1)}(X), f_o^{(j-1)}(X).\n\nTherefore, the tensor product protocol can be seen as the prover simultaneously executing a recursive algorithm on the univariate polynomial f to simulate the calculation process of \\tilde{f}.\n\nWe use a three-dimensional polynomial as an example to describe the calculation process in the j=1 round:\n\nThe split-and-fold calculation of univariate polynomials is as follows:\n\nCompared with the split-and-fold of multivariate polynomials, the variable X in the first round of univariate polynomials corresponds to X_0 in the multivariate, and in the second round, X^2 corresponds to X_1. In fact, these two processes are calculations on the coefficient vector \\vec{f} in different “bases”.\n\nWhen we need to calculate the value of a multivariate polynomial on the basis \\{X_0,X_1,X_2\\}, we only need to perform the same operation synchronously on \\{X^1,X^2,X^4\\} (i.e., on univariate polynomials) to simulate the evaluation process of multivariate polynomials using univariate polynomials.\n\nMore formally, we get a mapping relation from the multivariate basis vector space to the univariate basis vector space:\\iota: \\{ X_0,X_1,X_2 \\} \\rightarrow \\{ X^1, X^2, X^4\\}\n\nTherefore, we say that the tensor product protocol provides us with a proof method from multivariate to univariate, i.e., multi-to-uni IOP. As shown in the figure below, ideally, we hope that the prover can directly generate an Oracle of a multivariate polynomial and send it to the verifier. However, in engineering, there is a lack of efficient multivariate polynomial commitment schemes, so the prover can only construct a proof protocol for Tensor Product Check (i.e., Multi-Uni-IOP) to simulate the calculation process of multivariate polynomial evaluation on univariate polynomials.\n\nThe prover needs to send Oracles of n univariate polynomials, allowing the verifier to make queries and checks respectively. Since these checks are independent of each other, the verifier can make queries on all Oracles at a certain point \\beta at once, without needing O(n) points.","type":"content","url":"/gemini/gemini-pcs-1#multivariate-to-univariate-conversion","position":9},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Implementation Based on KZG"},"type":"lvl2","url":"/gemini/gemini-pcs-1#implementation-based-on-kzg","position":10},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"Implementation Based on KZG"},"content":"For the IOP protocol given above, we can deploy a univariate polynomial commitment scheme (KZG10) to compile it into an AoK (Argument of Knowledge). KZG10 can support the evaluation proof of a polynomial at a certain point, with the advantage of constant-sized proofs and support for batch proofs. The disadvantage is that it requires trusted initialization, and the proof complexity is relatively high (requiring FFT computation).\n\n[Note] Since we compile IOP into Argument of Knowledge, KZG10 needs to satisfy extractability, the proof of which is given in Marlin [CHM+19].\n\nLet’s briefly review the proof principle of KZG10: Given public parameters: \\mathbb{G}_1, \\mathbb{G}_2, \\mathbb{G}_T, G, H, e. In the initialization phase, randomly select \\tau \\in \\mathbb{}F and calculate \\tau H \\in \\mathbb{G}_2 and vector(G,\\tau G,\\ldots \\tau^{D-1}G,\\tau^{D}G)\\in \\mathbb{G}_1^{D+1}\n\nWe use bracket notation [a]_1 to represent scalar multiplication a \\cdot G on an elliptic curve group element. The KZG proof process is as follows:\n\nThe prover calculates the commitment of the univariate polynomial f(X) of degree d: [f(\\tau)]_1 = \\sum_{j=0}^d f_j \\cdot \\tau^j G.\n\nThe prover publicly announces the value of the polynomial at point \\rho as f(\\rho)=u, and calculates the quotient polynomialq(X) = \\frac{f(X)-f(\\rho)}{X-\\rho}\n\nGenerate evaluation proof [q(\\tau)]_1.\n\nThe verifier checks the evaluation proof e([f(\\tau)]_1-[u]_1, [1]_2) = e([q(\\tau)]_1, [\\tau-\\rho]_2).\n\nTherefore, to compile the IOP protocol, we only need to commit to the polynomials f^{(1)},...,f^{(n-1)} produced in the protocol respectively, and open them at specified points \\beta, -\\beta, \\beta^2. However, there are two points that still need our attention: (1) The degrees of these polynomials are not the same. To prevent the prover from cheating using polynomials that do not satisfy the degree requirements, we need to use the method in Marlin, Zeromorph [CHM+19, KT23] to limit the Degree Bound of polynomials. (2) Most polynomials need to be opened at three points \\beta, -\\beta, \\beta^2. If we generate evaluation proofs for each point separately, it will increase the proof size and verification complexity. Multi-point evaluation proof techniques can be used to optimize this problem.\n\nDegree Bound Proof: To prove deg(f)\\leq d\n\nThe prover provides [f(\\tau)]_1 and additionally sends [\\tau^{D-d}\\cdot f(\\tau)]_1 to the verifier\n\nThe verifier checks the equation e([f(\\tau)]_1, [\\tau^{D-d}]_2) = e([\\tau^{D-d}\\cdot f(\\tau)]_1, [1]_2)\n\nMulti-point Evaluation Proof: To prove that f(X) is opened as u_1,u_2,u_3 at \\beta_1, \\beta_2, \\beta_3\n\nThe prover randomly generates a polynomial g(X) of the same degree as f(X), which needs to pass through points (\\beta, u_1),(-\\beta, u_2),(\\beta^2, u_3)\n\nThe prover provides [f(\\tau)]_1 and the evaluation proof [q(\\tau)]_1 = \\frac{f(\\tau)-g(\\tau)}{(\\tau-\\beta_1)(\\tau-\\beta_2)(\\tau-\\beta_3)}\n\nThe verifier checks the equation e([f(\\tau)]_1 - [g(\\tau)]_1, [1]_2) = e([q(\\tau)]_1, [(\\tau-\\beta_1)(\\tau-\\beta_2)(\\tau-\\beta_3)]_2)\n\n[Note] Both of the above techniques require additional generation of (H,\\tau H,\\ldots \\tau^{D-1}H,\\tau^{D}H)\\in \\mathbb{G}_2^{D+1} in the Setup phase.\n\n[Protocol Description]\n\nBelow we first give the Multi-to-Uni AoK scheme compiled based on KZG:\n\nInstance\n\nUnivariate polynomial commitment [f(\\tau)]_1 of vector \\vec{f}, length N\n\nEvaluation point vector \\vec{\\rho}\n\nEvaluation result \\tilde{f}(\\rho) = u\n\nWitness\n\nCoefficient vector \\vec{f} of the multivariate polynomial\n\nInteraction Process\n\nThe prover generates polynomials f^{(1)},...,f^{(n-1)} and calculates and sends their commitments [f^{(1)}(\\tau)]_1,\\ldots,[f^{(n-1)}(\\tau)]_1\n\nThe prover calculates and sends degree bound proofs of polynomials f^{(0)},...,f^{(n-1)}: [\\tau^{D-N\\cdot 2^{-j} + 1}\\cdot f^{(j)}(\\tau)]_1, j = 0,\\ldots n-1\n\nThe verifier randomly selects point \\beta and sends it to the prover\n\nThe prover calculates the evaluation proof for each polynomial, where\n\n[q^{(0)}(\\tau)]_1 = \\frac{f^{(0)}(\\tau)-g^{(0)}(\\tau)}{(\\tau-\\beta)(\\tau+\\beta)}                    % f^{(0)}(\\beta), f^{(0)}(-\\beta)\n\n[q^{(j)}(\\tau)]_1 = \\frac{f^{(j)}(\\tau)-g^{(j)}(\\tau)}{(\\tau-\\beta)(\\tau+\\beta)(\\tau-\\beta^2)}              % f^{(j)}(\\beta), f^{(j)}(-\\beta), f^{(j)}(\\beta^2), j=1,...,n-1\n\nThe verifier checks:\n\nThe correctness of degree bound proofs [\\tau^{D-N\\cdot 2^{-j} + 1}\\cdot f^{(j)}(\\tau)]_1, j = 0,\\ldots n-1 for f^{(0)},...,f^{(n-1)}\n\nThe correctness of multi-point evaluation proofs [q^{(0)}(\\tau)]_1,\\ldots [q^{(n-1)}(\\tau)]_1 for f^{(0)},...,f^{(n-1)}\n\nThe correctness of split-and-fold relations, i.e., for j = 0,...,n-1, whether the following equation holds:\\hat{e}^{(j)} = \\frac{e^{(j)} + \\bar{e}^{(j)}}{2} + \\rho_j \\cdot \\frac{e^{(j)}-\\bar{e}^{(j)}}{2\\beta}\n\n[Performance Analysis]\n\nProof size: 3\\log N \\ \\mathbb{G}_1 elements\n\nVerifier computation: 4 \\log N \\ \\mathsf{Pairing},\\ O(\\log N) \\ \\mathsf{EccMul}^{\\mathbb{G}_1 \\text{ or } \\mathbb{G}_2}","type":"content","url":"/gemini/gemini-pcs-1#implementation-based-on-kzg","position":11},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"References"},"type":"lvl2","url":"/gemini/gemini-pcs-1#references","position":12},{"hierarchy":{"lvl1":"Gemini-PCS (Part I)","lvl2":"References"},"content":"[BCH+22] Bootle, Jonathan, Alessandro Chiesa, Yuncong Hu, **et al. “Gemini: Elastic SNARKs for Diverse Environments.” Cryptology ePrint Archive (2022). \n\nhttps://​eprint​.iacr​.org​/2022​/420\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[CHM+19] Chiesa, Alessandro, Yuncong Hu, Mary Maller, et al. “Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/1047","type":"content","url":"/gemini/gemini-pcs-1#references","position":13},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)"},"type":"lvl1","url":"/gemini/gemini-pcs-2","position":0},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)"},"content":"Tianyu ZHENG \n\ntian​-yu​.zheng@connect​.polyu​.hk\n\nIn the first part, we introduced the Tensor product check protocol in Gemini [BCH+22] for implementing multivariate polynomial evaluation proofs, and briefly explained how to apply it to practical proof systems to convert multivariate polynomials to univariate polynomials. In this part, we mainly focus on the security of the Tensor product check protocol and propose some optimizations based on Gemini.","type":"content","url":"/gemini/gemini-pcs-2","position":1},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Review"},"type":"lvl2","url":"/gemini/gemini-pcs-2#review","position":2},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Review"},"content":"For ease of reading, let’s first review the process of the tensor product check protocol:\n\n[Tensor-product Check Protocol]\n\nTarget relation: \\langle\\vec{f}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle = u\n\nProver input: Public parameters, instance x = (\\rho_0,...,\\rho_{n-1}, u), secret w = \\vec{f}\n\nVerifier input: Public parameters, instance x = (\\rho_0,...,\\rho_{n-1}, u)\n\nThe prover constructs a univariate polynomial f^{(0)}(X) = f(X).\n\nFor j \\in 1,...,n, the prover computesf^{(j)}(X) = f_e^{(j-1)}(X) + \\rho_{j-1} \\cdot f_o^{(j-1)}(X)\n\nwhere f_e^{(j-1)}, f_o^{(j-1)} are polynomials composed of even and odd order terms of f^{(j-1)} respectively, satisfying f^{(j-1)}(X) = f_e^{(j-1)}(X^2) + X \\cdot f_o^{(j-1)}(X^2).\n\nThe prover sends Oracles of f^{(0)},f^{(1)},...,f^{(n-1)} to the verifier.\n\nThe verifier randomly selects a challenge value \\beta \\leftarrow \\mathbb{F} and makes the following queries to the Oracles:e^{(j-1)}:= f^{(j-1)}(\\beta), \\bar{e}^{(j-1)} := f^{(j-1)}(-\\beta), \\hat{e}^{(j-1)} := f^{(j)}(\\beta^2)\n\nwhere j=1,...,n, when j=n, ignore the query f^{(n)}(\\beta^2), and directly set \\hat{e}^{(n-1)} := u.\n\nFor j = 0,...,n-1, the verifier checks\\hat{e}^{(j)} = \\frac{e^{(j)} + \\bar{e}^{(j)}}{2} + \\rho_j \\cdot \\frac{e^{(j)}-\\bar{e}^{(j)}}{2\\beta}","type":"content","url":"/gemini/gemini-pcs-2#review","position":3},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Security Analysis"},"type":"lvl2","url":"/gemini/gemini-pcs-2#security-analysis","position":4},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Security Analysis"},"content":"[Completeness]\n\nGiven a polynomial coefficient vector satisfying \\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle = u, a prover honestly executing the above protocol will certainly pass the verification. According to our discussion in the [Split-and-fold Check Protocol], when the prover correctly performs the split to obtain f_e^{(j-1)}(X), f_o^{(j-1)}(X), the folded f^{(j)}(X) will definitely pass the verification, and f^{(j)}(X) satisfies the following expressionf^{(j)}(X) = \\sum_{i_0,...,i_{n-1} \\in \\{ 0,1 \\}} f_{i_0\\cdots i_{n-1}} \\cdot \\rho_0^{i_0} \\cdots \\rho_{j-1}^{i_{j-1}} \\cdot X^{\\langle \\vec{i}_{[j:n-1]}, \\vec{2}^{n-1-j}\\rangle}\n\nwhere \\vec{2}^{n-1-j} = (2^0,...,2^{n-1-j}). Obviously, when j=n, f^{(n)}(X) = \\tilde{f}(\\vec{\\rho})= u.\n\n[Soundness]\n\nHere we adopt a different proof method from the original Gemini paper, which is easier to understand. Assume \\langle\\vec{f}, \\otimes_{j=0}^{2}(1,\\rho_j) \\rangle \\neq u, for a malicious prover, if it can output a series of interaction data, including Oracles of polynomials f^{(j)}, j=0,...,n-1, and pass the verification. Then there must be at least one pair of Oracles whose contained polynomials do not satisfy the split-and-fold relationship. That is, there exists a j such thatp_j(X) = f^{(j)}(X^2) - \\frac{f^{(j-1)}(X)+f^{(j-1)}(-X)}{2} - \\rho_{j-1}\\frac{f^{(j-1)}(X)-f^{(j-1)}(-X)}{2X}\n\nis a non-zero polynomial. Note that the highest order of p_j(X) is 2^{n-(j-1)}-1. Let event E_j represent that p_j(X) is a non-zero polynomial and p_j(\\beta) = 0. According to the Schwartz-Zippel lemma, Pr(E_j) \\leq deg(p_j)/|F|. Then, for all p_1(X),...,p_{n}(X), the probability that there exists a non-zero polynomial that happens to be 0 at point \\beta can be bounded by the union boundPr(E_1 \\vee \\cdots \\vee E_n) \\leq Pr(E_1) + \\cdots + Pr(E_n) = \\sum_{j=1}^n \\frac{deg(p_j)}{|F|} = \\frac{2N}{|F|}\n\n[About degree bound]\n\nNote that in the original Gemini paper, the verifier needs to check that the order of each f^{(j)} is strictly less than or equal to 2^{n-j}-1. This point is also reflected in the above proof: assuming that the highest order of p_j(X) is 2^{n-(j-1)}-1.\n\nHowever, after research, we find that the degree check is not necessary: even if a malicious prover is allowed to construct an illegal f^{(j')} in each round, whose order is greater than the legal f^{(j)} but less than or equal to N, we can still get a negligible soundness error (although slightly larger than the original). Specifically, for any E_j, we have Pr(E_j) \\leq N/|F|, so we can get Pr(E_1 \\vee \\cdots \\vee E_n) \\leq N \\log N/|F| (when N < D, it’s D \\log N / |F|).\n\nTherefore, the degree bound check in the Tensor-product check protocol based on KZG10 implemented in the first part can be ignored to reduce \\log N \\mathbb{G}_1 elements in the proof.","type":"content","url":"/gemini/gemini-pcs-2#security-analysis","position":5},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Implementing Zero-Knowledge"},"type":"lvl2","url":"/gemini/gemini-pcs-2#implementing-zero-knowledge","position":6},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"Implementing Zero-Knowledge"},"content":"Gemini did not discuss how to implement the ZK property of the tensor product check. Here we provide two feasible schemes.\n\n[Scheme One]\n\nAdopting a similar idea to implementing zk sumcheck in the paper [CAS17], we can directly add a blinding polynomial g(X) of the same size to the original polynomial f(X). That is, for each non-zero coefficient monomial in f(X), g(X) contains a corresponding monomial with a random coefficient. Let \\langle\\vec{g}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle = v.\n\nNext, the prover only needs to additionally commit to g(X) and send the commitment value cm(g(X)) and v to the verifier. The verifier then randomly selects a challenge c to combine the tensor product relations of f and g asu+c\\cdot v = \\langle\\vec{f}+c\\cdot \\vec{g}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle\n\nThen, the prover and verifier jointly complete the Tensor product check protocol for the above relation. We won’t elaborate further on the specific construction of this scheme.\n\n[Scheme Two]\n\nThe above method is very straightforward, but the disadvantage is that the prover needs to add an additional random polynomial as large as f(X) (length N). Referring to the optimization scheme of zk sumcheck in [CFS17] in Libra [XZZPS19], we can also propose an optimized scheme to implement the zk tensor product protocol, which can significantly reduce the size of the blinding polynomial.\n\nThe idea of this optimization scheme is: since the prover only sends a total of 3n point values in the tensor product check, the blinding polynomial needs to contain at least 3n random coefficients to ensure the zero-knowledge property of the protocol.\n\nSpecifically, let the blinding polynomial be:g(X) = a_{0,1}X + a_{0,2}X^2 + \\sum_{i=1}^{n-1} \\left( a_{i,1} X^{3i} + a_{i,2} X^{3i+1} + a_{i,3} X^{3i+2} \\right) + a_n\n\n[Zero-Knowledge Tensor Product Check Protocol]\n\nTo prove \\langle\\vec{f}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle = u\n\nThe prover constructs a blinding polynomial g(X) and pads its coefficient vector with zeros to length N\n\nThe prover computes and sends the following data to the verifierv = \\langle\\vec{g}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle \\\\ C_g = cm(g(X))\n\nThe verifier randomly selects c \\in F^* and sends it to the prover, then computes u + c\\cdot v, C_f + c\\cdot C_g.\n\nThe prover and verifier run the tensor product check protocol to prove the following relationu+c\\cdot v = \\langle\\vec{f}+c\\cdot \\vec{g}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle\n\nFor convenience, let h(X) = f(X) + c\\cdot g(X). The proof that the above construction satisfies zero-knowledge is as follows:\n\n[Proof]\n\nFirst, the simulator S can be constructed according to the following steps:\n\nS first inputs a random challenge value c \\neq 0\n\nS uniformly randomly generates vector \\vec{h} and computes polynomial h(X), as well as C_h = \\mathsf{Commit}(h(X)), w = \\langle\\vec{h}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle\n\nS computes v = (w-u)/c and commitment C_g = (C_h/C_f)^{1/c}\n\nS runs a tensor product check protocol with V^*, where the proof relation is w = \\langle\\vec{h}, \\otimes_{j=0}^{n-1}(1,\\rho_j) \\rangle.\n\nObviously, the messages in steps 1 and 3 are probabilistically indistinguishable from those sent by an honest prover P.\n\nNext, we only need to show that the tensor product check protocol run by S and V^* in step 4 also satisfies this property. Specifically, because the protocol satisfies soundness, for each oracle h^{(j)}, j=0,...,n-1, it satisfiesh^{(j)}(X^2)=\\frac{h^{(j-1)}(X)+h^{(j-1)}(-X)}{2} + \\rho \\cdot \\frac{h^{(j-1)}(X)-h^{(j-1)}(-X)}{2X}\n\nNote that for h^{(j-1)}(X) on the right side of the equation, its corresponding oracle also satisfies an equation related to h^{(j-2)}(X). Therefore, we can always expand the right expression satisfied by any h^{(j)}(X^2) into a form that only includes h^{(0)}(X), h^{(0)}(-X), h^{(0)}(X^2). Thus, the response obtained by V^* querying the oracle h^{(j)} at any point \\beta must be a linearly independent constraint on \\vec{h}.\n\nIn summary, after performing the tensor product check protocol, V^* will obtain 3\\cdot (n-1), 2 values on h^{(0)}(X), and one value of h^{(n)}(X), i.e., u+c\\cdot v. Because h contains a blinding polynomial of size 3n, the verifier cannot interpolate to obtain all coefficients of the blinding polynomial, so this protocol is indistinguishable from the protocol executed by an honest verifier.","type":"content","url":"/gemini/gemini-pcs-2#implementing-zero-knowledge","position":7},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"References"},"type":"lvl2","url":"/gemini/gemini-pcs-2#references","position":8},{"hierarchy":{"lvl1":"Gemini-PCS (Part II)","lvl2":"References"},"content":"[BCH+22] Bootle, Jonathan, Alessandro Chiesa, Yuncong Hu, **et al. “Gemini: Elastic SNARKs for Diverse Environments.” Cryptology ePrint Archive (2022). \n\nhttps://​eprint​.iacr​.org​/2022​/420\n\n[CFS17**]** Chiesa, Alessandro, Michael A. Forbes, and Nicholas Spooner. “A zero knowledge sumcheck and its applications.” \n\narXiv preprint arXiv:1704.02086 (2017).\n\n[XZZPS19] Xie, T., Zhang, J., Zhang, Y., Papamanthou, C., & Song, D. “Libra: Succinct zero-knowledge proofs with optimal prover computation.” \n\nhttps://​eprint​.iacr​.org​/2019​/317","type":"content","url":"/gemini/gemini-pcs-2#references","position":9},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)"},"type":"lvl1","url":"/gemini/gemini-pcs-3","position":0},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)"},"content":"","type":"content","url":"/gemini/gemini-pcs-3","position":1},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"1. Optimization Approach"},"type":"lvl2","url":"/gemini/gemini-pcs-3#id-1-optimization-approach","position":2},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"1. Optimization Approach"},"content":"","type":"content","url":"/gemini/gemini-pcs-3#id-1-optimization-approach","position":3},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"2. Protocol Description"},"type":"lvl2","url":"/gemini/gemini-pcs-3#id-2-protocol-description","position":4},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"2. Protocol Description"},"content":"The protocol below proves that a MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) evaluated at point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}) has value v = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}). Here, \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) is represented in coefficient form as:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{n-1} f_i\\cdot X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}","type":"content","url":"/gemini/gemini-pcs-3#id-2-protocol-description","position":5},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Common Input","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#common-input","position":6},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Common Input","lvl2":"2. Protocol Description"},"content":"Commitment C_f to vector \\vec{f}=(f_0, f_1, \\ldots, f_{n-1})C_f = \\mathsf{KZG10.Commit}(\\vec{f})\n\nEvaluation point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1})","type":"content","url":"/gemini/gemini-pcs-3#common-input","position":7},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Witness","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#witness","position":8},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Witness","lvl2":"2. Protocol Description"},"content":"Coefficients f_0, f_1, \\ldots, f_{n-1} of polynomial f(X)","type":"content","url":"/gemini/gemini-pcs-3#witness","position":9},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 1.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#round-1","position":10},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 1.","lvl2":"2. Protocol Description"},"content":"Prover computes h_1(X), h_2(X), \\ldots, h_{n-1}(X) such that:h_{i+1}(X^2) = \\frac{h_i(X) + h_i(-X)}{2} + u_i\\cdot \\frac{h_i(X) - h_i(-X)}{2X}\n\nwhere h_0(X) = f(X)\n\nProver computes commitments (H_1, H_2, \\ldots, H_{n-1}) such that:H_{i+1} = \\mathsf{KZG10.Commit}(h_{i+1}(X))\n\nProver sends (H_1, H_2, \\ldots, H_{n-1})","type":"content","url":"/gemini/gemini-pcs-3#round-1","position":11},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 2.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#round-2","position":12},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 2.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random point \\beta\\in\\mathbb{F}_p\n\nProver computes h_0(\\beta), h_1(\\beta), \\ldots, h_{n-1}(\\beta)\n\nProver computes h_0(-\\beta), h_1(-\\beta), \\ldots, h_{n-1}(-\\beta)\n\nProver computes h_0(\\beta^2)\n\nProver sends \\{h_i(\\beta), h_i(-\\beta)\\}_{i=0}^{n-1}, and h_0(\\beta^2)","type":"content","url":"/gemini/gemini-pcs-3#round-2","position":13},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 3.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#round-3","position":14},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 3.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random value \\gamma\\in\\mathbb{F}_p to aggregate multiple polynomials\n\nProver computes h(X)h(X) = h_0(X) + \\gamma\\cdot h_1(X) + \\gamma^2\\cdot h_2(X) + \\cdots + \\gamma^{n-1}\\cdot h_{n-1}(X)\n\nDefines a new Domain D containing 3 elements:D = \\{\\beta, -\\beta, \\beta^2\\}\n\nProver computes a quadratic polynomial h^*(X) such that it matches the values of h(X) on D:h^*(X) = h(\\beta)\\cdot \\frac{(X+\\beta)(X-\\beta^2)}{2\\beta(\\beta-\\beta^2)} + h(-\\beta)\\cdot \\frac{(X-\\beta)(X-\\beta^2)}{2\\beta(\\beta^2+\\beta)} + h(\\beta^2)\\cdot \\frac{X^2-\\beta^2}{\\beta^4-\\beta^2}\n\nProver computes the quotient polynomial q(X)q(X) = \\frac{h(X) - h^*(X)}{(X^2-\\beta^2)(X-\\beta^2)}\n\nProver computes the commitment C_q to q(X)C_q = \\mathsf{KZG10.Commit}(q(X))\n\nProver sends C_q","type":"content","url":"/gemini/gemini-pcs-3#round-3","position":15},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 4.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#round-4","position":16},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Round 4.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random point \\zeta\\in\\mathbb{F}_p\n\nProver computes the linearized polynomial r(X) such that r(\\zeta) = 0:r(X) = h(X) - h^*(\\zeta) - (\\zeta^2-\\beta^2)(\\zeta-\\beta^2)\\cdot q(X)\n\nProver computes the quotient polynomial w(X)w(X) = \\frac{r(X)}{(X-\\zeta)}\n\nProver computes the commitment C_w to w(X):C_w = \\mathsf{KZG10.Commit}(w(X))\n\nProver sends C_w","type":"content","url":"/gemini/gemini-pcs-3#round-4","position":17},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Proof Representation","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#proof-representation","position":18},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Proof Representation","lvl2":"2. Protocol Description"},"content":"As we can see, the proof consists of n+1 elements in \\mathbb{G}_1 and 2n+1 elements in \\mathbb{F}_p:\\pi=\\Big(H_1, H_2, \\ldots, H_{n-1}, C_q, C_w, \\{h_i(\\beta), h_i(-\\beta)\\}_{i=0}^{n-1}, h_0(\\beta^2) \\Big)","type":"content","url":"/gemini/gemini-pcs-3#proof-representation","position":19},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Verification Process","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-3#verification-process","position":20},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl3":"Verification Process","lvl2":"2. Protocol Description"},"content":"Verifier computes (h_1(\\beta^2), h_2(\\beta^2), \\ldots, h_{n-1}(\\beta^2))h_{i+1}(\\beta^2) = \\frac{h_i(\\beta) + h_i(-\\beta)}{2} + u_i\\cdot \\frac{h_i(\\beta) - h_i(-\\beta)}{2\\beta}\n\nVerifier checks if h_{n}(\\beta^2) equals the claimed polynomial evaluation v=\\tilde{f}(\\vec{u})h_n(\\beta^2) \\overset{?}{=} v\n\nVerifier computes the commitment C_h to h(X)C_h = C_f + \\gamma\\cdot H_1 + \\gamma^2\\cdot H_2 + \\cdots + \\gamma^{n-1}\\cdot H_{n-1}\n\nVerifier computes the commitment C_r to r_\\zeta(X):C_r = C_h - h^*(\\zeta)\\cdot[1]_1 - (\\zeta^2-\\beta^2)(\\zeta-\\beta^2)\\cdot C_q\n\nVerifier checks if C_w is a valid evaluation proof for C_r at X=\\zeta:\\mathsf{KZG10.Verify}(C_r, \\zeta, 0, C_w) \\overset{?}{=} 1\n\nOr directly expanded in pairing form:e\\Big(C_r + \\zeta\\cdot C_w, [1]_2\\Big) \\overset{?}{=} e\\Big(C_w, [\\tau]_2 \\Big)","type":"content","url":"/gemini/gemini-pcs-3#verification-process","position":21},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"3. Performance Analysis of Optimizations"},"type":"lvl2","url":"/gemini/gemini-pcs-3#id-3-performance-analysis-of-optimizations","position":22},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"3. Performance Analysis of Optimizations"},"content":"","type":"content","url":"/gemini/gemini-pcs-3#id-3-performance-analysis-of-optimizations","position":23},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"References"},"type":"lvl2","url":"/gemini/gemini-pcs-3#references","position":24},{"hierarchy":{"lvl1":"Gemini-PCS (Part III)","lvl2":"References"},"content":"","type":"content","url":"/gemini/gemini-pcs-3#references","position":25},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)"},"type":"lvl1","url":"/gemini/gemini-pcs-4","position":0},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)"},"content":"This article introduces a different optimized protocol that adopts the approach of selecting points from the Query-phase of the FRI protocol, challenging h_0(X) at X=\\beta, then challenging the folded polynomial h_1(X) at X=\\beta^2, and so on until h_{n-1}(\\beta^{2^{n-1}}). The advantage of this approach is that each opening point of h_i(X) can be reused when verifying the folding of h_{i+1}(X), thereby saving a total of n opening points.","type":"content","url":"/gemini/gemini-pcs-4","position":1},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"1. Optimization Approach"},"type":"lvl2","url":"/gemini/gemini-pcs-4#id-1-optimization-approach","position":2},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"1. Optimization Approach"},"content":"","type":"content","url":"/gemini/gemini-pcs-4#id-1-optimization-approach","position":3},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"2. Protocol Description"},"type":"lvl2","url":"/gemini/gemini-pcs-4#id-2-protocol-description","position":4},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"2. Protocol Description"},"content":"Proof objective: To prove that a multilinear extension (MLE) polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) with n variables has the value v = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) at point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}).\n\nThe MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) is represented in coefficient form as:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{n-1} c_i\\cdot X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}","type":"content","url":"/gemini/gemini-pcs-4#id-2-protocol-description","position":5},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Public Inputs","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#public-inputs","position":6},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Public Inputs","lvl2":"2. Protocol Description"},"content":"Commitment C_f to the vector \\vec{c}=(c_0, c_1, \\ldots, c_{n-1})C_f = \\mathsf{KZG10.Commit}(\\vec{c})\n\nEvaluation point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nv = \\tilde{f}(u_0, u_1, \\ldots, u_{n-1})","type":"content","url":"/gemini/gemini-pcs-4#public-inputs","position":7},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Witness","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#witness","position":8},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Witness","lvl2":"2. Protocol Description"},"content":"Coefficients \\vec{c}=(c_0, c_1, \\ldots, c_{n-1}) of the polynomial f(X)","type":"content","url":"/gemini/gemini-pcs-4#witness","position":9},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 1.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#round-1","position":10},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 1.","lvl2":"2. Protocol Description"},"content":"Prover denotes h_0(X) = f(X), then computes the folded polynomials h_1(X), h_2(X), \\ldots, h_{n-1}(X), such that:h_{i+1}(X^2) = \\frac{h_i(X) + h_i(-X)}{2} + u_i\\cdot \\frac{h_i(X) - h_i(-X)}{2X}\n\nProver computes commitments (C_{h_1}, C_{h_2}, \\ldots, C_{h_{n-1}}), such that:C_{h_{i+1}} = \\mathsf{KZG10.Commit}(h_{i+1}(X))\n\nProver sends (C_{h_1}, C_{h_2}, \\ldots, C_{h_{n-1}})","type":"content","url":"/gemini/gemini-pcs-4#round-1","position":11},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 2.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#round-2","position":12},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 2.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random point \\beta\\in\\mathbb{F}_p\n\nProver computes h_0(\\beta)\n\nProver computes h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\n\nProver sends \\big(h_0(\\beta), h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\\big)","type":"content","url":"/gemini/gemini-pcs-4#round-2","position":13},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 3.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#round-3","position":14},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 3.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random value \\gamma\\in\\mathbb{F}_p for polynomial aggregation\n\nProver computes q(X) satisfying the equation:q(X) = \\frac{h_0(X)-h_0(\\beta)}{X-\\beta}+ \\sum_{i=0}^{n-1} \\gamma^{i+1}\\cdot \\frac{h_i(X)-h_i(-\\beta^{2^i})}{X+\\beta^{2^i}}\n\nA new Domain D is defined with 3 elements:D = \\{\\beta, -\\beta, -\\beta^2, \\ldots, -\\beta^{2^{n-1}}\\}\n\nProver computes and sends commitment C_q=\\mathsf{KZG10.Commit}(q(X))","type":"content","url":"/gemini/gemini-pcs-4#round-3","position":15},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 4.","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#round-4","position":16},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Round 4.","lvl2":"2. Protocol Description"},"content":"Verifier sends a random point \\zeta\\in\\mathbb{F}_q\n\nProver computes the linearized polynomial L_\\zeta(X) which takes value 0 at X=\\zeta, i.e., L_\\zeta(\\zeta) = 0:L_\\zeta(X) = v_D(\\zeta)\\cdot q(X) - \\frac{v_D(\\zeta)}{\\zeta-\\beta}\\cdot(h_0(X)-h_0(\\beta)) - \\sum_{i=0}^{n-1} \\gamma^{i+1}\\cdot \\frac{v_D(\\zeta)}{\\zeta+\\beta^{2^i}}\\cdot(h_i(X)-h_i(-\\beta^{2^i}))\n\nProver computes the quotient polynomial w(X)w(X) = \\frac{L_\\zeta(X)}{(X-\\zeta)}\n\nProver computes and sends the commitment C_w for w(X):C_w = \\mathsf{KZG10.Commit}(w(X))","type":"content","url":"/gemini/gemini-pcs-4#round-4","position":17},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Proof Representation","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#proof-representation","position":18},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Proof Representation","lvl2":"2. Protocol Description"},"content":"It can be seen that a single proof consists of n+1 elements in \\mathbb{G}_1 and n+1 elements in \\mathbb{F}_q.\\pi=\\Big(C_{f_1}, C_{f_2}, \\ldots, C_{f_{n-1}}, C_{q}, C_w, h_0(\\beta), h_0(-\\beta), h_1(-\\beta^2), \\ldots, h_{n-1}(-\\beta^{2^{n-1}})\\Big)","type":"content","url":"/gemini/gemini-pcs-4#proof-representation","position":19},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Verification Process","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/gemini/gemini-pcs-4#verification-process","position":20},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl3":"Verification Process","lvl2":"2. Protocol Description"},"content":"Verifier computes (h_1(\\beta^2), h_2(\\beta^{2^2}), \\ldots, h_{n-1}(\\beta^{2^{n-1}}), h_n(\\beta^{2^n}))h_{i+1}(\\beta^{2^{i+1}}) = \\frac{h_i(\\beta^{2^i}) + h_i(-\\beta^{2^i})}{2} + u_i\\cdot \\frac{h_i(\\beta^{2^i}) - h_i(-\\beta^{2^i})}{2\\beta^{2^i}}\n\nVerifier checks if h_{n}(\\beta^{2^n}) equals the polynomial evaluation v=\\tilde{f}(\\vec{u}) to be provenh_n(\\beta^{2^n}) \\overset{?}{=} v\n\nVerifier computes the commitment C_L for L_\\zeta(X):C_L = v_D(\\zeta)\\cdot C_q - e_0\\cdot(C_{h_0} - h_0(\\beta)\\cdot[1]_1) - \\sum_{i=0}^{n-1} e_{i+1}\\cdot(C_{h_i} - h_i(-\\beta^{2^i})\\cdot[1]_1)\n\nwhere e_0, e_1, \\ldots, e_n are defined as:\\begin{aligned}\ne_0 &= \\frac{v_D(\\zeta)}{\\zeta - \\beta} \\\\\ne_{i+1} &= \\gamma^{i+1}\\cdot \\frac{v_D(\\zeta)}{\\zeta+\\beta^{2^i}}, \\quad i=0,1,\\ldots,n-1\n\\end{aligned}\n\nVerifier checks if C_w is the evaluation proof of C_L at X=\\zeta:\\mathsf{KZG10.Verify}(C_L, \\zeta, 0, C_w) \\overset{?}{=} 1\n\nOr expanded directly into Pairing form:e\\Big(C_L + \\zeta\\cdot C_w, [1]_2\\Big) \\overset{?}{=} e\\Big(C_w, [\\tau]_2 \\Big)","type":"content","url":"/gemini/gemini-pcs-4#verification-process","position":21},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"3. Performance Analysis of the Optimization"},"type":"lvl2","url":"/gemini/gemini-pcs-4#id-3-performance-analysis-of-the-optimization","position":22},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"3. Performance Analysis of the Optimization"},"content":"","type":"content","url":"/gemini/gemini-pcs-4#id-3-performance-analysis-of-the-optimization","position":23},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"References"},"type":"lvl2","url":"/gemini/gemini-pcs-4#references","position":24},{"hierarchy":{"lvl1":"Gemini-PCS (Part IV)","lvl2":"References"},"content":"","type":"content","url":"/gemini/gemini-pcs-4#references","position":25},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI"},"type":"lvl1","url":"/gemini/gemini-fri","position":0},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI"},"content":"Jade Xie  \n\njade@secbit.io\n\nThe Gemini protocol [BCH+22] provides an approach for converting a multilinear polynomial PCS into a univariate polynomial commitment scheme. To briefly review, in order to prove that the opening value of an MLE polynomial at a certain point is v, it can be transformed into an inner product proof. This inner product proof is obtained by repeatedly performing operations similar to sumcheck or split-and-fold in the FRI protocol on a univariate polynomial. This further converts the inner product proof into proving that some univariate polynomials have correct values at certain random points. In the original Gemini paper, KZG10’s univariate polynomial PCS was used to implement this proof. In fact, the univariate polynomial PCS can also use the FRI PCS scheme. One advantage of FRI PCS is that for openings of polynomials of different degrees at multiple points, they can be combined into one polynomial using random numbers, requiring only one call to FRI’s low degree test to complete all these proofs.\n\nBelow, drawing on the description in Appendix B of the HyperPlonk paper [BBBZ23], we provide a detailed protocol for Gemini interfacing with FRI PCS.","type":"content","url":"/gemini/gemini-fri","position":1},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl2":"Protocol Description"},"type":"lvl2","url":"/gemini/gemini-fri#protocol-description","position":2},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl2":"Protocol Description"},"content":"Proof goal: For an MLE polynomial with n variables \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}), represented in coefficient form:\\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) = \\sum_{i = 0}^{2^n - 1}c_i \\cdot X_0^{i_0} X_1^{i_1} \\cdots X_{n - 1}^{i_{n-1}}\n\nwhere (i_0, i_1,\\ldots,i_{n - 1}) is the binary representation of i, with i_0 being the least significant bit of the binary representation, satisfying i = \\sum_{j=0}^{n-1}i_j 2^{j}.\n\nThe goal of the proof is to prove that the value of \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) at the point \\vec{u} = (u_0,u_1, \\ldots, u_{n - 1}) is v = \\tilde{f}(u_0,u_1, \\ldots, u_{n - 1}).","type":"content","url":"/gemini/gemini-fri#protocol-description","position":3},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Public Inputs","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#public-inputs","position":4},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Public Inputs","lvl2":"Protocol Description"},"content":"FRI protocol parameters: Reed-Solomon encoding domain D_n \\subset D_{n-1} \\subset \\cdots \\subset D_0 = D, code rate \\rho, number of query rounds l.\n\nCommitment to polynomial f(X):C_f = \\mathsf{cm}([f(x)|_{x \\in D}]) = \\mathsf{MT.commit}([f(x)|_{x \\in D}])\n\nwhere f(X) is a polynomial of degree 2^n - 1, with the same coefficients \\vec{c} as \\tilde{f}:f(X) = \\sum_{i = 0}^{2^n - 1} c_i \\cdot X^i\n\nEvaluation point \\vec{u} = (u_0,u_1, \\ldots, u_{n - 1})\n\nv = \\tilde{f}(u_0,u_1, \\ldots, u_{n - 1})","type":"content","url":"/gemini/gemini-fri#public-inputs","position":5},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Witness","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#witness","position":6},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Witness","lvl2":"Protocol Description"},"content":"Coefficients \\vec{c} = (c_0,c_1, \\ldots, c_{2^n - 1}) of the multivariate polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1})","type":"content","url":"/gemini/gemini-fri#witness","position":7},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 1","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-1","position":8},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 1","lvl2":"Protocol Description"},"content":"Prover sets h_0(X) = f(X), computes folded polynomials h_1(X), h_2(X), \\ldots, h_{n-1}(X), such that for i = 1, \\ldots, n-1:h_{i}(X^{2}) = \\frac{h_{i - 1}(X) + h_{i - 1}(-X)}{2} + u_{i - 1} \\cdot \\frac{h_{i - 1}(X) - h_{i - 1}(-X)}{2X}\n\nProver computes commitments (C_{h_{1}},C_{h_{2}}, \\ldots, C_{h_{n-1}}), where for i = 1, \\ldots, n-1:C_{h_{i}} = \\mathsf{cm}([h_{i}(x)|_{x \\in D}]) = \\mathsf{MT.commit}([h_{i}(x)|_{x \\in D})","type":"content","url":"/gemini/gemini-fri#round-1","position":9},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 2","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-2","position":10},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 2","lvl2":"Protocol Description"},"content":"Verifier sends random number \\beta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}^* \\setminus D\n\nProver computes \\{h_i(\\beta), h_{i}(- \\beta), h_i(\\beta^2)\\}_{i = 0}^{n-1} and sends to Verifier.","type":"content","url":"/gemini/gemini-fri#round-2","position":11},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 3","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-3","position":12},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 3","lvl2":"Protocol Description"},"content":"Verifier sends random number r \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nFirst perform degree correction on each polynomial h_{i}(X)(i = 1, \\ldots, n) to align degrees to 2^{n}- 1. The degree of each polynomial is \\deg(h_i)=2^{n-i}-1. For i = 1, \\ldots, n - 1, compute h'_i(X):\n\nMethod 1:h'_i(X)=h_i(X)+r\\cdot X^{2^{n} - 2^{n-i}} \\cdot h_i(X)\n\nMethod 2: If using the degree correction method from the STIR paper [ACFY24]:h'_i(X)=\\sum_{j = 0}^{2^{n}- 2^{n - i}} r^{j} \\cdot X^{j} \\cdot h_i(X)=h_i(X)+r\\cdot X \\cdot h_i(X)+r^{2} \\cdot X^2 \\cdot h_i(X) + \\ldots + r^{2^n - 2^{n-i}} \\cdot X^{2^n - 2^{n-i}} \\cdot h_i(X)\n\n📝 Notes\n\nMethod 2 provides higher security compared to Method 1. ([ACFY24, 2.3])\n\nBatch h_0(X) with h_1'(X), \\ldots, h_{n-1}'(X) using powers of the random number r:\n\nMethod 1: Compute\\begin{align*}\nh^*(X) & = h_0(X) + r^{1 + (0)} \\cdot h_1'(X) + r^{2 + (0 + 1)} \\cdot h_2'(X) + r^{3 + (0 + 1 + 1)} \\cdot h_3'(X)+ \\ldots + r^{n - 1 + (0 + 1 \\cdot (n - 2))} \\cdot h_{n-1}'(X) \\\\\n& = h_0(X) + r \\cdot h_1'(X) + r^{3} \\cdot h_2'(X) + r^{5} \\cdot h_3'(X)+ \\ldots + r^{2n-3} \\cdot h_{n-1}'(X) \n\\end{align*} \\tag{1}\n\nMethod 2: If using the degree correction method from the STIR paper, compute the batched polynomial as:\\begin{align*}\nh^*(X) & = h_0(X) + r^{1 + (0)} \\cdot h_1'(X) + r^{2 + (0 + e_1)} \\cdot h_2'(X) + r^{3 + (0 + e_1 + e_2)} \\cdot h_3'(X)\\\\\n& \\quad + \\ldots + r^{n - 1 + (0 + e_1 + e_2 + \\ldots + e_{n-2})} \\cdot h_{n-1}'(X) \\\\\n& = h_0(X) + r \\cdot h_1'(X) + r^{2 + 2^n - 2^{n -1}} \\cdot h_2'(X) + r^{2 + \\sum_{i=1}^{2}(2^n-2^i)} \\cdot h_3'(X) \\\\\n& \\quad + \\ldots + r^{n - 1+\\sum_{i=1}^{n-2}(2^n-2^i)} \\cdot h_{n-1}'(X) \n\\end{align*} \\tag{2}\n\nCompute quotient polynomial q'(X) to verify if h^*(X) opens correctly at points (\\beta,-\\beta,\\beta^2):q'(X) = \\frac{h^*(X)-h^*(\\beta)}{X-\\beta} + \\frac{h^*(X)-h^*(-\\beta)}{X+\\beta} + \\frac{h^*(X)-h^*(\\beta^2)}{X-\\beta^2}\n\nThe construction of this quotient polynomial refers to the Multi-point queries section in paper [H22].","type":"content","url":"/gemini/gemini-fri#round-3","position":13},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 4","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-4","position":14},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 4","lvl2":"Protocol Description"},"content":"This round aligns the quotient polynomial q'(X) to a power of 2 to interface with the FRI protocol.\n\nVerifier sends random number \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver computesq(X) = (1 + \\lambda \\cdot X) q'(X)\n\non domain D.","type":"content","url":"/gemini/gemini-fri#round-4","position":15},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 5","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-5","position":16},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 5","lvl2":"Protocol Description"},"content":"Prover and Verifier engage in FRI’s low degree test proof interaction to prove that the degree of q(X) is less than 2^n:\\pi_{q} = \\mathsf{FRI.LDT}(q(X), 2^n)\n\nThis includes n rounds of interaction, until the original polynomial is folded into a constant polynomial. Using i to represent the i-th round, the specific interaction process is as follows:\n\nLet q^{(0)}(x)|_{x \\in D} := q(x)|_{x \\in D}\n\nFor i = 1,\\ldots, n:\n\nVerifier sends random number \\alpha^{(i)}\n\nFor any y \\in D_i, find x in D_{i - 1} satisfying x^2 = y, Prover computesq^{(i)}(y) = \\frac{q^{(i - 1)}(x) + q^{(i - 1)}(-x)}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(x) - q^{(i - 1)}(-x)}{2x}\n\nIf i < n, Prover sends Merkle Tree commitment of [q^{(i)}(x)|_{x \\in D_{i}}]:\\mathsf{cm}(q^{(i)}(X)) = \\mathsf{cm}([q^{(i)}(x)|_{x \\in D_{i}}]) = \\mathsf{MT.commit}([q^{(i)}(x)|_{x \\in D_{i}}])\n\nIf i = n, choose any x_0 \\in D_{n}, Prover sends the value of q^{(i)}(x_0).\n\n📝 Notes\n\nIf the number of folds r < n, then it will not fold to a constant polynomial at the end. Therefore, Prover will send a Merkle Tree commitment in the r-th round, instead of sending a value.","type":"content","url":"/gemini/gemini-fri#round-5","position":17},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 6","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#round-6","position":18},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Round 6","lvl2":"Protocol Description"},"content":"This round continues the FRI protocol’s low degree test interaction between Prover and Verifier in the query phase. Verifier repeats the query l times. Each time, Verifier selects a random number from D_0 and asks Prover to send the folded value in the i-th round and the corresponding Merkle Path, allowing Verifier to verify the correctness of each round of folding.\n\nRepeat l times:\n\nVerifier randomly selects a number s^{(0)} \\stackrel{\\$}{\\leftarrow} D_0 from D_0\n\nProver opens the commitments of \\{h_i(s^{(0)})\\}_{i = 0}^{n-1} and \\{h_i(-s^{(0)})\\}_{i = 0}^{n-1}, i.e., the values at these points and their corresponding Merkle Paths, and sends them to Verifier\\{(h_i(s^{(0)}), \\pi_{h_i}(s^{(0)}))\\}_{i = 0}^{n-1} \\leftarrow \\{\\mathsf{MT.open}([h_i(x)|_{x \\in D_0}], s^{(0)})\\}_{i = 0}^{n-1}\\{(h_i(-s^{(0)}), \\pi_{h_i}(-s^{(0)}))\\}_{i = 0}^{n-1} \\leftarrow \\{\\mathsf{MT.open}([h_i(x)|_{x \\in D_0}], -s^{(0)})\\}_{i = 0}^{n-1}\n\nProver computes s^{(1)} = (s^{(0)})^2\n\nFor i = 1, \\ldots, n - 1\n\nProver sends the values of q^{(i)}(s^{(i)}), q^{(i)}(-s^{(i)}), along with their Merkle Paths.\\{(q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], s^{(i)})\\{(q^{(i)}(-s^{(i)}), \\pi_{q}^{(i)}(-s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], -s^{(i)})\n\nProver computes s^{(i + 1)} = (s^{(i)})^2\n\nIf the number of folds r < n, then in the last step, the value of q^{(r)}(s^{(r)}) should be sent along with its Merkle Path.","type":"content","url":"/gemini/gemini-fri#round-6","position":19},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Proof","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#proof","position":20},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Proof","lvl2":"Protocol Description"},"content":"The proof sent by Prover is:\\pi = (C_{h_{1}},C_{h_{2}}, \\ldots, C_{h_{n-1}}, \\{h_i(\\beta), h_{i}(- \\beta), h_i(\\beta^2)\\}_{i = 0}^{n-1}, \\pi_{q})\n\nUsing the symbol \\{\\cdot\\}^l to represent the proof generated by repeating the query l times in the FRI low degree test query phase. Since each query is randomly selected, the proof in braces is also random. Then the proof of FRI’s low degree test is:\\begin{aligned}\n  \\pi_{q} = &  ( \\mathsf{cm}(q^{(1)}(X)), \\ldots, \\mathsf{cm}(q^{(n - 1)}(X)),q^{(n)}(x_0),  \\\\\n  & \\, \\{h_0(s^{(0)}), \\pi_{h_0}(s^{(0)}), h_0(- s^{(0)}), \\pi_{h_0}(-s^{(0)}), \\cdots ,\\\\\n  & \\quad h_{n-1}(s^{(0)}), \\pi_{h_{n-1}}(s^{(0)}), h_{n-1}(- s^{(0)}), \\pi_{h_{n-1}}(-s^{(0)}), \\\\\n  & \\quad q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)}),q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)}), \\ldots, \\\\\n  & \\quad q^{(n - 1)}(s^{(n - 1)}), \\pi_{q^{(n - 1)}}(s^{(n - 1)}),q^{(n - 1)}(-s^{(n - 1)}), \\pi_{q^{(i)}}(-s^{(n - 1)})\\}^l)\n\\end{aligned}","type":"content","url":"/gemini/gemini-fri#proof","position":21},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Verification","lvl2":"Protocol Description"},"type":"lvl3","url":"/gemini/gemini-fri#verification","position":22},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl3":"Verification","lvl2":"Protocol Description"},"content":"Verifier verifies if the folding process is correct. For i = 1, \\ldots, n - 1, calculate and verify based on the values sent by Prover:h_{i}(\\beta^{2}) \\stackrel{?}{=} \\frac{h_{i - 1}(\\beta) + h_{i - 1}(-\\beta)}{2} + u_{i - 1} \\cdot \\frac{h_{i - 1}(\\beta) - h_{i - 1}(-\\beta)}{2\\beta}\n\nVerifier verifies if it finally folds to the constant v, verify:\\frac{h_{n - 1}(\\beta) + h_{n - 1}(-\\beta)}{2} + u_{n - 1} \\cdot \\frac{h_{n - 1}(\\beta) - h_{n - 1}(-\\beta)}{2\\beta} \\stackrel{?}{=} v\n\nVerify the low degree test proof of q(X):\\mathsf{FRI.LDT.verify}(\\pi_{q}, 2^n) \\stackrel{?}{=} 1\n\nThe specific verification process is, repeat l times:\n\nVerify the correctness of \\{h_i(s^{(0)})\\}_{i = 0}^{n-1} and \\{h_i(-s^{(0)})\\}_{i = 0}^{n-1}. For i = 0, \\ldots, n - 1, verify:\\mathsf{MT.verify}(\\mathsf{cm}(h_i(X)), h_i(s^{(0)}), \\pi_{h_i}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(h_i(X)), h_i(-s^{(0)}), \\pi_{h_i}(-s^{(0)})) \\stackrel{?}{=} 1\n\nVerifier calculates the values of h^*(s^{(0)}) and h^*(-s^{(0)}) based on \\{h_i(s^{(0)})\\}_{i = 0}^{n-1} and \\{h_i(-s^{(0)})\\}_{i = 0}^{n-1}, calculates h^*(\\beta), h^*(-\\beta), h^*(\\beta^2) based on \\{h_i(\\beta)\\}_{i = 0}^{n-1}, \\{h_i(-\\beta)\\}_{i = 0}^{n-1}, \\{h_i(\\beta^2)\\}_{i = 0}^{n-1}. For x \\in \\{s^{(0)}, -s^{(0)}, \\beta, -\\beta, \\beta^2\\}, Verifier calculates h^*(x) as follows:\n\nMethod 1: For i = 1, \\ldots, n - 1, calculate:h'_i(x)=h_i(x)+r\\cdot (x)^{2^{n} - 2^{n-i}} \\cdot h_i(x)\n\nThen calculate:\\begin{align*}\nh^*(x) & = h_0(x) + r \\cdot h_1'(x) + r^{3} \\cdot h_2'(x) + r^{5} \\cdot h_3'(x)+ \\ldots + r^{2n-3} \\cdot h_{n-1}'(x) \n\\end{align*}\n\nMethod 2: If using the degree correction method from the STIR paper [ACFY24], for i = 1, \\ldots, n - 1, calculate:h'_i(x)=\\sum_{j = 0}^{2^{n}- 2^{n - i}} r^{j} \\cdot (x)^{j} \\cdot h_i(x) = \\begin{cases}\n h_i(x) \\cdot \\frac{1 - (r \\cdot x)^{2^n - 2^{n-i} + 1}}{1 - r \\cdot x} & \\text{if } r \\cdot x \\neq 0\\\\\nh_i(x) \\cdot (2^n - 2^{n-i} + 1) & \\text{if } r \\cdot x = 0\n\\end{cases}\n\nThen calculate:\\begin{align*}\nh^*(x) & = h_0(x) + r \\cdot h_1'(x) + r^{2 + 2^n - 2^{n -1}} \\cdot h_2'(x) + r^{2 + \\sum_{i=1}^{2}(2^n-2^i)} \\cdot h_3'(x) \\\\\n& \\quad + \\ldots + r^{n - 1+\\sum_{i=1}^{n-2}(2^n-2^i)} \\cdot h_{n-1}'(x) \n\\end{align*}\n\nVerifier calculates:\nq'(s^{(0)}) = \\frac{h^*(s^{(0)})-h^*(\\beta)}{s^{(0)}-\\beta} + \\frac{h^*(s^{(0)})-h^*(-\\beta)}{s^{(0)}+\\beta} + \\frac{h^*(s^{(0)})-h^*(\\beta^2)}{s^{(0)}-\\beta^2}q'(-s^{(0)}) = \\frac{h^*(-s^{(0)})-h^*(\\beta)}{-s^{(0)}-\\beta} + \\frac{h^*(-s^{(0)})-h^*(-\\beta)}{-s^{(0)}+\\beta} + \\frac{h^*(-s^{(0)})-h^*(\\beta^2)}{-s^{(0)}-\\beta^2}\n\nVerifier calculates:q^{(0)}(s^{(0)}) = (1 + \\lambda \\cdot s^{(0)}) q'(s^{(0)})q^{(0)}(-s^{(0)}) = (1 - \\lambda \\cdot s^{(0)}) q'(-s^{(0)})\n\nVerify the correctness of q^{(1)}(s^{(1)}), q^{(1)}(-s^{(1)}):\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)})) \\stackrel{?}{=} 1\n\nVerify if the folding in the 1st round is correct:q^{(1)}(s^{(1)}) \\stackrel{?}{=} \\frac{q^{(0)}(s^{(0)}) + q^{(0)}(- s^{(0)})}{2} + \\alpha^{(1)} \\cdot \\frac{q^{(0)}(s^{(0)}) - q^{(0)}(- s^{(0)})}{2 \\cdot s^{(0)}}\n\nFor i = 2, \\ldots, n - 1\n\nVerify the correctness of q^{(i)}(s^{(i)}), q^{(i)}(-s^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(-s^{(i)}), \\pi_{q^{(i)}}(-s^{(i)})) \\stackrel{?}{=} 1\n\nVerify if the folding in the i-th round is correct:q^{(i)}(s^{(i)}) \\stackrel{?}{=} \\frac{q^{(i-1)}(s^{(i - 1)}) + q^{(i - 1)}(- s^{(i - 1)})}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(s^{(i - 1)}) - q^{(i - 1)}(- s^{(i - 1)})}{2 \\cdot s^{(i - 1)}}\n\nVerify if it finally folds to a constant polynomial:q^{(n)}(x_0) \\stackrel{?}{=} \\frac{q^{(n-1)}(s^{(n - 1)}) + q^{(n - 1)}(- s^{(n - 1)})}{2} + \\alpha^{(n)} \\cdot \\frac{q^{(n - 1)}(s^{(n - 1)}) - q^{(n - 1)}(- s^{(n - 1)})}{2 \\cdot s^{(n - 1)}}","type":"content","url":"/gemini/gemini-fri#verification","position":23},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl2":"Reference"},"type":"lvl2","url":"/gemini/gemini-fri#reference","position":24},{"hierarchy":{"lvl1":"Gemini: Interfacing with FRI","lvl2":"Reference"},"content":"[BCH+22] Bootle, Jonathan, Alessandro Chiesa, Yuncong Hu, et al. “Gemini: Elastic SNARKs for Diverse Environments.” Cryptology ePrint Archive (2022). \n\nhttps://​eprint​.iacr​.org​/2022​/420\n\n[BBBZ23] Chen, Binyi, Benedikt Bünz, Dan Boneh, and Zhenfei Zhang. “Hyperplonk: Plonk with linear-time prover and high-degree custom gates.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 499-530. Cham: Springer Nature Switzerland, 2023.\n\n[H22] Haböck, Ulrich. “A summary on the FRI low degree test.” Cryptology ePrint Archive (2022).\n\n[ACFY24] Arnon, Gal, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.","type":"content","url":"/gemini/gemini-fri#reference","position":25},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/gemini/gemini-01","position":0},{"hierarchy":{"lvl1":""},"content":"################################################################################\n# SageMath code to convert from the \"point-value\" representation of a\n# 3D multilinear polynomial f: {0,1}^3 -> GF(17) to its \"coefficient form\",\n# and then construct the polynomial symbolically.\n################################################################################\n\n# 1) Choose q = 17 and define the field GF(17).\nq = 17\nF = GF(q)\n\n# 2) Create the polynomial ring in three variables over GF(17).\nR.<X0,X1,X2> = PolynomialRing(F, 3, 'X')\n\ndef point_value_to_coefficient(a):\n    \"\"\"\n    Given the values (a_0, ..., a_7) of a multilinear polynomial f\n    at the corners of the Boolean cube {0,1}^3,\n    return its coefficient vector (f_0, ..., f_7).\n    \n    The ordering of (a_0,...,a_7) is assumed to be:\n        a_0 = f(0,0,0)\n        a_1 = f(1,0,0)\n        a_2 = f(0,1,0)\n        a_3 = f(1,1,0)\n        a_4 = f(0,0,1)\n        a_5 = f(1,0,1)\n        a_6 = f(0,1,1)\n        a_7 = f(1,1,1)\n    \"\"\"\n    a0, a1, a2, a3, a4, a5, a6, a7 = a\n    \n    f0 = a0\n    f1 = a1 - a0\n    f2 = a2 - a0\n    f3 = a4 - a0            # because a4 = f(0,0,1)\n    f4 = a3 - a2 - a1 + a0  # x0*x1\n    f5 = a5 - a4 - a1 + a0  # x0*x2\n    f6 = a6 - a4 - a2 + a0  # x1*x2\n    f7 = a7 - a6 - a5 + a4 - a3 + a2 + a1 - a0\n    \n    return [f0, f1, f2, f3, f4, f5, f6, f7]\n\ndef polynomial_from_coefficients(coeffs):\n    \"\"\"\n    Given the coefficient vector [f0, f1, f2, f3, f4, f5, f6, f7]\n    for a multilinear polynomial in X0, X1, X2,\n    return the symbolic polynomial f(X0, X1, X2).\n    \"\"\"\n    f0, f1, f2, f3, f4, f5, f6, f7 = coeffs\n    return (f0\n            + f1*X0\n            + f2*X1\n            + f3*X2\n            + f4*X0*X1\n            + f5*X0*X2\n            + f6*X1*X2\n            + f7*X0*X1*X2)\n\n################################################################################\n# Example usage:\n# Let's pick a point-value vector a = (a0, a1, ..., a7) in GF(17).\n################################################################################\n\na_example = [\n    F(0),  # a0 = f(0,0,0)\n    F(1),  # a1 = f(1,0,0)\n    F(2),  # a2 = f(0,1,0)\n    F(5),  # a3 = f(1,1,0)\n    F(3),  # a4 = f(0,0,1)\n    F(9),  # a5 = f(1,0,1)\n    F(8),  # a6 = f(0,1,1)\n    F(16)  # a7 = f(1,1,1)\n]\n\n# 3) Convert the point-value vector to coefficient form\ncoeff_vector = point_value_to_coefficient(a_example)\nprint(\"Coefficient vector =\", coeff_vector)\n\n# 4) Build the symbolic polynomial\nf_poly = polynomial_from_coefficients(coeff_vector)\nprint(\"Multilinear polynomial f =\", f_poly)\n\n################################################################################\n# Verification: Evaluate the polynomial at each corner of {0,1}^3 and compare\n################################################################################\n\ntest_points = [\n    (0,0,0), (1,0,0), (0,1,0), (1,1,0),\n    (0,0,1), (1,0,1), (0,1,1), (1,1,1)\n]\n\nprint(\"\\nVerification of polynomial values:\")\nfor i, (x0v, x1v, x2v) in enumerate(test_points):\n     raw_val = f_poly(X0=x0v, X1=x1v, X2=x2v)\n     val = F(raw_val)  # ensures a representative in 0..16\n     print(f\"f({x0v},{x1v},{x2v}) = {val}, integer-rep = {raw_val}, expected = {a_example[i]}\")\n\ndef tensor_product(vec_u, vec_v):\n    \"\"\"\n    Return the Kronecker product of vec_u and vec_v.\n    If vec_u = (u_1, ..., u_m) and vec_v = (v_1, ..., v_n),\n    we return (u_1*v_1, ..., u_1*v_n, u_2*v_1, ..., u_m*v_n).\n    \"\"\"\n    return [u * v for u in vec_u for v in vec_v]\n\n\n\ndef monomial_vector_3d(X0, X1, X2):\n    \"\"\"\n    Return the 8-dimensional monomial vector\n        (1, X0, X1, X0*X1, X2, X0*X2, X1*X2, X0*X1*X2)\n    by doing the Kronecker product of (1,X0), (1,X1), and (1,X2).\n    \"\"\"\n    vec0 = [1, X0]\n    vec1 = [1, X1]\n    vec2 = [1, X2]\n    \n    # First take the product of vec0 and vec1, then tensor with vec2\n    tmp  = tensor_product(vec0, vec1)   # yields 4 monomials\n    full = tensor_product(tmp,  vec2)   # yields 8 monomials\n    return full\n\n# Set up GF(17) and a polynomial ring in X0, X1, X2:\nq  = 17\nF  = GF(q)\nR.<X0,X1,X2> = PolynomialRing(F, 3)\n\n# Example coefficient vector f = (f0, f1, ..., f7)\ncoeffs = [F(0), F(1), F(2), F(3), F(2), F(5), F(3), F(0)]\n\n# Build the 8-component monomial vector via tensor product\nmonom_vec = monomial_vector_3d(X0, X1, X2)\nprint(\"Monomial vector =\", monom_vec)\n# [1, X0, X1, X0*X1, X2, X0*X2, X1*X2, X0*X1*X2]\n\n# Now form the polynomial as the 'inner product' of coeffs and monom_vec\nf_poly = sum( c * m for (c,m) in zip(coeffs, monom_vec) )\nprint(\"f(X0,X1,X2) =\", f_poly)\n\n\n################################################################################\n# SageMath code to illustrate \"multivariate polynomial split-and-fold\" \n# for a 3-variable polynomial f^(0)(X0, X1, X2) over GF(17).\n################################################################################\n\n# 1) Define the field and ring\nq = 17\nF = GF(q)\nR.<X0,X1,X2> = PolynomialRing(F, 3)\n\n# 2) Define an example polynomial f^(0) = f_init\nf_init = (F(5)         # constant term\n          + F(1)*X0\n          + F(2)*X1\n          + F(3)*X0*X1\n          + F(4)*X2\n          + F(6)*X0*X2\n          + F(7)*X1*X2\n          + F(8)*X0*X1*X2)\n\nprint(\"Initial polynomial f^(0) =\")\nprint(f_init)\nprint(\"-\"*70)\n\n# ------------------------------------------------------------------------------\n# Helper function: split f into (f_even, f_odd), \n# where f = f_even + var*f_odd and 'var' is one of X0, X1, X2.\n# ------------------------------------------------------------------------------\ndef polynomial_split(f, var):\n    \"\"\"\n    Return (f_even, f_odd) such that\n      f(X0,X1,X2) = f_even(X0,X1,X2) + var*f_odd(X0,X1,X2).\n\n    Here 'var' should be one of the ring generators (X0, X1, or X2).\n    \"\"\"\n    R = f.parent()\n    var_idx = R.gens().index(var)  # find which generator var is: 0 for X0, 1 for X1, 2 for X2\n    f_even = R(0)\n    f_odd  = R(0)\n    \n    for monomial, coeff in f.dict().items():\n        # monomial is a tuple of exponents, e.g. (2,1,0).\n        exp_var = monomial[var_idx]  # exponent of 'var' in this monomial\n        # Make a copy with that exponent set to 0 for the base monomial:\n        base_monomial = list(monomial)\n        if exp_var > 0:\n            base_monomial[var_idx] -= 1  # factor out exactly one 'var'\n            f_odd += R({tuple(base_monomial): coeff})\n        else:\n            f_even += R({tuple(monomial): coeff})\n    \n    return (f_even, f_odd)\n\n\n# 3) Define challenges rho_0, rho_1, rho_2 in GF(17). In a real protocol, these might come from a verifier's randomness.\nrhos = [F(11), F(13), F(2)]  # Just an example set\n\n# We'll keep track of polynomials f^(j) in a list: f_list[j] = f^(j).\nf_list = [f_init]\n\n# We want to do j=1,2,3, where the variable to split on is X_{j-1}.\n# So for j=1 => X0, j=2 => X1, j=3 => X2:\nvars_in_order = [X0, X1, X2]\n\nfor j in [1,2,3]:\n    print(f\"=== Split-and-Fold Round j={j} (splitting on variable X{j-1}) ===\")\n    current_poly = f_list[j-1]         # f^(j-1)\n    var          = vars_in_order[j-1]  # X0 for j=1, X1 for j=2, X2 for j=3\n    rho          = rhos[j-1]          # rho_{j-1}\n\n    # ---- Split step ----\n    f_even, f_odd = polynomial_split(current_poly, var)\n    \n    print(f\"Split f^(j-1) into f_even + X{j-1} * f_odd:\")\n    print(f\"  f_even = {f_even}\")\n    print(f\"  f_odd  = {f_odd}\")\n    print(f\"Check: f^(j-1) = f_even + X{j-1}*f_odd ? => {f_even + var*f_odd}\")\n    print(\"-\"*70)\n\n    # ---- Fold step ----\n    #   f^(j) = f_even + rho * f_odd\n    folded_poly = f_even + rho * f_odd\n    \n    print(f\"Folded with rho_{j-1} = {rho}\")\n    print(f\" => f^(j) = f_even + rho * f_odd = {folded_poly}\")\n    print(\"=\"*70)\n\n    f_list.append(folded_poly)\n\n# 4) Final result after j=3 is f^(3)\nprint(\"Final polynomial f^(3) =\")\nprint(f_list[3])\n\n\n################################################################################\n# SageMath code demonstrating:\n#   1) \"Split-and-fold\" for a univariate polynomial f^(0)(X) --> f^(1)(X) --> ...\n#   2) The verifier checks from steps 4 and 5 of \"tensor-product check protocol.\"\n################################################################################\n\n# ------------------------------------------------------------------------------\n# 1) Setup environment\n# ------------------------------------------------------------------------------\nq = 17\nF = GF(q)\nR.<X> = PolynomialRing(F, 1)\n\nprint(\"=== Tensor-product Check Protocol with Verifier Checks ===\")\nprint(f\"Working over GF({q}).\\n\")\n\n# ------------------------------------------------------------------------------\n# 2) Construct an example initial polynomial f^(0)(X).\n#    You can tweak the coefficients or degree as needed.\n# ------------------------------------------------------------------------------\nf_init = (\n    F(2)*X^5 +\n    F(7)*X^3 +\n    F(9)*X^2 +\n    F(1)*X   +\n    F(11)    # constant term\n)\nn = 3  # number of \"rounds\" we want\nprint(f\"We will do {n} rounds of split-and-fold (hence building f^(0),...,f^({n})).\\n\")\nprint(\"Initial polynomial f^(0)(X) =\")\nprint(f_init)\nprint(\"-\"*70)\n\n# ------------------------------------------------------------------------------\n# 3) Split function: f(X) = f_e(X^2) + X * f_o(X^2)\n# ------------------------------------------------------------------------------\ndef split_even_odd(f):\n    \"\"\"\n    Given f(X) = sum_{k} a_k * X^k,\n    produce f_e(X) and f_o(X) such that:\n      f(X) = f_e(X^2) + X * f_o(X^2).\n\n    Returns (f_e, f_o).\n    \"\"\"\n    f_e = R(0)\n    f_o = R(0)\n    for (exp_tuple, coeff) in f.dict().items():\n        k = exp_tuple[0]\n        if k % 2 == 0:\n            # even exponent => a_k * X^(k/2) goes to f_e\n            f_e += coeff * X^(k//2)\n        else:\n            # odd exponent => a_k * X^((k-1)/2) goes to f_o\n            f_o += coeff * X^((k-1)//2)\n    return (f_e, f_o)\n\ndef reconstruct_from_even_odd(f_e, f_o):\n    \"\"\"\n    Return f_e(X^2) + X*f_o(X^2).\n    Should match original f(X) if (f_e, f_o) came from split_even_odd.\n    \"\"\"\n    fe_sub = R(0)\n    for (exp_tuple, coeff) in f_e.dict().items():\n        e = exp_tuple[0]\n        fe_sub += coeff * X^(2*e)\n\n    fo_sub = R(0)\n    for (exp_tuple, coeff) in f_o.dict().items():\n        e = exp_tuple[0]\n        fo_sub += coeff * X^(2*e)\n\n    return fe_sub + X*fo_sub\n\n# ------------------------------------------------------------------------------\n# 4) Folding: f^(j) = f_e(X) + rho * f_o(X).\n# ------------------------------------------------------------------------------\ndef fold_polynomial(f_e, f_o, rho):\n    return f_e + rho*f_o\n\n# ------------------------------------------------------------------------------\n# 5) Build the sequence of polynomials f^(0), f^(1), ..., f^(n).\n# ------------------------------------------------------------------------------\nf_list = [f_init]\n\n# Example: pick some rhos in GF(q).\nrhos = [F(3), F(5), F(7)]  # one for each round j=1..n\n\nfor j in range(1, n+1):\n    print(f\"Round j = {j}:\")\n    print(f\"  Current polynomial f^({j-1})(X) =\")\n    print(f_list[j-1])\n    \n    # Split\n    f_e, f_o = split_even_odd(f_list[j-1])\n    f_reconstructed = reconstruct_from_even_odd(f_e, f_o)\n    print(\"  Split into:\")\n    print(\"     f_even(X) =\", f_e)\n    print(\"     f_odd(X)  =\", f_o)\n    print(\"  Check f^({j-1})(X) == f_e(X^2) + X*f_o(X^2)? =>\", \n          (f_reconstructed == f_list[j-1]))\n    \n    # Fold\n    f_j = fold_polynomial(f_e, f_o, rhos[j-1])\n    print(f\"  Folding with rho_{j-1} = {rhos[j-1]}, yields f^({j})(X):\")\n    print(f_j)\n    print(\"-\"*70)\n    f_list.append(f_j)\n\nprint(f\"\\nAfter {n} rounds, we have polynomials f^(0)..f^({n}).\")\nprint(\"Final polynomial is f^({n})(X) =\")\nprint(f_list[n])\nprint(\"=\"*70)\n\n# ------------------------------------------------------------------------------\n# 6) Verifier checks (steps 4 & 5).\n#    We illustrate the queries:\n#       e^(j-1)     = f^(j-1)( beta )\n#       e~^(j-1)    = f^(j-1)(-beta )\n#       e^(j)       = f^(j)( beta^2 )    (for j < n)\n#    Then the check:\n#       e^(j) == (e^(j-1)+e~^(j-1))/2  +  rho_{j-1} * ( e^(j-1)-e~^(j-1) )/(2*beta)\n#    For j=n, we skip f^(n)( beta^2 ) and pretend the final claim is \"u\".\n# ------------------------------------------------------------------------------\ndef run_verifier_checks(f_list, rhos, beta, final_claim):\n    \"\"\"\n    f_list      : [ f^(0), f^(1), ..., f^(n) ]\n    rhos        : [ rho_0, rho_1, ..., rho_{n-1} ]\n    beta        : challenge in GF(q)\n    final_claim : the claimed value that would be f^(n)(beta^2), if we had an oracle.\n                  The protocol says \"ignore the actual query for j=n\" \n                  and just set e^(n) := final_claim.\n    \n    We'll check for j=1..n:\n       e^(j-1)     = f^(j-1)(beta)\n       e~^(j-1)    = f^(j-1)(-beta)\n       if j < n:\n         e^(j)   = f^(j)(beta^2)\n       else:\n         e^(j)   = final_claim\n       \n       Then verify\n         e^(j) == ( e^(j-1) + e~^(j-1) )/2  +  rho_{j-1} * [ e^(j-1) - e~^(j-1) ] / [2 beta]\n    \"\"\"\n    n = len(rhos)  # number of folds\n    print(f\"\\n=== Verifier Checks (beta={beta}, final_claim={final_claim}) ===\")\n    success = True\n    \n    for j in range(1, n+1):\n        # Evaluate e^(j-1) and e~^(j-1):\n        val_e_jm1 = f_list[j-1](beta)\n        val_etilde_jm1 = f_list[j-1](-beta)\n        \n        # Either query e^(j)=f^(j)(beta^2) if j<n, or set e^(j)=final_claim if j=n\n        if j < n:\n            val_e_j = f_list[j](beta^2)\n        else:\n            val_e_j = final_claim  # as per the protocol's \"ignore the last query, take u\"\n        \n        # The right-hand side of the check:\n        lhs = val_e_j\n        rhs = (val_e_jm1 + val_etilde_jm1)/2  +  rhos[j-1]*(val_e_jm1 - val_etilde_jm1)/(2*beta)\n        \n        print(f\"Round j={j}:\")\n        print(f\"  e^(j-1)       = {val_e_jm1}\")\n        print(f\"  e~^(j-1)      = {val_etilde_jm1}\")\n        print(f\"  e^(j)         = {val_e_j}  (queried or final claim)\")\n        print(f\"  Checking: e^(j) == (e^(j-1)+ e~^(j-1))/2 + rho_{j-1}*( e^(j-1)- e~^(j-1))/(2*beta)?\")\n        print(f\"    LHS = {lhs}\")\n        print(f\"    RHS = {rhs}\")\n        \n        if lhs != rhs:\n            print(\"  => Mismatch! Verification fails.\")\n            success = False\n            break\n        else:\n            print(\"  => OK.\")\n        print(\"-\"*70)\n    \n    if success:\n        print(\"All checks passed successfully.\")\n    else:\n        print(\"Verification failed in round j above.\")\n\n# ------------------------------------------------------------------------------\n# 7) Run the verifier checks.\n#    Suppose the final claimed value (u) is f^(n)(beta^2), i.e. if it *were* queried.\n#    We'll actually compute that here to see if the check passes.\n# ------------------------------------------------------------------------------\nbeta = F(11)\n# We'll \"simulate\" the final claim as if the prover didn't let us query it.\n# So let's see what f^(n)(beta^2) actually is, just to confirm correctness:\nactual_f_n_beta2 = f_list[n](beta^2)\n\nrun_verifier_checks(\n    f_list   = f_list,\n    rhos     = rhos,\n    beta     = beta,\n    final_claim = actual_f_n_beta2  # in a real protocol, the prover would just \"claim\" this\n)\n\n\n\n\n###############################################################################\n# Scalar-only KZG demonstration with Multi-to-Uni \"split-and-fold.\"\n# No sage.crypto.pairing needed; we do everything as field scalars.\n###############################################################################\n\n########################################################\n# 1) Basic Setup\n########################################################\n\np = 127  # a small prime field for demonstration\nF = GF(p)\nR.<x> = PolynomialRing(F, 'x')  # univariate polynomials over F\n\nprint(\"=== Multi-to-Uni + 'Scalar-Only' KZG Demo ===\")\nprint(f\"Using prime field GF({p}).\")\n\n# Max polynomial degree\nD = 6\n\n# \"Secret\" alpha in the field\nalpha = F(7)   # deterministically chosen (or random if you like)\n\n# Build the SRS as a list of powers [alpha^0, alpha^1, ..., alpha^D].\n# We'll keep them in python list 'srs_scalars'.\nsrs_scalars = [F(1)]\nfor i in range(1, D+1):\n    srs_scalars.append(srs_scalars[-1]*alpha)\n\nprint(\"\\nScalar SRS (alpha^i in F):\")\nfor i, val in enumerate(srs_scalars):\n    print(f\"  i={i}: alpha^i = {val}\")\n\n\n########################################################\n# 2) \"Scalar-Only\" KZG Routines\n########################################################\n\ndef kzg_commit(poly):\n    \"\"\"\n    Summation_{k} [poly[k] * alpha^k], in F.\n    That is, c_f = f(alpha).\n    \"\"\"\n    c = F(0)\n    for (k, coeff) in poly.dict().items():\n        c += coeff * srs_scalars[k]\n    return c\n\ndef poly_eval(poly, rho):\n    \"\"\" Just polynomial evaluation in R. \"\"\"\n    return poly(rho)\n\ndef kzg_eval_proof(poly, rho, val):\n    \"\"\"\n    Build q(x) = (f(x) - val)/(x-rho).\n    Return c_q = q(alpha) as the \"proof\".\n    \"\"\"\n    # f(x) - val\n    poly_minus = poly - val\n    # polynomial division\n    q = poly_minus // (x - rho)\n    # commit to q, i.e. q(alpha)\n    c_q = F(0)\n    for (k, ccoef) in q.dict().items():\n        c_q += ccoef * srs_scalars[k]\n    return (q, c_q)\n\ndef kzg_verify_eval(c_f, c_q, rho, val):\n    \"\"\"\n    Check: (c_f - val) == c_q*(alpha - rho).\n    If that equality holds in F, we say \"KZG => True.\"\n    \"\"\"\n    left  = c_f - val\n    right = c_q*(alpha - rho)\n    return (left == right)\n\n\n########################################################\n# 3) \"Split-and-Fold\" for univariate polynomials\n########################################################\n\ndef split_even_odd(fx):\n    \"\"\"\n    f(x) = f_even(x^2) + x*f_odd(x^2).\n    Return (f_even, f_odd).\n    \"\"\"\n    fe = R(0)\n    fo = R(0)\n    for e, coeff in fx.dict().items():\n        if e % 2 == 0:\n            fe += coeff * x^(e//2)\n        else:\n            fo += coeff * x^((e - 1)//2)\n    return (fe, fo)\n\ndef reconstruct_from_even_odd(fe, fo):\n    \"\"\"\n    Return fe(x^2) + x*fo(x^2).\n    \"\"\"\n    fe_sub = R(0)\n    for e, c in fe.dict().items():\n        fe_sub += c*(x^(2*e))\n    fo_sub = R(0)\n    for e, c in fo.dict().items():\n        fo_sub += c*(x^(2*e))\n    return fe_sub + x*fo_sub\n\ndef fold_polynomial(fe, fo, r):\n    \"\"\"\n    f_new(x) = fe(x) + r * fo(x).\n    \"\"\"\n    return fe + r*fo\n\n\n########################################################\n# 4) Build the chain f^(0)->f^(1)->...->f^(n_rounds).\n########################################################\n\nimport random\n\ndef random_poly_deg_at_most(d):\n    deg = random.randint(0, d)\n    coeffs = [F(random.randint(0, p-1)) for _ in range(deg+1)]\n    return R(coeffs)\n\nn_rounds = 2\n\n# Let's build f^(0) as a random polynomial deg <= D\nf0 = random_poly_deg_at_most(D)\nf_list = [f0]\n\n# pick random fold challenges\nfold_rhos = [F(random.randint(1, p-1)) for _ in range(n_rounds)]\n\nprint(f\"\\n=== Building chain of polynomials f^(0)->f^(1)->...->f^({n_rounds}) ===\")\nfor j in range(1, n_rounds+1):\n    f_prev = f_list[j-1]\n    fe, fo = split_even_odd(f_prev)\n    recon = reconstruct_from_even_odd(fe, fo)\n    check_ok = (recon == f_prev)\n    print(f\"Round j={j}: reconstructed == f^(j-1)? => {check_ok}\")\n    \n    f_new = fold_polynomial(fe, fo, fold_rhos[j-1])\n    f_list.append(f_new)\n\nfor j, fpoly in enumerate(f_list):\n    print(f\"f^({j})(x) = {fpoly}\")\n\n\n########################################################\n# 5) Commit to each f^(j) in the scalar-only KZG\n########################################################\ncomm_list = []\nfor j, polyj in enumerate(f_list):\n    c_j = kzg_commit(polyj)\n    comm_list.append(c_j)\n\nprint(\"\\nKZG Commitments (scalar-only):\")\nfor j, cval in enumerate(comm_list):\n    print(f\"  j={j}: commit = f^(j)(alpha) = {cval}\")\n\n\n########################################################\n# 6) Verifier picks beta. For j < n_rounds, we query f^(j)(beta), f^(j)(-beta), f^(j)(beta^2)\n#    For j=n_rounds, skip f^(n_rounds)(beta^2).\n#    Then do the KZG single-point proofs, verifying the scalar condition.\n########################################################\n\nbeta = F(random.randint(1, p-1))\nprint(f\"\\nVerifier picks beta = {beta}.\")\n\neval_points = []\nfor j in range(n_rounds+1):\n    if j < n_rounds:\n        eval_points.append((j, beta))\n        eval_points.append((j, -beta))\n        eval_points.append((j, beta^2))\n    else:\n        # final polynomial => skip beta^2 or treat as final claim\n        eval_points.append((j, beta))\n        eval_points.append((j, -beta))\n\neval_values = {}\neval_proofs = {}\n\n# Prover side: \nfor (j, pt) in eval_points:\n    val = f_list[j](pt)\n    eval_values[(j, pt)] = val\n    # produce proof\n    _, c_q = kzg_eval_proof(f_list[j], pt, val)\n    eval_proofs[(j, pt)] = c_q\n\n# Verifier checks:\nprint(\"\\n=== KZG Single-Point Checks (scalar-only) ===\")\nall_ok = True\nfor (j, pt) in eval_points:\n    c_f = comm_list[j]\n    val = eval_values[(j, pt)]\n    c_q = eval_proofs[(j, pt)]\n    \n    check_result = kzg_verify_eval(c_f, c_q, pt, val)\n    print(f\"Check f^(j={j})({pt}) = {val}:  KZG => {check_result}\")\n    if not check_result:\n        all_ok = False\n\nprint()\n########################################################\n# 7) Check the “split-and-fold” consistency with these evaluations\n#    e^(j)(beta^2) == ( e^(j-1)(beta)+ e^(j-1)(-beta) )/2\n#                    + rho_{j-1}*( e^(j-1)(beta)- e^(j-1)(-beta) )/(2*beta)\n########################################################\n\nprint(\"=== Checking the split-and-fold consistency at univariate points ===\")\nfor j in range(1, n_rounds+1):\n    # e^(j)(beta^2) might be missing if j=n_rounds\n    if (j, beta^2) not in eval_values:\n        print(f\"Round j={j}: no (beta^2) evaluation => skip final check.\")\n        continue\n    \n    lhs = eval_values[(j, beta^2)]\n    e_jm1_beta  = eval_values[(j-1, beta)]\n    e_jm1_mbeta = eval_values[(j-1, -beta)]\n    # rhos[j-1] is the fold challenge\n    r = fold_rhos[j-1]\n    \n    rhs = (e_jm1_beta + e_jm1_mbeta)/2  +  r*( e_jm1_beta - e_jm1_mbeta )/(2*beta)\n    eq_ok = (lhs == rhs)\n    print(f\"Round j={j}: LHS={lhs}, RHS={rhs}, match? {eq_ok}\")\n    if not eq_ok:\n        all_ok = False\n\nif all_ok:\n    print(\"\\nAll checks passed successfully!\")\nelse:\n    print(\"\\nSome checks have failed!\")\n","type":"content","url":"/gemini/gemini-01","position":1},{"hierarchy":{"lvl1":"Notes on HyperKZG"},"type":"lvl1","url":"/gemini/hyperkzg-pcs-01","position":0},{"hierarchy":{"lvl1":"Notes on HyperKZG"},"content":"In Gemini-PCS [BCHO22], a coefficient-form MLE polynomial corresponds to a univariate polynomial,\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = f_0 + f_1X_0 + f_2X_1 + f_3X_0X_1 + \\cdots + f_{2^n-1}X_0X_1\\cdots X_{n-1}\n\ncorresponds to a univariate polynomial:f(X) = f_0 + f_1X + f_2X^2 + \\cdots + f_{2^n-1}X^{2^n-1}\n\nWhen we determine a public evaluation point \\vec{u}=(u_0, u_1, \\ldots, u_{n-1}), the value of the MLE polynomial \\tilde{f} at \\vec{u} can be expressed as the following Tensor Product:\\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = \\langle\\vec{f}, \\otimes_{i=0}^{n-1}(1, u_i)\\rangle\n\nNext, Gemini-PCS uses the Split-and-fold approach to convert the above equation into the correctness of evaluations of multiple univariate polynomials, which can be proven using KZG10.\n\nHowever, MLE polynomials are usually represented in point-value form by default,\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{2^n-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nTo use Gemini-PCS, the Prover needs to first convert the point-value form of MLE to the coefficient form mentioned above, i.e., calculate the \\vec{f} vector from the \\vec{a} vector. This conversion algorithm is similar to FFT computation, with a time complexity of O(N\\log{N}), where N=2^n.\n\nThe idea of HyperKZG is to still utilize the core Split-and-fold approach of Gemini-PCS, but without the need for polynomial conversion similar to FFT. This may sound incredible at first, but the key point here is that MLE polynomials are essentially linear polynomials in multidimensional space. Whether in Evaluation-form or Coefficient-form, their computation process is actually a linear operation. At the same time, the Split-and-fold approach adopted by Gemini-PCS is also a mapping process that continuously reduces the dimensionality of high-dimensional space. Therefore, this Split-and-fold process can be transplanted to the Evaluation-form of MLE, achieving the same folding effect, but perfectly avoiding the complex calculations of polynomial Basis conversion.","type":"content","url":"/gemini/hyperkzg-pcs-01","position":1},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"1. Review of Gemini-PCS Principles"},"type":"lvl3","url":"/gemini/hyperkzg-pcs-01#id-1-review-of-gemini-pcs-principles","position":2},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"1. Review of Gemini-PCS Principles"},"content":"Gemini [BCHO22] provides a method for mapping MLE polynomials to univariate polynomials. Below is the definition of an MLE:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = f_0 + f_1X_0 + f_2X_1 + f_3X_0X_1 + \\cdots + f_{2^n-1}X_0X_1\\cdots X_{n-1}\n\nIf we use a coefficient vector \\vec{f} of length N=2^n to represent \\tilde{f}, then we can define a Univariate polynomial f(X) that has the same coefficients as \\tilde{f}(X_0,\\ldots, X_{n-1}):f(X) = f_0 + f_1X + f_2X^2 + \\cdots + f_{2^n-1}X^{2^n-1}\n\nIf we substitute X=-X into the above equation, we get:f(-X) = f_0 - f_1X + f_2X^2 - \\cdots - f_{2^n-1}X^{2^n-1}\n\nThen by adding and subtracting these two equations respectively, we can get:f(X) + f(-X) = 2(f_0 + f_2X^2 + \\cdots + f_{2^n-2}X^{2^n-2})\\begin{split}\nf(X) - f(-X) & = 2X(f_1 + f_3X^2 + \\cdots + f_{2^n-1}X^{2^n-2}) \\\\\n\\end{split}\n\nNow let’s observe the “partial computation” Partial Evaluation of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}), i.e., instantiating only one unknown X_0=u_0:\\begin{split}\n\\tilde{f}({\\color{red}u_0}, X_1, \\ldots, X_{n-1}) &= f_0 + f_1{\\color{red}u_0} + f_2X_1 + f_3{\\color{red}u_0}X_1 + f_4X_2 + f_5{\\color{red}u_0}X_2 + f_6X_1X_2 + f_7{\\color{red}u_0}X_1X_2 + \\cdots + f_{2^n-1}{\\color{red}u_0}X_1\\cdots X_{n-1}\\\\\n&= (f_0 + f_1{\\color{red}u_0}) + (f_2 + f_3{\\color{red}u_0})X_1 + (f_4 + f_5{\\color{red}u_0})X_2 + (f_6 + f_7{\\color{red}u_0})X_1X_2 + \\cdots + (f_{2^n-2} + f_{2^n-1}{\\color{red}u_0})X_1\\cdots X_{n-1} \\\\\n&= \\tilde{f}^{(1)}(X_1, X_2, \\ldots, X_{n-1}) \\\\\n\\end{split}\n\nHere, the coefficient vector of \\tilde{f}^{(1)}(X_1, X_2, \\ldots, X_{n-1}) is(f_0 + f_1{\\color{red}u_0}), (f_2 + f_3{\\color{red}u_0}), (f_4 + f_5{\\color{red}u_0}), (f_6 + f_7{\\color{red}u_0}), \\ldots, (f_{2^n-2} + f_{2^n-1}{\\color{red}u_0})\n\nIt is exactly the same as the coefficient vector of the following UniPoly:f^{(1)}(X) = (f_0 + f_1{\\color{red}u_0}) + (f_2 + f_3{\\color{red}u_0})X + (f_4 + f_5{\\color{red}u_0})X^2 + \\cdots + (f_{2^n-2} + f_{2^n-1}{\\color{red}u_0})X^{2^{n-1}-1}\n\nAnd the univariate polynomial f^{(1)}(X) plus f(X), f(-X), the three exactly satisfy the following relationship:\\begin{split}\nf^{(1)}(X^2) &= (f_0 + f_1{\\color{red}u_0}) + (f_2 + f_3{\\color{red}u_0})X^2 + (f_4 + f_5{\\color{red}u_0})X^4 + \\cdots + (f_{2^n-2} + f_{2^n-1}{\\color{red}u_0})X^{2^{n}-1} \\\\\n&=\\frac{1}{2}(f(X) + f(-X)) + u_0\\cdot \\frac{1}{2X}(f(X) - f(-X)) \\\\\n\\end{split}\n\nSo, if we want to prove that \\tilde{f}^{(1)}(X_1,\\ldots,X_{n-1}) is the Partial Evaluation of \\tilde{f}(X_0,X_1,\\ldots, X_{n-1}), we only need to prove that the above equation holds.\n\nSimilarly, if we want to prove \\tilde{f}(X_0, X_1, \\ldots,X_{n-1})=v, we can introduce several intermediate results, namely Partial Evaluated MLE polynomials, and their isomorphic mappings to univariate polynomials\\begin{split}\n\\tilde{f}^{(0)}(X_0, X_1, \\ldots, X_{n-1}) & \\mapsto f^{(0)}(X) \\\\\n\\tilde{f}^{(1)}(u_0, X_1, \\ldots, X_{n-1}) & \\mapsto f^{(1)}(X) \\\\\n\\tilde{f}^{(2)}(u_0, u_1, \\ldots, X_{n-1}) & \\mapsto f^{(2)}(X) \\\\\n\\vdots &  \\\\\n\\tilde{f}^{(n-1)}(u_0, u_1, \\ldots,u_{n-2}, X_{n-1}) & \\mapsto f^{(n-1)}(X) \\\\\n\\tilde{f}^{(n)}(u_0, u_1, \\ldots, u_{n-2}, u_{n-1}) & \\mapsto f^{(n)}(X) \\\\\n\\end{split}\n\nwhere the last f^{(n)}(X) is a constant polynomial, which is exactly the complete computation result of \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}), i.e., f^{(n)}(X)=v.\n\nAnd, these introduced univariate polynomials f^{(0)}(X), \\ldots, f^{(n-1)}(X) satisfy the following relationship between every two adjacent items:f^{(i+1)}(X^2) = \\frac{f^{(i)}(X) + f^{(i)}(-X)}{2} + u_i\\cdot \\frac{f^{(i)}(X) - f^{(i)}(-X)}{2X}\n\nwhere \\tilde{f}^{(n)}(u_0, u_1, \\ldots, u_{n-2})=v is the final MLE computation result, and h^{(0)}(X) is the UniPoly isomorphic to \\tilde{f}: h^{(0)}(X)=f(X).\n\nBack to our proof goal: \\tilde{f}^{(n)}(u_0, u_1, \\ldots, u_{n-1})=v, we split the proof of this computation process into the following steps:\n\nConstruct a Univariate polynomial f(X) such that its coefficient vector is equal to the coefficients of \\tilde{f}, and construct the polynomial commitment \\mathsf{cm}(f(X))\n\nThe polynomial \\tilde{f} computation process includes n steps of partial computation, each intermediate partial computation will produce a new MLE polynomial: \\tilde{f}^{(1)}, \\tilde{f}^{(2)}, \\ldots, \\tilde{f}^{(n-1)}\n\nProve that the Univariate polynomials corresponding to these intermediate MLEs satisfy a recursive relationship, which is randomly sampled and checked through the random number \\beta provided by the Verifier:f^{(i+1)}(\\beta^2) = \\frac{f^{(i)}(\\beta) + f^{(i)}(-\\beta)}{2} + u_i\\cdot \\frac{f^{(i)}(\\beta) - f^{(i)}(-\\beta)}{2\\beta}\n\nProve that f^{(n)}(\\beta^2)=v\n\nProve that all Univariate polynomials \\{f^{(i)}(X)\\} are correctly evaluated at X=\\beta,X=-\\beta, X=\\beta^2","type":"content","url":"/gemini/hyperkzg-pcs-01#id-1-review-of-gemini-pcs-principles","position":3},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl2","url":"/gemini/hyperkzg-pcs-01#id-2-linear-folding-of-evaluation-form","position":4},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl2":"2. Linear Folding of Evaluation-form"},"content":"If we use the evaluation-form of MLE in the PIOP proof system, then we need a conversion operation similar to FFT to convert it to the coefficient-form of MLE. The complexity of this conversion operation is O(N\\log{N}).\n\nIn Nova’s implementation, Setty provided an improved scheme for HyperKZG. It utilizes a general technique behind the Gemini PCS scheme, which is independent of whether \\tilde{f} is in Evaluation-form or Coefficient-form, as long as they can split the calculation process into multiple steps of linear operations.\n\nReviewing the MLE operation evaluation proof introduced in the Gemini paper in the previous section, it decomposes the Evaluation process of \\tilde{f}(X_0,X_1,\\ldots,X_{n-1}) into \\log{n} steps, then maps the coefficient vector of each intermediate MLE to the coefficients of a UniPoly, and then proves the relationship between these UniPolys to ensure the correctness of \\tilde{f} operation.\n\nFor an MLE polynomial \\tilde{f}(\\vec{X}) in Evaluation-form, let’s look at its evaluation process:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = a_0E_0(X_0, X_1, \\ldots, X_{n-1}) + a_1E_1(X_0, X_1, \\ldots, X_{n-1}) + \\cdots + a_{2^n-1}E_{2^n-1}(X_0, X_1, \\ldots, X_{n-1})\n\nHere E_i(\\vec{X}) are Lagrange Polynomials, defined as follows:E_i(X_0, X_1, \\ldots, X_{n-1}) = \\prod_{j=0}^{n-1} \\big(\\mathsf{bits}(i)_j\\cdot X_j + (1-\\mathsf{bits}(i)_j)(1-X_j)\\big)\n\nwhere \\mathsf{bits}(i)_j is the j-th bit of the binary representation of i (note that Big-endian representation is used here). For example, if i=5, its binary representation is 101, then \\mathsf{bits}(5)_0=1, \\mathsf{bits}(5)_1=0, \\mathsf{bits}(5)_2=1.fv\n\nIt’s easy to see from the definition that E_i(\\vec{X}) satisfies the following splitting property (Tensor Structure):E_{i\\parallel j}(\\vec{X}, \\vec{Y}) = E_i(\\vec{X})\\cdot E_j(\\vec{Y})\n\nwhere k = i\\parallel j is the splitting of the binary bit vector of k. For example, i=23, its binary representation is 10111, which can be split into 10\\parallel 111, or written as 23=2\\parallel 7. According to the splitting property, we can get:E_{b\\parallel i}(X_0, (X_1, \\ldots, X_{n-1})) = E_b(X_0)\\cdot E_i(X_1, \\ldots, X_{n-1})\n\nThen let’s observe what \\tilde{f} looks like after one Partial Evaluation, let X_0=u_0, \\vec{X}=(X_1, \\ldots, X_{n-1}):\\begin{split}\n\\tilde{f}(u_0, \\vec{X}) &= a_0E_0(u_0, \\vec{X}) + a_1E_1(u_0, \\vec{X}) + \\cdots + a_{2^n-2}E_{2^n-2}(u_0, \\vec{X}) + a_{2^n-1}E_{2^n-1}(u_0, \\vec{X}) \\\\\n&= \\Big(a_0E_0(u_0)\\Big)\\cdot E_{\\color{red}0}(\\vec{X}) + \\Big(a_1E_1(u_0)\\Big)\\cdot E_{\\color{red}0}(\\vec{X}) + \\cdots + \\Big(a_{2^n-1}E_{0}(u_0)\\Big)\\cdot E_{2^{\\color{red}n-1}-1}(\\vec{X})\n+ \\Big(a_{2^n-1}E_1(u_0)\\Big)\\cdot E_{2^{\\color{red}n-1}-1}(\\vec{X}) \\\\\n&= \\Big(a_0(1-u_0)\\Big)\\cdot E_{0}(\\vec{X}) + \\Big(a_1u_0\\Big)\\cdot E_{0}(\\vec{X}) + \\cdots + \\Big(a_{2^n-2}(1-u_0)\\Big)\\cdot E_{2^{n-1}-1}(\\vec{X}) + \\Big(a_{2^n-1}u_0\\Big)\\cdot E_{2^{n-1}-1}(\\vec{X}) \\\\\n&= \\Big(a_0(1-u_0)\\Big)\\cdot E_0(\\vec{X}) + \\Big(a_1u_0\\Big)\\cdot E_0(\\vec{X}) + \\cdots + \\Big(a_{2^n-2}(1-u_0)\\Big)\\cdot E_{2^{n-1}-1}(\\vec{X}) + \\Big(a_{2^n-1}u_0\\Big)\\cdot E_{2^{n-1}-1}(\\vec{X}) \\\\\n&= \\Big(a_0(1-u_0) + a_1u_0\\Big)\\cdot E_0(\\vec{X}) + \\cdots + \\Big(a_{2^n-2}(1-u_0) + a_{2^n-1}u_0\\Big)\\cdot E_{2^{n-1}-1}(\\vec{X}) \\\\\n&= \\tilde{h}^{(1)}(\\vec{X})\n\\end{split}\n\nWe can see that the Evaluation point value vector of \\tilde{h}^{(1)}(\\vec{X}) is:\\Big(a_0(1-u_0) + a_1u_0\\Big), \\Big(a_2(1-u_0) + a_3u_0\\Big), \\ldots, \\Big(a_{2^n-2}(1-u_0) + a_{2^n-1}u_0\\Big)\n\nComparing with the coefficient vector of \\tilde{f}(u_0, \\vec{X}), we will find that both are only half the original length, but the halving method is different. The former is \\big((1-u)\\cdot a + u\\cdot b\\big), while the latter is \\big(a + u\\cdot b\\big). This new halving method does not prevent us from using Gemini-PCS technology to ensure the correctness of Split-and-fold, but perfectly avoids the complex calculations of polynomial Basis conversion.\nWe can still introduce n-1 partially computed MLE polynomials, and map their Evaluations to the coefficients of multiple UniPolys:\\begin{split}\n\\tilde{h}^{(0)}(X_0, X_1, \\ldots, X_{n-1}) & \\mapsto h^{(0)}(X) \\\\\n\\tilde{h}^{(1)}(u_0, X_1, \\ldots, X_{n-1}) & \\mapsto h^{(1)}(X) \\\\\n\\tilde{h}^{(2)}(u_0, u_1, \\ldots, X_{n-1}) & \\mapsto h^{(2)}(X) \\\\\n\\vdots &  \\\\\n\\tilde{h}^{(n-1)}(u_0, u_1, \\ldots, u_{n-1}) & \\mapsto h^{(n-1)}(X) \\\\\n\\end{split}\n\nwhere \\tilde{h}^{(0)}(\\vec{X})=\\tilde{f}(\\vec{X}), and \\tilde{h}^{(n-1)}(u_0, u_1, \\ldots, u_{n-1})=v,\n\nAnd give a “similar” recursive relationship that their Evaluation forms satisfy:h^{(i+1)}(X^2) = (1-u_i)\\cdot \\frac{h^{(i)}(X) + h^{(i)}(-X)}{2} + u_i\\cdot \\frac{h^{(i)}(X) - h^{(i)}(-X)}{2X}\n\nTherefore, the Verifier can then send a unique random challenge point X=\\beta to check whether \\{h^{(i)}(\\beta)\\} satisfy the recursive relationship defined by the above equation. This is where the KZG10 PCS scheme for Univariate polynomials can be connected to complete the rest of the proof.","type":"content","url":"/gemini/hyperkzg-pcs-01#id-2-linear-folding-of-evaluation-form","position":5},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl3","url":"/gemini/hyperkzg-pcs-01#id-3-protocol-description","position":6},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"This section provides a description of the protocol flow. The protocol is to prove that the computation result of an MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) at a given point (u_0, u_1, \\ldots, u_{n-1}) equals v.","type":"content","url":"/gemini/hyperkzg-pcs-01#id-3-protocol-description","position":7},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Witness Input:","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#witness-input","position":8},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Witness Input:","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"\\vec{a}=(a_0, a_1, \\ldots, a_{2^n-1}): The coefficient vector of polynomial f(X).f(X) = a_0 + a_1X + a_2X^2 + \\cdots + a_{2^n-1}X^{2^n-1}","type":"content","url":"/gemini/hyperkzg-pcs-01#witness-input","position":9},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Public Input:","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#public-input","position":10},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Public Input:","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"C_f: The commitment of the isomorphic polynomial f(X) of MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}),C_f = \\mathsf{KZG10.Commit}(f(X))\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1}): The coordinates of the evaluation point\n\nv=\\tilde{f}(u_0, u_1, \\ldots, u_{n-1}): The result of the computation","type":"content","url":"/gemini/hyperkzg-pcs-01#public-input","position":11},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 1","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-1","position":12},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 1","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Prover computes h^{(1)}(X), h^{(2)}(X), \\ldots, h^{(n-1)}(X)\\begin{split}\nh^{(1)}(X) & = ((1-u_0)a_0+u_0a_1) + ((1-u_0)a_2+u_0a_3)X + \\cdots + ((1-u_0)a_{2^n-2}+u_0a_{2^n-1})X^{2^{n-1}-1} \\\\\nh^{(2)}(X) & = ((1-u_1)a^{(1)}_0+u_1a^{(1)}_1) + ((1-u_1)a^{(1)}_2+u_1a^{(1)}_3)X + \\ldots + ((1-u_1)a^{(1)}_{2^{n-1}-2}+u_1a^{(1)}_{2^{n-1}-1})X^{2^{n-2}-1} \\\\\n\\vdots & \\\\\nh^{(n-1)}(X) & = ((1-u_{n-1})a^{(n-2)}_0+u_{n-1}a^{(n-2)}_1) + ((1-u_{n-1})a^{(n-2)}_2+u_{n-1}a^{(n-2)}_3)X \\\\\n\\end{split}\n\nProver outputs commitments \\big(C_{h^{(1)}}, C_{h^{(2)}},\\ldots, C_{h^{(n-1)}}\\big)","type":"content","url":"/gemini/hyperkzg-pcs-01#round-1","position":13},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 2","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-2","position":14},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 2","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Verifier sends random challenge number \\beta\\in\\mathbb{F},\n\nProver computes and sends\n\\Big(h^{(0)}(\\beta), h^{(0)}(-\\beta)\\Big),\n\\Big(h^{(1)}(\\beta), h^{(1)}(-\\beta), h^{(1)}(\\beta^2)\\Big),\n\\Big(h^{(2)}(\\beta), h^{(2)}(-\\beta), h^{(2)}(\\beta^2)\\Big), \\ldots,\n\\Big(h^{(n-2)}(\\beta), h^{(n-2)}(-\\beta), h^{(n-2)}(\\beta^2)\\Big)\n\nProver proves the correctness of the above Evaluations and sends proofs: \\Big(\\pi_{0,\\beta}, \\pi_{0,-\\beta}\\Big),\n\\Big(\\pi_{1,\\beta}, \\pi_{1,-\\beta}, \\pi_{1,\\beta^2}\\Big), \\ldots, \\Big(\\pi_{n-1,\\beta}, \\pi_{n-1,-\\beta}, \\pi_{n-1,\\beta^2}\\Big)","type":"content","url":"/gemini/hyperkzg-pcs-01#round-2","position":15},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Verification","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#verification","position":16},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Verification","lvl3":"3. Protocol Description","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Verify if the values of h^{(0)}, \\ldots, h^{(n-1)} at X=\\beta, X=-\\beta and X=\\beta^2 satisfy the recursive formula:\\begin{split}\nh^{(1)}(\\beta^2) &\\overset{?}{=} (1-u_0)\\cdot \\frac{h^{(0)}(\\beta) + h^{(0)}(-\\beta)}{2} + u_0\\cdot \\frac{h^{(0)}(\\beta) - h^{(0)}(-\\beta)}{2\\beta} \\\\\nh^{(2)}(\\beta^2) &\\overset{?}{=} (1-u_1)\\cdot \\frac{h^{(1)}(\\beta) + h^{(1)}(-\\beta)}{2} + u_1\\cdot \\frac{h^{(1)}(\\beta) - h^{(1)}(-\\beta)}{2\\beta} \\\\\n\\vdots & \\\\\nh^{(n-1)}(\\beta^2) &\\overset{?}{=} (1-u_{n-2})\\cdot\\frac{h^{(n-2)}(\\beta) + h^{(n-2)}(-\\beta)}{2} + u_{n-2}\\cdot \\frac{h^{(n-2)}(\\beta) - h^{(n-2)}(-\\beta)}{2\\beta} \\\\\n v &\\overset{?}{=} (1-u_{n-1})\\cdot \\frac{h^{(n-1)}(\\beta) + h^{(n-1)}(-\\beta)}{2} + u_{n-1}\\cdot \\frac{h^{(n-1)}(\\beta) - h^{(n-1)}(-\\beta)}{2\\beta} \\\\\n\\end{split}\n\nBased on C_f, C_{h^{(1)}}, C_{h^{(2)}},\\ldots, C_{h^{(n-1)}}, verify the correctness of polynomial evaluation operations:\\begin{array}{lllll}\n\\mathsf{KZG10.Verify}&(C_f, &\\beta, &h^{(0)}(\\beta), &\\pi_{0,\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_f, &-\\beta, &h^{(0)}(-\\beta), &\\pi_{0,-\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(1)}}, &\\beta, &h^{(1)}(\\beta), &\\pi_{1,\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(1)}}, &-\\beta, &h^{(1)}(-\\beta), &\\pi_{1,-\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(1)}}, &\\beta^2, &h^{(1)}(\\beta^2), &\\pi_{1,\\beta^2}) &\\overset{?}{=} 1 \\\\\n& & \\vdots \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(n-1)}}, &\\beta, &h^{(n-1)}(\\beta), &\\pi_{n-1,\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(n-1)}}, &-\\beta, &h^{(n-1)}(-\\beta), &\\pi_{n-1,-\\beta}) &\\overset{?}{=} 1 \\\\\n\\mathsf{KZG10.Verify}&(C_{h^{(n-1)}}, &\\beta^2, &h^{(n-1)}(\\beta^2), &\\pi_{n-1,\\beta^2}) &\\overset{?}{=} 1 \\\\\n\\end{array}","type":"content","url":"/gemini/hyperkzg-pcs-01#verification","position":17},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl3","url":"/gemini/hyperkzg-pcs-01#id-4-protocol-optimization","position":18},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"There are several points that can be optimized in the above protocol:\n\nThe Prover doesn’t need to send all computation values at X=\\beta^2 except for h^{(0)}(\\beta^2), because the Verifier can calculate them through the following recursive relationship. This can reduce the Prover’s communication volume, and at the same time, the Verifier’s calculation process is equivalent to performing verification simultaneously, thus saving the recursive formula verification process.h^{(i+1)}(\\beta^2) = \\frac{h^{(i)}(\\beta) + h^{(i)}(-\\beta)}{2} + u_i\\cdot \\frac{h^{(i)}(\\beta) - h^{(i)}(-\\beta)}{2\\beta}\n\nThe Prover can aggregate h^{(0)}(X),\\ldots, h^{(n-1)}(X) together through a random number \\gamma, obtaining h(X), and then prove the values of h(X) at X=\\beta,-\\beta, \\beta^2. This can avoid the Prover sending 3n-1 independent KZG10 Evaluation proofs, and only need to send three Evaluation proofs.\n\nBelow is the protocol description after optimization","type":"content","url":"/gemini/hyperkzg-pcs-01#id-4-protocol-optimization","position":19},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 1","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-1-1","position":20},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 1","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Prover computes h^{(1)}(X), h^{(2)}(X), \\ldots, h^{(n-1)}(X)\\begin{split}\nh^{(1)}(X) & = ((1-u_0)a_0+u_0a_1) + ((1-u_0)a_2+u_0a_3)X + \\cdots + ((1-u_0)a_{2^n-2}+u_0a_{2^n-1})X^{2^{n-1}-1} \\\\\nh^{(2)}(X) & = ((1-u_1)a^{(1)}_0+u_1a^{(1)}_1) + ((1-u_1)a^{(1)}_2+u_1a^{(1)}_3)X + \\ldots + ((1-u_1)a^{(1)}_{2^{n-1}-2}+u_1a^{(1)}_{2^{n-1}-1})X^{2^{n-2}-1} \\\\\n\\vdots & \\\\\nh^{(n-1)}(X) & = ((1-u_{n-1})a^{(n-2)}_0+u_{n-1}a^{(n-2)}_1) + ((1-u_{n-1})a^{(n-2)}_2+u_{n-1}a^{(n-2)}_3)X \\\\\n\\end{split}\n\nProver sends polynomial commitments \\big(C_{h^{(1)}}, C_{h^{(2)}},\\ldots, C_{h^{(n-1)}}\\big)","type":"content","url":"/gemini/hyperkzg-pcs-01#round-1-1","position":21},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 2","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-2-1","position":22},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 2","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Verifier sends random challenge number \\beta\\in\\mathbb{F},\n\nProver computes and sends\n\\Big(h^{(0)}(\\beta), h^{(0)}(-\\beta), {\\color{blue}h^{(0)(\\beta^2)}}\\Big),\n\\Big(h^{(1)}(\\beta), h^{(1)}(-\\beta)\\Big),\n\\Big(h^{(2)}(\\beta), h^{(2)}(-\\beta)\\Big), \\ldots,\n\\Big(h^{(n-1)}(\\beta), h^{(n-1)}(-\\beta)\\Big)","type":"content","url":"/gemini/hyperkzg-pcs-01#round-2-1","position":23},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 3","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-3","position":24},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 3","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Verifier sends random challenge number \\gamma\\in\\mathbb{F},\n\nProver computes aggregated polynomial h(X),h(X) = h^{(0)}(X) + \\gamma\\cdot h^{(1)}(X) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(X)\n\nProver computes v_{\\beta}, v_{-\\beta}, v_{\\beta^2}\\begin{split}\nv_{\\beta} & = h^{(0)}(\\beta) + \\gamma\\cdot h^{(1)}(\\beta) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(\\beta) \\\\\nv_{-\\beta} & = h^{(0)}(-\\beta) + \\gamma\\cdot h^{(1)}(-\\beta) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(-\\beta) \\\\\nv_{\\beta^2} & = h^{(0)}(\\beta^2) + \\gamma\\cdot h^{(1)}(\\beta^2) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(\\beta^2) \\\\\n\\end{split}\n\nProver computes h^*(x)h^*(X) = h(\\beta)\\cdot L_\\beta(X) + h(-\\beta)\\cdot L_{-\\beta}(X) + h(\\beta^2)\\cdot L_{\\beta^2}(X)\n\nHere, assume Domain D=\\{\\beta, -\\beta, \\beta^2\\}, and \\{L_\\beta(X), L_{-\\beta}(X), L_{\\beta^2}(X)\\} are Lagrange Polynomials on D. Then h^*(X) satisfies the following equation: there exists a quotient polynomial q(X) such that\\mathsf{EQ1}: h(X) - h^*(X) = q(X)\\cdot(X-\\beta)(X+\\beta)(X-\\beta^2)\n\nProver computes quotient polynomial q(X) and sends its polynomial commitment C_q = \\mathsf{cm}(q(X))","type":"content","url":"/gemini/hyperkzg-pcs-01#round-3","position":25},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 4","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#round-4","position":26},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Round 4","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Verifier sends random challenge number \\zeta\\in\\mathbb{F}_p\n\nProver computes linearization polynomial r_\\zeta(X) of \\mathsf{EQ1}, satisfying r_\\zeta(\\zeta)=0,r_\\zeta(X) = h(X) - h^*(\\zeta) - q(X)\\cdot(\\zeta-\\beta)(\\zeta+\\beta)(\\zeta-\\beta^2)\n\nProver sends the commitment of the linearization polynomial C_r=[r(x)]_1\n\nProver computes quotient polynomial w(X) satisfying:w(X) = \\frac{r_\\zeta(X)}{X-\\zeta}\n\nProver sends C_w=[w(x)]_1","type":"content","url":"/gemini/hyperkzg-pcs-01#round-4","position":27},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Verification","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl4","url":"/gemini/hyperkzg-pcs-01#verification-1","position":28},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl4":"Verification","lvl3":"4. Protocol Optimization","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Compute \\Big(h^{(1)}(\\beta^2), h^{(2)}(\\beta^2), \\ldots, h^{(n-1)}(\\beta^2)\\Big),\\begin{split}\nh^{(1)}(\\beta^2) &= (1-u_0)\\cdot\\frac{h^{(0)}(\\beta) + h^{(0)}(-\\beta)}{2} + u_0\\cdot \\frac{h^{(0)}(\\beta) - h^{(0)}(-\\beta)}{2\\beta} \\\\\nh^{(2)}(\\beta^2) &= (1-u_1)\\cdot\\frac{h^{(1)}(\\beta) + h^{(1)}(-\\beta)}{2} + u_1\\cdot \\frac{h^{(1)}(\\beta) - h^{(1)}(-\\beta)}{2\\beta} \\\\\n\\vdots & \\\\\nh^{(n-1)}(\\beta^2) &=(1-u_{n-2})\\cdot\\frac{h^{(n-2)}(\\beta) + h^{(n-2)}(-\\beta)}{2} + u_{n-2}\\cdot \\frac{h^{(n-2)}(\\beta) - h^{(n-2)}(-\\beta)}{2\\beta} \\\\\n\\end{split}\n\nCompute h(\\beta), h(-\\beta), h(\\beta^2),h(\\beta) = h^{(0)}(\\beta) + \\gamma\\cdot h^{(1)}(\\beta) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(\\beta) \\\\\nh(-\\beta) = h^{(0)}(-\\beta) + \\gamma\\cdot h^{(1)}(-\\beta) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(-\\beta) \\\\\nh(\\beta^2) = h^{(0)}(\\beta^2) + \\gamma\\cdot h^{(1)}(\\beta^2) + \\cdots + \\gamma^{n-1}\\cdot h^{(n-1)}(\\beta^2) \\\\\n\nCompute the value of c(X) at X=\\zeta, c(\\zeta),\n\nCompute C_h=C_f+\\gamma\\cdot C_{h^{(1)}}+\\gamma^2\\cdot C_{h^{(2)}}+\\cdots+\\gamma^{n-1}\\cdot C_{h^{(n-1)}}\n\nCompute the commitment of C_r = [r_\\zeta(x)]_1:C_r = [r_\\zeta(x)]_1 = C_h - c(\\zeta)\\cdot [1]_1 - (\\zeta-\\beta)(\\zeta+\\beta)(\\zeta-\\beta^2)\\cdot C_q\n\nVerify the relationship between C_h and C_w:e(C_r + \\zeta\\cdot C_w, [1]_2) \\overset{?}{=} e(C_w, [x]_2)","type":"content","url":"/gemini/hyperkzg-pcs-01#verification-1","position":29},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"5. Performance Analysis","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl3","url":"/gemini/hyperkzg-pcs-01#id-5-performance-analysis","position":30},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"5. Performance Analysis","lvl2":"2. Linear Folding of Evaluation-form"},"content":"Proof size: (n+1)\\cdot\\mathbb{G}_1 + (2n+1)\\cdot\\mathbb{F}\\pi=\\big(C_{h^{(1)}}, C_{h^{(2)}},\\ldots, C_{h^{(n-1)}}, C_q, C_w, \\{h^{(i)}(\\beta), h^{(i)}(-\\beta)\\}_{i=0}^{n-1}, h^{(0)}(\\beta^2) \\big)\n\nVerifier Cost: (2n+2)\\cdot\\mathsf{EccMul}^{\\mathbb{G}_1} + (3n)\\cdot\\mathbb{F} + 2\\cdot\\mathsf{Pairing}\n\n2n\\cdot\\mathbb{F}.\\mathsf{Mult}\n\nCompute h(\\beta),h(-\\beta), h(\\beta^2)： 3n\\cdot\\mathbb{F}.\\mathsf{Mult}\n\nCompute c(\\zeta)： O(1)\\cdot\\mathbb{F}.\\mathsf{Mult}\n\nCompute C_h: n\\cdot\\mathbb{G}_1 Scalar Multiplication\n\nCompute P: 2\\cdot\\mathbb{G}_1 Scalar Multiplication","type":"content","url":"/gemini/hyperkzg-pcs-01#id-5-performance-analysis","position":31},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"References","lvl2":"2. Linear Folding of Evaluation-form"},"type":"lvl3","url":"/gemini/hyperkzg-pcs-01#references","position":32},{"hierarchy":{"lvl1":"Notes on HyperKZG","lvl3":"References","lvl2":"2. Linear Folding of Evaluation-form"},"content":"","type":"content","url":"/gemini/hyperkzg-pcs-01#references","position":33},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/gemini/hyperkzg","position":0},{"hierarchy":{"lvl1":""},"content":"The goal of this notebook is to show the working of HyperKZG PCS, and how it is better than Gemini PCS, when it comes to MLE polynomials provided in evaluation format.\n\nTo run this notebook, you need to run the following commands:\nconda create -n sage sage python=3.12.5\nconda activate sage\nsage -n jupyter\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\n# Coefficients of the polynomial\na = [var('a{}'.format(i)) for i in range(N)] # Coefficients a_0, a_1, ...., a_(N-1)\n\n# MLE polynomial\nf_tilde = sum(a[i]*eq_tilde(bits(i,n), X) for i in range(N))\n\nfor term in f_tilde.operands():\n    print(term)\n\n        \n# Generate all combinations of (1-u[i]) and u[i] based on binary representation\ndef generate_c_vector(n, u):\n    c_vector = []\n    for i in range(2^n):  # Loop over all binary numbers from 0 to 2^n - 1\n        binary = list(map(int, format(i, f'0{n}b')))  # Binary representation of i\n        binary_reverse = list(reversed(binary))  # Reverse the binary representation\n        product = 1\n        for j, bit in enumerate(binary_reverse):\n            if bit == 0:\n                product *= (1 - u[j])  # Use (1 - u[j]) for 0\n            else:\n                product *= u[j]  # Use u[j] for 1\n        c_vector.append(product)\n    return c_vector\n\n# Compute the c vector\nc = generate_c_vector(n, u)\n\n# Display the c vector\nshow(c)\n\n# Create a 3D plot of f_tilde\n# First, let's create a numerical version of f_tilde by substituting some values for coefficients\nimport numpy as np\n\n# Set some random coefficients for visualization\ncoeffs = [1, 2, 3, 4, 5, 6, 7, 8]  # 8 coefficients for n=3\n\n# Create a numerical function\ndef f_tilde_numerical(x0, x1, x2):\n    result = 0\n    for i in range(8):\n        bits_i = bits(i, 3)\n        result += coeffs[i] * eq_tilde(bits_i, [x0, x1, x2])\n    return result\n\n# Create points for all evaluations\npoints = []\nfor i in range(8):\n    bits_i = bits(i, 3)\n    x, y, z = bits_i\n    points.append((x, y, z, coeffs[i]))\n\n# Create point plot with axes labels\npoint_plot = point3d([(p[0], p[1], p[2]) for p in points], size=20, axes_labels=['X0', 'X1', 'X2'])\n\n# Create text labels with offset, only showing coordinate and a value\ntext_plots = []\n# offset = 0.22  # Spacing for clarity\n# fontsize = 16  # Large and readable\n\nfor i, p in enumerate(points):\n    label = f\"({p[0]},{p[1]},{p[2]})\\n\" + f\"a{i}={p[3]}\"\n    # If x=1, place label to the right; else to the left\n    if p[0] == 1:\n        label_pos = (p[0] , p[1], p[2])\n    else:\n        label_pos = (p[0] , p[1], p[2])\n    text_plots.append(text3d(label, label_pos, fontsize=20))\n\n# Combine all text labels into a single plot (no blue dots)\nfinal_plot = sum(text_plots)\nfinal_plot.show()\n\n# --- Separate a-vector display section ---\na_vector_text = \"a = [\" + \", \".join([f\"{c}\" for c in coeffs]) + \"]\"\nshow(html(f\"<pre style='font-size:14px'>{a_vector_text}</pre>\"))\n\nSo we have the evaluation vector with us which represents the evaluation of a boolean hypercube at individual points. For n=3, we have the points from 000 all the way up to 111.\n\nNow, in the Gemini Protocol, we take this evaluation vector, and convert this to the coefficient-value form using FFT/IFFT.\nThe followind code snippet shows that\n\nimport sys\nimport os\n!pip install -q py-ecc\n\nsrc_path = os.path.abspath(os.path.join(\"..\", \"src\"))\nif src_path not in sys.path:\n    sys.path.append(src_path)\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\ndef generate_c_vector(n, u):\n    c_vector = []\n    for i in range(2^n):  # Loop over all binary numbers from 0 to 2^n - 1\n        binary = list(map(int, format(i, f'0{n}b')))  # Binary representation of i\n        binary_reverse = list(reversed(binary))  # Reverse the binary representation\n        product = 1\n        for j, bit in enumerate(binary_reverse):\n            if bit == 0:\n                product *= (1 - u[j])  # Use (1 - u[j]) for 0\n            else:\n                product *= u[j]  # Use u[j] for 1\n        c_vector.append(product)\n    return c_vector\n\nfrom mle2 import MLEPolynomial\nfrom curve import Fr as BN254_Fr\n\nMLEPolynomial.set_field_type(BN254_Fr)\n\nevals = [1, 2, 3, 4, 5, 6, 7, 8]\nprint(\"Original evaluation at individual points\")\nprint(f\"evals = {[str(x) for x in evals]}\\n\")\n\nevals = [BN254_Fr(int(x)) for x in evals]\ncoeffs = MLEPolynomial.compute_coeffs_from_evals(evals)\nprint(\"Coefficients after NTT:\")\nprint(f\"coeffs = {[str(x) for x in coeffs]}\\n\")\n\nprint(\"What does this mean?\\n\")\n\n# Step 1: Setup for symbolic MLE polynomial\nN = len(evals)\nn = 3  # log2(N)\nX = var(['X{}'.format(i) for i in range(n)])\n\nprint(\"MLE polynomial in evaluation form:\\n\")\nfor i in range(N):\n    term = eq_tilde(bits(i, n), X)\n    print(f\"a[{i}] = {evals[i]} => term = {evals[i]}*({term})\")\n\nprint(\"\\ngets converted to the coefficient form, which is:\\n\")\n\n\nprint(\"Symbolic monomial basis (c_i * monomial):\")\nfor i in range(len(coeffs)):\n    i_bits = bits(i, n)\n    monomial_terms = [f\"X{j}\" for j, b in enumerate(i_bits) if b == 1]\n    monomial = \"*\".join(monomial_terms) if monomial_terms else \"1\"\n    print(f\"c{i} * {monomial}\")\n\n\nprint(\"\\nReal-valued coefficient vector:\")\nprint(f\"coeffs = {[str(c) for c in coeffs]}\")\n\n\nprint(\"\\nFull coefficient form (including zero coefficients):\")\nfor i in range(len(coeffs)):\n    mon_bits = bits(i, n)\n    monomial = \"*\".join([f\"X{j}\" if b == 1 else \"1\" for j, b in enumerate(mon_bits)])\n    coeff = coeffs[i]\n    print(f\"coeffs[{i}] = {coeff} => term = {coeff}*{monomial}\")\n\nprint(\"\\n\\\\ie. the coefficient form of the same multilinear extension\")\n\nFrom the above code snippet, it can be seen that conversion from Evaluation vector to Coefficient Vector requires the use of ntt_core function which takes O(NlogN) time.\n\nThe next step is mapping this is coefficient vector i.e. ['1', '1', '2', '0', '4', '0', '0', '0'] to a univariate polynomial and then use the split-and-fold technique thereafter. There is where the difference between Gemini and HyperKZG comes in.\n\nSo, if we already have a coefficient form of a MLE-polynomial, we won’t need the conversion from evaluation format. But what if we are presented with the evaluation form of the MLE polynomial. In case of Gemini, we will always be needing to first convert it to the coefficient form and then do the mapping. Instead, in the case of HyperKZG we do not need this conversion, therefore, be it evaluation form or coefficient form of the MLE, there is no effect on the time for proof generation.\n\nLet’s see how this works.\n\nimport sys\nimport os\n!pip install -q py-ecc\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\nfrom mle2 import MLEPolynomial\nfrom curve import Fr as BN254_Fr\n\nMLEPolynomial.set_field_type(BN254_Fr)\nevals = [1, 2, 3, 4, 5, 6, 7, 8]  \n\nevals = [BN254_Fr(int(x)) for x in evals] \ncoeffs = MLEPolynomial.compute_coeffs_from_evals(evals)\n# print(\"Coefficients after NTT:\")\n\n\nprint(\"Gemini: Mapping the coefficient vector to a unvariate polynomial\\n\")\n\nprint(f\"coeffs = {[str(x) for x in coeffs]}\\n\")\n\ndef uni_eval_from_coeffs(coeffs, z):\n    return sum(int(coeffs[i]) * z**i for i in range(len(coeffs)))\n\nx = var('X')  # Symbolic variable for univariate polynomial\nunivariate_poly_gemini = uni_eval_from_coeffs(coeffs, x)\n\n# 1. Symbolic representation (using c0, c1, ..., c7)\nsymbolic_terms = [f\"c{i}*X^{i}\" for i in range(len(coeffs))]\nsymbolic_poly_str = \" + \".join(symbolic_terms)\nprint(\"Symbolic coefficient form (before assigning actual values):\")\nprint(symbolic_poly_str)\n\n# 2. Actual coefficient vector\nprint(\"\\nReal-valued coefficient vector:\")\nprint(f\"coeffs = {[str(x) for x in coeffs]}\")\n\n# 3. Final univariate polynomial in explicit form\nterms = [f\"{coeffs[i]}*X^{i}\" for i in range(len(coeffs))]\npoly_str = \" + \".join(terms)\nprint(\"\\nUnivariate polynomial (explicit form with zero coeffs):\")\nprint(poly_str)\n\n# 4. Evaluatable symbolic polynomial expression (optional, for display)\nx = var('X')  # symbolic variable\nunivariate_poly_gemini = sum(int(coeffs[i]) * x**i for i in range(len(coeffs)))\nprint(\"\\nSimplified symbolic expression (optional):\")\nprint(univariate_poly_gemini)\nprint(\"\\n\")\nprint(\"HyperKZG: Mapping the evaluation vector to a unvariate polynomial\\n\")\nprint(\"Original evaluation at individual points\")\nprint(f\"evals = {[str(x) for x in evals]}\\n\")\nunivariate_poly_hyperKZG = uni_eval_from_coeffs(evals, x)\nprint(univariate_poly_hyperKZG)\n\n\nimport sys\nimport os\n!pip install -q py-ecc\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\nfrom mle2 import MLEPolynomial\nfrom curve import Fr as BN254_Fr\n\nMLEPolynomial.set_field_type(BN254_Fr)\nevals = [1, 2, 3, 4, 5, 6, 7, 8]  \n\nevals = [BN254_Fr(int(x)) for x in evals] \ncoeffs = MLEPolynomial.compute_coeffs_from_evals(evals)\n# print(\"Coefficients after NTT:\")\n\n\nprint(\"Gemini: Mapping the coefficient vector to a unvariate polynomial\\n\")\n\nprint(f\"coeffs = {[str(x) for x in coeffs]}\\n\")\n\ndef uni_eval_from_coeffs(coeffs, z):\n    return sum(int(coeffs[i]) * z**i for i in range(len(coeffs)))\n\nx = var('X')  # Symbolic variable for univariate polynomial\nunivariate_poly_gemini = uni_eval_from_coeffs(coeffs, x)\n\n# 1. Symbolic representation (using c0, c1, ..., c7)\nsymbolic_terms = [f\"c{i}*X^{i}\" for i in range(len(coeffs))]\nsymbolic_poly_str = \" + \".join(symbolic_terms)\nprint(\"Symbolic coefficient form (before assigning actual values):\")\nprint(symbolic_poly_str)\n\n# 2. Actual coefficient vector\nprint(\"\\nReal-valued coefficient vector:\")\nprint(f\"coeffs = {[str(x) for x in coeffs]}\")\n\n# 3. Final univariate polynomial in explicit form\nterms = [f\"{coeffs[i]}*X^{i}\" for i in range(len(coeffs))]\npoly_str = \" + \".join(terms)\nprint(\"\\nUnivariate polynomial (explicit form with zero coeffs):\")\nprint(poly_str)\n\n# 4. Evaluatable symbolic polynomial expression (optional, for display)\nx = var('X')  # symbolic variable\nunivariate_poly_gemini = sum(int(coeffs[i]) * x**i for i in range(len(coeffs)))\nprint(\"\\nSimplified symbolic expression (optional):\")\nprint(univariate_poly_gemini)\nprint(\"\\n\")\nprint(\"HyperKZG: Mapping the evaluation vector to a unvariate polynomial\\n\")\nprint(\"Original evaluation at individual points\")\nprint(f\"evals = {[str(x) for x in evals]}\\n\")\nunivariate_poly_hyperKZG = uni_eval_from_coeffs(evals, x)\nprint(univariate_poly_hyperKZG)\n\n\nAs it can be seen, we are mapping the coefficient vector to univariate polynomial for Gemini, and for HyperKZG, we use the evaluation vector i.e. [1, 2, 3, 4, 5, 6, 7, 8]\n\n                                    --------------------------------------------\n            ``HOW and WHY can we map the MLE Evaluation form and Coefficient form to a Univariate Polynomial ?``\n\n## Explanation to be added soon\n\nSo we have both our polynomials in the univariate form, from Evaluation to UniPoly as well as from Coefficients to UniPoly. Next step is to show how we can reduce down these univariate polynomials through the split-and-fold technique.\n\nLet’s see how this works:\n\nimport sys\nimport os\n!pip install -q py-ecc\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\nfrom mle2 import MLEPolynomial\nfrom curve import Fr as BN254_Fr\nfrom hyperkzg_pcs import HYPERKZG_PCS                        \nfrom unipoly import UniPolynomial\nfrom curve import Fr as Field\n\nprint(\"\\nCreating multilinear polynomial f and univariate polynomial g(X)...\")\n\n# 1) multilinear polynomial f  (from the evaluation vector we already have)\nf_mle = MLEPolynomial(evals,3)                # BN254 field is already set\n\n# 2) univariate poly g(X)  =  uₙ(evals)\ng_uni = UniPolynomial(evals)                # coefficients are evals[i]\n\nprint(\"\\nUnivariate poly via HYPERKZG map   g(X) =\")\nprint(g_uni)              \n\nx = var('X')                      # univariate indeterminate\ndef vec_poly(vec):                # vector → symbolic poly  P(X)=Σv[i]·X^i\n    return sum(int(v) * x**i for i, v in enumerate(vec))\n\ndef official_fold_verbose(f_eval_vec, u_vec):\n    print(\"\\n_____ HyperKZG fold __________________________________\")\n    h = f_eval_vec[:]                                     # h⁰\n    print(\"h⁰ =\", [int(v) for v in h])\n\n    for k, u_k in enumerate(u_vec):\n        print(f\"\\nStep {k+1}:  u_{k} = {int(u_k)}\")\n        h_even = h[::2]\n        h_odd  = h[1::2]\n        print(\"  h_even =\", [int(v) for v in h_even])\n        print(\"  h_odd  =\", [int(v) for v in h_odd])\n\n        # build h_next element-by-element and show each computation\n        h_next = []\n        for idx, (a, b) in enumerate(zip(h_even, h_odd)):\n            new_val = a + u_k * (b - a)\n            h_next.append(new_val)\n            print(f\"   h_next[{idx}] = {int(a)} + {int(u_k)}·({int(b)}−{int(a)}) = {int(new_val)}\")\n\n        print(\"  h_next =\", [int(v) for v in h_next])\n\n        # Eq.(24) verification\n        h_i_expr   = vec_poly(h)\n        h_ip1_expr = vec_poly(h_next)\n        h_i_neg    = h_i_expr.subs({x: -x})\n        u_int      = int(u_k)\n        rhs        = expand((1-u_int)*(h_i_expr + h_i_neg)/2 +\n                            u_int     *(h_i_expr - h_i_neg)/(2*x))\n        lhs        = h_ip1_expr.subs({x: x**2})\n        print(\"  LHS = h^{%d}(X²) =\" % (k+1), lhs)\n        print(\"  RHS              =\", rhs)\n        print(\"  LHS − RHS        =\", expand(lhs - rhs))\n\n        h = h_next                                       # advance to next stage\n\n    print(\"\\nFinal folded constant =\", int(h[0]))\n    return h[0]\n\ndef bits_le(i, n=3):\n    return [(i >> j) & 1 for j in range(n)]\n\ndef delta(bits_i, u_vec):\n    one = Field(int(1))\n    out = Field(int(1))\n    for j, (bit, u) in enumerate(zip(bits_i, u_vec)):\n        bit_f = Field(int(bit))\n        term = (one - bit_f) * (one - u) + bit_f * u\n        out  *= term\n    return out\n\n\n# choose challenge points\nu_vals = [Field(int(v)) for v in (2, 5, 3)]\nprint(\"\\nChallenge point vector u =\", [int(v) for v in u_vals])\n\n# run folding\nfold_constant = official_fold_verbose(evals, u_vals)\n\n# direct MLE evaluation \nprint(\"\\nComputing direct evaluation of f(u) using δ̃...\")\ndirect = Field(int(0))\nfor i, e in enumerate(evals):\n    b = bits_le(i)\n    d = delta(b, u_vals)\n    term = e * d\n    direct += term\nprint(direct)\n\nprint(\"\\n========= Final Check =========\")\nprint(\"Constant after folds =\", int(fold_constant))\nprint(\"Direct f(u)          =\", int(direct))\nprint(\"Match?               →\", fold_constant == direct)\n\n\nimport sys, os\n!pip install -q py-ecc\n\n\nfrom curve     import Fr  as Field\nfrom mle2      import MLEPolynomial\nfrom bcho_pcs  import BCHO_PCS           \nfrom unipoly   import UniPolynomial\nfrom curve     import Fr as BN254_Fr\n\n\nn = 3\nx = var('X')\n\ndef bits_le(i, n=n):\n    \"\"\"little-endian bits list\"\"\"\n    return [(i >> j) & 1 for j in range(n)]\n\ndef vec_poly(vec):\n    \"\"\"vector → Σ vec[i]·X^i\"\"\"\n    return sum(int(v)*x**i for i,v in enumerate(vec))\n\ndef delta(bits_i, u_vec):\n    \"\"\"δ̃_i(u) in the BN254 field\"\"\"\n    one = Field(int(1))\n    out = Field(int(1))\n    for bit, u in zip(bits_i, u_vec):\n        bit_f = Field(int(bit))\n        out  *= (one-bit_f)*(one-u) + bit_f*u\n    return out\n\n\nevals  = [Field(int(v)) for v in (1,2,3,4,5,6,7,8)]\nprint(\"evaluations=\",evals)\n\nMLEPolynomial.set_field_type(BN254_Fr)\ncoeffs = MLEPolynomial.compute_coeffs_from_evals(evals)  \nprint(\"coeffs =\", [int(c) for c in coeffs])\n\ng_uni  = UniPolynomial(coeffs)                 \nprint(\"\\nUnivariate poly P(X) =\", g_uni)\n\n\ndef gemini_fold_verbose(c_vec, u_vec):\n    print(\"\\n________  Gemini split-and-fold  ________\")\n    c = c_vec[:]                                         \n    print(\"c⁰ =\", [int(v) for v in c])\n\n    for k, u_k in enumerate(u_vec):\n        print(f\"\\nStep {k+1}, u_{k} = {int(u_k)}\")\n\n        # split\n        c_even, c_odd = c[::2], c[1::2]\n        print(\"  c_even =\", [int(v) for v in c_even])\n        print(\"  c_odd  =\", [int(v) for v in c_odd])\n\n        # update  c_next[i] = c_even[i] + u·c_odd[i]\n        c_next = []\n        for idx, (a, b) in enumerate(zip(c_even, c_odd)):\n            new = a + u_k * b\n            c_next.append(new)\n            print(f\"   c_next[{idx}] = {int(a)} + {int(u_k)}·{int(b)} = {int(new)}\")\n        print(\"  c_next =\", [int(v) for v in c_next])\n\n        # ----- verify Eq.(13):  Q(X²) = (P+P(−X))/2 + u·(P−P(−X))/(2X)\n        P      = vec_poly(c)          # current poly  P(X)\n        P_neg  = P.subs({x: -x})\n        Q      = vec_poly(c_next)     # poly after the fold\n        u_int  = int(u_k)\n\n        lhs = Q.subs({x: x**2})\n        rhs = expand((P + P_neg)/Integer(2) +\n                     u_int * (P - P_neg)/(Integer(2)*x))\n\n        print(\"  LHS = Q(X²) =\", lhs)\n        print(\"  RHS         =\", rhs)\n        print(\"  LHS − RHS   =\", expand(lhs - rhs))\n\n        c = c_next                    # proceed to next round\n\n    print(\"\\nFinal folded constant =\", int(c[0]))\n    return c[0]    \nu_vals = [Field(int(v)) for v in (2,5,3)]\nprint(\"evaluation point=\",u_vals)\nfold_const = gemini_fold_verbose(coeffs, u_vals)\n\n# ----------------------------- direct evaluation check ---------------------------\ndirect = Field(int(0))\nfor i, e in enumerate(evals):\n    b = bits_le(i)\n    d = delta(b, u_vals)\n    term = e * d\n    direct += term\nprint(direct)\n\nprint(\"\\n=========  Check  =========\")\nprint(\"Constant after folds =\", int(fold_const))\nprint(\"Direct f(u)          =\", int(direct))\nprint(\"Match?               →\", fold_const == direct)\n\n\nNow, finally we can have a look at the full commit-> prove-> verify process of HyperKZG. Let’s see how that would look like:\n\n\n\nfrom curve            import Fr  as Field, G1Point as G1, G2Point as G2\nfrom kzg10_non_hiding import KZG10_PCS\nfrom hyperkzg_pcs     import HYPERKZG_PCS\nfrom mle2             import MLEPolynomial\nfrom unipoly          import UniPolynomial\nfrom merlin.merlin_transcript import MerlinTranscript\n\n\nevals = [Field(v) for v in range(1, 9)]          # [1,2,3,4,5,6,7,8]\nf      = MLEPolynomial(evals, 3)                 # multilinear poly\nprint(f\"\\n f.evals = {[int(e) for e in evals]}\")\n\n\nkzg = KZG10_PCS(G1, G2, Field, 20, debug=True)   # SRS supports deg ≤ 19\nhyper = HYPERKZG_PCS(kzg, debug=True)\n\n\nP_uni = UniPolynomial(f.evals)                   # P(X)=Σ evals[i] X^i\nf_cm  = kzg.commit(P_uni)\n\nprint(\"\\n    commitment C    =\", f_cm)\n\n# ---------------- transcript ----------------------------------------------------\ntr = MerlinTranscript(b\"demo-hyperkzg\")\nprint(\" transcript initialised\")\n\n# ---------------- evaluation point & expected value ----------------------------\nu_point = [Field(int(2)), Field(int(5)), Field(int(3))]\nvalue   = f.evaluate(u_point)\nprint(\"\\n point u =\", [int(u) for u in u_point])\nprint(\"    f(u)    =\", int(value))\n\n# ---------------- prover   ------------------------------------------------------\nprint(\"\\n prover: split-and-fold + KZG opening (debug prints follow)\")\nval, proof = hyper.prove_eval(f_cm,\n                              f,                # multilinear polynomial\n                              u_point,\n                              transcript = tr.fork(b\"hyperkzg_pcs\"),\n                              debug      = 0)\nprint(\"\\n prover returns\")\nprint(\"    value =\", int(val))\nprint(\"    proof =\", proof)\n\n# ---------------- verifier ------------------------------------------------------\nprint(\"\\n verifier checking …\")\nok = hyper.verify_eval(f_cm,\n                       proof,\n                       u_point,\n                       val,\n                       tr.fork(b\"hyperkzg_pcs\"),\n                       debug = 0)\nprint(\"\\n verifier verdict =\", ok)\nassert ok\nprint(\"\\n✅ commit / prove / verify (n = 3, evals 1-8) succeeded\")\n","type":"content","url":"/gemini/hyperkzg","position":1},{"hierarchy":{"lvl1":"Greyhound Commitment"},"type":"lvl1","url":"/grey-hound/greyhound-pcs","position":0},{"hierarchy":{"lvl1":"Greyhound Commitment"},"content":"Greyhound is a lattice-based polynomial commitment scheme (PCS). For polynomials of degree N, the evaluation proof size is poly\\log(N), and the verification time is O(\\sqrt{N}). The main purpose of this article is to help readers understand the Greyhound polynomial commitment method, how evaluation proofs work, and how to use Greyhound to prove the evaluation of a polynomial over \\mathbb{F}_q.","type":"content","url":"/grey-hound/greyhound-pcs","position":1},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"1. Notation and Background"},"type":"lvl2","url":"/grey-hound/greyhound-pcs#id-1-notation-and-background","position":2},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"1. Notation and Background"},"content":"Before introducing the design principles of Greyhound, let’s establish some notation.\n\nLet d be a power of 2, \\mathcal{R} = \\mathbb{Z}[X]/(X^d + 1) be the ring of integers in the 2d-th cyclotomic field. Define the ring \\mathcal{R}_q = \\mathbb{Z}_q[X]/(X^d + 1) using an odd prime q, and define \\delta = \\lfloor \\log q \\rfloor. To avoid confusion, we will consistently use “vector” to refer to polynomial column vectors over \\mathcal{R} or \\mathcal{R}_q.\n\nFor an integer n \\geq 1, define a tool matrix for binary-to-decimal conversion \\mathbf{G}_n = \\mathbf{I} \\otimes [1 ~2~ 4~ \\cdots~ 2^\\delta] \\in \\mathcal{R}_q^{n \\times n\\delta}. Correspondingly, binary decomposition is denoted by \\mathbf{G}_n^{-1} : \\mathcal{R}_q^n \\to \\mathcal{R}_q^{\\delta n}.\n\nFor example, given a vector \\mathbf{t} \\in \\mathcal{R}_q^n, the notation \\mathbf{G}_n^{-1}(\\mathbf{t}) represents the process of decomposing all coefficients of \\mathbf{t} into binary and then filling them into a vector.\n\\mathbf{G}_n and \\mathbf{G}_n^{-1} are inverse operations, with \\mathbf{G}_n \\mathbf{G}_n^{-1}(\\mathbf{t}) = \\mathbf{t}.\n\nFor example, in \\mathcal{R}_{10} = \\mathbb{Z}_{10} [X]/(X^3+1), let \\mathbf{t} = (t_1, t_2) = (6+2X+5X^2, 4+X+9X^2) \\in \\mathcal{R}_{10}^2, and \\delta = \\lceil \\log 10 \\rceil = 4.\nThen \\mathbf{G}_3^{-1}(\\mathbf{t}) first converts (6,5,1),(4,9,2) into binary numbers (0110,0010,0101,0100,1001,0010), then fills them into a vector over \\mathcal{R}_q as \\hat{\\mathbf{t}} = (0+X+X^2, 0+0X+0X^2, 1+0X+0X^2, 1+0X+1X^2, 0+X+0X^2, 0+X+0X^2, 0+X+0X^2, 0+X+0X^2)\\in \\mathcal{R}^{2 \\delta}, so \\mathbf{G}_3^{-1}(\\mathbf{t}) = \\hat{\\mathbf{t}}.\nCorrespondingly, the matrix \\mathbf{G}_3 = \\begin{bmatrix} 1 & 2  & 2^3 & 2^4 & 0 &&&&&&\\cdots&0\\\\ 0 & & \\cdots &0 & 1 & 2  & 2^3 & 2^4 &0 && \\cdots & 0\\\\ 0 & &\\cdots & & & &  &0 & 1 & 2  & 2^3 & 2^4 \\end{bmatrix}\n\nrepresents the inverse operation, i.e., \\mathbf{G}_3 \\hat{\\mathbf{t}} = \\mathbf{t} = (6+2X+5X^2, 4+X+9X^2) \\in \\mathcal{R}_{10}^2.","type":"content","url":"/grey-hound/greyhound-pcs#id-1-notation-and-background","position":3},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl3":"Ajtai Commitment","lvl2":"1. Notation and Background"},"type":"lvl3","url":"/grey-hound/greyhound-pcs#ajtai-commitment","position":4},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl3":"Ajtai Commitment","lvl2":"1. Notation and Background"},"content":"Next, we introduce the definition of the SIS problem and Ajtai commitment.\n\nThe SIS problem is defined as follows: given a public matrix \\mathbf{A} \\in \\mathcal{R}_q^{n \\times m}, find a non-zero short vector \\mathbf{z} \\in \\mathcal{R}_q^m satisfying \\mathbf{A}\\mathbf{z}=\\mathbf{0}, |\\mathbf{z}|\\leq B.\n\nFor a binary message, after filling the coefficients into a vector \\mathbf{m} \\in \\mathcal{R}^n over \\mathcal{R}, the Ajtai commitment process is as follows:\n\nKeyGen: Input security parameter \\lambda, generate commitment key \\mathbf{A} \\in \\mathcal{R}_q^{n\\times m}\n\nCom: Input commitment key \\mathbf{A} \\in \\mathcal{R}_q^{n\\times m} and binary message \\mathbf{m} \\in \\mathcal{R}^m, compute commitment \\mathbf{t} = \\mathbf{Am}.\n\nRegarding the Ajtai commitment, three points can be further discussed: 1. Security of the commitment, 2. Norm of the commitment message, and 3. Compressibility of the commitment value.\n\nThe binding property of the commitment is based on the SIS problem. If a malicious attacker wants to change the commitment value \\mathbf{t} to another message’s commitment \\mathbf{t} = \\mathbf{Am}', it’s equivalent to finding a solution to the SIS problem \\mathbf{m-m'} that satisfies \\mathbf{0} = \\mathbf{A(m-m)'}, |\\mathbf{m-m}'|_\\infty \\leq 2.\n\nNote that we can find a solution to the SIS problem satisfying |\\mathbf{m-m}'|_\\infty \\leq 2 because we’ve required the committed messages to be binary. This requirement that messages be binary is actually constraining the norm of \\mathbf{m} to be small. In other cases, it could be that the norm of \\mathbf{m} is less than some bound B, then the binding property of the commitment would reduce to finding a solution with bound B: |\\mathbf{m-m}'|_\\infty \\leq 2B.\n\nThe compressibility of the commitment value is reflected in the fact that the commitment value \\mathbf{t} \\in \\mathcal{R}_q^n is an n-dimensional vector over \\mathcal{R}_q, independent of the committed message length (an m-dimensional vector over \\mathcal{R}_q), and under the security definition of the SIS problem, the length of the commitment value n < m.","type":"content","url":"/grey-hound/greyhound-pcs#ajtai-commitment","position":5},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"2. Greyhound Commitment Scheme"},"type":"lvl2","url":"/grey-hound/greyhound-pcs#id-2-greyhound-commitment-scheme","position":6},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"2. Greyhound Commitment Scheme"},"content":"Greyhound commitment can be understood as a two-layer Ajtai commitment.\n\nSuppose we want to commit to a group (e.g., r) of arbitrary vectors \\mathbf{f}_1, \\dots, \\mathbf{f}_r \\in \\mathcal{R}_q^m. Note that each \\mathbf{f}_i is an m-dimensional vector over \\mathcal{R}_q, with each component being an element of \\mathcal{R}_q.\n\nThe commitment key consists of the inner public matrix \\mathbf{A}\\in \\mathcal{R}_q^{n\\times m\\delta} and the outer public matrix \\mathbf{B} \\in \\mathcal{R}_q^{n \\times r n \\delta}.\n\nInner commitment: As mentioned in the previous section, Ajtai commitment messages should be binary strings. Therefore, to use Ajtai commitment for \\mathbf{f}_1, \\dots, \\mathbf{f}_r, we first need to use the base conversion tool matrix \\mathbf{G}^{-1} to convert this group of vectors into binary vectors \\mathbf{s}_i \\in \\mathcal{R}_q^{m \\delta}, i.e., \\mathbf{s}_i = \\mathbf{G}_m^{-1}(\\mathbf{f}_i). Now, we can make an Ajtai commitment to the message \\mathbf{s}_i: \\mathbf{t}_i := \\mathbf{As}_i = \\mathbf{AG}^{-1}_m(\\mathbf{f}_i)\n\nThis gives us commitment values \\mathbf{t}_1, ..., \\mathbf{t}_r \\in \\mathcal{R}_q^n for the r messages \\mathbf{f}_1, ..., \\mathbf{f}_r.\n\nOuter commitment: After completing the inner commitment, we have r vectors \\mathbf{t}_1, ..., \\mathbf{t}_r \\in \\mathcal{R}_q^n. Currently, the commitment value has a sublinear relationship \\mathcal{O}(mr) with the message length (r groups of vectors of length m). To achieve a more compressed commitment value, i.e., \\mathcal{O}(1), we want to make another Ajtai commitment to the inner commitment values \\mathbf{t}_1, ..., \\mathbf{t}_r. This can be done by viewing \\mathbf{t}_1, ..., \\mathbf{t}_r as commitment messages and repeating the above process. First, use \\mathbf{G}^{-1} to convert \\mathbf{t}_1, ..., \\mathbf{t}_r into vectors with binary coefficients \\hat{\\mathbf{t}}_i = \\mathbf{G}^{-1}(\\mathbf{t}_i), then compute the outer commitment value: \\mathbf{u} := \\mathbf{B}\\begin{bmatrix} \\hat{\\mathbf{t}}_1 \\\\ \\vdots \\\\ \\hat{\\mathbf{t}}_r \\end{bmatrix} \\in \\mathcal{R}_q^n\n\nFinally, the commitment to the group of vectors \\mathbf{f}_1, \\dots, \\mathbf{f}_r \\in \\mathcal{R}_q^m is the vector \\mathbf{u} \\in \\mathcal{R}_q^n.","type":"content","url":"/grey-hound/greyhound-pcs#id-2-greyhound-commitment-scheme","position":7},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"3. Polynomial Evaluation in Greyhound Commitment"},"type":"lvl2","url":"/grey-hound/greyhound-pcs#id-3-polynomial-evaluation-in-greyhound-commitment","position":8},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"3. Polynomial Evaluation in Greyhound Commitment"},"content":"In the previous section, we only discussed how to commit to a group of vectors \\mathbf{f}_1, \\dots, \\mathbf{f}_r \\in \\mathcal{R}_q^m, but our goal is to construct a PCS, so we need to discuss the relationship between this group of vectors and a polynomial f(\\mathsf{X}) = \\sum_{i=0}^{N-1} f_i \\mathsf{X}^i \\in \\mathcal{R}_q^{<N}[\\mathsf{X}] for which we want to perform evaluation.\n\nNote that \\mathsf{X} is the variable of polynomial f and has no relation to X in \\mathcal{R} = \\mathbb{Z}[X]/(X^d + 1).\n\nAssume N=m \\cdot r. We want to prove that the evaluation of polynomial f at point x \\in \\mathcal{R}_q is y, i.e., f(x) = \\sum_{i=0}^{N-1} f_i x^i = y.\n\nSimilar to Mercury, we can represent the above evaluation process using matrix-vector multiplication:f(x) = [1~x~x^2~\\cdots~x^{m-1}] \\begin{bmatrix} f_0 & f_m & \\cdots & f_{(r-1)m} \\\\ f_{1} & f_{m+1} &\\cdots & f_{(r-1)m+1} \\\\ f_{2} & f_{m+2} &\\cdots & f_{(r-1)m+2} \\\\ &&\\cdots& \\\\ f_{m-1} & f_{2m-1} &\\cdots & f_{rm-1} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ x^m \\\\ (x^m)^2\\\\ \\vdots \\\\ (x^m)^r \\end{bmatrix}.\n\nTherefore, if we define vectors \\mathbf{f}_i = [ f_{(i-1)m}, f_{(i-1)m+1}, ... , f_{im-1}] \\in \\mathcal{R}_q^m, consisting of the coefficients of polynomial f, then the commitment to \\mathbf{f}_1, \\dots, \\mathbf{f}_r \\in \\mathcal{R}_q^m is the commitment to polynomial f.\n\nNow, we define vectors \\mathbf{a}^\\top = [1~x~x^2~\\cdots~x^{m-1}] and \\mathbf{b}^\\top = \\begin{bmatrix} 1 ~ x^m ~\\dots ~ (x^m)^r \\end{bmatrix}, then the polynomial evaluation can be represented as f(x) = \\mathbf{a}^\\top [\\mathbf{f}_1 ~ \\cdots ~\\mathbf{f}_r] \\mathbf{b}.","type":"content","url":"/grey-hound/greyhound-pcs#id-3-polynomial-evaluation-in-greyhound-commitment","position":9},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"4. Proof of Polynomial Evaluation"},"type":"lvl2","url":"/grey-hound/greyhound-pcs#id-4-proof-of-polynomial-evaluation","position":10},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"4. Proof of Polynomial Evaluation"},"content":"The proof of polynomial evaluation includes two points: 1. The polynomial commitment is correctly computed; 2. The polynomial evaluation operation is correct.\n\nFirst, a prover needs to commit to polynomial f according to the method in section 1.1. Then proving the correctness of the commitment computation means proving:\n\\begin{align*} \\mathbf{s}_i &= \\mathbf{G}^{-1}_m(\\mathbf{f}_i), \\\\ \\mathbf{t}_i &= \\mathbf{G}_n\\hat{\\mathbf{t}}_i = \\mathbf{As}_i,  \\\\ \\mathbf{u} &= \\mathbf{B}\\begin{bmatrix} \\hat{\\mathbf{t}}_1 \\\\ \\vdots \\\\ \\hat{\\mathbf{t}}_r \\end{bmatrix}. \\end{align*}\n\nSince the transformation \\mathbf{G} is reversible, the first equation can also be written as \\mathbf{f}_i = \\mathbf{G}_m \\mathbf{s}_i.\n\nProving the correctness of polynomial evaluation means proving: f(x) = \\mathbf{a}^\\top [\\mathbf{f}_1 ~ \\cdots ~\\mathbf{f}_r] \\mathbf{b}.\n\nGreyhound’s approach to proving the above four equations is to have the prover compute parts directly related to polynomial \\mathbf{f}_i, and the verifier compute the remaining parts. We’ll first show the proof process and then discuss this approach.\n\nAssume the public parameters of the protocol are commitment keys \\mathbf{A,B}, commitment u, and polynomial f. The entire protocol consists of 2 rounds of interaction:\n\nThe prover first calculates the first half of f(x) and sends \\mathbf{w} to the verifier:\\mathbf{w}^\\top := \\mathbf{a}^\\top[\\mathbf{f}_1 ~ \\cdots ~\\mathbf{f}_r].\n\nThe verifier selects a random challenge vector \\mathbf{c} = (c_1, ..., c_r) \\in \\mathcal{R}_q^r and sends it to the prover.\n\nThe prover sends intermediate commitments (\\hat{\\mathbf{t}}_1, ...,\\hat{\\mathbf{t}}_r) and uses \\mathbf{c} to make a linear combination of \\mathbf{s}_i: \\mathbf{z} := [\\mathbf{s}_1 ~ \\cdots~ \\mathbf{s}_r] \\mathbf{c} = \\mathbf{s}_1c_1 + ... +\\mathbf{s}_r c_r\\in \\mathcal{R}_q^m.\n\nFinally, the verifier uses all the information sent by the prover \\mathbf{w}, \\hat{\\mathbf{t}}_i, \\mathbf{z} to verify whether the following equations hold:\\begin{align}\n\\mathbf{w}^\\top \\mathbf{b} &= y, \\\\\n\\mathbf{w}^\\top \\mathbf{c} &= \\mathbf{a}^\\top \\mathbf{G}_m \\mathbf{z},\\\\\n\\mathbf{Az} & = c_1\\mathbf{G}_n \\hat{\\mathbf{t}}_1 + \\cdots + c_r \\mathbf{G}_n \\hat{\\mathbf{t}}_r,\\\\\n\\mathbf{u} &= \\mathbf{B}\\begin{bmatrix} \\hat{\\mathbf{t}}_1 \\\\ \\vdots \\\\ \\hat{\\mathbf{t}}_r \\end{bmatrix}.\n\\end{align}\n\nThe first equation actually verifies the second half of the polynomial f(x) evaluation operation, because if \\mathbf{w} is correct (which is guaranteed by the second equation), then f(x) = \\mathbf{a}^\\top[\\mathbf{f}_1 ~ \\cdots ~\\mathbf{f}_r] \\mathbf{b} = \\mathbf{w}^\\top \\mathbf{b} = y.\n\nThe second equation, with the participation of the challenge vector \\mathbf{c}, verifies the correctness of the first half of the f(x) evaluation operation:\\begin{align*} \\mathbf{w}^\\top \\mathbf{c} &= \\mathbf{a}^\\top [\\mathbf{f}_1 ~ \\cdots ~\\mathbf{f}_r] \\mathbf{c} \\\\ & = \\mathbf{a}^\\top \\mathbf{G}_m [\\mathbf{s}_1~ \\cdots ~\\mathbf{s}_r] \\mathbf{c} \\\\\n& = \\mathbf{a}^\\top \\mathbf{G}_m \\mathbf{z}\n\\end{align*}.\n\nThe third equation verifies the correctness of the inner commitment of the PCS:\\begin{align*} \\mathbf{Az} & = \\mathbf{A s}_1 c_1 + ... + \\mathbf{A s}_r c_r\\\\ &= c_1\\mathbf{G}_n \\hat{\\mathbf{t}}_1 + \\cdots + c_r \\mathbf{G}_n \\hat{\\mathbf{t}}_r. \\\\\n\\end{align*}\n\nThe fourth equation verifies the outer commitment of the PCS.\n\nThis proof can be further optimized using Labrador’s recursive proof to reduce the proof size by rewriting the equations in (1) into the standard form of Labrador proof. For specific details, please refer to the Greyhound documentation.","type":"content","url":"/grey-hound/greyhound-pcs#id-4-proof-of-polynomial-evaluation","position":11},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"5. Using \\mathcal{R}_q to Prove Polynomial Evaluation over \\mathbb{F}_q"},"type":"lvl2","url":"/grey-hound/greyhound-pcs#id-5-using-mathcal-r-q-to-prove-polynomial-evaluation-over-mathbb-f-q","position":12},{"hierarchy":{"lvl1":"Greyhound Commitment","lvl2":"5. Using \\mathcal{R}_q to Prove Polynomial Evaluation over \\mathbb{F}_q"},"content":"In the previous introduction, we ignored an important issue: all operations and proofs are performed over \\mathcal{R}_q, while most practical applications use polynomials over \\mathbb{F}_q. This section discusses how to equivalently represent polynomials over \\mathbb{F}_q as polynomials over \\mathcal{R}_q, thereby using the proof methods from the previous sections.\n\nThe transformation from \\mathbb{F}_q to \\mathcal{R}_q requires filling vectors over \\mathbb{F}_q into \\mathcal{R}_q through coefficient filling, and then using operations over \\mathcal{R}_q to represent operations over \\mathbb{F}_q.\n\nDefine an automorphism \\sigma: \\mathcal{R}_q \\to \\mathcal{R}_q that maps elements in \\mathcal{R}_q to negative powers, \\sigma(X) = X^{-1}. For example, if a = a_0 + \\sum_{i=1}^{d-1} a_i X^i \\in \\mathcal{R}_q, then \\sigma(a) = a_0 +\\sum_{i=1}^{d-1} a_i X^{-i} \\in \\mathcal{R}_q.\nIn \\mathcal{R}_q, the constant term of a\\sigma(a) is: a_0 a_0 + \\sum_{i=1}^{d-1} a_i a_i = \\sum_{i=0}^{d-1} a_i a_i, which is the sum of all terms where the product of a term in a and a term in \\sigma(a) does not involve X.\n\nLet’s clarify the notation: a polynomial over \\mathbb{F}_q is denoted as F(U) = \\sum_{i=0}^{N'-1} F_i U^i = V \\in \\mathbb{F}_q, and a polynomial over \\mathcal{R}_q is denoted as f(x) = \\sum_{i=0}^{N-1} f_i x^i = y \\in \\mathcal{R}_q. We use f to represent the polynomial obtained by filling the coefficients of F into \\mathcal{R}_q. To distinguish, we use N-1 to represent the degree of polynomial f and N'-1 for the degree of F. After coefficient filling, N and N' are not equal.\n\nWhen N' \\leq d, without loss of generality, we discuss N'=d. When N' < d, we can pad F with zeros to make N'=d. In this case, only one element in \\mathcal{R}_q is needed to store all coefficients of F, so N = 1.\n\nWe define the evaluation after filling the coefficients of F as f(x) = f_0 \\sigma(x).\nHere, f_0 = \\sum_{i=0}^{d-1} F_i X^i \\in \\mathcal{R}_q is obtained by filling all the coefficients of F (F_0, ..., F_{N'-1}), and \\sigma(x) is obtained by first filling all powers of the evaluation point U of F (1, U, ..., U^{N'-1}) into x = \\sum_{i=0}^{d-1} U^i X^i\\in \\mathcal{R}_q, and then applying the \\sigma mapping to get \\sigma(x) = 1+ \\sum_{i=1}^{d-1} U^i X^i \\in \\mathcal{R}_q.\nIt should be reminded again that X here is a symbol in \\mathcal{R}_q and has no actual meaning.\n\nUsing the \\sigma mapping discussed above, the constant term of f(x) = f_1 \\sigma(x) is \\sum_{i=0}^{N'-1} F_i U^{i} = V.\n\n This means that a Greyhound verifier can check if F(U) = V holds by checking if the constant term of f(x) is V.\n\nWhen N' > d, we discuss N' = Nd. In this case, the coefficients of F (F_0, ..., F_{N'-1}) will be filled into N elements in \\mathcal{R}_q f_0, f_1 ..., f_{N-1} \\in \\mathcal{R}_q, but the operation method of polynomial f(x) is similar to when N'=d.\n\nWe just need to fill the coefficients of F sequentially into (f_0, f_1 ..., f_{N-1}), then fill (1, U, ..., U^{N'-1}) sequentially into x_1, x_2, ..., x_N \\in \\mathcal{R}_q, and then define f(x) = \\sum_{i=0}^{N} f_i x_i.\n\nThen, the multiplication operation between f_i and x_i in f(x) will segment-wise store \\sum_{j=0}^{d} F_{id+j}{U^{id+j}} in the constant term of f_i x_i, similar to our previous discussion.\nSince addition in \\mathcal{R}_q adds corresponding coefficients, the outer addition operation \\sum_{i=0}^{N-1} f_i x_i from i=0 to N will further combine the \\sum_{j=0}^{d} F_{id+j}{U^{id+j}} in each constant term, i.e., the constant term of f(x) is \\sum_{i=0}^{N-1} \\sum_{j=0}^{d} F_{id+j}{U^{id+j}} = \\sum_{i=0} ^{N'-1} F_i U^i = V.\n\nNow, we are able to use Greyhound to commit to any polynomial over \\mathbb{F}_q and prove their evaluations. Thank you for reading to this point.\n\nReferences:\n[1] Ngoc Khanh Nguyen, Gregor Seiler: Greyhound: Fast Polynomial Commitments from Lattices. CRYPTO (10) 2024: 243-275.","type":"content","url":"/grey-hound/greyhound-pcs#id-5-using-mathcal-r-q-to-prove-polynomial-evaluation-over-mathbb-f-q","position":13},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS"},"type":"lvl1","url":"/hyrax-pcs/hyrax-01","position":0},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS"},"content":"This article briefly introduces the principles of Hyrax-PCS, whose security assumption is Discrete Log. Its main idea is to prove an Inner Product and adopts the recursive folding approach to gradually fold two vectors of length n into two vectors of length n/2, reducing the inner product calculation to the inner product calculation of vectors with only half the length. The recursive folding idea mainly comes from [BCC+16] and [BBB+18], with the main problem being that the Verifier’s computational complexity is O(n). To make Hyrax meet the requirements of zkSNARK, Hyrax rearranges the vector into a \\sqrt{n}\\times\\sqrt{n} matrix, then reduces the inner product calculation to the inner product calculation of vectors with length \\sqrt{n}. This results in the Verifier’s computational complexity being optimized to O(\\sqrt{n}). At the same time, after optimization by [BCC+16], Hyrax’s Proof size (communication complexity) is also optimized to O(\\log{n}).","type":"content","url":"/hyrax-pcs/hyrax-01","position":1},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"1. Evaluation Proof of MLE Polynomials"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#id-1-evaluation-proof-of-mle-polynomials","position":2},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"1. Evaluation Proof of MLE Polynomials"},"content":"Whether the MLE polynomial is in Coefficients form or Evaluation form, we can prove the evaluation of the polynomial at certain points through the “Inner Product Argument”.f(X_0, X_1, ..., X_{n-1}) = \\sum_{i=0}^{2^n-1} a_i \\cdot \\tilde{eq}(\\mathsf{bits}(i), X_0, X_1, ..., X_{n-1})f(X_0, X_1, ..., X_{n-1}) = \\sum_{i=0}^{2^n-1} c_i \\cdot X_0^{i_0} \\cdot X_1^{i_1} \\cdot ... \\cdot X_{n-1}^{i_{n-1}},  \\quad \\text{where } \\mathsf{bits}(i) = (i_0, i_1, ..., i_{n-1})\n\nIf we have an inner product proof protocol, we can easily construct an evaluation proof for MLE polynomials.","type":"content","url":"/hyrax-pcs/hyrax-01#id-1-evaluation-proof-of-mle-polynomials","position":3},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"1. Evaluation Proof of MLE Polynomials"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-input","position":4},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"1. Evaluation Proof of MLE Polynomials"},"content":"Commitment of \\vec{a}: C_a=\\mathsf{cm}(a_0, a_1, ..., a_{2^n-1})\n\n\\vec{u}=(u_0, u_1, ..., u_{n-1})\n\nv=\\tilde{f}(u_0, u_1, ..., u_{n-1})","type":"content","url":"/hyrax-pcs/hyrax-01#public-input","position":5},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"1. Evaluation Proof of MLE Polynomials"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#witness","position":6},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"1. Evaluation Proof of MLE Polynomials"},"content":"\\vec{a}","type":"content","url":"/hyrax-pcs/hyrax-01#witness","position":7},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Inner Product Protocol","lvl2":"1. Evaluation Proof of MLE Polynomials"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#inner-product-protocol","position":8},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Inner Product Protocol","lvl2":"1. Evaluation Proof of MLE Polynomials"},"content":"Prover computes vector \\vec{e}, length 2^n,\\begin{aligned}\ne_0 &= \\tilde{eq}(\\mathsf{bits}(0), u_0, u_1, ..., u_{n-1}) \\\\\ne_1 &= \\tilde{eq}(\\mathsf{bits}(1), u_0, u_1, ..., u_{n-1}) \\\\\n\\cdots \\\\\ne_{2^n-1} &= \\tilde{eq}(\\mathsf{bits}(2^n-1), u_0, u_1, ..., u_{n-1}) \\\\\n\\end{aligned}\n\nProver and Verifier use an Inner Product Argument protocol to prove that the inner product of \\vec{a} and \\vec{e} equals v. Below, we introduce a simple inner product proof that proves the inner product of two hidden vectors equals a public value.","type":"content","url":"/hyrax-pcs/hyrax-01#inner-product-protocol","position":9},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"2. Mini-IPA"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#id-2-mini-ipa","position":10},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"2. Mini-IPA"},"content":"Let’s start with the simplest case. Suppose Prover has two vectors \\vec{a} and \\vec{b}, satisfying \\langle \\vec{a}, \\vec{b} \\rangle = c (note: c is a public value here).","type":"content","url":"/hyrax-pcs/hyrax-01#id-2-mini-ipa","position":11},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Proof Goal","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#proof-goal","position":12},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Proof Goal","lvl2":"2. Mini-IPA"},"content":"Prover has knowledge (\\vec{a},\\vec{b}) (two vectors of equal length, denoted as m, so there are 2m witnesses in total), and \\langle \\vec{a}, \\vec{b} \\rangle = c","type":"content","url":"/hyrax-pcs/hyrax-01#proof-goal","position":13},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Parameters","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-parameters","position":14},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Parameters","lvl2":"2. Mini-IPA"},"content":"To compute the Pedersen Commitment of vectors, we need to select a set of random group elements g_1, g_2, g_3, \\ldots, g_m, h \\in \\mathbb{G}.","type":"content","url":"/hyrax-pcs/hyrax-01#public-parameters","position":15},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-input-1","position":16},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"2. Mini-IPA"},"content":"Inner product result c\n\nCommitment of vector \\vec{a}: C_a=\\mathsf{cm}(\\vec{a};\\rho_a),\n\nCommitment of vector \\vec{b}: C_b=\\mathsf{cm}(\\vec{b};\\rho_b)","type":"content","url":"/hyrax-pcs/hyrax-01#public-input-1","position":17},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witnesses","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#witnesses","position":18},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witnesses","lvl2":"2. Mini-IPA"},"content":"(\\vec{a}, \\rho_a)\n\n(\\vec{b}, \\rho_b)","type":"content","url":"/hyrax-pcs/hyrax-01#witnesses","position":19},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Basic Protocol Idea","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#basic-protocol-idea","position":20},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Basic Protocol Idea","lvl2":"2. Mini-IPA"},"content":"Prover introduces two “blinder vectors”, \\vec{r} and \\vec{s}. These two vectors are flattened into one vector through a challenge number \\mu (from Verifier):\\vec{a}' = \\vec{r}+ \\mu\\cdot \\vec{a}  \\qquad \\vec{b}' = \\vec{s}+ \\mu\\cdot \\vec{b}\n\nThen calculate the inner product (or dot product) of \\vec{a}'\\cdot \\vec{b}'.\\langle \\vec{a}', \\vec{b}' \\rangle = (\\vec{r}+ \\mu\\cdot \\vec{a})(\\vec{s}+ \\mu\\cdot \\vec{b}) =\\mu^2(\\langle \\vec{a}, \\vec{b} \\rangle) + \\mu(\\langle \\vec{a}, \\vec{s} \\rangle + \\langle \\vec{b}, \\vec{r} \\rangle) + \\langle \\vec{r}, \\vec{s} \\rangle\n\nObserving \\vec{a}' and \\vec{b}', we find that both vectors have \\mu terms. After flattening the vectors and performing the inner product operation, we get a quadratic polynomial in \\mu, where the “coefficient” of the \\mu^2 term is exactly the inner product of vectors \\vec{a} and \\vec{b} (should equal c), and the constant term \\vec{r}\\cdot\\vec{s} is exactly the inner product of the two “blinder vectors”. However, the coefficient of \\mu looks somewhat messy. We can ignore the messy coefficient of \\mu for now and focus on the coefficient of the \\mu^2 term. According to the Schwartz-Zippel theorem, as long as Prover can successfully respond to Verifier’s challenge, the coefficients of all terms in the polynomial must be (with high probability) correct.\n\nWe let Prover not only commit to the “blinder vectors” in the first step of the protocol but also commit to the coefficients of the polynomial (in \\mu) after expanding the inner product. Then in the third step, Prover only needs to send the two flattened vectors \\vec{a}' and \\vec{b}', which is just right. Verifier first verifies if a' can open A', then verifies if b' can open B', and finally Verifier verifies if \\vec{a}'\\cdot\\vec{b}' can open the commitment C'. Let’s see how the protocol is specifically defined:","type":"content","url":"/hyrax-pcs/hyrax-01#basic-protocol-idea","position":21},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#protocol","position":22},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"content":"","type":"content","url":"/hyrax-pcs/hyrax-01#protocol","position":23},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-1","position":24},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"content":"Prover sends commitments C_r and C_s of two “blinder vectors” \\vec{r} and \\vec{s}; also sends polynomial coefficient commitments C_0 and C_1:\\begin{aligned}\nC_r&=\\mathsf{cm}(\\vec{r};\\rho_{r}) \\\\\nC_s&=\\mathsf{cm}(\\vec{s} ;\\rho_{s}) \\\\\nC_0&=\\mathsf{cm}(\\langle \\vec{r}, \\vec{s} \\rangle; \\rho_0) \\\\\nC_1&=\\mathsf{cm}(\\langle \\vec{a}, \\vec{s} \\rangle + \\langle \\vec{b}, \\vec{r} \\rangle; \\rho_1) \\\\\n\\end{aligned}","type":"content","url":"/hyrax-pcs/hyrax-01#round-1","position":25},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-2","position":26},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"content":"Verifier replies with a challenge number \\mu\n\nProver sends two flattened vectors \\vec{a}', \\vec{b}', three random numbers mixed with \\mu: \\rho'_a, \\rho'_b, \\rho'_{ab}\\begin{aligned}\n\\vec{a}'&=\\vec{r} + \\mu \\cdot \\vec{a}\\\\\n\\vec{b}'&=\\vec{s} + \\mu \\cdot \\vec{b}\\\\\n\\end{aligned}\\begin{aligned}\n\\rho'_a&=\\rho_{r} + \\mu \\cdot \\rho_a \\\\\n\\rho'_b&=\\rho_{s} + \\mu \\cdot \\rho_b \\\\\n\\rho'_{ab}&=\\rho_0 + \\mu \\cdot \\rho_1 \\\\\n\\end{aligned}","type":"content","url":"/hyrax-pcs/hyrax-01#round-2","position":27},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#verification","position":28},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Protocol","lvl2":"2. Mini-IPA"},"content":"Verifier homomorphically verifies in group \\mathbb{G}: \\vec{a}' and \\vec{b}', and their inner product\\begin{aligned}\n\\mathsf{cm}(\\vec{a}'; \\rho'_a)&\\overset{?}{=} C_r + \\mu\\cdot C_a \\\\\n\\mathsf{cm}(\\vec{b}'; \\rho'_b)&\\overset{?}{=} C_s + \\mu \\cdot C_b \\\\\n\\end{aligned}\\mathsf{cm}(\\langle \\vec{a}', \\vec{b}' \\rangle; \\rho'_{ab})\\overset{?}{=} \\mu^2\\cdot\\mathsf{cm}(c;0) + \\mu\\cdot C_1 + C_0\n\nThe biggest problem with this protocol is that Verifier’s computational complexity is O(n), because Verifier needs to compute \\langle \\vec{a}', \\vec{b}' \\rangle. Also, \\vec{b} is hidden information, but for the MLE Evaluation proof protocol, \\vec{e} (computed from \\vec{u}) is a public value, so we need to adjust the protocol.","type":"content","url":"/hyrax-pcs/hyrax-01#verification","position":29},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"3. Square-root inner product argument"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#id-3-square-root-inner-product-argument","position":30},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"3. Square-root inner product argument"},"content":"The Hyrax paper proposes a simple and direct approach to reduce Verifier’s computational complexity to O(\\sqrt{n}). A Sublinear Verifier is a basic requirement of zkSNARK. We still only consider the Coefficients form of \\tilde{f}, i.e., \\vec{c} is the coefficient of \\tilde{f}.\n\nWe assume n=4, so \\vec{a} has a length of 16, and we can arrange this vector into a matrix:\\begin{bmatrix}\nc_0 & c_1 & c_2 & c_3 \\\\\nc_4 & c_5 & c_6 & c_7 \\\\\nc_8 & c_9 & c_{10} & c_{11} \\\\\nc_{12} & c_{13} & c_{14} & c_{15}\n\\end{bmatrix}\n\nThe MLE polynomial represented by \\vec{a} can be expressed in the following form:\\tilde{f}(X_0, X_1, X_2, X_3) = \n\\begin{bmatrix}\n1 & X_2 & X_3 & X_2X_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 & c_1 & c_2 & c_3 \\\\\nc_4 & c_5 & c_6 & c_7 \\\\\nc_8 & c_9 & c_{10} & c_{11} \\\\\nc_{12} & c_{13} & c_{14} & c_{15}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nX_0 \\\\\nX_1 \\\\\nX_0X_1 \\\\\n\\end{bmatrix}\n\nThe result of this matrix calculation is as follows:\\tilde{f}(X_0, X_1, X_2, X_3) = c_0 + c_1X_0 + c_2X_1 + c_3X_0X_1 + c_4X2 + \\cdots + c_{14}X_1X_2X_3 + c_{15}X_0X_1X_2X_3\n\nWe first split \\vec{u} into two short vectors:\\vec{u} = (u_0, u_1, u_2, u_3) = (u_0, u_1) \\parallel (u_2, u_3)\n\nThen \\tilde{f}(u_0, u_1, u_2, u_3) can be represented as:\\tilde{f}(u_0, u_1, u_2, u_3) = \\begin{bmatrix}\n1 & u_2 & u_3 & u_2u_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 & c_1 & c_2 & c_3 \\\\\nc_4 & c_5 & c_6 & c_7 \\\\\nc_8 & c_9 & c_{10} & c_{11} \\\\\nc_{12} & c_{13} & c_{14} & c_{15}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nu_0 \\\\\nu_1 \\\\\nu_0u_1 \\\\\n\\end{bmatrix}\n\nThen, we calculate the commitments of the matrix composed of \\vec{c} by rows, obtaining C_0, C_1, C_2, C_3,\\begin{aligned}\nC_0 &= \\mathsf{cm}(c_0, c_1, c_2, c_3; \\rho_0) \\\\\nC_1 &= \\mathsf{cm}(c_4, c_5, c_6, c_7; \\rho_1) \\\\\nC_2 &= \\mathsf{cm}(c_8, c_9, c_{10}, c_{11}; \\rho_2) \\\\\nC_3 &= \\mathsf{cm}(c_{12}, c_{13}, c_{14}, c_{15}; \\rho_3)\n\\end{aligned}\n\nThen we can use (1, u_2, u_3, u_2u_3) and (C_0, C_1, C_2, C_3) to perform an inner product operation, obtaining C^*:C^* = C_0 + u_2C_1 + u_3C_2 + u_2u_3C_3\n\nThen C^* can be seen as the inner product of the column vector of matrix M_c and (1, u_2, u_3, u_2u_3), denoted as \\vec{d}=(d_0, d_1, d_2, d_3),\\begin{split}\nd_0 &= c_0 + c_4\\cdot u_0 + c_8\\cdot u_2 + c_{12}\\cdot u_2u_0 \\\\\nd_1 &= c_1 + c_5\\cdot u_0 + c_9\\cdot u_2 + c_{13}\\cdot u_2u_0 \\\\\nd_2 &= c_2 + c_6\\cdot u_0 + c_{10}\\cdot u_2 + c_{14}\\cdot u_2u_0 \\\\\nd_3 &= c_3 + c_7\\cdot u_0 + c_{11}\\cdot u_2 + c_{15}\\cdot u_2u_0 \\\\\n\\end{split}\n\nIt’s easy to verify:C^* = \\mathsf{cm}(d_0, d_1, d_2, d_3; \\rho^*)\n\nHere \\rho^*=\\rho_0 + \\rho_1u_2 + \\rho_2u_3 + \\rho_3u_2u_3.\n\nUsing this approach, we construct a simple MLE polynomial commitment scheme.","type":"content","url":"/hyrax-pcs/hyrax-01#id-3-square-root-inner-product-argument","position":31},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"3. Square-root inner product argument"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-input-2","position":32},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"3. Square-root inner product argument"},"content":"Commitment of \\vec{a}: C_a=\\mathsf{cm}(a_0, a_1, ..., a_{2^n-1})\n\n\\vec{u}=(u_0, u_1, ..., u_{n-1})\n\nv=\\tilde{f}(u_0, u_1, ..., u_{n-1})","type":"content","url":"/hyrax-pcs/hyrax-01#public-input-2","position":33},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"3. Square-root inner product argument"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#witness-1","position":34},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"3. Square-root inner product argument"},"content":"\\vec{a}","type":"content","url":"/hyrax-pcs/hyrax-01#witness-1","position":35},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Commitment","lvl2":"3. Square-root inner product argument"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#commitment","position":36},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Commitment","lvl2":"3. Square-root inner product argument"},"content":"Prover rearranges \\vec{a} into a matrix M_a\\in\\mathbb{F}_p^{l\\times h}:M_a =\n\\begin{bmatrix}\na_0 & a_1 & a_2 & \\cdots & a_{l-1} \\\\\na_l & a_{l+1} & a_{l+2} & \\cdots & a_{2l-1} \\\\\na_{2l} & a_{2l+1} & a_{2l+2} & \\cdots & a_{3l-1} \\\\\na_{(h-1)l} & a_{(h-1)l+1} & a_{(h-1)l+2} & \\cdots & a_{hl-1} \\\\\n\\end{bmatrix}\n\nProver calculates commitments C_0, C_1, ..., C_{h-1} by row\\begin{aligned}\nC_0 &= \\mathsf{cm}(a_0, a_1, ..., a_{l-1}; \\rho_0) \\\\\nC_1 &= \\mathsf{cm}(a_l, a_{l+1}, ..., a_{2l-1}; \\rho_1) \\\\\nC_2 &= \\mathsf{cm}(a_{2l}, a_{2l+1}, ..., a_{3l-1}; \\rho_2) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nC_{h-1} &= \\mathsf{cm}(a_{(h-1)l}, a_{(h-1)l+1}, ..., a_{hl-1}; \\rho_{h-1}) \\\\\n\\end{aligned}","type":"content","url":"/hyrax-pcs/hyrax-01#commitment","position":37},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#evaluation-proof-protocol","position":38},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"content":"Prover and Verifier split \\vec{u} into two short vectors, represented by \\vec{u}_L and \\vec{u}_R respectively:\\begin{aligned}\n\\vec{u}_L &= (u_0, u_1, ..., u_{\\log(l)-1}) \\\\\n\\vec{u}_R &= (u_{\\log(l)}, u_{\\log(l)+1}, ..., u_{n-1}) \\\\\n\\end{aligned}\n\nObviously\\vec{u} = \\vec{u}_L \\parallel \\vec{u}_R","type":"content","url":"/hyrax-pcs/hyrax-01#evaluation-proof-protocol","position":39},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-1-1","position":40},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"content":"Prover calculates \\vec{e}=(e_0, e_1, ..., e_{h-1}), length h:\\begin{split}\ne_0 &= \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_R) \\\\\ne_1 &= \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_R) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\ne_{h-1} &= \\tilde{eq}(\\mathsf{bits}(h-1), \\vec{u}_R) \\\\\n\\end{split}\n\nProver calculates the matrix multiplication of \\vec{e} and M_a, obtaining a new vector \\vec{b}, length l\\begin{split}\nb_0 &= \\langle \\vec{e}, (a_0, a_l, ..., a_{(h-1)l}) \\rangle \\\\\nb_1 &= \\langle \\vec{e}, (a_1, a_{l+1}, ..., a_{(h-1)l+1}) \\rangle \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nb_{l-1} &= \\langle \\vec{e}, (a_{l-1}, a_{l+l-1}, ..., a_{(h-1)l+l-1}) \\rangle \\\\\n\\end{split}\n\nProver calculates the commitment C^* of \\vec{b}C^* = \\mathsf{cm}(\\vec{b}; \\rho^*)","type":"content","url":"/hyrax-pcs/hyrax-01#round-1-1","position":41},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-2-1","position":42},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"content":"Prover and Verifier conduct an Inner Product Argument protocol to complete the inner product proof of \\vec{b} and \\vec{d}.\\begin{split}\nd_0 &= \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_L) \\\\\nd_1 &= \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_L) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nd_{h-1} &= \\tilde{eq}(\\mathsf{bits}(h-1), \\vec{u}_L) \\\\\n\\end{split}\n\nProver first samples a random number vector \\vec{r}, used to protect the information of \\vec{b}, then calculates its commitment:C_{r} = \\mathsf{cm}(\\vec{r}; \\rho_{r})\n\nProver calculates the inner product of \\vec{r} and \\vec{b}, obtaining vs = \\vec{r}\\cdot\\vec{d}C_0 = \\mathsf{cm}(s; \\rho_s)C_1 = \\mathsf{cm}(0; \\rho_t)","type":"content","url":"/hyrax-pcs/hyrax-01#round-2-1","position":43},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-3","position":44},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"content":"Verifier sends a random number \\mu\n\nProver calculates and sends the following values:\\vec{z_b} = \\vec{r} + \\mu\\cdot\\vec{b}z_\\rho = \\rho_r + \\mu\\cdot\\rho^*z_t = \\rho_s + \\mu^{-1}\\cdot\\rho_t","type":"content","url":"/hyrax-pcs/hyrax-01#round-3","position":45},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#verification-1","position":46},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"3. Square-root inner product argument"},"content":"Verifier verifies:C_r + \\mu\\cdot{}C^*\\overset{?}{=} \\mathsf{cm}(\\vec{z_b}; z_\\rho)C_0 + \\mu^{-1}\\cdot{}C_1 + \\mu\\cdot\\mathsf{cm}(v; 0)\\overset{?}{=} \\mathsf{cm}(\\langle\\vec{z}_b, \\vec{d}\\rangle; z_t)","type":"content","url":"/hyrax-pcs/hyrax-01#verification-1","position":47},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"4. Bulletproofs Optimization"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#id-4-bulletproofs-optimization","position":48},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"4. Bulletproofs Optimization"},"content":"In the Round-2 of the “inner product proof” protocol (Mini-IPA) mentioned above, since Prover needs to send a vector of length l (\\vec{z}_b), the overall communication of the protocol is O(l). If the vector is quite long, the final proof size will be relatively large. J. Bootle et al. proposed a very interesting idea in the [BCC+16] paper, using a recursive method to gradually fold the proof, achieving compression of the proof size.\n\nSuppose we have a vector \\vec{a} = (a_1, a_2, a_3, a_4) of length 4, we can split it in half into two vectors \\vec{a}_1 = (a_1, a_2) and \\vec{a}_2 = (a_3, a_4), then stack them vertically to form a matrix:\\begin{bmatrix}\na_1 & a_2\\\\\na_3 & a_4 \\\\\n\\end{bmatrix}\n\nThen we perform a “flattening” operation on this 2\\times 2 matrix (with the help of a random number vector):(x, x^{-1})\n\\left[\n\\begin{array}{ll}\na_1 &a_2\\\\\na_3 &a_4\n\\end{array}\n\\right]\n= (a_1x + a_3x^{-1},\\quad a_2x + a_4x^{-1})\n= \\vec{a}'\n\nAs shown in the above formula, we left multiply the matrix by a random number vector (x, x^{-1}), then obtain a vector \\vec{a}' of length 2. We can view this action as a special vertical flattening. This trick is slightly different from the vertical flattening in the previous text, not using the naive flattening vector (x^0, x^1). We call this action “folding”.\n\nWe can notice that the length of the folded vector \\vec{a}' is only half of \\vec{a}. Doing this recursively, by Verifier continuously sending challenge numbers and Prover repeatedly recursively folding, the vector can eventually be folded into a vector of prime length. However, if we are allowed to append some redundant values to the vector to align the vector length to 2^k, then after k folds, we can fold the vector into a number with a length of only 1. This is equivalent to performing horizontal flattening on a matrix.\n\nThis preliminary idea faces the first problem, which is that after the vector is cut in half, the commitment A of the original vector \\vec{a} seems unusable. So how can Verifier obtain the commitment of the folded vector after Prover performs a folding action? From another perspective, Pedersen commitment, in a sense, is also a kind of inner product, that is, the inner product of the “vector to be committed” and the “base vector”:\\mathsf{cm}_{\\vec{G}}(\\vec{a}) = a_1G_1 + a_2G_2 + \\cdots + a_mG_m\n\nThe next trick is crucial. We split the base vector \\vec{G} in the same way, then fold it similarly, but use a different “challenge vector”:(x^{-1}, x)\n\\left[\n\\begin{array}{ll}\nG_1 &G_2\\\\\nG_3 &G_4\n\\end{array}\n\\right]\n= (G_1x^{-1} + G_3x, \\quad G_2x^{-1} + G_4x) = \\vec{G}'\n\nPlease note that the two challenge vectors (x, x^{-1}) and (x^{-1}, x) above look symmetric. The purpose of doing this is to create a special constant term. When calculating the inner product of the new vectors \\vec{a}' and \\vec{G}' together, its constant term is exactly the inner product of the original vector \\vec{a} and \\vec{G}. But for the non-constant terms (coefficients of x^2 and x^{-2} terms), they are some values that look rather messy.\n\nLet’s expand the calculation. First, we split \\vec{a} into two sub-vectors \\vec{a}=(\\vec{a}_1, \\vec{a}_2), \\vec{G}=(\\vec{G}_1, \\vec{G}_2), then perform folding respectively to get \\vec{a}'=\\vec{a}_1x + \\vec{a}_2x^{-1}, \\vec{G}'=\\vec{G}_1x^{-1} + \\vec{G}_2x,\\vec{a}'\\cdot\\vec{G}'= \n(1,1)\\left(\\left[\n\\begin{array}{cc}\n\\vec{a}_1\\cdot\\vec{G}_1 &  \\vec{a}_1\\cdot\\vec{G}_2 \\\\\n\\vec{a}_2\\cdot\\vec{G}_1 &  \\vec{a}_2\\cdot\\vec{G}_2\n\\end{array}\n\\right]\n\\circ\n\\left[\n\\begin{array}{cc}\n1 & x^2 \\\\\nx^{-2} &  1\n\\end{array}\n\\right]\\right)\n\\left(\n\\begin{array}{c}\n1\\\\\n1\n\\end{array}\n\\right)\n\nThe right side of the above formula is a polynomial in x, so we can clearly see the coefficients of x^2 and x^{-2}.\n\nNext, let’s solve the problem raised above: How to let Verifier calculate the commitment of the folded vector \\vec{a}':A'=A + (\\vec{a}_1\\cdot\\vec{G}_2)~x^2 + (\\vec{a}_2\\cdot\\vec{G}_1)~x^{-2}\n\nWe let Verifier calculate A'=\\vec{a}'\\cdot \\vec{G}'. Obviously, Verifier can calculate \\vec{G}' on their own, then Verifier can calculate A' based on \\vec{a}' sent by Prover.\n\nThe commitment A' is a polynomial in x, where the constant term is the commitment A of the original vector, and the coefficients of the x^2 and x^{-2} terms can be calculated and sent by Prover. What about the coefficients of the x^1 and x^{-1} terms? They happen to be zero. Because we cleverly used two symmetric challenge vectors to perform folding operations on \\vec{a} and \\vec{G} respectively, the coefficients of the x and x^{-1} terms are exactly canceled out, while making the constant term equal to the inner product of the original vectors.\n\nThe new commitment A' is easy to calculate A'=A + Lx^2 + Rx^{-2}, where L=(\\vec{a}_1\\cdot \\vec{G}_2), R=(\\vec{a}_2\\cdot \\vec{G}_1). The commitments L and R appear to be the result of cross inner products of the two sub-vectors. In this way, the problem is solved. When Prover needs to send a vector \\vec{v} of length m, Prover can choose to send the vector \\vec{v}' that is folded and flattened, which has a length of only m/2. Then Verifier can also verify the correctness of the original vector by verifying the opening of the folded and flattened vector.\n\nRecursive folding also has its cost. In addition to Verifier having to calculate \\vec{G}' extra, Prover also has to calculate L and R extra, and they also need to add one round of interaction. We can see the complete process of recursive folding through the following recursive folding inner product proof protocol.","type":"content","url":"/hyrax-pcs/hyrax-01#id-4-bulletproofs-optimization","position":49},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Public Parameters","lvl2":"4. Bulletproofs Optimization"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#public-parameters-1","position":50},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Public Parameters","lvl2":"4. Bulletproofs Optimization"},"content":"\\vec{G}, \\vec{H}\\in\\mathbb{G}^n; U, T\\in\\mathbb{G}","type":"content","url":"/hyrax-pcs/hyrax-01#public-parameters-1","position":51},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Public Input","lvl2":"4. Bulletproofs Optimization"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#public-input-3","position":52},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Public Input","lvl2":"4. Bulletproofs Optimization"},"content":"P = \\vec{a}\\vec{G} + \\vec{b}\\vec{H} + cU + \\rho T\n\nWitnesses: \\vec{a}, \\vec{b}\\in\\mathbb{Z}^n_p; c\\in\\Z_p\n\nStep 1 (Commitment Step): Prover sends commitments of two mask vectors P_0 and C_1, C_2:\n\nP_0=\\vec{a}_0\\vec{G} + \\vec{b}_0\\vec{H} + \\rho_{0}T\n\nC_1=\\vec{a}_0\\cdot\\vec{b}_0\\cdot{}U + \\rho_{1}T\n\nC_2 = (\\vec{a}\\cdot\\vec{b}_0 + \\vec{a}_0\\cdot\\vec{b})\\cdot U + \\rho_{2}T\n\nStep 2 (Challenge): Verifier replies with a challenge number z\n\nStep 3: Prover calculates the flattened vectors \\vec{a}', \\vec{b}', \\rho' with z, and sends \\rho_c\n\n\\vec{a}' = \\vec{a} + z\\vec{a}_0\n\n\\vec{b}' = \\vec{b} + z\\vec{b}_0\n\n\\rho' = \\rho + z\\rho_0\n\n\\rho_c = z^2\\rho_1 + z\\rho_2","type":"content","url":"/hyrax-pcs/hyrax-01#public-input-3","position":53},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl2":"4. Bulletproofs Optimization"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#verification-2","position":54},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl2":"4. Bulletproofs Optimization"},"content":"Verifier can calculate P'\n\nP' = P + zP_0 + zC_2 + z^2C_1 - \\rho_cT\n\nThus, Prover and Verifier can continue to run the rIPA protocol to prove \\vec{a}'\\cdot\\vec{b}'\\overset{?}{=}c', where the commitments of these three values are merged into P' = \\vec{a}'\\vec{G} + \\vec{b}'\\vec{H} + (\\vec{a}'\\cdot\\vec{b}')U + \\rho'T.","type":"content","url":"/hyrax-pcs/hyrax-01#verification-2","position":55},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"5. Complete Protocol"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#id-5-complete-protocol","position":56},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"5. Complete Protocol"},"content":"Below is the complete protocol combining recursive folding and Square-root IPA, which supports the Zero-Knowledge property. If the ZK property is not needed, simply remove the H part related to \\rho.","type":"content","url":"/hyrax-pcs/hyrax-01#id-5-complete-protocol","position":57},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Parameters","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-parameters-2","position":58},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Parameters","lvl2":"5. Complete Protocol"},"content":"G_0, G_1, G_2, \\ldots, G_{2^n-1}, H, U \\in \\mathbb{G}.","type":"content","url":"/hyrax-pcs/hyrax-01#public-parameters-2","position":59},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Calculating Commitment","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#calculating-commitment","position":60},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Calculating Commitment","lvl2":"5. Complete Protocol"},"content":"Prover rearranges \\vec{a} into a matrix M_a\\in\\mathbb{F}_p^{l\\times h}:M_a =\n\\begin{bmatrix}\na_0 & a_1 & a_2 & \\cdots & a_{l-1} \\\\\na_l & a_{l+1} & a_{l+2} & \\cdots & a_{2l-1} \\\\\na_{2l} & a_{2l+1} & a_{2l+2} & \\cdots & a_{3l-1} \\\\\na_{(h-1)l} & a_{(h-1)l+1} & a_{(h-1)l+2} & \\cdots & a_{hl-1} \\\\\n\\end{bmatrix}\n\nProver calculates commitments C_0, C_1, ..., C_{h-1} by row\\begin{aligned}\nC_0 &= \\mathsf{cm}(a_0, a_1, ..., a_{l-1}; \\rho_0) \\\\\nC_1 &= \\mathsf{cm}(a_l, a_{l+1}, ..., a_{2l-1}; \\rho_1) \\\\\nC_2 &= \\mathsf{cm}(a_{2l}, a_{2l+1}, ..., a_{3l-1}; \\rho_2) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nC_{h-1} &= \\mathsf{cm}(a_{(h-1)l}, a_{(h-1)l+1}, ..., a_{hl-1}; \\rho_{h-1}) \\\\\n\\end{aligned}\n\nHere, \\mathsf{cm}(\\vec{a};\\rho) is defined as:\\mathsf{cm}(\\vec{a};\\rho) = \\sum_{i=0}^{l-1} a_iG_i + \\rho H","type":"content","url":"/hyrax-pcs/hyrax-01#calculating-commitment","position":61},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Evaluation Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#evaluation-proof-protocol-1","position":62},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Evaluation Proof Protocol","lvl2":"5. Complete Protocol"},"content":"","type":"content","url":"/hyrax-pcs/hyrax-01#evaluation-proof-protocol-1","position":63},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#public-input-4","position":64},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Public Input","lvl2":"5. Complete Protocol"},"content":"Commitment of \\vec{a}: (C_0, C_1, ..., C_{h-1})\n\n\\vec{u}=(u_0, u_1, ..., u_{n-1})=\\vec{u}_L \\parallel \\vec{u}_R, where |\\vec{u}_L|=\\log(h), |\\vec{u}_R|=\\log(l)\n\nv=\\tilde{f}(u_0, u_1, ..., u_{n-1})","type":"content","url":"/hyrax-pcs/hyrax-01#public-input-4","position":65},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#witness-2","position":66},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Witness","lvl2":"5. Complete Protocol"},"content":"\\vec{a}\n\n(\\rho_0, \\rho_1, ..., \\rho_{h-1})","type":"content","url":"/hyrax-pcs/hyrax-01#witness-2","position":67},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl3","url":"/hyrax-pcs/hyrax-01#proof-protocol","position":68},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"","type":"content","url":"/hyrax-pcs/hyrax-01#proof-protocol","position":69},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-1-2","position":70},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 1","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"Prover calculates \\vec{e}:\\begin{split}\ne_0 &= \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_R) \\\\\ne_1 &= \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_R) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\ne_{h-1} &= \\tilde{eq}(\\mathsf{bits}(h-1), \\vec{u}_R) \\\\\n\\end{split}\n\nProver calculates the matrix multiplication of \\vec{e} and M_a, obtaining \\vec{b}, length l\\begin{split}\nb_0 &= \\langle \\vec{e}, (a_0, a_l, ..., a_{(h-1)l}) \\rangle \\\\\nb_1 &= \\langle \\vec{e}, (a_1, a_{l+1}, ..., a_{(h-1)l+1}) \\rangle \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nb_{l-1} &= \\langle \\vec{e}, (a_{l-1}, a_{l+l-1}, ..., a_{(h-1)l+l-1}) \\rangle \\\\\n\\end{split}\n\nProver calculates the commitment C^* of \\vec{b}C^* = \\mathsf{cm}(\\vec{b}; \\rho^*)","type":"content","url":"/hyrax-pcs/hyrax-01#round-1-2","position":71},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-2-2","position":72},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 2.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"Prover and Verifier conduct an IPA protocol to complete the inner product proof of \\vec{b} and \\vec{d}, \\vec{d} is calculated as follows:\\begin{split}\nd_0 &= \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_L) \\\\\nd_1 &= \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_L) \\\\\n\\cdots\\ &=\\quad \\cdots \\\\\nd_{h-1} &= \\tilde{eq}(\\mathsf{bits}(h-1), \\vec{u}_L) \\\\\n\\end{split}\n\nVerifier sends a random number \\gamma\n\nProver and Verifier calculate U'\\in\\mathbb{G}U' = \\gamma\\cdot U","type":"content","url":"/hyrax-pcs/hyrax-01#round-2-2","position":73},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 3 (Repeated i=0, 1, ..., n-1).","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-3-repeated-i-0-1-n-1","position":74},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 3 (Repeated i=0, 1, ..., n-1).","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"First introduce the following symbols, for example, \\vec{b}_L represents the first half of \\vec{b}, \\vec{b}_R represents the second half of \\vec{b}.\\begin{aligned} \n\\vec{b}^{(i)}_L &= (b^{(i)}_0, b^{(i)}_1, ..., b^{(i)}_{2^{n-1}-1}) \\\\\n\\vec{b}^{(i)}_R &= (b^{(i)}_{2^{n-1}}, b^{(i)}_{2^{n-1}+1}, ..., b^{(i)}_{2^n-1}) \\\\\n\\vec{d}^{(i)}_L &= (d^{(i)}_0, d^{(i)}_1, ..., d^{(i)}_{2^{n-1}-1}) \\\\\n\\vec{d}^{(i)}_R &= (d^{(i)}_{2^{n-1}}, d^{(i)}_{2^{n-1}+1}, ..., d^{(i)}_{2^n-1}) \\\\\n\\vec{G}^{(i)}_L &= (G^{(i)}_0, G^{(i)}_1, ..., G^{(i)}_{2^{n-1}-1}) \\\\\n\\vec{G}^{(i)}_R &= (G^{(i)}_{2^{n-1}}, G^{(i)}_{2^{n-1}+1}, ..., G^{(i)}_{2^n-1}) \\\\\n\\end{aligned}\n\nNote the initial values here, \\vec{b}^{(0)} = \\vec{b}, \\vec{d}^{(0)} = \\vec{d}, \\vec{G}^{(0)}_L=\\vec{G}_L, \\vec{G}^{(0)}_R=\\vec{G}_R.\n\nProver sends L^{(i)} and R^{(i)}:\\begin{aligned}\nL^{(i)} &= \\mathsf{cm}_{\\vec{G}^{(i)}_L}(\\vec{b}^{(i)}; \\rho^{(i)}_L) + \\langle\\vec{b}^{(i)}_R, \\vec{d}^{(i)}_L\\rangle\\cdot{}U' \\\\\nR^{(i)} &= \\mathsf{cm}_{\\vec{G}^{(i)}_R}(\\vec{b}^{(i)}; \\rho^{(i)}_R) + \\langle\\vec{b}^{(i)}_L, \\vec{d}^{(i)}_R\\rangle\\cdot{}U' \\\\\n\\end{aligned}\n\nVerifier sends a random number \\mu^{(i)},\n\nProver calculates and sends the following values:\\begin{aligned}\n\\vec{b}^{(i+1)} &= \\vec{b}^{i}_L + \\mu^{(i)}\\cdot\\vec{b}^{i}_R \\\\\n\\vec{d}^{(i+1)} &= \\vec{d}^{i}_L + {\\mu^{(i)}}^{-1}\\cdot\\vec{d}^{i}_R \\\\\n\\end{aligned}\n\nProver and Verifier calculate \\vec{G}^{(i+1)}\\begin{aligned}\n\\vec{G}^{(i+1)} &= \\vec{G}^{(i)}_L + {\\mu^{(i)}}^{-1}\\cdot\\vec{G}^{(i)}_R \\\\\n\\end{aligned}\n\nProver and Verifier recursively perform Round 3 until i=n-1\n\nProver calculates\\hat{\\rho} = \\rho^* + \\sum_{i=0}^{n-1}\\mu^{(i)}\\cdot\\rho^{(i)}_L + {\\mu^{(i)}}^{-1}\\cdot\\rho^{(i)}_R","type":"content","url":"/hyrax-pcs/hyrax-01#round-3-repeated-i-0-1-n-1","position":75},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 4.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-4","position":76},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round 4.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"Prover calculates and sends R, where r, \\rho_r\\in\\mathbb{F}_p are random numbers sampled by ProverR = r\\cdot(G^{(n-1)} + b^{(n-1)}\\cdot{U'}) + \\rho_r\\cdot{}H","type":"content","url":"/hyrax-pcs/hyrax-01#round-4","position":77},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round  5.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#round-5","position":78},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Round  5.","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"Verifier sends a random number \\zeta\\in\\mathbb{F}_p\n\nProver calculates z and z_rz = r + \\zeta\\cdot b^{(n-1)}z_r = \\rho_r + \\zeta\\cdot\\hat{\\rho}","type":"content","url":"/hyrax-pcs/hyrax-01#round-5","position":79},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"type":"lvl4","url":"/hyrax-pcs/hyrax-01#verification-3","position":80},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl4":"Verification","lvl3":"Proof Protocol","lvl2":"5. Complete Protocol"},"content":"Verifier calculates C^* and PC^* = d_0C_0 + d_1C_1 + ... + d_{h-1}C_{h-1}P = C^* + \\sum_{i=0}^{n-1}\\mu^{(i)}L^{(i)} + {\\mu^{(i)}}^{-1}R^{(i)}\n\nVerifier verifies if the following equation holdsR + \\zeta\\cdot P \\overset{?}{=} z\\cdot (G^{(n-1)} + b^{(n-1)}\\cdot{U'}) + z_r\\cdot{}H","type":"content","url":"/hyrax-pcs/hyrax-01#verification-3","position":81},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"References"},"type":"lvl2","url":"/hyrax-pcs/hyrax-01#references","position":82},{"hierarchy":{"lvl1":"Notes on Hyrax-PCS","lvl2":"References"},"content":"[WTSTW16] Riad S. Wahby, Ioanna Tzialla, abhi shelat, Justin Thaler, and Michael Walfish. “Doubly-efficient zkSNARKs without trusted setup.”  In 2018 IEEE Symposium on Security and Privacy (SP), pp. 926-943. IEEE, 2018.  \n\nhttps://​eprint​.iacr​.org​/2016​/263\n\n[BBB+18] Bünz, Benedikt, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter Wuille, and Greg Maxwell. “Bulletproofs: Short proofs for confidential transactions and more.” In 2018 IEEE symposium on security and privacy (SP), pp. 315-334. IEEE, 2018. \n\nhttps://​eprint​.iacr​.org​/2017​/1066\n\n[BCC+16] Jonathan Bootle, Andrea Cerulli, Pyrros Chaidos, Jens Groth, and Christophe Petit. “Efficient Zero-Knowledge Arguments for Arithmetic Circuits in the Discrete Log Setting.”  In Advances in Cryptology–EUROCRYPT 2016: 35th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Vienna, Austria, May 8-12, 2016, Proceedings, Part II 35, pp. 327-357. Springer Berlin Heidelberg, 2016.  \n\nhttps://​eprint​.iacr​.org​/2016​/263","type":"content","url":"/hyrax-pcs/hyrax-01#references","position":83},{"hierarchy":{"lvl1":"Land of Polynomial Commitment Schemes"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"Land of Polynomial Commitment Schemes"},"content":"🔐 KZG Polynomial Commitments\n\nExplore KZG10 and related schemes »\n\n📊 FRI-based Schemes\n\nLearn about FRI and proximity testing »\n\n🔄 BaseFold & Folding\n\nDiscover BaseFold and folding techniques »\n\n⚡ Gemini & HyperKZG\n\nSee Gemini and HyperKZG schemes »\n\n🎯 Hyrax PCS\n\nExplore Hyrax polynomial commitments »\n\n📈 Libra PCS\n\nLearn about Libra scheme »\n\n🌟 Ligerito PCS\n\nDiscover Ligerito commitments »\n\n🔬 PH23 & Advanced Topics\n\nExplore PH23, Zeromorph, and more »\n\n🐺 Zeromorph PCS\n\nLearn about Zeromorph polynomial commitments »\n\n🌙 Mercury PCS\n\nExplore Mercury polynomial commitments »\n\n🦅 Virgo PCS\n\nDiscover Virgo polynomial commitments »\n\n🦮 GreyHound PCS\n\nLearn about GreyHound scheme »\n\n🔵 Binius & FRI\n\nExplore Binius with FRI »\n\n🌪️ WHIR PCS\n\nDiscover WHIR polynomial commitments »\n\n📚 Coding Theory\n\nLearn about coding theory fundamentals »\n\n🧮 Mathematical Foundations\n\nExplore mathematical concepts »\n\n🔄 Reductions & Optimizations\n\nLearn about reduction techniques »","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1"},"type":"lvl1","url":"/kzg10/kzg-notes/kzg-soundness-1","position":0},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1"},"content":"Tianyu ZHENG \n\ntian​-yu​.zheng@connect​.polyu​.hk\n\nThe KZG polynomial commitment scheme is widely used in zk-SNARKs systems, blockchain scaling solutions (such as Verkle trees), and data availability solutions due to its concise proof length, efficient verification algorithm, and updatable structured reference string (SRS). However, different application scenarios have different security requirements for KZG. Since the KZG10 paper, researchers have conducted in-depth studies on the security of KZG. The following are the main studies related to zk-SNARKs:\n\nKZG10: The first paper on the KZG polynomial commitment scheme, which only proved that the scheme satisfies the evaluation binding property.\n\nSonic: Used KZG to construct zk-SNARKs and provided the first proof of KZG extractability in the Algebraic Group Model (AGM).\n\nPlonk: Also provided a proof of KZG extractability in AGM, but its definition has some flaws.\n\nMarlin: Provided a formal definition and gave extractability proofs in both the standard model and AGM.\n\nAGMOS: Proposed the AGMOS assumption and proved KZG extractability based on a falsifiable assumption.\n\nLPS24: Proved KZG extractability based on a falsifiable assumption only in the Random Oracle Model (ROM).\n\nIn this article, we first review the discussion of the evaluation binding property in the KZG10 paper. Next, we introduce the necessity of the KZG extractability property and provide its proof in AGM. Finally, we briefly mention some conclusions from AGMOS.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-1","position":1},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Soundness in KZG10"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-1#soundness-in-kzg10","position":2},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Soundness in KZG10"},"content":"[KZG10 Protocol]\n\nSetup: Given \\mathbb{G}_1, \\mathbb{G}_2, \\mathbb{G}_T, e:\\mathbb{G}_1\\times \\mathbb{G}_2 \\mapsto \\mathbb{G}_T, randomly select \\tau \\in \\mathbb{F} and computeSRS = ([1]_1,[\\tau]_1,[\\tau^2]_1,\\ldots,[\\tau^{D}]_1, [1]_2, [\\tau]_2)\n\nCommit: The prover computes a degree d univariate polynomial f(X) \\in \\mathbb{F}[X] to generate a commitment C_f = [f(\\tau)]C_f = \\mathsf{Commit}(f(X)) = f_0\\cdot[1]_1+f_1\\cdot[\\tau]_1+ \\cdots + f_d\\cdot[\\tau^d]_1\n\nOpen: The prover reveals the value of the polynomial at point z as f(z)=v, and generates the quotient polynomialq(X) = \\frac{f(X)-f(z)}{X-z}\n\nGenerate the proof \\pi = [q(\\tau)]_1\\pi = q_0\\cdot[1]_1 + q_1\\cdot[\\tau]_1+\\cdots+q_d\\cdot[\\tau^d]_1\n\nCheck: The verifier checks the evaluation proofe([f(\\tau)]_1-f(z)\\cdot[1]_1, [1]_2) = e([q(\\tau)]_1, [\\tau -z]_2)\n\n[Evaluation Binding]\n\nGiven an SRS, if for any adversary \\mathcal{A}, the probability of finding a polynomial commitment value C_f and point z, and generating two valid proofs \\pi, \\pi' for f(z) = v, f(z) = v' respectively, is negligible, then we consider the above polynomial commitment algorithm to satisfy the evaluation binding property.\n\nProving this conclusion requires the t-Strong Diffie-Hellman Assumption:\n\nLet \\alpha \\in \\mathbb{F} be a random value. Given the input ([1]_1, [\\alpha]_1, [\\alpha^2]_1, \\ldots, [\\alpha^t]_1, [\\alpha]_2), it is difficult for any adversary \\mathcal{A} to find an element [\\frac{1}{\\alpha+c}]_1 in \\mathbb{G}_1, where c can be any value in \\mathbb{F}/\\{-\\alpha\\}.\n\n[Security Proof]\n\nAssume there exists an adversary \\mathcal{A} that can break the evaluation binding property of the KZG10 polynomial commitment scheme: that is, given the input SRS, \\mathcal{A} computes a commitment value C_f and two tuples (z, v, \\pi), (z, v', \\pi') that can both pass the verifier’s Check algorithm. Then we can construct an efficient algorithm \\mathcal{B} based on \\mathcal{A}, which can break the t-SDH assumption.\n\nIn simple terms, we prove that if \\mathcal{A} exists, then we can construct \\mathcal{B}. The contrapositive is also true: if we cannot construct \\mathcal{B}, then \\mathcal{A} does not exist. In fact, since algorithm \\mathcal{B} (i.e., the algorithm that breaks the t-SDH assumption) is considered difficult to construct, we can conclude that the adversary \\mathcal{A} attacking KZG10 also does not exist.\n\nNext, we only need to provide the construction algorithm for \\mathcal{B}:\n\n\\mathcal{B} sends a t-SDH instance (\\mathbb{G}_1, [1]_1, [\\alpha]_1, [\\alpha^2]_1, \\ldots, [\\alpha^t]_1, [\\alpha]_2) as SRS to \\mathcal{A}.\n\nAccording to the assumption, \\mathcal{A} can output C_f, (z, v, \\pi), (z, v', \\pi') with non-negligible probability in polynomial time, satisfying\ne([C_f-v\\cdot[1]_1, [1]_2) = e(\\pi, [\\alpha-z]_2)\\\\ e([C_f-v'\\cdot[1]_1, [1]_2) = e(\\pi', [\\alpha-z]_2)\n\nWithout loss of generality, assume \\pi = [q(\\alpha)]_1, \\pi' = [q'(\\alpha)]_1, they satisfy\nq(\\alpha) \\cdot (\\alpha-z) + v= f(\\alpha) = q'(\\alpha) \\cdot (\\alpha-z) + v' \\\\ \\frac{q(\\alpha)/q'(\\alpha)}{v'-v} = \\frac{1}{\\alpha-z}\n\n\nNotice that the right-hand side is our target relation, so \\mathcal{B} only needs to compute the following to break the t-SDH assumption\n\\frac{(\\pi-\\pi')}{v'-v} = [\\frac{1}{\\alpha-z}]_1","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-1#soundness-in-kzg10","position":3},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Extractable KZG"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-1#extractable-kzg","position":4},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Extractable KZG"},"content":"[Why is Extractability Needed?]\n\nThe above evaluation binding property only ensures that the prover cannot open two different values at the same point, which indeed satisfies the requirements of some application scenarios, such as membership proofs. However, with the development of SNARKs, polynomial commitment proofs like KZG10 are used to construct SNARKs, instantiating polynomial Oracle in an Interactive Oracle Proof (IOP). The security properties required by IOPs impose new requirements on KZG.\n\nFor an ideal IOP protocol, its Knowledge Soundness is very easy to guarantee. According to the conclusion in HypePlonk, when an IOP satisfies the soundness property, it also satisfies the Knowledge Soundness property: there exists an extractor that only needs to query a polynomial oracle d+1 times to compute the entire polynomial itself.\n\nHowever, when we consider instantiating polynomial Oracles in IOPs with PCS, ensuring that the instantiated SNARK satisfies the Knowledge Soundness property is not so easy.\n\nCompared to the prover sending an oracle containing the entire polynomial in IOP (in the IOP model, the length of the oracle is the same as the polynomial, but the verifier does not fully read it), in SNARK, the prover only sends the commitment of the polynomial, which contains very little information: only the values of the polynomial at a few points.\n\nTherefore, we can only ensure that the polynomial commitment itself is “Extractable” to guarantee the Knowledge Soundness of SNARK. (For detailed arguments, refer to Interactive Oracle Proofs by Eli Ben-Sasson et al. and DARK)\n\nSonic, Plonk, Marlin, and other SNARKs based on polynomial commitments have successively discussed this issue. Below we mainly refer to the description in Sonic to introduce the proof of KZG10 Extractability based on the AGM model.\n\n[Algebraic Group Model]\n\nAGM (Algebraic Group Model) is an idealized security model that is stronger than the standard model but weaker than the Generic Group Model (GGM).\n\nIn AGM, we assume that the adversary algorithm \\mathcal{A}_{alg} is “algebraic”: that is, whenever \\mathcal{A}_{alg} outputs a group element Z, it will simultaneously output a representation of Z, which includes a vector of field elements (z_1,...,z_t) \\in F^t to describe how the group element Z is computed, i.e., Z = \\prod_{i=1}^t g_i^{z_i}, where *\\{ g_1,...,g_t\\}* is the list of all group elements received by \\mathcal{A}_{alg} from the beginning of the protocol until now.\n\nAs in the standard model, we also use reductions to prove security relations under AGM.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-1#extractable-kzg","position":5},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Extractability under AGM"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-1#extractability-under-agm","position":6},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"Extractability under AGM"},"content":"[Extractability Definition]\n\nFor any algebraic adversary \\mathcal{A}_{alg}, if it can output a valid KZG evaluation proof, then there must exist another efficient algorithm \\mathcal{B}_{alg} that can extract the public value f(X) of the commitment C_f, satisfying f(z) = v.\n\n[q-DLOG Assumption]\n\nLet \\alpha \\in \\mathbb{F} be a random value. Given the input ([1]_1, [\\alpha]_1, [\\alpha^2]_1, \\ldots, [\\alpha^t]_1, [1]_2,[\\alpha]_2), it is difficult for any adversary \\mathcal{A} to compute \\alpha.\n\n[Security Proof]\n\nGoal: Adv_{\\mathcal{A}_{alg}}^{Extract} \\leq Adv_{\\mathcal{B}_{alg}}^{qDL}, i.e., the extractability of KZG broken by \\mathcal{A}_{alg} can be reduced to the q-DLOG problem broken by \\mathcal{B}_{alg}.\n\nProof: Assume there exists an adversary algorithm \\mathcal{A}_{alg} that can provide a valid proof without knowing the correct f(X). Then we can construct an algorithm \\mathcal{B}_{alg} that simulates an extractability game with \\mathcal{A}_{alg} based on its input q-DLOG problem instance ([1]_1, [\\alpha]_1, [\\alpha^2]_1, \\ldots, [\\alpha^D]_1, [1]_2,[\\alpha]_2):\n\n\\mathcal{B}_{alg} sends the q-DLOG instance as SRS to \\mathcal{A}_{alg}SRS = ([1]_1,[\\alpha]_1,[\\alpha^2]_1,\\ldots,[\\alpha^{D}]_1, [1]_2, [\\alpha]_2)\n\nAccording to the assumption, \\mathcal{A}_{alg} can output the message C_f, (z, v, \\pi) with non-negligible probability in polynomial time, passing verification.\n\nSince \\mathcal{A}_{alg} is an algebraic algorithm, it will simultaneously output the representations of group elements C_f, \\pi, denoted as f'(X), q'(X), satisfyingC_f = [f'(\\alpha)]_1, \\pi = [q'(\\alpha)]_1\n\nMeanwhile, since the proof passes verification, C_f, \\pi satisfye(C_f-v\\cdot[1]_1, [1]_2) = e(\\pi, [\\alpha -z]_2)\n\nBased on the relationship satisfied by the exponents in the verification equation, \\mathcal{B}_{alg} can construct a polynomial Q(X) using f'(X), q'(X)Q(X) = f'(X) - v - (X-z)q'(X)\n\nIf Q(X) = 0, then \\mathcal{B}_{alg} terminates the algorithm.\n\nIf Q(X) \\neq 0, \\mathcal{B}_{alg} factors the polynomial and finds all its roots.\n\nTest each root in turn to see if it satisfies the given q-DLOG instance. If a certain x satisfies it, output x as the solution to the q-DLOG problem.\n\nWe argue that in the above reduction, if the success probability of \\mathcal{A}_{alg} is a non-negligible \\epsilon, then the success probability of \\mathcal{B}_{alg} is \\epsilon+\\mathsf{negl}(\\lambda). Clearly, the event that \\mathcal{B}_{alg} cannot output a solution to a q-DLOG problem can occur in two places:\n\nQ(X) = 0, then \\mathcal{B}_{alg} terminates the algorithm. From Q(X) = 0, it can be deduced that f'(X) - v = (X-z)q'(X), i.e., f'(z) = v, which contradicts the initial assumption.\n\nThe roots obtained by factoring Q(X) do not satisfy the q-DLOG instance. Since \\mathcal{B}_{alg} did not terminate the algorithm in the previous step, it is known that Q(X) \\neq 0, and the proof output by \\mathcal{A}_{alg} can pass verification, i.e., [f'(\\alpha)-v]_T = [(\\alpha-z)q'(\\alpha)]_T. Unless \\mathcal{A}_{alg} breaks the DL problem in G_T, \\alpha must be a root of Q(X).","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-1#extractability-under-agm","position":7},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"AGMOS"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-1#agmos","position":8},{"hierarchy":{"lvl1":"KZG-Soundness-zh-1","lvl2":"AGMOS"},"content":"Recently, Lipmaa et al. proposed AGMOS (Algebraic Group Model with Oblivious Sampling). AGMOS is a more realistic variant of AGM, which gives the adversary the additional ability to sample group elements obliviously without knowing the discrete logarithm (e.g., hash-to-group in practice).\n\nFurthermore, Lipmaa pointed out that there are two different definitions of KZG extractability in practical protocol design:\n\nThe extractor algorithm extracts the polynomial after the Commit and Open phases, as in Marlin and Sonic.\n\nThe extractor algorithm extracts the polynomial only after the Commit phase, as in Plonk and Lunar.\n\nLipmaa noted in the article that the latter leads to a spurious knowledge assumption that is secure under AGM but insecure in the standard model.\n\n[KZG10] Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. “Constant-size commitments to polynomials and their applications.” International conference on the theory and application of cryptology and information security. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010.\n\n[Sonic] Maller, Mary, et al. “Sonic: Zero-knowledge SNARKs from linear-size universal and updatable structured reference strings.” Proceedings of the 2019 ACM SIGSAC conference on computer and communications security. 2019.\n\n[Plonk] Gabizon, Ariel, Zachary J. Williamson, and Oana Ciobotaru. “Plonk: Permutations over lagrange-bases for oecumenical noninteractive arguments of knowledge.” Cryptology ePrint Archive (2019).\n\n[Marlin] Chiesa, Alessandro, et al. “Marlin: Preprocessing zkSNARKs with universal and updatable SRS.” Advances in Cryptology–EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part I 39. Springer International Publishing, 2020.\n\n[AGMOS] Lipmaa, Helger, Roberto Parisella, and Janno Siim. “Algebraic group model with oblivious sampling.” Theory of Cryptography Conference. Cham: Springer Nature Switzerland, 2023.\n\n[LPS24] Lipmaa, Helger, Roberto Parisella, and Janno Siim. “Constant-size zk-SNARKs in ROM from falsifiable assumptions.” Annual International Conference on the Theory and Applications of Cryptographic Techniques. Cham: Springer Nature Switzerland, 2024.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-1#agmos","position":9},{"hierarchy":{"lvl1":"KZG Extractability based on ROM"},"type":"lvl1","url":"/kzg10/kzg-notes/kzg-soundness-2","position":0},{"hierarchy":{"lvl1":"KZG Extractability based on ROM"},"content":"KZG10 and other polynomial commitment proofs are often used to construct SNARKs, typically by compiling polynomial oracles in an Interactive Oracle Proof using PCS.\n\nConsidering security, the Knowledge Soundness of the IOP itself is easy to guarantee. However, for the SNARK obtained after compiling the IOP with PCS, proving its Knowledge Soundness property is not as straightforward.\n\nCompared to the IOP model where the prover sends an oracle containing an entire polynomial (in the IOP model, the length of the oracle is the same as the polynomial, but the verifier doesn’t read it entirely), in SNARK, the prover only sends commitments to the polynomials, which contain very little information: just the values of the polynomial at a few points.\n\nTherefore, we can only guarantee the Knowledge Soundness of SNARK if we ensure that the polynomial commitment itself is “extractable”. (For a detailed argument, refer to Interactive Oracle Proofs by Eli Ben-Sasson et al.)\n\nUnfortunately, Kate et al. did not prove that the KZG10 protocol has extractability. Therefore, to use KZG10 in constructing SNARKs, we must reprove its security.\n\nA series of previous works, including Sonic [MBK+19], Plonk [GWC19], Marlin [CHM+19], proposed schemes to prove that the KZG10 scheme satisfies extractability based on non-falsifiable assumptions (Knowledge Assumptions) or based on idealized group models (Idealized Group Model) such as GGM, AGM. It can be said that most SNARK systems constructed based on the KZG scheme currently indirectly rely on the idealized group model.\n\nAt the same time, SNARK systems use the Fiat-Shamir transform to achieve non-interactive proofs, which means they also rely on another strong idealized model, namely the Random Oracle Model (ROM). This situation puts us in a rather bad position: our SNARK systems would have the flaws of both models! In recent years, some papers have attacked them separately.\n\nCompared to the idealized group model, the ROM model assumption is weaker (which means stronger security). If we can prove the security of the KZG scheme under the ROM model, we can remove the SNARK system’s dependence on the idealized group model, thus increasing our confidence in its security.\n\nIn this context, Lipmaa, Parisella, and Siim published their work “Constant-Size zk-SNARKs in ROM from Falsifiable Assumptions” (hereinafter referred to as [LPS24]) this year, making a significant step towards our goal. Their contributions include:\n\nProving the special soundness property of the KZG scheme under the ROM model based on a newly proposed falsifiable assumption\n\nFurther proving that the KZG scheme satisfies black-box extractability, for use in compiling IOPs\n\nMaking partial progress in proving the knowledge soundness property of Plonk under the ROM model\n\nIn this article, we will focus on introducing the work of the first point.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2","position":1},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Special Soundness"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-2#special-soundness","position":2},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Special Soundness"},"content":"To introduce special soundness, we first need to understand interactive proofs and their security definitions.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#special-soundness","position":3},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"Interactive Proofs and Knowledge Soundness","lvl2":"Special Soundness"},"type":"lvl3","url":"/kzg10/kzg-notes/kzg-soundness-2#interactive-proofs-and-knowledge-soundness","position":4},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"Interactive Proofs and Knowledge Soundness","lvl2":"Special Soundness"},"content":"【Definition 1: Public-coin Interactive Proofs】\n\nAn interactive protocol between two parties (prover and verifier) for proving a target relation R is called an Interactive Proof, denoted as \\Pi = (P, V), where P,V are the prover and verifier algorithms respectively. Specifically,\n\nProver input: public statement (denoted as x), secret witness (denoted as w)\n\nVerifier input: public statement (denoted as x)\n\nThe prover and verifier interact through a series of exchanges, and the collection of all interaction messages is called a transcript\n\nThe verifier outputs 1 for acceptance, 0 for rejection.\n\nIf all random numbers used by the verifier during the interaction are public, we call such an interactive protocol a Public-coin Interactive Proof. Additionally, if the prover sends k messages and the verifier sends k-1 messages during the entire interaction, we call it a (2k-1)-step protocol.\n\nAs is well known, to ensure that an interactive proof is secure, it needs to satisfy two security properties:\n\nCompleteness: For any honest prover P executing the protocol, and if there exists w such that (x,w) satisfies relation R, then P can cause the verifier to output acceptance by executing the protocol.\n\nSoundness: For any potentially malicious prover, and if there does not exist w such that (x,w) satisfies relation R, then P cannot cause the verifier to output acceptance by executing the protocol.\n\nThe above two security properties ensure the basic security of interactive proofs, but the definition of Soundness can only ensure that a certain statement x indeed belongs to relation R, and cannot meet the security requirements of some application scenarios. For example, in an identity authentication system, we require the prover to prove their identity: that is, they “possess” a private key sk corresponding to the public key pk satisfying pk = sk\\cdot G. If the proof only ensures the Soundness property, then the verifier only knows the conclusion that “pk belongs to the cyclic group \\mathbb{G} constituted by the generator G”. But this conclusion cannot guarantee that the prover actually possesses the private key sk. In fact, we can prove pk \\in \\mathbb{G} without knowing sk, for example, using Fermat’s Little Theorem.\n\nTherefore, we need a stronger security definition, namely **“**Knowledge Soundness”\n\n【Definition 2: Knowledge Soundness】\n\nFor an interactive proof \\Pi = (P, V), if there exists a polynomial-time algorithm P^* that can forge a proof to make the verifier accept with a non-negligible probability \\epsilon without knowing the w corresponding to x, then there must exist a polynomial-time extractor algorithm E, which uses P^* as a rewindable Oracle, can extract a w satisfying x with a non-negligible probability \\epsilon'. We call |\\epsilon'-\\epsilon| the soundness error, and if the size of this error is negligible, then \\Pi satisfies the Knowledge Soundness property.\n\n【Note】If a Public-coin Interactive Proof satisfies Completeness and Soundness for any adversary, we call it a Proof of Knowledge. If Soundness is only satisfied for polynomial-time adversaries, we call it an Argument of Knowledge.\n\nIt can be seen that the key to the definition of Knowledge Soundness lies in emphasizing the feasibility of constructing an extractor algorithm. That is to say, if a malicious prover claims that it is feasible to forge a legal proof without knowing w, then constructing an extractor for w based on this malicious prover is equally feasible, which contradicts the claim of the malicious prover. This ensures that any prover who can output a legal proof must “possess” the secret value w.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#interactive-proofs-and-knowledge-soundness","position":5},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"Proof of Knowledge Soundness (Taking Schnorr Protocol as an Example)","lvl2":"Special Soundness"},"type":"lvl3","url":"/kzg10/kzg-notes/kzg-soundness-2#proof-of-knowledge-soundness-taking-schnorr-protocol-as-an-example","position":6},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"Proof of Knowledge Soundness (Taking Schnorr Protocol as an Example)","lvl2":"Special Soundness"},"content":"We have already given a relatively specific definition of Knowledge Soundness. So how do we prove that an interactive proof protocol satisfies this property? Obviously, the most direct answer is to construct an extractor, but how to construct an extractor is a deep subject (plainly speaking, LPS24 is doing this). To facilitate explaining the work of LPS24, let’s start with a relatively simple example to explain the proof idea of knowledge soundness.\n\nAs shown in the figure below, the Schnorr protocol [Sch90] is a 3-step interactive proof conducted between the prover and the verifier. By executing this protocol, the prover can prove to the verifier that she possesses a secret value w satisfying the discrete logarithm relation W=wG\n\nTheir interaction process is as follows\n\nThe prover generates a random value r \\leftarrow \\mathbb{F}, calculates R=rG and sends it to the verifier\n\nThe verifier generates a random value c \\leftarrow \\mathbb{F} as a challenge and sends it to the prover\n\nThe prover calculates the public value z = r+cw and sends it to the verifier\n\nFinally, the verifier checks zG \\overset{?}{=} R +cW based on the messages received in the protocol. For convenience, we denote the transcript of the Schnorr protocol as (R,c,z).\n\nIt’s easy to prove that the Schnorr protocol satisfies the Completeness property, which we won’t elaborate on here.\n\nNext, we focus on the Knowledge Soundness property:\n\nAccording to Definition 2, we first give the conclusion: If there exists a polynomial-time algorithm P^* that can forge a legal Schnorr proof, then there must exist a polynomial-time extractor algorithm E that can extract the satisfying secret value w by rewinding P^*.\n\nSo how to construct an E to complete the proof? It might be difficult to write out the algorithm directly, so let’s break down this work into the following steps:\n\nFirst, we construct a sub-algorithm E_{ss}, given two transcripts about W as input, denoted as (R,c_1,z_1), (R,c_2,z_2), requiring R to be the same and c_1,c_2 to be different. This sub-algorithm should be able to output w satisfying W=wG\n\nThen, we construct another sub-algorithm E_{rw}, E_{rw} calls P^* as an Oracle, first obtains a legal transcript (R, c_1, z_1), then E_{rw} rewinds P^* to the second step of the Schnorr protocol, tries to send a challenge value different from c_1 to P^* with the same R, until P^* outputs another legal transcript (R, c_2, z_2)\n\nFinally, the E algorithm first runs E_{rw} to obtain two transcripts that meet the conditions, and then runs E_{ss} to obtain w\n\n【Implementing sub-algorithm E_{ss}】\n\nThe implementation of sub-algorithm E_{ss} often appears in the security proofs of various papers. Simply put, E_{ss} can obtain the public values z_1, z_2 from the two input transcripts. Assuming P^* honestly calculated these two values, they should satisfy the following form:z_1 = r + c_1 w' \\\\ z_2 = r+c_2w'\n\nBy solving the equation, we can calculate w' = (z_1-z_2)/(c_1-c_2) as a possible secret value, and we only need to check w'G\\overset{?}{=}W to know if it’s legal. If they are equal, E' directly outputs the legal secret value w=w', and the algorithm completes. If they are not equal, E' can use the obtained result to construct a reduction to break the discrete logarithm assumption:\\frac{(z_1-z_2)}{c_1-c_2} G = W\n\nSince the probability of breaking the discrete logarithm assumption is negligible, we can conclude that the difference between the success probability of E_{ss} and the success probability of P^* is also negligible.\n\n【Obtaining transcripts】\n\nWe have implemented the first step, now let’s look at the second step, which requires algorithm E_{rw} to call P^* to obtain two legal transcripts. It should be noted that Definition 2 assumes that P^* can only successfully output legal proofs with probability \\epsilon each time, which means that P^* cannot always succeed. Moreover, the running time of P^* is assumed to be within polynomial time, which also limits that E_{rw} cannot call P^* indefinitely, because considering the feasibility of the algorithm, the total running time of E_{rw} also needs to be within polynomial time.\n\nTherefore, to successfully complete the second step, we must prove the following two points\n\nE_{rw} is a polynomial-time algorithm\n\nE_{rw} also successfully outputs w with a non-negligible probability","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#proof-of-knowledge-soundness-taking-schnorr-protocol-as-an-example","position":7},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"From Knowledge Soundness to Special Soundness","lvl2":"Special Soundness"},"type":"lvl3","url":"/kzg10/kzg-notes/kzg-soundness-2#from-knowledge-soundness-to-special-soundness","position":8},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl3":"From Knowledge Soundness to Special Soundness","lvl2":"Special Soundness"},"content":"The paper [Cra96] gives a quite elegant proof of the properties of the E_{rw} algorithm. Since the process is quite long and similar to the content of [LPS24] that will be introduced later, we won’t describe it here. In any case, the above process is summarized as a lemma:\n\n【Rewinding Lemma】\n\nFor a 3-step interactive proof \\Pi = (P, V), if there exists a polynomial-time algorithm P^* that can forge a legal transcript with a non-negligible probability, then there must exist a polynomial-time extractor algorithm E_{rw} that can obtain another legal transcript (satisfying the same R and different c) by rewinding P^*, and the success probability of E_{rw} is also non-negligible.\n\nThe Rewinding Lemma is not limited to the Schnorr protocol. In fact, for any 3-step Sigma protocol, the extractor algorithm E_{rw} can rewind to obtain additional k-1 (polynomial number) legal transcripts.\n\nTherefore, the Rewinding Lemma actually simplifies the process of proving Knowledge Soundness for researchers designing specific protocols. For protocols designed based on the Sigma model, we usually only need to give the construction of sub-algorithm E_{ss} in the security proof. To formally describe this process, cryptographers proposed a new definition, namely Special Soundness\n\nDefinition 3: Special Soundness\n\nFor a 3-step interactive proof \\Pi = (P, V), if there exists a polynomial-time extraction algorithm E_{ss}, given its input as x and two legal transcripts, denoted as (R,c_1,z_1), (R,c_2,z_2), can output the secret value w, then we say \\Pi satisfies Special Soundness.\n\n【Note】The above definition is also called 2-special soundness. If the input of extraction algorithm E includes k transcripts, it is called k-special soundness\n\nWith the development of interactive proofs, researchers are not limited to constructing only 3-step protocols. To meet this demand, Special Soundness has been further extended to (2n-1)-step interactive proofs. That is, for the j\\in[1,n] round, the extractor algorithm E_{rw} needs to obtain additional k_j-1 transcripts by rewinding P^*. Finally, the input of sub-algorithm E_{ss} is no longer a simple transcript vector, but a transcript tree with height n, denoted as (k_1,...,k_n)-transcript tree. Correspondingly, the property satisfied by this protocol is called (k_1,...,k_n)-special soundness. For the specific definition of this part, interested readers can read [BCC+16] and [ACK21]. The [LPS24] introduced in this article only uses k-special soundness.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#from-knowledge-soundness-to-special-soundness","position":9},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"LPS24: KZG10 with Special Soundness"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-2#lps24-kzg10-with-special-soundness","position":10},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"LPS24: KZG10 with Special Soundness"},"content":"We have already introduced the basic process of the KZG10 polynomial commitment scheme in our previous articles. In [LPS24], the authors first write the KZG10 scheme in a form that conforms to interactive proofs, where the prover has public input ck = (p, [(\\sigma^i)_{i=0}^n]_1, [1,\\sigma]_2) (i.e., parameters p \\leftarrow Pgen(1^{\\lambda}) and SRS), secret input f(X) polynomial, and the verifier only has public input ck. The two parties conduct the following interactive protocol:\n\nThe prover calculates the polynomial commitment C = [f(\\sigma)]_1 and sends it to the verifier\n\nThe verifier chooses a random r as the evaluation point and sends it to the prover\n\nThe prover calculates and sends the value v = f(r), proof \\pi = [q(\\sigma)]_1, where q(X) = (f(X)-v)/(X-r)\n\nThe verifier checks e(C-[v]_1, [1]_2) \\overset{?}{=} e(\\pi, [\\sigma-r]_2) based on the interaction data\n\nSimilarly, we call the collection of interaction messages between the two parties a transcript. If a transcript tr can pass verification, it is called accepting. Furthermore, if a vector \\vec{tr} containing n+1 transcripts satisfies the following two requirements, it is admissible:\n\nAll transcripts in \\vec{tr} contain the same polynomial commitment C\n\nFor any two transcripts tr_i, tr_j, i,j\\in [0,n], their evaluation points are different, i.e., r_i \\neq r_j\n\nIn addition to defining the interactive form of the KZG10 scheme, the authors of [LPS24] also proposed a new difficult problem assumption, named Adaptive Rational Strong Diffie-Hellman assumption, abbreviated as ARSDH assumption, defined as follows\n\n【(n+1)-ARSDH assumption】\n\nIf for any polynomial-time adversary algorithm A, given parameters p \\leftarrow Pgen(1^{\\lambda}), SRS generated by random value \\sigma, ([(\\sigma^i)_{i=0}^n]_1, [1,\\sigma]_2), A is required to output a pair [g]_1, [\\varphi]_1, and a set S of size n+1, satisfying the following relation:[g]_1 \\neq [0]_1 \\wedge e([g]_1, [1]_2) = e([\\varphi]_1, [Z_S(\\sigma)]_2)\n\nIf the probability of A succeeding is negligible, then the (n+1)-ARSDH assumption is said to hold for the bilinear group parameter generation algorithm p \\leftarrow Pgen(1^{\\lambda}).\n\nARSDH is a relaxation of a known assumption RSDH, where RSDH requires that A cannot choose the set S by itself. In addition, [LPS24] also proves that (n+1)-ARSDH can imply the (n+1)-SDH assumption (ARSDH implies SDH), that is, if SDH can be broken, then ARSDH can also be broken. Because SDH can imply the evaluation binding property of KZG10, we get the following conclusion(n+1)\\text{-ARSDH} \\rightarrow (n+1)\\text{-SDH} \\rightarrow \\text{KZG10's binding}\n\nThe preparatory knowledge has been introduced. Next, following the proof idea of the Schnorr protocol introduced earlier, we first give the construction of the extractor algorithm E_{ss} based on transcripts, that is, proving that KZG satisfies special soundness, and then prove the rewinding lemma.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#lps24-kzg10-with-special-soundness","position":11},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Special Soundness of KZG"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-2#special-soundness-of-kzg","position":12},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Special Soundness of KZG"},"content":"First, let’s give the definition:\n\nFor a polynomial commitment scheme PC, if there exists a polynomial-time extraction algorithm E_{ss}, given its input as ck and a vector \\vec{tr} of length n+1 of transcripts, satisfying\n\nAny tr_j \\in \\vec{tr} is accepting (passes verification)\n\n\\vec{tr} is admissible (C is the same, r is different)\n\nE can output the secret value f(X), satisfying C = \\mathrm{Com}(ck; f) \\wedge f(r_j) = v_j, \\forall j \\in [0,n], then we say PC satisfies (n+1)-Special Soundness\n\nObviously, the idea of designing the E_{ss} algorithm is to try to extract a polynomial f'(X) from \\vec{tr}, and f'(X) is either a legal secret value or an instance that breaks the (n+1)\\text{-ARSDH} assumption.\n\nLet’s write out the verification relation corresponding to each tr_j:e({\\color{red} C-[v_0]_1}, [1]_2) = e({\\color{blue}\\pi_0}, [\\sigma-r_0]_2) \\\\ \\vdots \\\\ e({\\color{red}C-[v_n]_1}, [1]_2) = e({\\color{blue}\\pi_n}, [\\sigma-r_n]_2)\n\nLet I = [0,n], L_0(X),\\ldots,L_n(X) be the Lagrange polynomials interpolating the values S=\\{v_i\\}_{i\\in I} on the set I, the expression of L_j(X) is\\color{purple} L_j(X) = \\frac{\\prod_{i\\in I/\\{j\\}} (X-r_i)}{\\prod_{i\\in I/\\{j\\}} (r_j-r_i)}\n\nNow, multiply both sides of each verification equation by the value of the Lagrange polynomial at \\sigma, for example, the j-th equation (j\\in[0,n])  ise({\\color{red}C-[v_j]_1}, [1]_2)\\cdot {\\color{purple}L_j(\\sigma)} = e({\\color{blue}\\pi_j}, [\\sigma-r_j]_2) \\cdot {\\color{purple}L_j(\\sigma)}e({\\color{red}C-[v_j]_1}, [1]_2)\\cdot {\\color{purple}L_j(\\sigma)} = e({\\color{blue}\\pi_j} \\cdot {\\color{purple} \\frac{\\prod_{i\\in I/\\{j\\}} (\\sigma-r_i)}{\\prod_{i\\in I/\\{j\\}} (r_j-r_i)}}, [\\sigma-r_j]_2)\n\n$$\n\ne({\\color{red}C-[v_j]_1}, [1]2)\\cdot {\\color{purple}L_j(\\sigma)} = e({\\color{blue}\\pi_j} \\cdot {\\color{purple} \\frac{1}{\\prod{i\\in I/{j}} (r_j-r_i)}}, [Z_S(\\sigma)]_2) $$\n\nBy adding all n+1 equations, we can gete(\\sum_{j \\in I}{\\color{red}(C-[v_j]_1)}\\cdot {\\color{purple}L_j(\\sigma)}, [1]_2) = e(\\sum_{j \\in I} {\\color{blue}\\pi_j} \\cdot {\\color{purple} \\frac{1}{\\prod_{i\\in I/\\{j\\}} (r_j-r_i)}}, [Z_S(\\sigma)]_2)\n\nLet \\sum_{j \\in I} {\\color{red} [v_j]_1} \\cdot {\\color{purple} L_j(\\sigma)} = [{\\color{purple} L(\\sigma)}]_1, the left-hand side isLHS = e({\\color{red}C}-\\sum_{j \\in I}{\\color{red} [v_j]_1}\\cdot {\\color{purple}L_j(\\sigma)}, [1]_2)  \\\\ = e({\\color{red}C}-{\\color{purple}[L(\\sigma)]_1}, [1]_2)\n\nLet \\sum_{j\\in I} \\left( {\\color{blue} \\pi_j} / {\\color{purple} \\prod_{i\\in I/\\{j\\}}(r_j -r_i)} \\right) = {\\color{blue} \\varphi}, the right-hand side isRHS  = e(\\sum_{j \\in I} {\\color{blue}\\pi_j} \\cdot {\\color{purple} \\frac{1}{\\prod_{i\\in I/\\{j\\}} (r_j-r_i)}}, [Z_S(\\sigma)]_2) \\\\ = e([\\sum_{j\\in I} \\frac{\\color{blue} q_j(\\sigma)}{\\color{purple} \\prod_{i \\in I/\\{j\\}}(r_j-r_i)}]_1, [Z_S(\\sigma)]_2) \\\\ = e([{\\color{blue} \\varphi}]_1, [Z_S(\\sigma)]_2)\n\nFinally, we get the equationLHS = e({\\color{red}C}-{\\color{purple}[L(\\sigma)]_1}, [1]_2) = e([{\\color{blue} \\varphi}]_1, [Z_S(\\sigma)]_2) = RHS\n\nBased on this equation, the extraction algorithm E_{ss} first obtains v_0,\\ldots,v_n from n+1 transcripts, and calculates {\\color{purple} L(X)} and [{\\color{purple} L(\\sigma)}]_1 = [\\sum_{j\\in I}v_j \\cdot L_j(\\sigma)]_1. Compare [{\\color{purple} L(\\sigma)}]_1 \\overset{?}{=} {\\color{red} C}, and perform the following operations based on the result\n\nIf [{\\color{purple} L(\\sigma)}]_1 = {\\color{red} C}, E_{ss} directly outputs {\\color{purple} L(X)} as the secret polynomial, and the algorithm completes.\n\nIf [{\\color{purple} L(\\sigma)}]_1 \\neq {\\color{red} C}, E_{ss} uses {\\color{purple} L(X)} to construct a reduction to break the (n+1)\\text{-ARSDH} assumption:\n\nE_{ss} calculates {\\color{red} [g]_1} = {\\color{red} C}-[{\\color{purple} L(\\sigma)}]_1, and outputs {\\color{red} [g]_1}, [{\\color{blue} \\varphi}]_1 as an instance to break (n+1)\\text{-ARSDH}\n\nObviously, the above instance satisfiese({\\color{red} [g]_1}, [1]_2) = e([{\\color{blue} \\varphi}]_1, [Z_S(\\sigma)]_2)\n\nProof completed.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#special-soundness-of-kzg","position":13},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Rewinding Lemma"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-2#rewinding-lemma","position":14},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"Rewinding Lemma"},"content":"The above proof ensures that KZG10 satisfies (n+1)-special soundness, but to further ensure knowledge soundness, we also need to prove that it is feasible for E_{rw} to obtain n+1 satisfying transcripts by rewinding, that is, the rewinding lemma.\n\nSpecifically, for the following E_{rw} algorithm, we need to prove that it can succeed with a non-negligible probability in polynomial time\n\nE_{rw} randomly selects r and calls P^* to obtain tr_0\n\nCheck the validity of tr_0, if valid, continue; if not valid, return to step 1 and select another r'\n\nE_{rw} runs a loop algorithm, selecting a new r in each round, and rewinds P^* to obtain a new transcript, with the termination condition being\n\nE_{rw} obtains (n+1) transcripts that meet the requirements (i.e., satisfying accepting and admissible) → Algorithm succeeds\n\nE_{rw} has traversed all possible r, but still hasn’t obtained (n+1) transcripts that meet the requirements → Algorithm fails\n\nThe [LPS24] paper adopts the same proof idea as [ACK21], letting H be a Boolean matrix with row indices as the set \\{ \\vec{r} = ( r_p, r_{ck}, r_A) \\in \\{ 0,1 \\}^{poly(\\lambda)} \\}, where r_p, r_{ck},r_A are the random numbers used by the Pgen algorithm, SRS, and the adversary respectively. The column indices of H are the challenge value space \\mathbb{F}. When P^* generates a legal transcript for challenge value r under a certain random number setting \\vec{r}, we set the corresponding element in H to 1, i.e., H[\\vec{r}][r] = 1.\n\nNext, we analyze the success probability and running time of the E_{rw} algorithm respectively\n\n【Probability Analysis】\n\nDefine events as follows:\n\nEvent A: tr_0 passes verification\n\nEvent B: \\forall j \\in [1,n], tr_j passes verification\n\nThen the probability of E_{rw} succeeding is calculated as the probability of A \\rightarrow B, that isPr[A \\rightarrow B] = Pr[A \\wedge (A \\rightarrow B)] + Pr[\\neg{A} \\wedge (A \\rightarrow B)] \\\\ = Pr[A \\wedge B] + Pr(\\neg{A})\n\n【Note】: The truth table of A \\rightarrow B is\n\nA\n\nB\n\nA \\rightarrow B\n\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nT\n\nT\n\nF\n\nF\n\nT\n\nConsider the probability Pr[A \\wedge B], the event A \\wedge B occurs if and only if P^* outputs a legal tr_0 under the random parameter setting \\vec{r}, and the row H[\\vec{r}] where \\vec{r} is located has at least n+1 “1” elements.\n\nLet R_j be the number of rows in H that contain j “1” elements, for example, R_2 = 3 in the above figure. The number of rows containing \\ge n+1 “1” elements can be calculated as \\sum_{j=n+1}^{|\\mathbb{F}|} j \\cdot R_j, the probability Pr[A \\wedge B] is calculated as followsPr[A \\wedge B]=\\frac{\\sum_{j=n+1}^{|\\mathbb{F}|} j \\cdot R_j }{|\\vec{r}|\\cdot |\\mathbb{F}|} \\\\ =\\frac{\\sum_{j=0}^{|\\mathbb{F}|} j \\cdot R_j }{|\\vec{r}|\\cdot |\\mathbb{F}|} - \\frac{\\sum_{j=0}^{n} j \\cdot R_j }{|\\vec{r}|\\cdot |\\mathbb{F}|} \\\\= Pr[A] - \\frac{\\sum_{j=0}^{n} j \\cdot R_j }{|\\vec{r}|\\cdot |\\mathbb{F}|}\n\nAnd because \\sum_{j=0}^{n} j \\cdot R_j = \\sum_{j=1}^{n} j \\cdot R_j \\leq \\sum_{j=1}^{n} n \\cdot R_j \\leq n|\\vec{r}|\n\nWe can get the lower bound of Pr[A \\wedge B]Pr[A \\wedge B] \\ge Pr[A] - \\frac{n|\\vec{r}|}{|\\vec{r}||\\mathbb{F}|} = Pr[A] - \\frac{n}{|\\mathbb{F}|}\n\nFinally, we get the lower bound of the success probability of E_{rw}Pr[A \\rightarrow B] = Pr[A \\wedge B] + Pr(\\neg{A}) \\\\ \\ge Pr[A] - \\frac{n}{|\\mathbb{F}|} + Pr[\\neg A]  \\\\ = 1-\\frac{n}{|\\mathbb{F}|}\n\n【Running Time Analysis】\n\nFor the E_{rw} algorithm, it can be considered that its running time is mainly related to the time of calling the P^* algorithm. And because P^* is a polynomial-time algorithm, we only need to calculate the number of times E_{rw} calls the P^* algorithm, denoted as Q, to deduce that the time complexity of E_{rw} algorithm is poly(\\lambda)\\cdot Q.\n\nConsider that E_{rw} successfully obtains a legal tr_0 in step 2 (i.e., event A occurs), E_{rw} continues to run the loop in step 3. Since E_{rw} needs to call the P^* algorithm once in each round of the loop, we can obtain Q by calculating the expected number of loop iterations.\n\nLet’s first discuss the problem of calculating the number of loop iterations separately: Given a random parameter \\vec{r}, assuming the corresponding row vector H[\\vec{r}] in H contains \\delta_{\\vec{r}}|\\mathbb{F}| “1” elements, |\\mathbb{F}| is the length of the vector H[\\vec{r}]. On the premise of already selecting one “1” element in H[\\vec{r}] (i.e., tr_0), solve for the expected number of times E_{rw} selects n “1” elements from the remaining |\\mathbb{F}|-1 items.\n\nTo calculate the expectation of Q, we need to introduce the concept of Negative HyperGeometric distribution (NHG distribution)\n\nNHG distribution: Given a blind box containing N balls, of which K balls are marked, it is required to take out only one ball at a time, and not put it back, until k\\leq K marked balls are taken out. Let X be the total number of all balls taken out when the ball-taking ends, the expectation of X is E[NHG_{N,K,k}] = k(N+1)/(K+1).\n\nCorrespondingly, when the number of “1” elements contained in H[\\vec{r}] is greater than n, Q conforms to the NHG distribution. Assuming that each H[\\vec{r}] contains \\delta_{\\vec{r}}|\\mathbb{F}| “1” elements, we can calculate the expectation of Q as follows:\n\nH[\\vec{r}] contains at least n+1 “1” elements, E[Q|A \\wedge \\vec{r}] = E[NHG_{N,K,k}] + 1 = n/\\delta_{\\vec{r}} + 1, where N = \\mathbb{F}-1, K = \\delta_{\\vec{r}}|\\mathbb{F}|-1, k=n\n\nH[\\vec{r}] contains less than n+1 “1” elements, i.e., \\delta_{\\vec{r}}|\\mathbb{F}| \\leq n, algorithm E_{rw} will keep executing the loop until traversing all elements in H[\\vec{r}], obviously E[Q|A \\wedge \\vec{r}] = |\\mathbb{F}| \\leq n/\\delta_{\\vec{r}}\n\nThe above considers the case when event A occurs. Since for any \\vec{r}, H[\\vec{r}] contains \\delta_{\\vec{r}}|\\mathbb{F}| “1” elements, the probability of event A occurring is Pr[A] = \\delta_{\\vec{r}}, calculateE[Q|\\vec{r}] = E[Q|A \\wedge \\vec{r}]\\cdot Pr[A] + E[Q|\\neg A \\wedge \\vec{r}]\\cdot Pr[\\neg A] \\\\ \\leq \\frac{n}{\\delta_{\\vec{r}}}\\cdot \\delta_{\\vec{r}} + 1\\cdot (1-\\delta_{\\vec{r}}) = n+1- \\delta_{\\vec{r}} \\leq n+1\n\nFor all \\vec{r} \\in \\{ 0,1 \\}^{poly(\\lambda)}, calculate the expectation of Q as followsE[Q] = \\sum_{\\vec{r}} E[Q|\\vec{r}]\\cdot Pr[\\vec{r}] \\leq \\sum_{1}^{|\\vec{r}|} \\frac{n+1}{|\\vec{r}|} = n+1\n\nProof completed.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#rewinding-lemma","position":15},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"References"},"type":"lvl2","url":"/kzg10/kzg-notes/kzg-soundness-2#references","position":16},{"hierarchy":{"lvl1":"KZG Extractability based on ROM","lvl2":"References"},"content":"[CHM+19] Chiesa, Alessandro, Yuncong Hu, Mary Maller, et al. “Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/1047\n\n[MBK+19] Maller Mary, Sean Bowe, Markulf Kohlweiss, et al. “Sonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updatable Structured Reference Strings.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/099\n\n[GWC19] Ariel Gabizon, Zachary J. Williamson, Oana Ciobotaru. “PLONK: Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/953\n\n[LPS24] Helger Lipmaa, Roberto Parisella, Janno Siim. “Constant-Size zk-SNARKs in ROM from Falsifiable Assumptions.” Cryptology ePrint Archive (2024). \n\nhttps://​eprint​.iacr​.org​/2024​/173\n\n[ACK21] Thomas Attema, Ronald Cramer, and Lisa Kohl “A Compressed Sigma-Protocol Theory for Lattices” Cryptology ePrint Archive (2021). \n\nhttps://​eprint​.iacr​.org​/2021​/307\n\n[Sch90] Claus-Peter Schnorr. “Efficient identification and signatures for smart cards.” In Gilles Brassard, editor, CRYPTO’89, volume 435 of LNCS, pages 239–252. Springer, Heidelberg, August 1990.\n\n[Cra96] Ronald Cramer. “Modular Design of Secure yet Practical Cryptographic Protocols”. PhD thesis, CWI and University of Amsterdam, 1996.","type":"content","url":"/kzg10/kzg-notes/kzg-soundness-2#references","position":17},{"hierarchy":{"lvl1":"Understanding Hiding KZG10"},"type":"lvl1","url":"/kzg10/kzg-hiding","position":0},{"hierarchy":{"lvl1":"Understanding Hiding KZG10"},"content":"Hiding KZG10 is a variant of the KZG10 protocol that produces polynomial commitments with a random blinding factor, thus possessing the property of Perfect Hiding. This means that even if an attacker has unlimited computational power, they cannot reverse-engineer any information about the polynomial from the commitment. While Hiding KZG10 is not common, it is an important component in constructing zkSNARKs with Zero-knowledge properties or other secure protocols.\n\nThis article introduces two different Hiding KZG10 schemes. The first scheme is from [KT23], and its main technique is a simplified version of multivariate polynomial commitment from [PST13], [ZGKPP17], and [XZZPS19]. The second scheme is from [CHMMVW19], which is an improvement on the original KZG10 protocol paper [KZG10].","type":"content","url":"/kzg10/kzg-hiding","position":1},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"None-hiding KZG10"},"type":"lvl2","url":"/kzg10/kzg-hiding#none-hiding-kzg10","position":2},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"None-hiding KZG10"},"content":"Let’s first recall the basic KZG10 protocol. KZG10 is based on a pre-setup (Universal Trusted Setup) SRS:SRS = ([1]_1, [\\tau]_1, [\\tau^2]_1, [\\tau^3]_1, \\ldots, [\\tau^D]_1, [1]_2, [\\tau]_2)\n\nHere, \\tau is a secret value that needs to be forgotten after the Setup phase, otherwise, any party knowing \\tau could launch an attack. We use bracket notation [a]_1 to represent scalar multiplication (Scalar Multiplication) a\\cdot G on an elliptic curve group element, where G\\in\\mathbb{G}_1 is the generator of the group. The SRS consists of group elements from \\mathbb{G}_1 and \\mathbb{G}_2, which we call Base elements, as subsequent commitments to polynomials are based on linear operations of these Base elements.\n\nSince division operations on elliptic curve group elements are a difficult problem, if our computing power is limited, we cannot calculate a from [a]_1. This can be seen as, once we multiply a value a\\in\\mathbb{F}_r by a Base element, a is hidden.\n\nKZG10 requires a pairing friendly curve, meaning there exists another elliptic curve group \\mathbb{G}_2 (with generator G'), where each element is represented as [b]_2, i.e., b\\cdot G'. And there exists a bilinear pairing operation that satisfies the following bilinearity and non-degeneracy properties:e(a\\cdot G, b\\cdot G') = (ab)\\cdot e(G, G')\n\nNow assume we have a univariate polynomial f(X)\\in\\mathbb{F}_r[X]f(X) = f_0 + f_1 X + f_2 X^2 + \\cdots + f_d X^d\n\nHere, the Degree d of the polynomial needs to satisfy d< D.\nThen the commitment C_f of the polynomial is calculated as follows:C_f = \\mathsf{Commit}(f(X)) = f_0 \\cdot [1]_1 + f_1 \\cdot [\\tau]_1 + f_2 \\cdot [\\tau^2]_1 + \\cdots + f_d \\cdot [\\tau^d]_1\n\nAfter derivation, it’s not hard to find that the following equation holdsC_f = [f(\\tau)]","type":"content","url":"/kzg10/kzg-hiding#none-hiding-kzg10","position":3},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation proof","lvl2":"None-hiding KZG10"},"type":"lvl3","url":"/kzg10/kzg-hiding#evaluation-proof","position":4},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation proof","lvl2":"None-hiding KZG10"},"content":"For any polynomial f(X) we choose, which is an element of the polynomial ring \\mathbb{F}_r[X], it satisfies the following division formula:f(X) = q(X) \\cdot g(X) + r(X)\n\nwhere g(X) is the divisor polynomial, r(X) is the remainder polynomial. Obviously, \\deg(r)<\\deg(g).\n\nIf we let g(X) = X-z, then clearly r(X) is a constant polynomial, so the above division decomposition formula can be written as:f(X) = q(X) \\cdot (X-z) + r\n\nFurthermore, substituting X=z into the above equation, we can get f(z)=r. So, the above division decomposition formula can be rewritten as:f(X) = q(X) \\cdot (X-z) + f(z)\n\nThis formula is the core formula of the KZG10 protocol. That is, if we want to prove f(z)=r, we only need to prove that f(X)-f(z) can be divided by (X-z). Or in other words, there exists a quotient polynomial q(X) satisfying:q(X) = \\frac{f(X)-f(z)}{X-z}\n\nIf the value of f(X) at X=z is not equal to r, then q(X) is not a polynomial, but a Rational Function. And for any Rational Function whose denominator is not a constant polynomial, we cannot use the above SRS to calculate its commitment.\n\nTherefore, the Prover only needs to send the commitment of q(X) to prove the existence of q(X) to the Verifier, which is equivalent to proving that the evaluation of f(X) is correct.\\pi_{eval} = q_0 \\cdot [1]_1 + q_1 \\cdot [\\tau]_1 + \\cdots + q_{d-1} \\cdot [\\tau^{d-1}]_1 = [q(\\tau)]_1\n\nThen the Verifier uses the Base elements provided by SRS to check the correctness of the decomposition formula. For the Verifier, f(z) and z are public, so the Verifier can check the decomposition formula through the following formula:e\\Big({\\color{red}[f(\\tau)]_1} - {\\color{blue}f(z)}\\cdot[1]_1,\\ [1]_2\\Big) = e\\Big({\\color{red}[q(\\tau)]_1},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big)\n\nThe red parts in the above formula are provided by the Prover and do not expose the values inside [\\cdot]_1.\n\nThe Polynomial Evaluation proof of the KZG10 protocol only contains one element [q(\\tau)]_1 of \\mathbb{G}_1, with a size of O(1). And the verification algorithm of the Verifier is also O(1). It should be mentioned that the Verifier needs to complete two Pairing calculations, which, although of O(1) complexity, are quite expensive.","type":"content","url":"/kzg10/kzg-hiding#evaluation-proof","position":5},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Degree Bound proof","lvl2":"None-hiding KZG10"},"type":"lvl3","url":"/kzg10/kzg-hiding#degree-bound-proof","position":6},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Degree Bound proof","lvl2":"None-hiding KZG10"},"content":"KZG10 also supports proving that the Degree of a polynomial f(X)\\in\\mathbb{F}_r[X] is less than or equal to d.\n\nThe Prover’s proof method is very straightforward, which is to construct a new polynomial \\hat{f}(X),\\hat{f}(X) = X^{D-n}\\cdot f(X)\n\nObviously, the Degree of \\hat{f}(X) is less than or equal to D. And because the highest power of \\tau in the Base elements contained in the SRS is [\\tau^D]_1, theoretically, anyone (who doesn’t know the value of \\tau) cannot construct the commitment of any polynomial with a degree greater than or equal to D.\n\nTherefore, the Prover can construct the Degree Bound proof \\pi as:\\pi_{deg} = f_0\\cdot [\\tau^{D-d}]_1 + f_1\\cdot [\\tau^{D-d+1}]_1 + \\cdots + f_{d}\\cdot [\\tau^D]_1 = [\\tau^{D-d}\\cdot f(\\tau)]_1\n\nThe Verifier’s verification method is also straightforward, checking whether \\hat{f}(X) is obtained by multiplying f(X) with X^{D-d}.e\\Big({\\color{red}[\\tau^{D-d}\\cdot f(\\tau)]_1},\\ [1]_2\\Big) = e\\Big({\\color{red}[f(\\tau)]_1},\\ [{\\color{blue}\\tau^{D-d}}]_2\\Big)\n\nHere, [\\tau^{D-d}]_2 should also be a Base element in the SRS. This requires the SRS of KZG10 to include more Base elements of \\mathbb{G}_2:SRS = \\left(\\begin{array}{ccccccc}[1]_1, &[\\tau]_1, &[\\tau^2]_1, &[\\tau^3]_1, &\\ldots, &[\\tau^D]_1\\\\[1ex]\n[1]_2, &[\\tau]_2, &[\\tau^2]_2, &[\\tau^3]_2, &\\ldots, &[\\tau^D]_2\\\\\n\\end{array}\\right)\n\nContinuing to think, if the Prover needs to prove both the Degree Bound and Evaluation of f(X) at the same time, then when generating the commitment of f(X), he needs to produce two elements of \\mathbb{G}_1, ([\\hat{f}(\\tau)]_1, [\\tau^{D-d}\\hat{f}(\\tau)]_1). Then the Verifier needs to complete 4 Pairing calculations to check both the Evaluation and Degree Bound proofs simultaneously.","type":"content","url":"/kzg10/kzg-hiding#degree-bound-proof","position":7},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Perfect Hiding"},"type":"lvl2","url":"/kzg10/kzg-hiding#perfect-hiding","position":8},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Perfect Hiding"},"content":"If we care more about privacy protection during protocol interaction, then KZG10 needs to enhance the protection of the polynomial f(X). In the above KZG10 protocol, we assume that the attacker’s computational power is limited, meaning the attacker cannot reverse-engineer any information about f(X) through [f(\\tau)]_1.\n\nIn comparison, the traditional Pedersen Commitment has the property of Perfect Hiding, because its commitment carries a random number as a blinding factor, so even if the attacker has infinite computing power, they cannot reverse-engineer any information about f(X) through [f(\\tau)]_1.\\mathsf{Pedersen.Commit}(f(X), r) = f_0 \\cdot G_0 + f_1 \\cdot G_1 + \\cdots + f_d \\cdot G_d + {\\color{red}r} \\cdot G'\n\nWhere \\{G_i\\}_{i=0}^d\\in\\mathbb{G}^{d+1}_1 and G'\\in\\mathbb{G}_1 are the public parameters of Pedersen Commitment. And r\\in\\mathbb{F}_r is the so-called blinding factor.\n\nWe will now explain how to convert the KZG10 protocol into a Perfect Hiding protocol. This scheme is from [KT23], and its basic idea comes from [ZGKP17] and [PST13].\n\nFirst, we can “try” to consider adding a random number as a blinding factor when KZG10 commits to f(X), for example:\\mathsf{KZG.Commit}(f(X), r) = f_0 \\cdot [1]_1 + f_1 \\cdot [\\tau]_1 + \\cdots + f_d \\cdot [\\tau^d]_1 + {\\color{red}r} \\cdot [1]_1\n\nBut such a commitment would have security issues. Because in Pedersen Commitment, the element G' used to commit r is a special Base element, and its relationship with other G_i is unknown (i.e., independent). Therefore, the introduction of r does not affect the commitment of the constant term f_0 of f(X).\n\nTherefore, we need to expand the SRS and introduce an additional preset random value \\gamma, specifically used to commit the blinding factor r:SRS = ([1]_1, [\\tau]_1, [\\tau^2]_1, [\\tau^3]_1, \\ldots, [\\tau^D]_1, {\\color{red}[\\gamma]_1}, [1]_2, [\\tau]_2, {\\color{red}[\\gamma]_2})\n\nThen the commitment of f(X) is defined as:\\mathsf{KZG.Commit}(f(X), r) = f_0 \\cdot [1]_1 + f_1 \\cdot [\\tau]_1 + \\cdots + f_d \\cdot [\\tau^d]_1 + r \\cdot {\\color{red}[\\gamma]_1}\n\nWe will use a shorter symbol \\mathsf{cm}(f) below to represent the commitment of f(X).","type":"content","url":"/kzg10/kzg-hiding#perfect-hiding","position":9},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation proof of Hiding-KZG10","lvl2":"Perfect Hiding"},"type":"lvl3","url":"/kzg10/kzg-hiding#evaluation-proof-of-hiding-kzg10","position":10},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation proof of Hiding-KZG10","lvl2":"Perfect Hiding"},"content":"Although we add a Blinding Factor to the Commitment, the Evaluation proof of f(X) may still expose information about f(X).\n\nImagine if the Prover wants to prove f(z)=v to the Verifier, he needs to calculate the quotient polynomial q(X), compute and send its commitment [q(\\tau)]_1. If [q(\\tau)]_1 is sent directly, this would break the Perfect Hiding property we want, because an attacker with “infinite computing power” could reverse-engineer q(X) from [q(\\tau)]_1, and then continue to calculate f(X).\n\nTherefore, we also need to add another different blinding factor to [q(\\tau)]_1, denoted as \\color{green}s:\\begin{aligned}\n\\mathsf{KZG.Commit}(q(X), {\\color{green}s}) & = q_0 \\cdot [1]_1 + q_1 \\cdot [\\tau]_1 + \\cdots + q_d \\cdot [\\tau^{d-1}]_1 + {\\color{green}s} \\cdot {\\color{red}[\\gamma]_1} \\\\\n& = [q(\\tau) + {\\color{green}s}\\cdot{\\color{red}\\gamma}]_1\n\\end{aligned}\n\nWe denote the commitment of q(X) with the blinding factor added as the short symbol \\mathsf{cm}(q).\n\nContinuing to recall, the Verifier of Non-hiding KZG10 needs to check the following equation to verify the commitment of q(X):e\\Big({\\color{red}[f(\\tau)]_1} - {\\color{blue}f(z)}\\cdot[1]_1,\\ [1]_2\\Big) = e\\Big({\\color{red}[q(\\tau)]_1},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big)\n\nHowever, in Hiding-KZG10, since both the polynomial commitment \\mathsf{cm}(f) and the quotient polynomial commitment \\mathsf{cm}(q) have blinding factors, the Verifier can no longer complete the verification according to the above Pairing equation:e\\Big([f(\\tau) + {\\color{red}r\\cdot\\gamma}]_1 - {\\color{blue}f(z)}\\cdot[1]_1,\\ [1]_2\\Big) \\neq e\\Big([q(\\tau)+{\\color{red}s\\cdot\\gamma}]_1,\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big)\n\nLet’s reason why the above equation doesn’t hold. First, look at the left side of the equation, which is equivalent to calculating\\begin{aligned}\nlhs & = f(\\tau) + r\\cdot\\gamma - f(z) \\\\\n& = f(\\tau) - f(z) + r\\cdot\\gamma\n\\end{aligned}\n\nThe right side of the equation is equivalent to calculating\\begin{aligned}\nrhs & = (q(\\tau) + s\\cdot\\gamma) \\cdot (\\tau - z) \\\\\n& = q(\\tau)\\cdot(\\tau - z) + s\\cdot(\\tau - z)\\cdot\\gamma\\\\\n& = f(\\tau) - f(z) + s\\cdot(\\tau - z)\\cdot\\gamma\\\\\n\\end{aligned}\n\nThe difference between the left and right sides is\\begin{aligned}\nlhs - rhs & = r\\cdot\\gamma - s\\cdot(\\tau - z)\\cdot\\gamma \\\\\n& = (r - s\\cdot(\\tau - z)) \\cdot \\gamma \\\\ \n\\end{aligned}\n\nTo allow the Verifier to verify, we need to introduce an additional “group element” to balance the Pairing verification formula:E = r \\cdot [1]_1 - s \\cdot [\\tau]_1 + (s\\cdot z)\\cdot [1]_1\n\nThus, the Verifier can verify through the following formula:e\\Big([f(\\tau) + r\\cdot\\gamma] - f(z)\\cdot[1]_1,\\ [1]_2\\Big) = e\\Big([q(\\tau)+s\\cdot\\gamma],\\ [\\tau] - z\\cdot[1]_2\\Big) + e\\Big(E,\\ [\\gamma]_2\\Big)\n\nOr written as:e\\Big({\\color{red}\\mathsf{cm}(f)} - {\\color{blue}f(z)}\\cdot[1]_1,\\ [1]_2\\Big) = e\\Big({\\color{red}\\mathsf{cm}(q)},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big) + e\\Big({\\color{red}E},\\ [\\gamma]_2\\Big)\n\nWhere the red parts are provided by the Prover, and the blue parts are public values.","type":"content","url":"/kzg10/kzg-hiding#evaluation-proof-of-hiding-kzg10","position":11},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Degree Bound proof of Hiding-KZG10","lvl2":"Perfect Hiding"},"type":"lvl3","url":"/kzg10/kzg-hiding#degree-bound-proof-of-hiding-kzg10","position":12},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Degree Bound proof of Hiding-KZG10","lvl2":"Perfect Hiding"},"content":"To prove the Degree Bound of f(X), we also need to add a Blinding Factor to the polynomial \\hat{f}(X), then calculate its commitment as the Degree Bound proof of f(X):\\mathsf{cm}(\\hat{f}) = [\\tau^{D-d}\\cdot f(\\tau)]_1 + {\\color{red}\\eta}\\cdot[\\gamma]_1\n\nAt the same time, an additional element E \\in\\mathbb{G}_1 is needed for balancing,E = \\rho\\cdot[\\tau^{D-d}]_1 - {\\color{red}\\eta}\\cdot[1]_1\n\nThis way, the Verifier can verify the Degree Bound proof of f(X) through the following equation:e\\Big({\\color{red}\\mathsf{cm}(f)},\\ [\\tau^{D-d}]_2\\Big) = e\\Big({\\color{red}\\mathsf{cm}(\\hat{f})},\\ [1]_2\\Big) + e\\Big({\\color{red}E},\\ [\\gamma]_2\\Big)\n\nReaders can verify for themselves why the above equation holds.","type":"content","url":"/kzg10/kzg-hiding#degree-bound-proof-of-hiding-kzg10","position":13},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation-and-degree-bound proof of Hiding KZG10","lvl2":"Perfect Hiding"},"type":"lvl3","url":"/kzg10/kzg-hiding#evaluation-and-degree-bound-proof-of-hiding-kzg10","position":14},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation-and-degree-bound proof of Hiding KZG10","lvl2":"Perfect Hiding"},"content":"Suppose for the same Polynomial f(X), the Prover needs to prove both the Evaluation and Degree Bound of f(X) simultaneously. If we use the above Evaluation and Degree Bound proof protocols separately, the Prover would need to send two \\mathbb{G}_1 elements, and then the Verifier would need to complete 4 Pairing calculations. In fact, we can combine these two proof steps into one: the Prover only sends two \\mathbb{G}_1 elements, and the Verifier only needs to use two Pairings to complete the verification.\n\nThe Prover needs to construct two \\mathbb{G}_1 elements,\\mathsf{cm}(q) = [\\tau^{D-d}\\cdot q(\\tau)]_1 + \\eta\\cdot[\\gamma]_1\n\nAnother element E is defined as:E = \\rho\\cdot[\\tau^{D-d}]_1 - \\eta\\cdot[\\tau]_1 + (\\eta\\cdot z)\\cdot[1]_1\n\nThe Prover sends the proof\\pi = (\\mathsf{cm}(q), E)\n\nAnd the Verifier needs to verify the following equation:e\\Big({\\color{red}\\mathsf{cm}(f)} - {\\color{blue}f(z)}\\cdot[1]_1,\\ [\\tau^{D-d}]_2\\Big) = e\\Big({\\color{red}\\mathsf{cm}(q)},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big) + e\\Big({\\color{red}E},\\ [\\gamma]_2\\Big)","type":"content","url":"/kzg10/kzg-hiding#evaluation-and-degree-bound-proof-of-hiding-kzg10","position":15},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Another construction of Hiding KZG10"},"type":"lvl2","url":"/kzg10/kzg-hiding#another-construction-of-hiding-kzg10","position":16},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Another construction of Hiding KZG10"},"content":"In the original [KZG10] paper, a scheme for achieving Perfect Hiding was also provided. We can compare these two different styles of Hiding KZG10 variants.\n\nThe idea of this scheme is to add a random polynomial r(X) when committing to f(X), rather than just a single random blinding factor. Here, f(X) and r(X) are defined as follows:f(X)=\\sum_{i=0}^{d}f_i\\cdot X^i\\qquad r(X)=\\sum_{i=0}^{d}r_i\\cdot X^i\n\nNote that here, the Degree of the blinding polynomial r(X) is consistent with the Degree of f(X). To support the blinding polynomial (Blinding Polynomial), the SRS produced in the initial Setup phase needs to introduce a random number \\gamma to isolate the blinding factor from the normal message to be committed. So the SRS is expanded to:SRS = \\left(\n    \\begin{array}{ccccccc}\n    [1]_1, &[\\tau]_1, &[\\tau^2]_1, &[\\tau^3]_1, &\\ldots, &[\\tau^D]_1\\\\[1ex]\n    [{\\color{red}\\gamma}]_1, &[{\\color{red}\\gamma}\\tau]_1, &[{\\color{red}\\gamma}\\tau^2]_1, &[{\\color{red}\\gamma}\\tau^3]_1, &\\ldots, &[{\\color{red}\\gamma}\\tau^D]_1\\\\[1ex]\n    [1]_2, &[\\tau]_2, &[\\tau^2]_2, &[\\tau^3]_2, &\\ldots, &[\\tau^D]_2\\\\\n\\end{array}\\right)\n\nBelow we define the calculation formula for \\mathsf{cm}(f):\\begin{split}\n\\mathsf{KZG10.Commit}(f(X), r(X)) & = \\sum_{i=0}^{d}f_i\\cdot[\\tau^i]_1 + \\sum_{i=0}^{d}r_i\\cdot[{\\color{red}\\gamma}\\tau^i]_1 \\\\\n& = [f(\\tau) + {\\color{red}\\gamma}\\cdot r(\\tau)]_1\n\\end{split}\n\nEssentially, the commitment to the polynomial f(X) is actually a commitment to \\bar{f}(X) = f(X) + {\\color{red}\\gamma}\\cdot r(X).\\mathsf{cm}(f) = [f(\\tau) + {\\color{red}\\gamma}\\cdot r(\\tau)]_1 = [\\bar{f}(\\tau)]_1\n\nWhen the Prover needs to prove f(z)=v, he not only needs to send the commitment of the quotient polynomial q(X), but also needs to calculate the value of r(X) at X=z.\\pi = (\\mathsf{cm}(q), r(z))\n\nWhere the polynomial \\bar{q}(X) is the quotient polynomial after dividing \\bar{f}(X) with blinding polynomial by (X-z):\\bar{q}(X) = q(X) + \\gamma\\cdot q'(X) = \\frac{f(X)-f(z)}{X-z} + \\gamma\\cdot \\frac{r(X)-r(z)}{X-z}\n\nWhen the Verifier receives \\pi_{eval}=(\\mathsf{cm}(\\bar{q}), r(z)), he can verify the following equation:e\\Big({\\color{red}\\mathsf{cm}(\\bar{f})} - {\\color{blue}f(z)}\\cdot[1]_1 - {\\color{red}r(z)}\\cdot[\\gamma]_1,\\ [1]_2\\Big) = e\\Big({\\color{red}\\mathsf{cm}(\\bar{q})},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big)\n\nIntuitively, although the Prover sent the value of r(X) at r(z), as long as the Degree of r(X) is greater than or equal to 1, the attacker cannot reverse-engineer r(X) through the value of r(z) alone, so there is at least one random factor still protecting f(X).\n\nIn fact, if we know that f(X) will be opened at most k<d times throughout its lifecycle, then we don’t need to force the Degree of r(X) to be d, but it can be a polynomial of Degree k. Because the k-degree blinding factor polynomial consists of k+1 random factors, when r(X) is calculated k times, there is still one random factor protecting the commitment of f(X).\n\nTake an extreme example where the Degree of r(X) is 1, then when the Prover proves the value at a different point again, say f(z')=v', the Verifier would have the ability to recover r(X), thus breaking the Perfect Hiding property of the commitment to f(X).","type":"content","url":"/kzg10/kzg-hiding#another-construction-of-hiding-kzg10","position":17},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"type":"lvl3","url":"/kzg10/kzg-hiding#evaluation-with-degree-bound-proof","position":18},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"content":"The next question is, in this Hiding-KZG10 scheme, can we prove f(z)=v and \\deg{f}\\leq d simultaneously like in the first scheme? The paper [CHMMVW19] provided a scheme, which is different from the first scheme. This scheme requires an interactive process (or using Fiat-Shamir transformation) when proving Evaluation with degree bound, that is, the Verifier needs to provide a public random challenge number.","type":"content","url":"/kzg10/kzg-hiding#evaluation-with-degree-bound-proof","position":19},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl4":"Commit","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"type":"lvl4","url":"/kzg10/kzg-hiding#commit","position":20},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl4":"Commit","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"content":"Assuming f(X) is opened at most e times, then the Degree of the blinding polynomial r(X) only needs to be equal to e.\\begin{aligned}\nC_{f}=\\mathsf{Commit}(f(X),r(X)) & = \\Big(\\sum_{i=0}^{d}f_i\\cdot[\\tau^i]_1\\Big) + \\Big(\\sum_{i=0}^{e}r_i\\cdot[{\\color{red}\\gamma}\\tau^i]_1\\Big) \\\\\n& = [f(\\tau) + {\\color{red}\\gamma}\\cdot r(\\tau)]_1\n\\end{aligned}\n\nTo prove the Degree Bound, we also need to commit to X^{D-d}\\cdot f(X):\\begin{aligned}\nC_{xf}=\\mathsf{Commit}(X^{D-d}\\cdot f(X),s(X)) & = \\Big(\\sum_{i=0}^{d}f_i\\cdot[\\tau^{D-d+i}]_1\\Big) + \\Big(\\sum_{i=0}^{d}s_i\\cdot[{\\color{red}\\gamma}\\cdot \\tau^{i}]_1\\Big) \\\\\n& = [\\tau^{D-d}\\cdot f(\\tau) + {\\color{red}\\gamma}\\cdot s(\\tau)]_1\n\\end{aligned}\n\nSo overall, the commitment \\mathsf{cm}(f) of f(X) is defined as:\\mathsf{cm}(f) = (C_{f}, C_{xf})","type":"content","url":"/kzg10/kzg-hiding#commit","position":21},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl4":"Evaluation with degree bound protocol","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"type":"lvl4","url":"/kzg10/kzg-hiding#evaluation-with-degree-bound-protocol","position":22},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl4":"Evaluation with degree bound protocol","lvl3":"Evaluation-with-degree-bound proof","lvl2":"Another construction of Hiding KZG10"},"content":"Public inputs:\n\nCommitment C_f of polynomial f(X)\n\nCommitment C_{xf} of polynomial X^{D-d}\\cdot f(X)\n\nEvaluation point of polynomial f(X), X=z\n\nEvaluation result of polynomial: f(z)=v\n\nWitness:\n\nBlinding polynomial r(X) of polynomial f(X)\n\nBlinding polynomial s(X) of polynomial X^{D-d}\\cdot f(X)\n\nStep 1: Verifier sends random number \\alpha\\leftarrow\\mathbb{F}_r,\n\nStep 2: Prover follows these steps\n\nProver calculates quotient polynomial q(X):\nq(X) = \\frac{f(X) - f(z)}{X-z}\n\nProver calculates aggregated blinding polynomial t(X), obviously \\deg(t)\\leq d\nt(X) = r(X)  + \\alpha\\cdot s(X)\n\nProver calculates quotient polynomial q_t(X)\nq_t(X) = \\frac{t(X) - t(z)}{X-z}\n\nProver introduces an auxiliary polynomial f^*(X), which takes value 0 at X=z, i.e., f^*(z)=0\nf^*(X)=X^{D-d}\\cdot f(X)-X^{D-d}\\cdot f(z)\n\nProver calculates the quotient polynomial q^*(X) of f^*(X) divided by (X-z),\n\\begin{aligned}\nq^*(X) & = \\frac{f^*(X) - f^*(z)}{X-z} \\\\\n& = \\frac{\\big(X^{D-d}\\cdot f(X) - X^{D-d}\\cdot f(z)\\big) - 0}{X-z} \\\\\n& = X^{D-d}\\cdot q(X)\n\\end{aligned}\n\nProver commits to quotient polynomial q(X), without adding any blinding factor\nQ = \\sum_{i=0}^{d-1}q_i\\cdot[\\tau^{i}]_1 = [q(\\tau)]_1\n\nProver commits to quotient polynomial q^*(X), without adding any blinding factor\nQ^* = \\sum_{i=0}^{d-1}q_i\\cdot[\\tau^{D-d+i}]_1 = [q^*(\\tau)]_1\n\nProver commits to quotient polynomial q_t(X) of blinding polynomial\n\\begin{aligned}\nQ_{t} & = \\sum_{i=0}^{d-1}q_{t,i}\\cdot[{\\color{red}\\gamma}\\tau^{i}]_1 \\\\\n& = [{\\color{red}\\gamma}\\cdot q_t(\\tau)]_1\n\\end{aligned}\n\nProver calculates merged commitment Q\n\\begin{aligned}\nQ & = Q + \\alpha\\cdot {Q^*} + Q_{t} \\\\\n& = [q(\\tau)]_1 + \\alpha\\cdot [q^*(\\tau)]_1 + [{\\color{red}\\gamma}\\cdot q_t(\\tau)]_1\n\\end{aligned}\n\nProver outputs proof \\pi = \\big(Q, t(z)\\big)\n\nThe principle of this protocol can actually be understood from another perspective. The construction process can be decomposed into: Batch of evaluations of two polynomials at the same point (using random number \\alpha). One is to prove that the polynomial f(X) takes value f(z) at X=z, and the other is to prove that f^*(X) takes value 0 at X=z. We can introduce an auxiliary polynomial g(X) to represent the random linear combination of these two polynomials about \\alpha:g(X) = f(X) + \\alpha\\cdot (X^{D-d}\\cdot f(X) - X^{D-d}\\cdot f(z))\n\nAnd the quotient polynomial q_g(X) of this aggregated polynomial g(X) divided by (X-z) can be expressed as:q_g(X) = \\frac{g(X) - g(z)}{X-z} = q(X) + \\alpha\\cdot q^*(X)\n\nFinally, the commitment Q calculated by the Prover is exactly equal to the commitment [q_g(\\tau)] of the quotient polynomial plus the commitment of the random polynomial [{\\color{red}\\gamma}\\cdot q_t(\\tau)].\n\nTherefore, this proof idea is actually consistent with the idea of Evaluation proof.\n\nVerification\n\nThe proof received by the Verifier is \\pi = \\big(Q, t(z)\\big), then verify according to the following steps:\n\nCalculate the commitment of g(X)+t(X), denoted as C_{g+t}:\nC_{g+t} = {\\color{red}C_{f}} + \\alpha\\cdot ({\\color{red}C_{xf}} - {\\color{blue}f(z)}\\cdot[\\tau^{D-d}]_1)\n\nCalculate the commitment of the value of g(X)+t(X) at X=z, denoted as V_{g+t}:\nV_{g+t} = f(z)\\cdot[1]_1 + {\\color{red}t(z)}\\cdot[\\gamma]_1\n\nVerify the correctness of C_{g+t}:\ne\\Big(C_{g+t} - V_{g+t},\\ [1]_2\\Big) = e\\Big({\\color{red}Q},\\ [\\tau] - {\\color{blue}z}\\cdot[1]_2\\Big)","type":"content","url":"/kzg10/kzg-hiding#evaluation-with-degree-bound-protocol","position":23},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Comparison"},"type":"lvl2","url":"/kzg10/kzg-hiding#comparison","position":24},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"Comparison"},"content":"In the first scheme, the Prover doesn’t need to care about how many times the polynomial will be opened in the future when committing, and only needs to add one random factor to achieve Perfect Hiding. The second scheme requires the Prover to add enough random factors (in the form of random polynomials) at once, and ensure that the number of times the polynomial is opened in the future will not exceed this random factor.\n\nAn advantage brought by the second scheme is that in each proof of Evaluation, the proof only includes one \\mathbb{G}_1 element, plus one \\mathbb{F}_r element; while the first scheme requires two \\mathbb{G}_1 elements.\n\nFurthermore, the first advantage brought by the second scheme is that the Verifier only needs to calculate two Pairings, while the first scheme requires three Pairings.","type":"content","url":"/kzg10/kzg-hiding#comparison","position":25},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"References"},"type":"lvl2","url":"/kzg10/kzg-hiding#references","position":26},{"hierarchy":{"lvl1":"Understanding Hiding KZG10","lvl2":"References"},"content":"[KZG10] Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. “Constant-size commitments to polynomials and their applications.” Advances in Cryptology-ASIACRYPT 2010: 16th International Conference on the Theory and Application of Cryptology and Information Security, Singapore, December 5-9, 2010. Proceedings 16. Springer Berlin Heidelberg, 2010.\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[PST13] Papamanthou, Charalampos, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” Theory of Cryptography Conference. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. \n\nhttps://​eprint​.iacr​.org​/2011​/587\n\n[ZGKPP17] “A Zero-Knowledge Version of vSQL.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2017​/1146\n\n[XZZPS19] Tiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song. “Libra: Succinct Zero-Knowledge Proofs with Optimal Prover Computation.” \n\nhttps://​eprint​.iacr​.org​/2019​/317\n\n[CHMMVW19] Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Psi Vesely, and Nicholas Ward. “Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS.” \n\nhttps://​eprint​.iacr​.org​/2019​/1047","type":"content","url":"/kzg10/kzg-hiding#references","position":27},{"hierarchy":{"lvl1":"Notes on Libra-PCS"},"type":"lvl1","url":"/libra-pcs/libra-pcs","position":0},{"hierarchy":{"lvl1":"Notes on Libra-PCS"},"content":"","type":"content","url":"/libra-pcs/libra-pcs","position":1},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"1. MLE Polynomials"},"type":"lvl2","url":"/libra-pcs/libra-pcs#id-1-mle-polynomials","position":2},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"1. MLE Polynomials"},"content":"Of course, an MLE polynomial can also be represented in “coefficient form”, which is expressed as follows:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{1}\\sum_{i_1=0}^{1}\\cdots \\sum_{i_{n-1}=0}^{1} f_{i_0i_1\\cdots i_{n-1}} X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}\n\nFor the example of a three-dimensional MLE polynomial shown above, we can write it as:\\tilde{f}(X_0, X_1, X_2) = f_0 + f_1X_0 + f_2X_1 + f_3X_2 + f_4X_0X_1 + f_5X_0X_2 + f_6X_1X_2 + f_7X_0X_1X_2\n\nwhere (f_0, f_1, \\ldots, f_7) is the coefficient vector of the MLE polynomial. Note that because MLE polynomials belong to multivariate polynomials, any representation needs to determine the ordering of terms in the polynomial in advance. In this article and subsequent discussions, we will base on the Lexicographic Order.\n\nFor the “point-value form” representation of MLE polynomials, we can define it as:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{1}\\sum_{i_1=0}^{1}\\cdots \\sum_{i_{n-1}=0}^{1} a_{i_0i_1\\cdots i_{n-1}}\\cdot eq(i_0, i_1, \\ldots, i_{n-1}, X_0, X_1, \\ldots, X_{n-1})\n\nwhere eq is a set of Lagrange Polynomials for the n-dimensional Boolean HyperCube \\{0, 1\\}^n:eq(i_0, i_1, \\ldots, i_{n-1}, X_0, X_1, \\ldots, X_{n-1}) = \\prod_{j=0}^{n-1} \\Big((1-i_j)\\cdot (1-X_j)+ i_j\\cdot X_j\\Big)\n\nThere exists an N\\log(N) conversion algorithm between the “point-value form” and “coefficient form” of MLE polynomials, which we won’t discuss in depth here.","type":"content","url":"/libra-pcs/libra-pcs#id-1-mle-polynomials","position":3},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"2. Division of MLE Polynomials"},"type":"lvl2","url":"/libra-pcs/libra-pcs#id-2-division-of-mle-polynomials","position":4},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"2. Division of MLE Polynomials"},"content":"For a univariate polynomial f(X)\\in\\mathbb{F}_p[X], if the value of f(X) at X=u is v, then we have the following equation:f(X)= q(X)\\cdot (X-u) + v\n\nwhere q(X) is the quotient polynomial of f(X) divided by (X-u), and v is the remainder.\n\nWe can derive this equation simply. When we substitute X=u into the equation, we get f(u)=v. This shows that the problem of polynomial evaluation is equivalent to finding the remainder of polynomial division. Then, we can subtract this remainder from f(X), and the resulting polynomial g(X)=f(X)-v can obviously be divided by (X-u), meaning there exists a quotient polynomial, denoted as q(X).\n\nFor a multivariate polynomial f(X_0, X_1, \\ldots, X_{n-1}), the paper [PST13] provides a similar division relation equation:f(X_0, X_1, \\ldots, X_{n-1}) - f(u_0, u_1, \\ldots, u_{n-1})= \\sum_{k=0}^{n-1}q_k(X_0, X_1, \\ldots, X_{n-1}) \\cdot (X_k - u_k)\n\nIf f(X_0, X_1, \\ldots, X_{n-1}) is an MLE polynomial, it can be simplified to the following equation:\\begin{split}\n\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) - \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) & = \\tilde{q}_{n-1}(X_0, X_1, \\ldots, X_{\\color{red}n-2}) \\cdot (X_{n-1} - u_{n-1}) \\\\\n& + \\tilde{q}_{n-2}(X_0, X_1, \\ldots, X_{\\color{red}n-3}) \\cdot (X_{n-2} - u_{n-2}) \\\\\n& + \\cdots \\\\\n& + \\tilde{q}_{1}(X_{\\color{red}0}) \\cdot (X_{1} - u_{1}) \\\\\n& + \\tilde{q}_{0} \\cdot (X_{0} - u_{0}) \\\\\n\\end{split}\n\nThis is because in the MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}), the highest degree of each unknown X_k is 1. For f(X_0, X_1, \\ldots, X_{k}), after dividing by the factor (X_k-u_k), the remainder polynomial will no longer contain the unknown X_k. So when f(X_0, X_1, \\ldots, X_{n-1}) is sequentially divided by factors from (X_{n-1} - u_{n-1}) to (X_0 - u_0), the number of unknowns in the resulting quotient polynomials and remainder polynomials will decrease successively, until we finally get a constant quotient polynomial \\tilde{q}_0. Of course, after n divisions, a constant remainder polynomial will appear, which is exactly the evaluation of the MLE polynomial at (u_0, u_1, \\ldots, u_{n-1}). This is known as Ruffini’s rule [Ruffini].\n\nLet’s assume this final evaluation is v, i.e.,\\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = v","type":"content","url":"/libra-pcs/libra-pcs#id-2-division-of-mle-polynomials","position":5},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"3. Construction of Libra-PCS"},"type":"lvl2","url":"/libra-pcs/libra-pcs#id-3-construction-of-libra-pcs","position":6},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"3. Construction of Libra-PCS"},"content":"Similar to the construction of KZG10, Libra-PCS also requires a structured SRS. It should be noted that the Libra paper is based on the KOE security assumption, while the scheme introduced in this article is based on the AGM security assumption, so the scheme’s SRS has a smaller size and can also prove the correctness and Extractibility properties of the scheme under the AGM assumption.\n\nIt is generated by a Trusted Setup:SRS = \\left(\n\\begin{array}{l}\n[1]_1, [\\tau_0]_1, [\\tau_1]_1, [\\tau_0\\tau_1]_1, [\\tau_2]_1, [\\tau_0\\tau_2]_1, [\\tau_1\\tau_2]_1, [\\tau_0\\tau_1\\tau_2]_1, \\ldots, [\\tau_0\\tau_1\\cdots\\tau_{n-1}]_1, [\\xi]_1\\\\[1.5ex]\n[1]_2,\\ [\\tau_0]_2,\\ [\\tau_1]_2,\\ [\\tau_2]_2,\\ \\ldots,\\ [\\tau_{n-1}]_2,\\ [\\xi]_2\n\\end{array}\n\\right)\n\nWith this SRS, we can calculate the commitment of an n-variable MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}). That is, use the SRS as a basis for linear combination with its coefficient vector of length N=2^n to obtain an element on \\mathbb{G}_1.\\begin{split}\n\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) & = f_0 + f_1X_0 + f_2X_1 + f_3X_0X_1 + f_4X_2 + f_5X_0X_2 + f_6X_1X_2 \\\\\n& + f_7X_0X_1X_2 + \\cdots + f_{N-1}X_0X_1\\cdots X_{n-1} \\\\\n\\end{split}\n\nwhere \\vec{f}=(f_0, f_1, f_2, \\ldots, f_{N-1}) is the coefficient vector of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}). The commitment is calculated as follows:\\begin{split}\nF=\\mathsf{Commit}(\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}); \\rho) & = f_0 \\cdot [1]_1 + f_1 \\cdot [\\tau_0]_1 + f_2 \\cdot [\\tau_1]_1 + f_3 \\cdot [\\tau_0\\tau_1]_1 \\\\\n& + f_4 \\cdot [\\tau_2]_1 + f_5 \\cdot [\\tau_0\\tau_2]_1 + f_6 \\cdot [\\tau_1\\tau_2]_1 + f_7 \\cdot [\\tau_0\\tau_1\\tau_2]_1 \\\\\n& + \\cdots + f_{N-1} \\cdot [\\tau_0\\tau_1\\cdots\\tau_{n-1}]_1 + \\rho \\cdot [\\xi]_1\n\\end{split}\n\nHere \\rho is a random number used to hide the information of \\vec{f}.\n\nThen if the Prover wants to prove \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = v, they need to commit to the quotient polynomials \\tilde{q}_0, \\tilde{q}_1, \\ldots, \\tilde{q}_{n-1}.\\begin{split}\nQ_0=\\mathsf{Commit}(\\tilde{q}_0; \\eta_0) & = \\tilde{q}_0 \\cdot [1]_1 + \\eta_0 \\cdot [\\xi]_1 \\\\[1ex]\nQ_1=\\mathsf{Commit}(\\tilde{q}_1; \\eta_1) & = \\tilde{q}_{1,0} \\cdot [1]_1 + \\tilde{q}_{1,1} \\cdot [\\tau_0]_1 +  \\eta_1 \\cdot [\\xi]_1 \\\\[1ex]\nQ_2=\\mathsf{Commit}(\\tilde{q}_2; \\eta_2) & = \\tilde{q}_{2,0} \\cdot [1]_1 + \\tilde{q}_{2,1} \\cdot [\\tau_0]_1 + \\tilde{q}_{2,2} \\cdot [\\tau_1]_1 + \\tilde{q}_{2,3} \\cdot [\\tau_0\\tau_1]_1 + \\eta_2 \\cdot [\\xi]_1 \\\\[1ex]\n\\cdots & = \\cdots \\\\[1ex]\nQ_{n-1}=\\mathsf{Commit}(\\tilde{q}_{n-1}; \\eta_{n-1}) & = \\tilde{q}_{n-1,0} \\cdot [1]_1 + \\tilde{q}_{n-1,1} \\cdot [\\tau_0]_1 + \\tilde{q}_{n-1,2} \\cdot [\\tau_1]_1 + \\cdots \\\\\n& \\ + \\tilde{q}_{n-1,2^{n-1}-1} \\cdot [\\tau_0\\tau_1\\cdots\\tau_{n-2}]_1 + \\eta_{n-1} \\cdot [\\xi]_1 \\\\[1ex]\n\\end{split}\n\nTo ensure that the Verifier can verify these Commitments, and to allow each Commitment to have a Blinding Factor, we need to modify the division equation of the MLE polynomial after adding these Blinding Factors:\\begin{split}\n&(q_0 + \\eta_0\\xi)(X_0-u_0) + (q_1 + \\eta_1\\xi)(X_1-u_1) + \\cdots + (q_{n-1} + \\eta_{n-1}\\xi)(X_{n-1}-u_{n-1}) \\\\\n=\\ & q_0(X_0-u_0) + q_1(X_0)(X_1-u_1) + \\cdots + q_{n-1}(X_0,\\ldots,X_{n-2})(X_{n-1}-u_{n-1}) \\\\\n& + \\eta_0\\xi(X_0-u_0) + \\eta_1\\xi(X_1-u_1) + \\cdots + \\eta_{n-1}\\xi(X_{n-1}-u_{n-1}) \\\\\n=\\ & f(\\vec{X}) - f(\\vec{u}) + ({\\color{red}\\eta_0(X_0-u_0) + \\eta_1(X_1-u_1) + \\cdots + \\eta_{n-1}(X_{n-1}-u_{n-1})})\\cdot \\xi \\\\\n=\\ & f(\\vec{X}) + \\rho\\xi- f(\\vec{u}) + ({\\color{red}\\eta_0(X_0-u_0) + \\eta_1(X_1-u_1) + \\cdots + \\eta_{n-1}(X_{n-1}-u_{n-1})-\\rho})\\cdot \\xi\\\\\n\\end{split}\n\nTherefore, the Prover needs to calculate an additional Commitment, collecting all the Blinding Factors, which is the red part on the right side of the equation above:\\begin{split}\nR & = \\rho\\cdot[1]_1 + (-\\eta_0\\cdot[\\tau_0]_1 + (\\eta_0\\cdot u_0)[1]_1) + (-\\eta_1\\cdot[\\tau_1]_1 + (\\eta_1\\cdot u_1)[1]_1) \\\\\n& + \\cdots + (-\\eta_{n-1}\\cdot[\\tau_{n-1}]_1 + (\\eta_{n-1}\\cdot u_{n-1})[1]_1)\n\\end{split}\n\nHere R is clearly also an element on \\mathbb{G}_1.\n\nSo after the Prover sends (Q_0, Q_1, \\ldots, Q_{n-1}, R) to the Verifier, the Verifier can verify through the following Pairing equation:e(F - v[1]_1, [1]_2) \\overset{?}{=} e(R, [\\xi]_2) + \\sum_{i=0}^{n-1} e(Q_i, [\\tau_i]_2-u_i[1]_2)","type":"content","url":"/libra-pcs/libra-pcs#id-3-construction-of-libra-pcs","position":7},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"4. Supporting MLE Evaluation Form"},"type":"lvl2","url":"/libra-pcs/libra-pcs#id-4-supporting-mle-evaluation-form","position":8},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"4. Supporting MLE Evaluation Form"},"content":"In the Libra-PCS described above, when the Prover calculates the Commitment, they need to first obtain the “coefficient form” of the MLE polynomial before calculating the Commitment. However, in many Sumcheck protocols, the Evaluation Form of MLE is used, that is, the point-value form. In other words, the n-variable MLE polynomial is represented by its evaluation values on the n-dimensional Boolean Hypercube:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{1}\\sum_{i_1=0}^{1}\\cdots \\sum_{i_{n-1}=0}^{1} a_{i_0i_1\\cdots i_{n-1}}\\cdot eq(i_0, i_1, \\ldots, i_{n-1}, X_0, X_1, \\ldots, X_{n-1})\n\nIf we need to convert the Evaluation form of the MLE polynomial to the Coefficient form, this conversion algorithm requires O(n\\cdot2^n) time complexity. So is it possible to directly support the Evaluation form of MLE?\n\nThe Evaluation form of MLE brings two difficult problems. The first is how to calculate a commitment for an Evaluation Form based on SRS, and the second is how to calculate n divisions of the MLE polynomial to obtain n quotient polynomials q_0, q_1, \\ldots, q_{n-1}.\n\nLet’s look at the first problem: how to calculate the Commitment for the Evaluation Form. There are two ways to do this. A more direct way is to produce the Commitment of the Multilinear Basis of the MLE polynomial directly when calculating the SRS, rather than the Commitment of the Monomial Basis.\n\nFor example, suppose n=3, then we produce a new set of SRS parameters to calculate the Commitment of the Evaluation Form of a three-variable MLE polynomial.SRS^{(3)} = ([eq_0(\\tau_0,\\tau_1, \\tau_2)]_1, [eq_1(\\tau_0,\\tau_1, \\tau_2)]_1, [eq_2(\\tau_0,\\tau_1, \\tau_2)]_1, \\ldots, [eq_{7}(\\tau_0,\\tau_1, \\tau_2)]_1, [\\xi]_1)\n\nTo simplify the notation, we use eq_i(\\vec{X}) to represent eq(i_0,i_1,\\ldots,i_{n-1},X_0,X_1,\\ldots,X_{n-1}). Suppose we have a polynomial f(X_0, X_1, X_2):f(X_0, X_1, X_2) = a_0\\cdot eq_0(X_0, X_1, X_2) + a_1\\cdot eq_1(X_0, X_1, X_2) + \\cdots + a_7\\cdot eq_7(X_0, X_1, X_2)\n\nSo we calculate its Commitment as follows:\\mathsf{cm}(f) = a_0\\cdot [eq_0(\\tau_0,\\tau_1, \\tau_2)]_1 + a_1\\cdot [eq_1(\\tau_0,\\tau_1, \\tau_2)]_1 + \\cdots + a_7\\cdot [eq_7(\\tau_0,\\tau_1, \\tau_2)]_1 + \\rho\\cdot [\\xi]_1\n\nHowever, we need to realize that SRS^{(3)} is not enough, because the Multilinear Basis like eq_i(\\vec{X}) is related to the size of the Domain. We should realize that if we want to commit to the Evaluation Form of a bivariate polynomial, we cannot use SRS^{(3)} to calculate. Instead, we need to produce another set of SRS parameters for bivariate MLE polynomials, denoted as SRS^{(2)}SRS^{(2)} = ([eq_0(\\tau_0,\\tau_1)]_1, [eq_1(\\tau_0,\\tau_1)]_1, \\ldots, [eq_{3}(\\tau_0,\\tau_1)]_1)\n\nSimilarly, we also need a set of SRS parameters for univariate polynomials, denoted as SRS^{(1)}:SRS^{(1)} = ([eq_0(\\tau_0)]_1, [eq_1(\\tau_0)]_1)\n\nFinally, for constant polynomials, we only need [1]_1 as a Basis to calculate the commitment. We combine all these SRS^{(k)} based on k-MLE together, denoted as SRS^*:SRS^* = \\left(\n\\begin{array}{l}\n[eq_0(\\tau_0,\\tau_1, \\tau_2)]_1, [eq_1(\\tau_0,\\tau_1, \\tau_2)]_1, [eq_2(\\tau_0,\\tau_1, \\tau_2)]_1, \\ldots, [eq_{7}(\\tau_0,\\tau_1, \\tau_2)]_1, \\\\[1ex]\n[eq_0(\\tau_0,\\tau_1)]_1, [eq_1(\\tau_0,\\tau_1)]_1, \\ldots, [eq_{3}(\\tau_0,\\tau_1)]_1, \\\\[1ex]\n[eq_0(\\tau_0)]_1, [eq_1(\\tau_0)]_1, \\\\[1ex]\n[1]_1, [\\xi]_1 \\\\[1ex]\n[1]_2,\\ [\\tau_0]_2,\\ [\\tau_1]_2,\\ [\\tau_2]_2,\\ \\ldots,\\ [\\tau_{n-1}]_2,\\ [\\xi]_2\n\\end{array}\n\\right)\n\nNext, suppose we have obtained three quotient polynomials q_0, q_1(X_0), q_2(X_0,X_1) of f(X_0,X_1,X_2). Then we calculate the corresponding Commitments through their Evaluation Form:\\begin{split}\n\\mathsf{cm}(q_0) & = q_0\\cdot [1]_1 + \\eta_0\\cdot [\\xi]_1 \\\\[1.5ex]\n\\mathsf{cm}(q_1) & = q_{1,0}\\cdot [eq_0(\\tau_0)]_1 + q_{1,1}\\cdot [eq_1(\\tau_0)]_1 + \\eta_1\\cdot [\\xi]_1 \\\\[1.5ex]\n\\mathsf{cm}(q_2) & = q_{2,0}\\cdot [eq_0(\\tau_0,\\tau_1)]_1 + q_{2,1}\\cdot [eq_1(\\tau_0,\\tau_1)]_1 + q_{2,2}\\cdot [eq_2(\\tau_0,\\tau_1)]_1 + q_{2,3}\\cdot [eq_3(\\tau_0,\\tau_1)]_1 + \\eta_2\\cdot [\\xi]_1 \\\\[1.5ex]\n\\end{split}\n\nNext, let’s consider the second difficult problem: how to calculate n divisions of the MLE polynomial to obtain n quotient polynomials q_0, q_1, \\ldots, q_{n-1} in Evaluation Form. The familiar polynomial long division is only applicable to the Coefficient Form of polynomials, while the Libra paper [XZZPS19] provides an O(n) algorithm that supports division of MLE polynomials in Evaluation Form.\n\nLet’s first consider a simple case, assuming a bivariate MLE polynomial f(X_0, X_1) in Evaluation Form:f(X_0, X_1) = a_0 \\cdot (1-X_0)(1-X_1) + a_1 \\cdot X_0(1-X_1) + a_2 \\cdot (1-X_0)X_1 + a_3 \\cdot X_0X_1\n\nThe first step is to calculate f(X_0, X_1)/(X_1-u_1),\n\nWe expand the Evaluation Form of f(X_0, X_1) according to the degree of X_1:\\begin{split}\nf(X_0, X_1) & = ({\\color{blue}a_0(1-X_0) + a_1X_0})(1-X_1) + ({\\color{red}a_2(1-X_0) + a_3X_0})X_1 \\\\\n& = ({\\color{blue}a_0(1-X_0) + a_1X_0}) + ({\\color{red}a_2(1-X_0) + a_3X_0} - ({\\color{blue}a_0(1-X_0) + a_1X_0})) \\cdot X_1\n\\end{split}\n\nThen the quotient polynomial q_1(X_0) is equal to:q_1(X_0) = ({\\color{red}a_2(1-X_0) + a_3X_0}) - ({\\color{blue}a_0(1-X_0) + a_1X_0})\n\nWe rearrange the terms of q_1(X_0) and can get:q_1(X_0) = ({\\color{red}a_2}-{\\color{blue}a_0})(1-X_0) + ({\\color{red}a_3}-{\\color{blue}a_1})X_0\n\nThe above equation is exactly the Evaluation Form of q_1(X_0), which is not difficult to calculate. We just need to subtract the first half of the Evaluation vector of f(X_0, X_1) from the second half to obtain the Evaluation vector of q_1(X_0).\n\nNext, let’s look at the remainder polynomial after dividing f(X_0, X_1)/(X_1-u_1), which is a univariate polynomial in terms of X_0, denoted as f'(X_0):\\begin{split}\nf'(X_0) &= f(X_0, X_1) - q_1(X_0)(X_1-u_1)  \\\\\n & = ({\\color{blue}a_0(1-X_0) + a_1X_0}) + u_1\\cdot \\Big(({\\color{red}a_2(1-X_0) + a_3X_0}) - ({\\color{blue}a_0(1-X_0) + a_1X_0})\\Big) \\\\\n & = (1-u_1)\\cdot ({\\color{blue}a_0(1-X_0) + a_1X_0}) + u_1\\cdot ({\\color{red}a_2(1-X_0) + a_3X_0}) \\\\\n & = ((1-u_1)\\cdot {\\color{blue}a_0} + u_1\\cdot {\\color{red}a_2})\\cdot(1-X_0) + ((1-u_1)\\cdot {\\color{blue}a_1} + u_1\\cdot {\\color{red}a_3})\\cdot X_0 \\\\\n\\end{split}\n\nAfter reorganization, we see that the Evaluation vector of the remainder polynomial f'(X_0) is exactly the combination of the first half and the second half of the Evaluation vector of f(X_0, X_1), that is:\\mathsf{evaluations}(f)= (a_0, a_1, a_2, a_3)\n\nThen\\mathsf{evaluations}(f') = (\\mathsf{fold}(u_1, a_0, a_2),\\ \\mathsf{fold}(u_1, a_1, a_3))\n\nHere the definition of the fold function \\mathsf{fold} is:\\mathsf{fold}(u, a, b) = (1-u)\\cdot a + u\\cdot b\n\nTherefore, we can obtain a clear algorithm. Each time, split the Evaluation vector into two halves, subtract the lower half from the higher half to get the Evaluation vector of the quotient polynomial; fold the high and low halves to get the Evaluation vector of the remainder polynomial. Then continue recursively to get all the quotient polynomials. Below we give the Python implementation of this algorithm:def decompose_by_div(evaluations, point) -> tuple[list, int]:\n    e = evaluations.copy()\n    k = log_2(len(e))\n    quotients = []\n    half = pow_2(k - 1)\n    for i in range(k):\n        q = [0] * half  # init quotient MLE (evalations)\n        for j in range(half):\n            q[j] = e[j + half] - e[j]  # compute quotient MLE\n            e[j] = e[j] * (1 - point[k-i-1]) + e[j + half] * point[k-i-1]  # fold by point[k-i-1]\n        quotients.insert(0, q)\n        half >>= 1\n    return quotients, e[0] # e[0] = f(point)","type":"content","url":"/libra-pcs/libra-pcs#id-4-supporting-mle-evaluation-form","position":9},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"References"},"type":"lvl2","url":"/libra-pcs/libra-pcs#references","position":10},{"hierarchy":{"lvl1":"Notes on Libra-PCS","lvl2":"References"},"content":"[PST13] Papamanthou, Charalampos, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” Theory of Cryptography Conference. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. \n\nhttps://​eprint​.iacr​.org​/2011​/587\n\n[XZZPS19] Tiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song. “Libra: Succinct Zero-Knowledge Proofs with Optimal Prover Computation.” Cryptology ePrint Archive (2019). \n\nhttps://​eprint​.iacr​.org​/2019​/317\n\n[Ruffini] Ruffini’s rule. (\n\nhttps://en.wikipedia.org/wiki/Ruffini’s_rule)","type":"content","url":"/libra-pcs/libra-pcs#references","position":11},{"hierarchy":{"lvl1":"Ligerito-PCS Notes"},"type":"lvl1","url":"/ligerito-pcs/ligerito-01","position":0},{"hierarchy":{"lvl1":"Ligerito-PCS Notes"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nLast updated: 2025-06-05\n\nLigerito [NA25] is a multilinear extension (MLE) polynomial commitment scheme built on Ligero [AHIV17], employing a recursive proof strategy similar to the FRI protocol. After understanding the Ligerito protocol, I found that its recursive approach is similar to the FRI protocol, while the incorporation of a Partial Sumcheck protocol likely draws inspiration from Basefold. During the recursive process, the Sumcheck protocol can be merged with the Partial Sumcheck protocol from the previous iteration - an approach nearly identical to the handling of Shift Queries in the Whir protocol. In the Discussion section on page 13 of the paper, the authors write:\n\nRemco Bloemen and Giacomo Fenzi have commented that this protocol is structurally similar to the WHIR protocol of [ACFY24], though we note that Ligerito uses general linear codes, the logarithmic randomness of [DP24], and as far as we can tell, results in concretely different (and smaller) numbers in the unique decoding regime. A natural open question is whether there is a simple generalization of both protocols that can recast them in a common framework.\n\nThis article provides an overview of Ligerito’s core ideas and compares them with the Whir protocol. Ultimately, I believe there is no substantial difference between the two protocols - despite their different design paths, their core processes are almost identical. Whir additionally incorporates an Out-of-domain Query step and tends to use larger domains to compute intermediate polynomial RS Codes during iterations, aiming to improve code rate and reduce query counts.\n\nApproach:\n\nReview the Ligero protocol\n\nThe crucial role of Sumcheck\n\nRecursive implementation","type":"content","url":"/ligerito-pcs/ligerito-01","position":1},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"1. Ligero"},"type":"lvl2","url":"/ligerito-pcs/ligerito-01#id-1-ligero","position":2},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"1. Ligero"},"content":"Let’s work through the Ligero protocol with a simple example to help readers quickly grasp the concept. We’ll consider an MLE polynomial with just four variables, defined as:f(x_0, x_1, x_2, x_3) = \\sum_{i=0}^{2^4-1} a_i \\cdot eq(\\mathsf{bits}(i), (x_0, x_1, x_2, x_3))\n\nwhere the vector \\vec{a}=(a_0, a_1, \\cdots, a_{15}) represents the evaluations of polynomial f on the 4-dimensional Boolean Hypercube (\\{0, 1\\}^4).","type":"content","url":"/ligerito-pcs/ligerito-01#id-1-ligero","position":3},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Computing the Commitment","lvl2":"1. Ligero"},"type":"lvl3","url":"/ligerito-pcs/ligerito-01#computing-the-commitment","position":4},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Computing the Commitment","lvl2":"1. Ligero"},"content":"We can rearrange the vector \\vec{a} into a 4\\times 4 matrix, denoted as A:A=\n\\begin{bmatrix}\na_0 & a_1 & a_2 & a_3 \\\\\na_4 & a_5 & a_6 & a_7 \\\\\na_8 & a_9 & a_{10} & a_{11} \\\\\na_{12} & a_{13} & a_{14} & a_{15}\n\\end{bmatrix}\n\nWe use a linear code, C[\\mathbb{F}, 4, 8], with an associated generator matrix G\\in\\mathbb{F}^{4\\times 8}, to encode each row of the matrix (this encoding scheme has a rate of 1/2):\\mathsf{enc}(a_0, a_1, a_2, a_3) = \n\\begin{bmatrix}\na_0 & a_1 & a_2 & a_3\n\\end{bmatrix}\nG\n\nWe can choose RS Code or any linear code defined over the finite field \\mathbb{F}_q.\nIn the encoding example above, \\mathsf{enc}(a_0, a_1, a_2, a_3) results in a row vector of length 8, which we’ll denote as (e_0, e_1, e_2, e_3, e_4, e_5, e_6, e_7). Similarly, we encode all rows of A using G to compute the encoded matrix B of size 4\\times 8:B= AG =\n\\begin{bmatrix}\ne_0 & e_1 & e_2 & e_3 & e_4 & e_5 & e_6 & e_7 \\\\\ne_8 & e_9 & e_{10} & e_{11} & e_{12} & e_{13} & e_{14} & e_{15} \\\\\ne_{16} & e_{17} & e_{18} & e_{19} & e_{20} & e_{21} & e_{22} & e_{23} \\\\\ne_{24} & e_{25} & e_{26} & e_{27} & e_{28} & e_{29} & e_{30} & e_{31}\n\\end{bmatrix}\n\nLike any hash-based PCS protocol, we use a Merkle Tree to compute the commitment of B. Importantly, we use a column-wise approach to commit to B. First, we treat each column of the matrix as a leaf node of a Merkle Tree, then compute its root. This gives us 8 Merkle Tree roots for the matrix B:\\begin{aligned}\n\\mathsf{t}_0 &= \\mathsf{MerkleRoot}(e_0, e_8, e_{16}, e_{24}) \\\\\n\\mathsf{t}_1 &= \\mathsf{MerkleRoot}(e_1, e_9, e_{17}, e_{25}) \\\\\n\\vdots \\\\\n\\mathsf{t}_7 &= \\mathsf{MerkleRoot}(e_7, e_{15}, e_{23}, e_{31}) \\\\\n\\end{aligned}\n\nWe then use these 8 roots as leaf nodes of a new Merkle Tree and compute their root:\\mathsf{t} = \\mathsf{MerkleRoot}(\\mathsf{t}_0, \\mathsf{t}_1, \\cdots, \\mathsf{t}_7)\n\nFinally, \\mathsf{t} is our commitment to B. Of course, this commitment computation method can also be viewed as flattening matrix B column-by-column into a one-dimensional vector of length 32, treating these as leaf nodes of a Merkle Tree, and computing its root to obtain the same value \\mathsf{t}.","type":"content","url":"/ligerito-pcs/ligerito-01#computing-the-commitment","position":5},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Proving Correctness of Evaluation at a Random Point","lvl2":"1. Ligero"},"type":"lvl3","url":"/ligerito-pcs/ligerito-01#proving-correctness-of-evaluation-at-a-random-point","position":6},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Proving Correctness of Evaluation at a Random Point","lvl2":"1. Ligero"},"content":"The Prover can commit to an MLE polynomial using the method above and send the commitment \\mathsf{t} to the Verifier. Then the Verifier can randomly choose a point (r_0, r_1, r_2, r_3) and send it to the Prover. The Prover returns a value v along with a proof \\pi_{eval} proving the correctness of the evaluation:\\pi_{eval}: v=f(r_0, r_1, r_2, r_3)\n\nLet’s quickly go through the Ligero approach:\n\nStep 1: The Prover computes B and its commitment \\mathsf{t}, then sends it to the Verifier.\n\nStep 2: The Verifier randomly selects a point (r_0, r_1, r_2, r_3)\\in\\mathbb{F}_q^{4} and sends it to the Prover.\n\nStep 3: The Prover uses r_3, r_2 to perform a “vertical folding” of all “rows” in matrix A, and sends the calculated folded row vector \\vec{a}':\\vec{a}' = \n\\begin{bmatrix}\n(1-r_2)(1-r_3) & r_2(1-r_3) & r_3(1-r_2) & r_2r_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\na_0 & a_1 & a_2 & a_3 \\\\\na_4 & a_5 & a_6 & a_7 \\\\\na_8 & a_9 & a_{10} & a_{11} \\\\\na_{12} & a_{13} & a_{14} & a_{15}\n\\end{bmatrix}\n\nNote that while the matrix has 4 rows, the Prover only needs two random numbers to fold them. This is because we can use k random numbers to construct 2^k linearly independent values:\\bar{r} = (1-r_2, r_2) \\otimes (1-r_3, r_3)\n\nWe use the notation \\bar{r} from the Ligerito paper to represent a vector of n linearly independent values constructed from \\log{n} random factors - an approach from [DP24]. Here, \\otimes represents the Kronecker product. Expanded, \\bar{r} can be written as:\\bar{r} = \n\\begin{bmatrix}\n(1-r_2)(1-r_3) & \n(1-r_2)r_3 &\nr_2(1-r_3) &\nr_2r_3\n\\end{bmatrix}\n\nIf the mathematical notation is difficult to understand, let me explain the calculation of \\vec{a}' more intuitively.\n\nI’ll introduce a new notation \\mathsf{fold}(r, \\vec{c}), which represents folding a column vector \\vec{c} using a random number r:\\mathsf{fold}(r, (c_0,c_1,c_2,\\ldots, c_7)^T) = \n\\begin{bmatrix}\n(1-r)\\cdot c_0 + r\\cdot c_4 \\\\\n(1-r)\\cdot c_1 + r\\cdot c_5 \\\\\n(1-r)\\cdot c_2 + r\\cdot c_6 \\\\\n(1-r)\\cdot c_3 + r\\cdot c_7\n\\end{bmatrix}\n=\\begin{bmatrix}\nc_0' \\\\\nc_1' \\\\\nc_2' \\\\\nc_3'\n\\end{bmatrix}\n=\\vec{c}'\n\nThis fold function can be understood as splitting the vector in half, multiplying the first half by the factor (1-r), the second half by r, then adding the corresponding elements to obtain a column vector half as long.\n\nIf we continue folding the resulting vector \\vec{c}'=(c_0', c_1', c_2', c_3')^T with a new random number r', we get:\\mathsf{fold}(r', \\vec{c}') = \n\\begin{bmatrix}\n(1-r')\\cdot c_0' + r'\\cdot c_2' \\\\\n(1-r')\\cdot c_1' + r'\\cdot c_3' \\\\\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n(1-r')(1-r)\\cdot c_0 + (1-r')r\\cdot c_4 + r'(1-r)\\cdot c_2 + r'r\\cdot c_6 \\\\\n(1-r')(1-r)\\cdot c_1 + (1-r')r\\cdot c_5 + r'(1-r)\\cdot c_3 + r'r\\cdot c_7 \\\\\n\\end{bmatrix}\n= \\vec{c}''\n\nIf we allow \\mathsf{fold} to be composable, we can express composite folding as:\\mathsf{fold}(r', r, \\vec{a}) = \\mathsf{fold}(r', \\mathsf{fold}(r, \\vec{a}))\n= \\vec{a}''\n\nContinuing our example, we can express the calculation of \\vec{a}' using the \\mathsf{fold} function:\\vec{a}' = \n\\begin{bmatrix}\n\\mathsf{fold}(r_2, r_3, (a_0, a_4, a_8, a_{12})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_1, a_5, a_9, a_{13})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_2, a_6, a_{10}, a_{14})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_3, a_7, a_{11}, a_{15}))\n\\end{bmatrix}\n\nStep 4: The Verifier samples Q\\subset\\{0, 1, 2, \\ldots, 7\\} and sends it to the Prover.Q = \\{q_0, q_1, \\ldots, q_{l-1}\\}\n\nThe Verifier’s random sampling aims to check whether the Prover honestly calculated \\vec{a}' using the \\mathsf{fold}(r2,r_3,\\cdot) function. Due to the remarkable “Proximity Gap” property of linear coding, a folded linear code is either still a correct code or, with high probability, far from any correct code. This distance between codes is measured by the Hamming Distance. Thus, the Verifier can check the correctness of the folding process by sampling just a few positions in the codeword. Of course, more sampling is more secure, but increases the proof size. We can determine the minimum number of samples needed to ensure security using probability formulas. Let’s denote the number of samples as l.\n\nStep 5: The Prover responds to the Verifier’s random sampling by sending, for each query point q_i \\in Q, the column vector \\vec{b}_{q_i} of B, along with its corresponding Merkle Tree root \\mathsf{t}_{q_i}, and the Merkle Path \\pi_{q_i} from \\mathsf{t}_{q_i} to \\mathsf{t}.\n\nFor example, if q_0=2, the Prover would send (e_2, e_{10}, e_{18}, e_{26}), their Merkle Root \\mathsf{t}_{2}, and the Merkle Path from \\mathsf{t}_{2} to \\mathsf{t}.\n\nVerification Step: The Verifier needs to check if the following equations hold:\n\nAll \\mathsf{t}_{q_i} are consistent with \\mathsf{t}:\\mathsf{t}_{q_i} \\overset{?}{=} \\mathsf{MerkleRoot}(\\vec{b}_{q_i}), \\quad \\forall q_i\\in Q\n\nEach t_{q_i} is a leaf of \\mathsf{t}:\\mathsf{MerkleTree.verify}(\\mathsf{t}, \\mathsf{t}_{q_i}, \\pi_{q_i}) \\overset{?}{=} 1, \\quad \\forall q_i\\in Q\n\nThe folding of \\vec{b}_{q_i} is consistent with the corresponding point of encoded \\vec{a}'G:\\mathsf{fold}(r_2, r_3,\\vec{b}_{q_i}) \\overset{?}{=} \\vec{a}'^TG_{q_i}, \\quad \\forall q_i\\in Q\n\nwhere G_{q_i} represents the q_i-th column of the generator matrix G.\n\nVerify the vector \\vec{a}':\\vec{a}'^T\\cdot\\big((1-r_0,r_0)\\otimes(1-r_1,r_1)\\big)\\overset{?}{=}v\n\nIf all four verification steps pass, the Verifier accepts the proof:\\pi_{eval}:f(r_0, r_1, r_2, r_3) = v","type":"content","url":"/ligerito-pcs/ligerito-01#proving-correctness-of-evaluation-at-a-random-point","position":7},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"2. The Crucial Role of Sumcheck"},"type":"lvl2","url":"/ligerito-pcs/ligerito-01#id-2-the-crucial-role-of-sumcheck","position":8},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"2. The Crucial Role of Sumcheck"},"content":"The protocol in the previous section has an obvious limitation: the Verifier must provide the evaluation point only after the Prover has committed to polynomial f, and this point must be sufficiently random - it cannot be a predetermined public point. This means the protocol doesn’t satisfy the general application scenarios of polynomial commitments, where we want to support MLE polynomial evaluation at arbitrary, even non-random points.\n\nThis is where we need to introduce the Sumcheck protocol. Specifically, by adding a partially executed Sumcheck sub-protocol to the Ligero protocol, we can complete an evaluation proof for an MLE polynomial at any arbitrary point. This approach actually originated from Basefold [ZCF23]. I assume readers are already familiar with the Sumcheck protocol, or please refer to our previous articles. (TODO)\n\nWe’ll continue with our four-variable MLE polynomial f, but now we want the Prover to prove f’s evaluation at an arbitrary point \\vec{u}=(u_0, u_1, u_2, u_3). According to the definition of MLE polynomials, it can be uniquely represented as an interpolation polynomial of (a_0, a_1, a_2, \\ldots, a_{15}) over the Boolean Hypercube:f(u_0, u_1, u_2, u_3) = \\sum_{\\vec{b}\\in\\{0,1\\}^4} f(b_0,b_1,b_2,b_3) \\cdot eq\\big((b_0, b_1, b_2, b_3), (u_0, u_1, u_2, u_3)\\big)\n\nTo emphasize: \\vec{u}=(u_0, u_1, u_2, u_3) is an arbitrary predetermined point. The eq(\\vec{b}, \\vec{u}) in the equation above is a Lagrange polynomial, defined as:eq(\\vec{b}, \\vec{u}) = \\prod_{i=0}^{n-1} \\big((1-b_i)(1-u_i) + b_i\\cdot u_i\\big)\n\nwhere n represents the dimension of the BooleanHypercube, here n=4. If we fix \\vec{b}\\in\\{0,1\\}^4, we’ll introduce a notation w_i to represent eq(\\mathsf{bits}(i), \\vec{u}):w_i = eq(\\mathsf{bits}(i), (u_0, u_1, u_2, u_3))\n\nAt this point, we can see that the following equation holds:f(u_0, u_1, u_2, u_3) = \\sum_{i=0}^{2^4-1} a_i \\cdot w_i\n\nAs mentioned earlier, \\vec{a}=(a_0, a_1, a_2, \\ldots, a_{15}) represents f’s values on the 4-dimensional Boolean Hypercube:\\vec{a} = \\begin{bmatrix}\nf(0,0,0,0) \\\\\nf(1,0,0,0) \\\\\nf(0,1,0,0) \\\\\nf(1,1,0,0) \\\\\n\\vdots \\\\\nf(1,1,1,1)\n\\end{bmatrix}\n\nThat is:a_i=f(\\mathsf{bits})\n\nSince f(u_0, u_1, u_2, u_3) can be expressed as a sum, specifically the inner product of \\vec{a} and \\vec{w}, we can consider using the Sumcheck protocol to prove this inner product operation:f(u_0, u_1, u_2, u_3) = \\sum_{\\vec{b}\\in\\{0,1\\}^4} f(b_0, b_1, b_2, b_3) \\cdot \\tilde{w}(b_0, b_1, b_2, b_3)\n\nwhere \\tilde{w} is the MLE polynomial corresponding to \\vec{w}. One round of the Sumcheck protocol can reduce the above inner product equation to an inner product equation of half the length. For example, after just one round of Sumcheck protocol between Prover and Verifier, we get a sum equation of length 8:f(u_0, u_1, u_2, {\\color{red}r_3}) = \\sum_{\\vec{b}\\in\\{0,1\\}^3} f(b_0, b_1, b_2, {\\color{red}r_3}) \\cdot \\tilde{w}(b_0, b_1, b_2, {\\color{red}r_3})\n\nHere r_3 is a challenge value randomly generated by the Verifier during the first round of Sumcheck interaction. If we go another round, the length of this sum equation will be halved again:f(u_0, u_1, {\\color{red}r_2}, r_3) = \\sum_{\\vec{b}\\in\\{0,1\\}^2} f(b_0, b_1, {\\color{red}r_2}, r_3) \\cdot \\tilde{w}(b_0, b_1, {\\color{red}r_2}, r_3)\n\nAt this point, the f(X_0, X_1, r_2, r_3) on the right side of the sum equation can be viewed as an MLE polynomial in terms of X_0,X_1, whose values on the Boolean Hypercube form a vector of length 4, with each element being a random linear combination of matrix A based on r_2, r_3 (expanding by Multilinear Basis):\\begin{bmatrix}\nf(0,0,r_2,r_3) \\\\\nf(0,1,r_2,r_3) \\\\\nf(1,0,r_2,r_3) \\\\\nf(1,1,r_2,r_3)\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n\\mathsf{fold}(r_2, r_3, (a_0, a_4, a_8, a_{12})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_1, a_5, a_9, a_{13})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_2, a_6, a_{10}, a_{14})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_3, a_7, a_{11}, a_{15}))\n\\end{bmatrix}\n\nWe can use the first element of the vector, f(0,0,r_2,r_3), as an example to derive why the above equation holds:\\begin{aligned}\nf(0,0,r_2,r_3) &= \\sum_{i=0}^{3} a_i \\cdot eq(\\mathsf{bits}(i), (0,0,r_2,r_3)) \\\\\n&= f(0,0,0,0) \\cdot eq(\\mathsf{bits}(0), (0,0,r_2,r_3)) + f(0,0,1,0) \\cdot eq(\\mathsf{bits}(4), (0,0,r_2,r_3)) \\\\\n& + f(0,0,0,1) \\cdot eq(\\mathsf{bits}(8), (0,0,r_2,r_3)) + f(0,0,1,1) \\cdot eq(\\mathsf{bits}(12), (0,0,r_2,r_3)) \\\\\n&= a_0 \\cdot eq(\\mathsf{bits}(0), (r_2,r_3)) + a_4 \\cdot eq(\\mathsf{bits}(1), (r_2,r_3)) \\\\\n& + a_8 \\cdot eq(\\mathsf{bits}(2), (r_2,r_3)) + a_{12} \\cdot eq(\\mathsf{bits}(3), (r_2,r_3)) \\\\\n& = a_0 \\cdot eq(0,r_2)eq(0,r_3) + a_4 \\cdot eq(1,r_2)eq(0,r_3) + a_8 \\cdot eq(0,r_2)eq(1,r_3) + a_{12} \\cdot eq(1,r_2)eq(1,r_3) \\\\\n& = eq(0,r_2)\\cdot \\Big(a_0 \\cdot eq(0,r_3) + a_8 \\cdot eq(1,r_3)\\Big) + eq(1,r_2)\\cdot \\Big(a_4 \\cdot eq(0,r_3) + a_{12} \\cdot eq(1,r_3)\\Big) \\\\\n&=eq(0,r_2)\\cdot \\Big(\\mathsf{fold}(r_3,(a_0,a_8))\\Big) + eq(1,r_2)\\cdot \\Big(\\mathsf{fold}(r_3,(a_4,a_{12}))\\Big) \\\\\n&= \\mathsf{fold}(r_2, r_3, (a_0, a_4, a_8, a_{12})) \\\\\n\\end{aligned}\n\nAll four values together are exactly the vector \\vec{a}' produced by folding A as described in the previous section’s protocol. However, unlike the previous protocol, here r_2, r_3 are not directly randomly generated by the Verifier, but are generated by the Verifier during a Sumcheck protocol execution. The purpose of this Sumcheck protocol is to prove the correctness of an evaluation at an arbitrary point f(u_0, u_1, u_2, u_3).\n\nIn other words, the utility of the Sumcheck protocol is that it can prove the evaluation of an MLE polynomial at any public point, transforming it into the evaluation of f at a random point. The transformed proof target can then be completed using the protocol from the previous section. This random number is effectively reused twice. However, it’s worth noting that we don’t run all rounds of the Sumcheck protocol, but stop here, leaving the remaining unproven sum equation for the Verifier to verify directly.\n\nAt this point, the Prover only needs to send \\vec{a}' directly to the Verifier, who then has two verification tasks: first, check if \\vec{a}'\\cdot (1-u_0,u_0)\\otimes(1-u_1,u_1) equals \\tilde{f}(u_0, u_1, r_2, r_3); second, randomly check if \\vec{a}' is indeed the result of folding matrix A using r_2, r_3.\n\nBelow, I provide the protocol flow, which readers can understand in conjunction with the explanation above.","type":"content","url":"/ligerito-pcs/ligerito-01#id-2-the-crucial-role-of-sumcheck","position":9},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"type":"lvl3","url":"/ligerito-pcs/ligerito-01#protocol-flow","position":10},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"content":"Public Input: \\mathsf{t}, \\vec{u}=(u_0, u_1, u_2, u_3)\n\nWitness: \\vec{a}=(a_0, a_1, a_2, \\ldots, a_{15})","type":"content","url":"/ligerito-pcs/ligerito-01#protocol-flow","position":11},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl4":"Commitment:","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"type":"lvl4","url":"/ligerito-pcs/ligerito-01#commitment","position":12},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl4":"Commitment:","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"content":"The Prover computes the encoded matrix B=AG:B = \\begin{bmatrix}\n\\mathsf{enc}(a_0, a_1, a_2, a_3) \\\\\n\\mathsf{enc}(a_4, a_5, a_6, a_7) \\\\\n\\mathsf{enc}(a_8, a_9, a_{10}, a_{11}) \\\\\n\\mathsf{enc}(a_{12}, a_{13}, a_{14}, a_{15})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ne_0 & e_1 & e_2 & e_3 & e_4 & e_5 & e_6 & e_7 \\\\\ne_8 & e_9 & e_{10} & e_{11} & e_{12} & e_{13} & e_{14} & e_{15} \\\\\ne_{16} & e_{17} & e_{18} & e_{19} & e_{20} & e_{21} & e_{22} & e_{23} \\\\\ne_{24} & e_{25} & e_{26} & e_{27} & e_{28} & e_{29} & e_{30} & e_{31}\n\\end{bmatrix}\n\nThen calculates the Merkle Tree root for each column of matrix B:\\begin{aligned}\n\\mathsf{t}_0 &= \\mathsf{MerkleRoot}(e_0, e_8, e_{16}, e_{24}) \\\\\n\\mathsf{t}_1 &= \\mathsf{MerkleRoot}(e_1, e_9, e_{17}, e_{25}) \\\\\n\\vdots \\\\\n\\mathsf{t}_7 &= \\mathsf{MerkleRoot}(e_7, e_{15}, e_{23}, e_{31}) \\\\\n\\end{aligned}\n\nFinally calculates a Merkle Tree from these roots to get a single root \\mathsf{t}:\\mathsf{t} = \\mathsf{MerkleRoot}(\\mathsf{t}_0, \\mathsf{t}_1, \\cdots, \\mathsf{t}_7)","type":"content","url":"/ligerito-pcs/ligerito-01#commitment","position":13},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl4":"Evaluation Proof","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"type":"lvl4","url":"/ligerito-pcs/ligerito-01#evaluation-proof","position":14},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl4":"Evaluation Proof","lvl3":"Protocol Flow","lvl2":"2. The Crucial Role of Sumcheck"},"content":"Step 1: The Prover and Verifier conduct the first round of the Sumcheck protocol: Prover sends h^{(0)}(X)h^{(0)}(X) = f(u_0, u_1, u_2, X) = \\sum_{\\vec{b}\\in\\{0,1\\}^3} \\tilde{a}(b_0, b_1, b_2, X) \\cdot \\tilde{w}(b_0, b_1, b_2, X)\n\nStep 2: Verifier sends a random number r_3\\in\\mathbb{F} and checks the following equation:f(u_0, u_1, u_2, u_3)\\overset{?}{=}h^{(0)}(0)+h^{(0)}(1)\n\nStep 3: Prover sends h^{(1)}(X)h^{(1)}(X) =f(u_0, u_1, X, r_3) = \\sum_{\\vec{b}\\in\\{0,1\\}^2} \\tilde{a}(b_0, b_1, X, r_3) \\cdot \\tilde{w}(b_0, b_1, X, r_3)\n\nStep 4: Verifier sends a random number r_2\\in\\mathbb{F}_q and checks the following equation:h^{(0)}(r_3) = f(u_0, u_1, u_2, r_3) \\overset{?}{=}h^{(1)}(0)+h^{(1)}(1)\n\nStep 5: Prover stops the Sumcheck protocol and sends the folded vector \\vec{a}':\\vec{a}' = \\begin{bmatrix}\n\\mathsf{fold}(r_2, r_3, (a_0, a_4, a_8, a_{12})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_1, a_5, a_9, a_{13})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_2, a_6, a_{10}, a_{14})) \\\\\n\\mathsf{fold}(r_2, r_3, (a_3, a_7, a_{11}, a_{15}))\n\\end{bmatrix}\n\nStep 6: Verifier samples Q\\subset\\{0, 1, 2, \\ldots, 7\\} and sends it to the Prover, where |Q|=lQ = \\{q_0, q_1, \\ldots, q_{l-1}\\}\n\nStep 7: Prover sends, for each query point q_i \\in Q, the column vector \\vec{b}_{q_i} of encoded matrix B, along with its corresponding Merkle Tree root \\mathsf{t}_{q_i}, and the Merkle Path \\pi_{q_i} from \\mathsf{t}_{q_i} to \\mathsf{t}.\n\nVerification Step: The Verifier checks if the following equations hold:\n\nAll \\mathsf{t}_{q_i} are consistent with \\mathsf{t}:\\mathsf{t}_{q_i} = \\mathsf{MerkleRoot}(\\vec{b}_{q_i}), \\quad \\forall q_i\\in Q\n\nEach t_{q_i} is a leaf of \\mathsf{t}:\\mathsf{MerkleTree.verify}(\\mathsf{t}, \\mathsf{t}_{q_i}, \\pi_{q_i}) = 1, \\quad \\forall q_i\\in Q\n\nThe folding of \\vec{b}_{q_i} is consistent with the corresponding point of encoded \\vec{a}'^T G:\\mathsf{fold}(r_2, r_3,\\vec{b}_{q_i}) \\overset{?}{=} \\vec{a}'^T G_{q_i}, \\quad \\forall q_i\\in Q\n\nwhere G_{q_i} is the q_i-th column of the generator matrix.\n\nCheck the inner product of vectors:\\vec{a}'\\cdot\\tilde{w}(u_0, u_1, r_2, r_3)\\overset{?}{=}h^{(1)}(r_2) = \\tilde{f}(u_0, u_1, r_2, r_3)","type":"content","url":"/ligerito-pcs/ligerito-01#evaluation-proof","position":15},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"3. Recursive Implementation"},"type":"lvl2","url":"/ligerito-pcs/ligerito-01#id-3-recursive-implementation","position":16},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"3. Recursive Implementation"},"content":"Let’s introduce a new polynomial f'(X_0, X_1) to represent the polynomial of length \n\n22 obtained after partially evaluating f(X_0, X_1, X_2, X_3) with X_2=r_2, X_3=r_3:f'(X_0, X_1) = f(X_0, X_1, r_2, r_3)\n\nStarting from Step 5 in the previous section’s protocol, the Prover and Verifier are effectively proving:f'(u_0, u_1) = h^{(1)}(r_2)\n\nAnd it’s easy to verify that the values of f'(X_0, X_1) on the 2-dimensional Boolean Hypercube are exactly \\vec{a}'=(a'_0, a'_1, a'_2, a'_3):\\begin{aligned}\nf'(X_0, X_1) &= f(X_0, X_1, r_2, r_3) \\\\\n&= a'_0\\cdot\\tilde{w}(X_0, X_1, r_2, r_3) + a'_1\\cdot\\tilde{w}(X_0, X_1, r_2, r_3) + a'_2\\cdot\\tilde{w}(X_0, X_1, r_2, r_3) + a'_3\\cdot\\tilde{w}(X_0, X_1, r_2, r_3)\n\\end{aligned}\n\nHowever, this proof process is compressed: the Prover simply sends the polynomial representation (i.e., \\vec{a}' as the Evaluation Form of f') directly to the Verifier, who performs the inner product calculation to complete the verification.\n\nNote! We can approach this differently. The first half of the protocol described in the previous section effectively transforms the verification target (step 4 of the verification):f(u_0, u_1, u_2, u_3) \\overset{?}{=} v\n\ninto the following proof target:f'(u_0, u_1) = f(u_0, u_1, r_2, r_3) \\overset{?}{=} h^{(1)}(r_2)\n\nThat is, proving the correctness of evaluating the MLE polynomial f'(X_0, X_1) at the point (u_0, u_1). Although the new target is still an MLE evaluation proof, the polynomial length has been reduced from 16 to 4.\n\nTherefore, we can have the Prover and Verifier recursively invoke the protocol from the previous section, continuing to reduce this MLE evaluation proof f'(u_0, u_1)=v' to an even smaller proof target, until the new polynomial is short enough for the Verifier to easily verify.\n\nLooking at the third item in the verification step, the Verifier is checking l inner product calculations:\\mathsf{fold}(r_2, r_3,\\vec{b}_{q}) \\overset{?}{=} \\vec{a}'G_{q}, \\quad \\forall q\\in Q\n\nNote that the left side of the equation can be calculated by the Verifier, and the right side is an inner product of a vector of length N/N'. Let’s introduce the symbol N to represent the length of polynomial f, N=2^n, where n is the number of variables; and the symbol N' to indicate that we organize f’s Evaluations over the boolean hypercube into an N'\\times N_1 matrix. It’s easy to verify that after one protocol call, the length of the folded vector \\vec{a}' sent by the Prover is N_1=N/N'.\n\nThe matrix multiplication on the right can be broken down into l vector inner product calculations, so this process can still use the Sumcheck protocol, allowing the Prover to handle it and further reducing the Verifier’s computational load.\nMoreover, this Sumcheck protocol can be merged with the Sumcheck protocol in the next recursive call, further reducing the Verifier’s computational burden. This idea of merging Sumcheck protocols first appeared in Whir [ACFY24], and I suspect the authors of Ligerito [NA25] likely arrived at this idea independently.","type":"content","url":"/ligerito-pcs/ligerito-01#id-3-recursive-implementation","position":17},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Recursive Protocol Flow","lvl2":"3. Recursive Implementation"},"type":"lvl3","url":"/ligerito-pcs/ligerito-01#recursive-protocol-flow","position":18},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl3":"Recursive Protocol Flow","lvl2":"3. Recursive Implementation"},"content":"Let’s assume the initial MLE polynomial is f^{(0)}(X_0, X_1, \\cdots, X_{n-1}) with length N=2^n.\n\nThe first protocol call folds the evaluation form of the polynomial into an N'\\times N_1 matrix, denoted as A_0, where N_1=2^{n_1}, N'=2^{n_0}, satisfying n_0 + n_1 = n, i.e., N_1\\cdot N'=N.A_0 = \\begin{bmatrix}\na_0 & a_1 & \\cdots & a_{N_1-1} \\\\\na_{N_1} & a_{N_1+1} & \\cdots & a_{2N_1-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{(N_1-1)N'} & a_{(N_1-1)N'+1} & \\cdots & a_{N-1}\n\\end{bmatrix}\n\nThe commitment calculation is:B_0 = A_0G_1 = \\begin{bmatrix}\nb_0 & b_1 & \\cdots & b_{N_1-1} \\\\\nb_{N_1} & b_{N_1+1} & \\cdots & b_{2N_1-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{N'-1} & b_{N'-1+1} & \\cdots & b_{N-1}\n\\end{bmatrix}\\mathsf{t}^{(0)} = \\mathsf{MerkleRoot}(\\mathsf{t}_0, \\mathsf{t}_1, \\cdots, \\mathsf{t}_{N_1-1})\n\nPublic Input:\n\n\\vec{u}=(u_0, u_1, \\cdots, u_{n_1-1}, u_{n_1}, u_{n_1+1}, \\cdots, u_{n-1})\n\n\\mathsf{t}^{(0)}\n\nProtocol Loop: i=0, 1, \\cdots, l-1, where variable i represents the recursion layer.\n\nLoop Step 1: If i=0, the Prover skips directly to Loop Step 2;\n\nIf i\\neq 0, Prover commits to f^{(i)} by arranging it into an N'\\times N_i matrix, denoted as A^{(i)}, calculates its encoded matrix B^{(i)} and the Merkle Tree root \\mathsf{t}^{(i)}. Prover sends \\mathsf{t}^{(i)}.\n\nLoop Step 2: Prover and Verifier perform k=\\log_2(N') rounds of the Sumcheck protocol. During the protocol, the Verifier sequentially sends random numbers r^{(i)}_{k-1}, r^{(i)}_{k-2}, \\cdots, r^{(i)}_{0}, and finally the Prover sends the polynomial h^{(i)}(X), satisfying:h^{(i)}(X) = \\tilde{f}^{(i)}(u_0, u_1, u_{n_1-1}, X, r^{(i)}_{1},\\ldots, r^{(i)}_{k-1}) = \\sum_{\\vec{b}\\in\\{0,1\\}^{n_1}} \\tilde{a}^{(i)}(\\vec{b}, X) \\cdot \\tilde{w}^{(i)}(\\vec{b}, X)\n\nAnd h^{(i)}(r^{(i)}_{0}) is the evaluation of \\tilde{f}^{(i+1)} at the point (u_0, u_1, \\cdots, u_{n_i-1}), denoted as v^{(i+1)}:v^{(i+1)}= h^{(i)}(r^{(i)}_{0}) = \\tilde{f}^{(i+1)}(u_0, u_1, \\cdots, u_{n_i-1})\n\nIt’s easy to derive that v^{(i+1)} is the inner product of the folded vector \\vec{a}^{(i+1)} and \\vec{w}^*:v^* = \\vec{a}^{(i+1)}\\cdot\\vec{w}^*\n\nHere:\\vec{a}^{(i+1)} = \\mathsf{fold}((r^{(i)}_{0}, r^{(i)}_{1}, \\cdots, r^{(i)}_{k-1}), \\vec{a}^{(i)})\\vec{w}^*= \\mathsf{fold}((r^{(i)}_{0}, r^{(i)}_{1}, \\cdots, r^{(i)}_{k-1}), \\vec{w}^{(i)})\n\nNote that we’re not rushing to use the symbol w^{(i+1)} here, as the vector \\vec{w}^* is just an intermediate result that will later be merged with other vectors.\n\nLoop Step 3: Verifier samples Q\\subset\\{0, 1, 2, 3\\} and sends it to the Prover. The number |Q| is predetermined by the protocol security parameter:Q = \\{q_0, q_1, \\ldots, q_{|Q|-1}\\}\n\nLoop Step 4: For each query point q\\in Q, the Prover sends the column vector \\vec{b}_{q} of the encoded matrix B, along with its corresponding Merkle Tree root \\mathsf{t}^{(i)}_{q}, and the Merkle Path \\pi^{(i)}_{q} from \\mathsf{t}^{(i)}_{q} to \\mathsf{t}^{(i)}.\n\nLoop Step 5: The Verifier checks if the following equations hold:\n\nAll \\mathsf{t}^{(i)}_{q} are consistent with \\mathsf{t}^{(i)}:\\mathsf{t}^{(i)}_{q} = \\mathsf{MerkleRoot}(\\vec{b}_{q}), \\quad \\forall q\\in Q\n\nEach \\mathsf{t}^{(i)}_{q} is a leaf of \\mathsf{t}^{(i)}:\\mathsf{MerkleTree.verify}(\\mathsf{t}^{(i)}, \\mathsf{t}^{(i)}_{q}, \\pi^{(i)}_{q}) = 1, \\quad \\forall q\\in Q\n\nLoop Step 6: For each q\\in Q, the Prover calculates the inner product of \\vec{a}^{(i+1)} and vector G^{(i)}_q, denoted as y^{(i)}_q, and sends it to the Verifier:y^{(i)}_q = \\vec{a}^{(i+1)}\\cdot G^{(i)}_q\n\nHere, G^{(i)}_q is the q-th column of the generator matrix G^{(i)} of the linear code C[\\mathbb{F}_q,N_i, \\mathcal{R}N_i].\n\nLoop Step 7: The Verifier sends a random number \\beta^{(i)} to combine the y^{(i)}_q with the inner product sum v^* from Step 2. The new sum value is:{v}^{(i+1)} =v^* + \\beta^{(i)}\\cdot y^{(i)}_0 + {\\beta^{(i)}}^2\\cdot y^{(i)}_1 + \\cdots + {\\beta^{(i)}}^{|Q|}\\cdot y^{(i)}_{|Q|-1}\n\nIf the Prover is honest, this should be the inner product of the following two vectors:\\vec{w}^{(i+1)} = \\vec{w}^* + \\beta^{(i)}\\cdot G^{(i)}_{q_0} + {\\beta^{(i)}}^2\\cdot G^{(i)}_{q_1} + \\cdots + {\\beta^{(i)}}^{|Q|}\\cdot G^{(i)}_{q_{|Q|-1}}\n\nwhere G_{q_i} is the q_i-th column of the generator matrix.\n\nLoop Termination Step: If n_i \\leq n', the Prover and Verifier end the loop; otherwise, they return to Loop Step 1, setting i\\leftarrow i+1.\n\nOutside Loop Step 1: The Prover directly sends \\vec{a}^{(l)} to the Verifier.\n\nOutside Loop Step 2: The Verifier calculates \\vec{w}^{(l)} and checks the following equation:v^{(l)} \\overset{?}{=} \\vec{a}^{(l)}\\cdot\\vec{w}^{(l)}\n\nProtocol Complete.\n\nNote that \\vec{a}^{(l)} and \\vec{w}^{(l)} are both vectors of length N_l.","type":"content","url":"/ligerito-pcs/ligerito-01#recursive-protocol-flow","position":19},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"4. Reducing the Verifier’s Computational Load"},"type":"lvl2","url":"/ligerito-pcs/ligerito-01#id-4-reducing-the-verifiers-computational-load","position":20},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"4. Reducing the Verifier’s Computational Load"},"content":"In Step 7 of the above protocol, the complexity of calculating \\vec{w}^{(i+1)} is related to the length of \\vec{w}^*. If the Verifier directly calculates according to the formula in Step 7, the computational complexity will be O(|Q|\\cdot N), which clearly doesn’t meet the Succinctness requirement.\n\nThe paper [NA25] discusses that if the linear code is a special type where each column of the generator matrix has a Tensor Structure, then the Verifier can calculate \\vec{w}^{(l)} in exponential time complexity.\n\nThe main idea is that if two vectors \\vec{r} and \\vec{w} both have a Tensor Structure:\\begin{aligned}\n\\vec{r} &= r_0 \\otimes r_1 \\otimes \\cdots \\otimes r_{k-1} \\\\\n\\vec{w} &= w_0 \\otimes w_1 \\otimes \\cdots \\otimes w_{k-1} \\\\\n\\end{aligned}\n\nThen the inner product of \\vec{r} and \\vec{w} can be expressed as:\\vec{r}\\cdot\\vec{w} = \\prod_{i=0}^{k-1} r_i \\cdot w_i","type":"content","url":"/ligerito-pcs/ligerito-01#id-4-reducing-the-verifiers-computational-load","position":21},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"References"},"type":"lvl2","url":"/ligerito-pcs/ligerito-01#references","position":22},{"hierarchy":{"lvl1":"Ligerito-PCS Notes","lvl2":"References"},"content":"[NA25] Andrija Novakovic and Guillermo Angeris. Ligerito: A Small and Concretely Fast Polynomial Commitment Scheme. 2025. \n\nhttps://​angeris​.github​.io​/papers​/ligerito​.pdf.\n\n[ACFY24] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi and Eylon Yogev. WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification. 2024. \n\nhttps://​eprint​.iacr​.org​/2024​/1586​.pdf\n\n[DP23] Benjamin Diamond and Jim Posen. Proximity Testing with Logarithmic Randomness. 2023. \n\nhttps://​eprint​.iacr​.org​/2023​/630​.pdf\n\n[AHIV17] Scott Ames, Carmit Hazay, Yuval Ishai, and Muthuramakrishnan Venkitasubramaniam. Ligero: lightweight sublinear arguments without a trusted setup\". 2022. \n\nhttps://​eprint​.iacr​.org​/2022​/1608​.pdf","type":"content","url":"/ligerito-pcs/ligerito-01#references","position":23},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields"},"type":"lvl1","url":"/math/field-ext-inverse","position":0},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields"},"content":"For an extension field \\mathbb{F}_{p^n} over a base field \\mathbb{F}_p, if we directly apply the Extended Euclidean Algorithm or Fermat’s Little Theorem to compute inverses, it requires multiple multiplication operations in \\mathbb{F}_{p^n}, which is computationally expensive. An optimization approach is to convert the inverse computation in the extension field to inverse computation in the base field or a subfield. This conversion introduces a small number of additional multiplication operations, but reduces the overall computational complexity.","type":"content","url":"/math/field-ext-inverse","position":1},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Quadratic Extension"},"type":"lvl2","url":"/math/field-ext-inverse#quadratic-extension","position":2},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Quadratic Extension"},"content":"Let’s consider a simple case where the base field is \\mathbb{F}_p and the extension field is \\mathbb{F}_{p^2}.\n\nWe specify an irreducible polynomial f(X)\\in\\mathbb{F}_p[X], and construct the extension field \\mathbb{F}_{p^2} as \\mathbb{F}_p[X]/(f(X)).f(X) = X^2 - c_0\n\nThen\\mathbb{F}_{p^2} = \\mathbb{F}_{p}[X]/(X^2 - c_0)\n\nLet’s assume \\alpha is a root of X^2-c_0 in \\mathbb{F}_{p^2}. Then \\mathbb{F}_{p^2} can be written as \\mathbb{F}_{p}[\\alpha].\n\nAccording to finite field theory, elements in \\mathbb{F}_{p^2} can be represented as a_0 + a_1 \\alpha, where a_0, a_1 \\in \\mathbb{F}_{p}.\\mathbb{F}_{p^2} = \\{a_0 + a_1 \\alpha \\mid a_0, a_1 \\in \\mathbb{F}_{p}\\}\n\nTo find the inverse of a = a_0 + a_1 \\alpha, denoted as a^{-1}, we can convert it to elements in \\mathbb{F}_{p}.\nLet a^{-1} = b_0 + b_1 \\alpha, then we have(a_0 + a_1 \\alpha)(b_0 + b_1 \\alpha) = 1\n\nSimplifying the left side of the equation and using \\alpha^2 = c_0, we get:(a_0b_0 + c_0\\cdot a_1b_1) + (a_0b_1 + a_1b_0)\\alpha = 1\n\nSince there is no \\alpha term on the right side, we have a_0b_1 + a_1b_0 = 0 and a_0b_0 + c_0\\cdot a_1b_1 = 1. This can also be represented in matrix form:\\begin{bmatrix}\na_0 & c_0\\cdot a_1 \\\\\na_1 & a_0\n\\end{bmatrix}\n\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix}\n\nBy inverting the matrix on the left, we can obtain the computation matrix for b_0 and b_1:\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na_0 & c_0\\cdot a_1 \\\\\na_1 & a_0\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}\n=\n(a_0^2 - c_0\\cdot a_1^2)^{-1}\n\\begin{bmatrix}\na_0 & - c_0\\cdot a_1 \\\\\n-a_1 & a_0\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}\n\nThe term (a_0^2 - c_0\\cdot a_1^2)^{-1} is an element in \\mathbb{F}_{p}. Thus, computing a^{-1} is converted to an inverse operation in \\mathbb{F}_{p}, with a constant number (five in this case) of additional multiplication operations in \\mathbb{F}_{p}.def quadratic_inv(t: K, c0: F):\n    if t == K(0):\n        raise ValueError(\"t=0\")\n    g = K.gen()   # root of f(X)=(X^2 - c0)\n    [a0, a1] = t.list()\n    a0_sq = a0 * a0\n    a1_sq = a1 * a1\n    \n    scalar = a0_sq - c0 * a1_sq\n    scalar_inv = scalar.inverse()\n\n    b0 = scalar_inv * a0\n    b1 = scalar_inv * (-a1)\n    \n    return b0 + b1 * g\n\nThis method is not limited to binomial extensions but is applicable to any irreducible polynomial. For example, if we generate \\mathbb{F}_{p^2} using the polynomial:g(X) = X^2 + c_1X + c_0\n\nThen exploring the product of two elements in \\mathbb{F}_{p^2}:\\begin{aligned}\n(a_0 + a_1 \\alpha)(b_0 + b_1 \\alpha) &= a_0b_0 + (a_0b_1 + a_1b_0)\\alpha + a_1b_1 \\alpha^2 \\\\\n&= a_0b_0 + (a_0b_1 + a_1b_0)\\alpha + a_1b_1 (c_1\\alpha + c_0) \\\\\n&= (a_0b_0 + c_0a_1b_1) + (a_0b_1 + a_1b_0 + c_1a_1b_1)\\alpha\n\\end{aligned}\n\nIn matrix form:\\begin{bmatrix}\na_0 & c_0\\cdot a_1 \\\\\na_1 & a_0+c_1a_1\n\\end{bmatrix}\n\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix}\n\nInverting the matrix on the left:\\begin{bmatrix}\na_0 & c_0\\cdot a_1 \\\\\na_1 & a_0+c_1a_1\n\\end{bmatrix}^{-1}\n=\n(a_0^2 -a_1^2c_0 + a_0a_1c_1)^{-1}\n\\begin{bmatrix}\na_0+a_1c_1 & -a_1c_0 \\\\\n-a_1 & a_0\n\\end{bmatrix}","type":"content","url":"/math/field-ext-inverse#quadratic-extension","position":3},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Cubic Extension"},"type":"lvl2","url":"/math/field-ext-inverse#cubic-extension","position":4},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Cubic Extension"},"content":"We still assume the base field is \\mathbb{F}_p and the cubic extension field is \\mathbb{F}_{p^3}.\n\nFor simplicity, we continue to specify a binomial irreducible polynomial f(X)\\in\\mathbb{F}_p[X] and construct the extension field \\mathbb{F}_{p^3} as \\mathbb{F}_p[X]/(f(X)).f(X) = X^3 - c_0\n\nLet \\alpha be a root of X^3-c_0 in \\mathbb{F}_{p^3}. Then elements in \\mathbb{F}_{p^3} can be represented as a_0 + a_1 \\alpha + a_2 \\alpha^2, where a_0, a_1, a_2 \\in \\mathbb{F}_{p}.\n\nLet a^{-1} = b_0 + b_1 \\alpha + b_2 \\alpha^2, then we have(a_0 + a_1 \\alpha + a_2 \\alpha^2)(b_0 + b_1 \\alpha + b_2 \\alpha^2) = 1\n\nAfter expansion and substituting \\alpha^3 = c, we get:(a_0b_0 + c_0\\cdot a_2b_1 + c_0\\cdot a_1b_2) + (a_0b_1 + a_1b_0 + c_0\\cdot a_2b_2) \\alpha + (a_0b_2 + a_1b_1 + a_2b_0) \\alpha^2 = 1\n\nWe can write this equation in matrix form:\\begin{bmatrix}\na_0 & c_0\\cdot a_2 & c_0\\cdot a_1 \\\\\na_1 & a_0 & c_0\\cdot a_2 \\\\\na_2 & a_1 & a_0\n\\end{bmatrix}\n\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\nb_2 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\nWe invert the matrix on the left:\\begin{bmatrix}\na_0 & c_0\\cdot a_2 & c_0\\cdot a_1 \\\\\na_1 & a_0 & c_0\\cdot a_2 \\\\\na_2 & a_1 & a_0\n\\end{bmatrix}^{-1}\n=\n(s)^{-1}\n\\begin{bmatrix}\na_0^2 - c_0\\cdot a_1\\cdot a_2 \n& c_0\\cdot a_1^2 - c_0\\cdot a_0\\cdot a_2 \n& c_0^2\\cdot a_2^2 - c_0\\cdot a_0\\cdot a_1 \n\\\\\nc_0\\cdot a_2^2 - a_0\\cdot a_1 \n& a_0^2 - c_0\\cdot a_1\\cdot a_2 \n& c_0\\cdot a_1^2 - c_0\\cdot a_0\\cdot a_2 \n\\\\\na_1^2 - a_0\\cdot a_2 \n& c\\cdot a_2^2 - a_0\\cdot a_1 \n& c\\cdot a_1\\cdot a_2 - a_0^2\n\\end{bmatrix}\n\nWhere s=a_0^3 + c_0\\cdot a_1^3 + c_0^2\\cdot a_2^3 - 3c_0\\cdot a_0\\cdot a_1\\cdot a_2.\n\nNext, we can convert the computation of b_0, b_1, b_2 to an inverse operation in \\mathbb{F}_{p}, s^{-1}, multiplied by elements in the base field:\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\nb_2 \\\\\n\\end{bmatrix}\n= s^{-1}\n\\begin{bmatrix}\na_0^2 - c_0\\cdot a_1\\cdot a_2  \\\\\nc_0\\cdot a_2^2 - a_0\\cdot a_1  \\\\\na_1^2 - a_0\\cdot a_2 \n\\end{bmatrix}def cubic_inv(t: K, w: F):\n    [a0, a1, a2] = t.list()\n    g = K.gen()\n    a0_sq = a0 * a0\n    a1_sq = a1 * a1\n    a2_w = w * a2\n    a0_a1 = a0 * a1\n\n    scalar = a0*a0_sq + w*a1*a1_sq + a2_w * a2_w * a2 - F(3) * a2_w * a0_a1\n    scalar_inv = scalar.inverse()\n    b0 = scalar_inv * (a0_sq - a1 * a2_w)\n    b1 = scalar_inv * (a2_w * a2 - a0_a1)\n    b2 = scalar_inv * (a1_sq - a0 * a2)\n    return b0 + b1 * g + b2 * g^2\n\nBy combining some reusable multiplication operations in \\mathbb{F}_{p}, we ultimately require 14 multiplication operations in \\mathbb{F}_{p} plus one inverse operation in \\mathbb{F}_{p}.","type":"content","url":"/math/field-ext-inverse#cubic-extension","position":5},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Quartic Extension"},"type":"lvl2","url":"/math/field-ext-inverse#quartic-extension","position":6},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Quartic Extension"},"content":"For quartic extensions, we can still use the approach above, though the complexity of matrix inversion increases rapidly.\n\nWe can, of course, convert a quartic extension into two steps of quadratic extensions.\\mathbb{F}_{p^4} = \\mathbb{F}_{p}(\\alpha)(\\beta)\n\nThen the first step of the inverse operation converts the inverse in the quartic extension field to an inverse in the quadratic extension field, and finally uses the quadratic extension inverse formula to convert it to an inverse operation in the base field.\n\nIf \\mathbb{F}_{p^4} is a direct quartic extension, but the irreducible polynomial used is a quartic binomial, then we have a more optimized approach (this approach comes from the implementation of RicsZero).\\mathbb{F}_{p^4}\\cong\\mathbb{F}_{p}[X]/(X^4-c)\n\nTo calculate the multiplicative inverse \\theta^{-1} of an element \\theta, i.e.,\\frac{1}{\\theta} = \\frac{1}{a_0 + a_1\\alpha + a_2\\alpha^2 + a_3\\alpha^3}\n\nwhere \\alpha is a root of the irreducible polynomial X^4-c=0, satisfying \\alpha^4-c=0. We multiply both the numerator and denominator of the above equation by an auxiliary element \\zeta\\zeta = a_0 - a_1\\alpha + a_2\\alpha^2 - a_3\\alpha^3\n\nThus\\frac{1}{\\theta} = \\frac{\\zeta}{\\zeta\\cdot\\theta}\n\nSince two coefficients in \\theta and \\zeta have opposite signs, two coefficients (related to \\alpha and \\alpha^3) in the product \\zeta\\cdot\\theta will be eliminated:\\zeta\\cdot\\theta = (a_0^2 + a_2^2\\cdot c - 2a_1a_3\\cdot c) + (2a_0a_2 - a_1^2 + a_3^2\\cdot c)\\cdot \\alpha^2\n\nWe let \\zeta\\cdot\\theta = b_0 + b_1\\alpha^2, where b_0, b_1\\in\\mathbb{F}_{p}, defined as:\\begin{aligned}\nb_0 &= a_0^2 + a_2^2\\cdot c - 2a_1a_3\\cdot c \\\\\nb_1 &= 2a_0a_2 - a_1^2 + a_3^2\\cdot c\n\\end{aligned}\n\nWe introduce another auxiliary element \\xi=b_0 - b_1\\alpha^2, and again multiply both the numerator and denominator of \\theta^{-1} by \\xi:\\frac{1}{\\theta} = \\frac{\\zeta}{\\zeta\\cdot\\theta} \n= \\frac{\\zeta\\cdot\\xi}{\\zeta\\cdot\\xi\\cdot\\theta}\n\nSo the denominator \\zeta\\cdot\\xi\\cdot\\theta will be an element in \\mathbb{F}_{p}, derived as follows:\\zeta\\cdot\\xi\\cdot\\theta = (b_0 + b_1\\alpha^2)(b_0-b_1\\alpha^2) = b_0^2 - b_1^2\\alpha^4 = b_0^2 - b_1^2\\cdot c\n\nNote that since this approach requires the irreducible polynomial to be a binomial, i.e., X^4-c, the \\alpha^4 term in the above equation can be completely replaced by c, ensuring that \\zeta\\cdot\\xi\\cdot\\theta is an element in \\mathbb{F}_{p}. If the irreducible polynomial were of the form X^4-c'\\cdot X+ c, the right side of the equation would introduce an extra \\alpha term, which would be difficult to handle.\n\nFinally, we can derive the computation formula for \\theta^{-1}:\\frac{1}{\\theta} \n= \\frac{\\zeta\\cdot\\xi}{\\zeta\\cdot\\xi\\cdot\\theta}\n= \\frac{(a_0 - a_1\\alpha + a_2\\alpha^2 - a_3\\alpha^3)\n\\cdot (b_0 - b_1\\alpha^2)}{b_0^2 - b_1^2\\cdot c}\n\nClearly, the denominator of the above equation is an element in \\mathbb{F}_{p}. And the coefficients of \\alpha in the numerator are products of several elements in \\mathbb{F}_{p}.","type":"content","url":"/math/field-ext-inverse#quartic-extension","position":7},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Inverse Algorithm Based on Frobenius Map"},"type":"lvl2","url":"/math/field-ext-inverse#inverse-algorithm-based-on-frobenius-map","position":8},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl2":"Inverse Algorithm Based on Frobenius Map"},"content":"The core idea of the algorithms above is to convert the inverse operation in the extension field to an inverse operation in the base field.\n\nBased on the Frobenius Map, we can also convert the inverse operation in the extension field to an inverse operation in the base field.\n\nFor an n-degree extension field \\mathbb{F}_{p^n}, its Frobenius Map is Frob_n: \\mathbb{F}_{p^n} \\to \\mathbb{F}_{p^n}, defined as Frob_n(\\alpha) = \\alpha^{p}.\n\nFor any element \\alpha in \\mathbb{F}_{p^n}, we can obtain all its conjugate elements through the Frobeinus Map:\\alpha, \\alpha^p, \\alpha^{p^2}, \\cdots, \\alpha^{p^{n-1}}\n\nThe product of these conjugate elements (called the Norm of \\alpha) is precisely an element in \\mathbb{F}_{p}:\\alpha \\cdot \\alpha^p \\cdot \\alpha^{p^2} \\cdots \\alpha^{p^{n-1}} = c\n\nWhy? Here’s a simple proof.\n\nSuppose the characteristic polynomial of \\alpha is f(X), thenf(X) = (X-\\alpha)(X-\\alpha^p)(X-\\alpha^{p^2})\\cdots(X-\\alpha^{p^{n-1}})\n\nwhere the constant term of f(X) is exactly c=\\alpha \\cdot \\alpha^p \\cdot \\alpha^{p^2} \\cdots \\alpha^{p^{n-1}}, and since f(X)\\in\\mathbb{F}_{p}[X], we have c\\in\\mathbb{F}_{p}. Besides, the Norm Map is also a commonly used homomorphism from \\mathbb{F}_{p^n} to \\mathbb{F}_{p}.\n\nSo we can leverage the properties of the Norm Map to calculate the multiplicative inverse \\theta^{-1} of any element \\theta\\in\\mathbb{F}_{p^n}.\\theta^{-1} = \\theta^{-r} \\cdot \\theta^{r-1}\n\nHere r is calculated as:r = \\frac{p^n-1}{p-1} = 1 + p + p^2 + \\cdots + p^{n-1}\n\nThen according to the definition, \\theta^{r} is exactly the Norm of \\theta:\\theta^{r} = \\theta\\cdot\\theta^p\\cdot\\theta^{p^2}\\cdots\\theta^{p^{n-1}} = c\n\nTo calculate \\theta^{-1}, we also need to calculate \\theta^{r-1}, which is an element in \\mathbb{F}_{p^n}. However, after analysis, we can use the following formula to calculate \\theta^{-1} “recursively”:\\begin{aligned}\n\\theta^{r-1} &= \\prod_{i=1}^{n-1} \\theta^{p^i} \\\\\n        &= (((1\\cdot \\theta)^p \\cdot \\theta)^p \\cdot \\cdots \\cdot \\theta)^p\n\\end{aligned}\n\nThe Python code is as follows:def frobenius_map(a: K):\n    return a^(K.characteristic())\n\ndef frobenius_inv(a: F, degree: int):\n    # compute a^{r-1}\n    s = F(1)\n    for i in range(1, degree):\n        s = frobenius_map(s * a)\n    t = s * a\n    t_inv = t.inverse()\n\n    return s * t_inv\n\nAs you can see, this algorithm is a general algorithm, and there are no assumptions about the structure of the extension field or the irreducible polynomial.","type":"content","url":"/math/field-ext-inverse#inverse-algorithm-based-on-frobenius-map","position":9},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl3":"Optimization of Frobenius Map for Binomial Extension Fields","lvl2":"Inverse Algorithm Based on Frobenius Map"},"type":"lvl3","url":"/math/field-ext-inverse#optimization-of-frobenius-map-for-binomial-extension-fields","position":10},{"hierarchy":{"lvl1":"Computation of Inverse in Extension Fields","lvl3":"Optimization of Frobenius Map for Binomial Extension Fields","lvl2":"Inverse Algorithm Based on Frobenius Map"},"content":"If we only consider binomial extension fields, we can further optimize the computation of the Frobenius Map.\n\nAssume the extension field is defined as:\\mathbb{F}_{p^n} = \\mathbb{F}_{p}[X]/(X^n - e)\n\nwhere e is an element in \\mathbb{F}_{p}. Let \\beta^n-e=0, then \\beta is an element in \\mathbb{F}_{p^n} and satisfies:\\beta^n = c\n\nAny element \\alpha can be represented as a coefficient vector in the power basis of \\beta:\\alpha = a_0 + a_1\\beta + a_2 \\beta^2 + \\cdots + a_{n-1} \\beta^{n-1}\n\nThen the Frobenius Map of \\alpha is \\alpha^p, which can be derived as follows:\\begin{aligned}\n\\alpha^p &= (a_0 + a_1\\beta + a_2 \\beta^2 + \\cdots + a_{n-1} \\beta^{n-1})^p \\\\\n    &= a_0^p + a_1^p\\beta^p + a_2^p\\beta^{2p} + \\cdots +  a_{n-1}^p\\beta^{p(n-1)} \\\\\n    &= a_0 + a_1\\beta^p + a_2\\beta^{2p} + \\cdots +  a_{n-1}\\beta^{p(n-1)} \\\\\n    &= a_0 + a_1\\cdot (\\beta^{p-1})\\cdot \\beta  + a_2\\cdot (\\beta^{p-1})^{2}\\cdot \\beta^2 + \\cdots +  a_{n-1}\\cdot (\\beta^{p-1})^{(n-1)} \\cdot \\beta^{n-1}\n\\end{aligned}\n\nSo \\alpha^p can be represented as the dot product of the following vector and (1,\\beta,\\beta^2,\\cdots,\\beta^{n-1}):(a_0, a_1\\cdot (\\beta^{p-1}), a_2\\cdot (\\beta^{p-1})^{2}, \\cdots, a_{n-1}\\cdot (\\beta^{p-1})^{(n-1)})\n\nLet z = \\beta^{p-1}, and if n\\mid (p-1), then z\\in\\mathbb{F}_{p}, so its computation only involves calculations in the base field.z = \\beta^{p-1} = c^{\\frac{p-1}{n}}\n\nThus, the coefficient representation of \\alpha^p is as follows:(a_0, a_1\\cdot z, a_2\\cdot z^2, \\cdots, a_{n-1}\\cdot z^{n-1})","type":"content","url":"/math/field-ext-inverse#optimization-of-frobenius-map-for-binomial-extension-fields","position":11},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule"},"type":"lvl1","url":"/math/mle-div","position":0},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule"},"content":"","type":"content","url":"/math/mle-div","position":1},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule","lvl2":"Univariate Polynomial Division and Ruffini’s Rule"},"type":"lvl2","url":"/math/mle-div#univariate-polynomial-division-and-ruffinis-rule","position":2},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule","lvl2":"Univariate Polynomial Division and Ruffini’s Rule"},"content":"According to Ruffini’s rule, there is a close relationship between polynomial division and polynomial evaluation.\n\nIf we use polynomial long division to calculate f(X)/(X-d), we get a quotient polynomial q(X) and a remainder r, satisfying:f(X) = (X-d)\\cdot q(X) + r\n\nThis is the well-known polynomial remainder theorem, stating that the remainder equals f(d). So when f(X) minus the remainder, it can be divided evenly by (X-d). Is there also a connection between the quotient polynomial q(X) and f(d)?\nHere’s a conclusion: the quotient polynomial q(X) is exactly the intermediate calculation result produced during the evaluation of f(X) at X=d.\n\nRepresenting f(X) in coefficient form, we have:f(X) = a_0 + a_1X + a_2X^2 + \\ldots + a_nX^n\n\nRearranging, we get:f(X) = a_0 + X(a_1 + X(a_2 + \\ldots + X(a_{n-1} + Xa_n)\\ldots)\n\nTo calculate the “polynomial evaluation” f(d), we substitute X=d from left to right into the above equation:\\begin{split}\n    q_{n-1} &= d\\cdot 0 + a_n\\\\\n    q_{n-2} &= d\\cdot q_{n-1} + a_{n-1} \\\\\n    \\vdots & \\vdots \\\\\n    q_1     &= d\\cdot q_2 + a_2 \\\\\n    q_0     &= d\\cdot q_1 + a_1 \\\\\n    v       &= d\\cdot q_0 + a_0 \\\\\n\\end{split}\n\nAnd (q_0, q_1, \\ldots, q_{n-1}) are precisely the coefficients of the quotient polynomial q(X). In other words, the linear division process of a polynomial is actually the polynomial evaluation process.\n\nLet’s take a simple example: f(X) = 5 + 3X + 2X^2 + X^3. To calculate f(-2), which is equivalent to calculating f(X)/(X+2), the process is:\\begin{array}{lll}\n    q_2 &= (-2)\\cdot 0 + 1 &= 1\\\\\n    q_1 &= (-2)\\cdot 1 + 2 &= 0\\\\\n    q_0 &= (-2)\\cdot 0 + 3 &= 3\\\\\n    v   & = (-2)\\cdot 3 + 5 &= -1\\\\\n\\end{array}\n\nWe thus get the result f(-2) = -1 and incidentally obtain the quotient polynomial q(X) = 3 + X^2.\n\nWe can implement this calculation process with a simple loop:fn div_poly(f: &[Scalar], d: Scalar) -> (Vec<Scalar>, Scalar) {\n    let mut rem = Scalar::zero();\n    let mut quo = Vec::new();\n\n    for c in coeffs.iter().rev() {\n        rem = rem * d + c;\n        quo.insert(0, rem);\n    }\n    (quo, rem)\n}\n\nActually, Ruffini’s rule applies not just to linear polynomial division but to division by polynomials of any degree. This generalized algorithm is known as Synthetic Division.\n\nSo far, we’ve been discussing univariate polynomial division. What about MLE (Multilinear Extension) division? Fortunately, Ruffini’s rule also applies to MLE polynomial division; the calculation process of MLE polynomial division is equivalent to the MLE polynomial evaluation process.","type":"content","url":"/math/mle-div#univariate-polynomial-division-and-ruffinis-rule","position":3},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule","lvl2":"MLE Polynomial Division"},"type":"lvl2","url":"/math/mle-div#mle-polynomial-division","position":4},{"hierarchy":{"lvl1":"Polynomial Division and Ruffini’s Rule","lvl2":"MLE Polynomial Division"},"content":"To warm up, when we divide an MLE by a univariate linear polynomial, we can view the MLE as a univariate polynomial, with other variables seen as mutually non-calculable constants. This way, we can use univariate polynomial division to quickly obtain the quotient polynomial (an (n-1)-variate MLE) and remainder.\n\nFor example:\\tilde{f}(X_0, X_1, X_2) = 2\\cdot X_0X_2 + 3\\cdot X_1 + 4\\cdot X_0\n\nThe division is:\\frac{2\\cdot X_0X_2 + 3\\cdot X_1 + 4\\cdot X_0}{(X_2-a)} = \\frac{(2\\cdot X_0)X_2 + (3\\cdot X_1 + 4\\cdot X_0)}{(X_2-a)} = 2\\cdot X_0, 3X_1 + (4+ 2a)X_0\n\nThe remainder is 3X_1 + (4+ 2a)X_0, and the quotient polynomial is an MLE, 2\\cdot X_0. Next, we can continue to divide the remainder polynomial by (X_1-b):\\frac{3X_1 + (4+ 2a)X_0}{(X_1-b)} = 3, (4+ 2a)X_0 + 3b\n\nThe remainder becomes (4+ 2a)X_0 + 3b, and the quotient is 3. Finally, we continue division with (X_0-c):\\frac{(4+ 2a)X_0 + 3b}{(X_0-c)} = 4+2a, 3b + (4+2a)c\n\nThe remainder is 3b + (4+2a)c, and the quotient is 4+2a.\n\nWe’ve now performed three complete MLE polynomial divisions, dividing by (X_2-a), (X_1-b), and (X_0-c). We’ve obtained three quotients and a final remainder. Looking at the final remainder, it’s exactly the value of \\tilde{f}(\\vec{X}) at (c,b,a), perfectly matching Ruffini’s rule:r= 3b + (4+2a)c  = \\tilde{f}(c, b, a)\n\nThe three quotient polynomials are:\\begin{split}\n    q_2(X_0,X_1) &= 2\\cdot X_0\\\\\n    q_1(X_0) &= 3\\\\\n    q_0 &= 4+2a\\\\\n\\end{split}\n\nComputing polynomial division isn’t so intuitive, so next we’ll use Ruffini’s rule to calculate MLE evaluation and simultaneously compute the quotient polynomial, making the process clearer. The MLE calculation can be viewed as a folding process. For every variable X_k\\in\\{X_{n-1},\\ldots, X_0\\}, we can view the polynomial as a univariate linear polynomial:\\tilde{f}(\\vec{X},X_k)=A + B\\cdot X_k\n\nWhen we substitute X_k = \\alpha, we get:\\tilde{f}(\\vec{X},X_k)=A + \\alpha\\cdot B\n\nIf we represent \\tilde{f} as a coefficient vector, this process can be seen as a folding based on the factor \\alpha. For example, for \\tilde{f}(X_0, X_1, X_2) = 2\\cdot X_0X_2 + 3\\cdot X_1 + 4\\cdot X_0, its coefficient vector is:\\begin{array}{cccccccc}\n(&0, &4, &3, &0, &0, &2, &0, &0 &) \\\\\n&1, &X_0, &X_1, &X_0X_1, &X_2, &X_0X_2, &X_1X_2, &X_0X_1X_2& \\\\\n\\end{array}\n\nEach coefficient corresponds to the monomial in the second row. Next, if we want to calculate f(c, b, a), we can use a split-and-fold approach:\\begin{split}\n(0,4,3,0) + a\\cdot {\\color{blue}(0,2,0,0)} & = (0, 4+2a, 3, 0)\\\\    \n(0, 4+2a) + b\\cdot {\\color{blue}(3, 0)} & = (3b, 4+2a)\\\\\n(3b) + c\\cdot {\\color{blue}(4+2a)} & = 3b+ (4+2a)c\\\\    \n\\end{split}\n\nIn each round, we split the vector in half, add the left half to the right half multiplied by a factor. After three recursive rounds, we finally get the result 3b+ (4+2a)c, which matches the remainder from our earlier polynomial division. As mentioned before, according to Ruffini’s rule, the quotient polynomial should be the intermediate result during the evaluation. So where do we find traces of the quotient polynomial in this folding process?\n\nThe quotient polynomials are the MLEs with the blue-marked vectors as “coefficient vectors”, i.e., the right half of the vector in each round of the split-and-fold process. Let’s verify: the first row’s quotient polynomial is 2\\cdot X_0, which is the coefficient of monomials (1,X_0,X_1,X_0X_1), so the first quotient polynomial is 2X_0; the second row’s quotient is (3,0), which is a constant polynomial; the third row’s quotient is (4+2a), also a constant polynomial. This matches the results we obtained through manual division.\n\nTODO: q_k = f(... 1+u_k, ...) - f(... u_k, ...)\n\nHowever, in the ZeroMorph protocol, MLEs are represented in “value form”. If we want to use the above method for division, we’d need to first convert the MLE from value form to coefficient form. The generalized algorithm for this conversion (similar to FFT for univariate polynomials) has a time complexity of O(N\\log^2N), or O(2^n\\cdot n^2). And after calculating the quotient, we’d need to perform the inverse transformation on the n-1 quotient polynomials to go from coefficient form back to value form. This would introduce non-negligible conversion overhead.\n\nIn fact, we can use the split-and-fold approach to directly compute evaluation on the MLE’s “value form” and simultaneously calculate the quotient polynomial, completely avoiding the back-and-forth conversion between “value form” and “coefficient form”.\n\nLet’s see how evaluation is calculated on the MLE’s value form:\\tilde{f}(X_0,X_1,\\ldots, X_{n-2}, X_{n-1}) = \\sum_{\\vec{i}\\in\\{0,1\\}^n} f_{\\vec{i}} \\cdot \\tilde{eq}(\\vec{i}, \\vec{X})\n\nSuppose the value form of \\tilde{f} is the vector (f_{000}, f_{100}, \\ldots, f_{111}), corresponding to the HyperCube \\{0,1\\}^3.\n\nWe substitute values for X_{n-1},\\ldots, X_0 from right to left. For instance, if we first substitute X_{n-1}=u_{n-1}, then the value form of \\tilde{f}^{(1)}(X_0,X_1,\\ldots, X_{n-2}) is:(1-u_{n-1})\\cdot(f_0, f_1, \\ldots, f_{2^{n-2}-1}) +  u_k\\cdot(f_{2^{n-2}}, f_{2^{n-2}+1}, \\ldots, f_{2^{n-1}-1})\n\nThis calculation is also a split-and-fold process, which can be recursively continued until the vector is folded into one dimension.\n\nUsing our previous example MLE polynomial, \\tilde{f}(X_0, X_1, X_2) = 2\\cdot X_0X_2 + 3\\cdot X_1 + 4\\cdot X_0, its value form is:\\begin{array}{cccccccc}\n(&0, &4, &3, &7, &0, &6, &3, &9 &) \\\\\n&f_{000}, &f_{100}, &f_{010}, &f_{110}, &f_{001}, &f_{101}, &f_{011}, &f_{111}& \\\\\n\\end{array}\n\nFor instance, if we substitute (X_0=1,X_1=0,X_2=1), then \\tilde{f}(1, 0, 1)=2+4=6, which corresponds to the f_{101} element in the value vector.\n\nNow, let’s substitute (X_0=c,X_1=b,X_2=a) and use the split-and-fold approach to complete the evaluation:\\begin{split}\n(1-a)\\cdot(0,4,3,7) + a\\cdot (0,6,3,9) & = (0, 4+2a, 3, 7+2a)\\\\    \n(1-b)\\cdot(0, 4+2a) + b\\cdot (3, 7+2a) & = (3b, (1-b)(4+2a)+b(7+2a)) = (3b, 4+2a+3b)\\\\\n(1-c)\\cdot(3b) + c\\cdot (4+2a+3b) & =  3b - 3bc +4c+2ac + 3bc = 3b+ (4+2a)c\\\\    \n\\end{split}\n\nAs expected, the final result is still 3b+ (4+2a)c, though it’s not as easy to find the quotient polynomial in this calculation.\n\nThe quotient polynomial for each row is the right vector minus the left vector. Let’s examine each row:\\begin{split}\nq_2(X_0,X_1): & (0, 6, 3, 9) - (0, 4, 3, 7) = (0, 2, 0, 2)\\\\\nq_1(X_0):     & (3, 7+2a) - (0, 4+2a) = (3, 3)\\\\\nq_0:          & (4+2a+3b) - (3b) = 4+2a\\\\\n\\end{split}\n\nRemember, these are the value forms of the quotient polynomials. We can convert them to “coefficient form” through polynomial interpolation and compare with the earlier quotients.\n\nAnother method, as described earlier, is to map a low-dimensional HyperCube to a high-dimensional one, which is actually a repetition of the low-dimensional Cube. So, seeing that the value form of q_2(X_0,X_1) is (0, 2, 0, 2), which is a repetition in a 2-dimensional HyperCube, indicates that the coefficient related to the variable X_1 is zero. Thus, we can quickly deduce q_2(X_0,X_1) = 2X_0. Similarly, the value form of q_1(X_0) is (3,3), also a repeating pattern, indicating that the coefficients of monomials containing the X_0 variable are zero. Hence, q_1(X_0) = 3. Finally, with q_0() = 4+2a, these matches perfectly with the quotient polynomials calculated using the coefficient form earlier.","type":"content","url":"/math/mle-div#mle-polynomial-division","position":5},{"hierarchy":{"lvl1":"Classic Montgomery REDC"},"type":"lvl1","url":"/math/reductions/logjump-reduction","position":0},{"hierarchy":{"lvl1":"Classic Montgomery REDC"},"content":"","type":"content","url":"/math/reductions/logjump-reduction","position":1},{"hierarchy":{"lvl1":"Classic Montgomery REDC"},"type":"lvl1","url":"/math/reductions/logjump-reduction#classic-montgomery-redc","position":2},{"hierarchy":{"lvl1":"Classic Montgomery REDC"},"content":"Montgomery reduction rewrites a 2·n-word integerc = t = \\sum_{i=0}^{2n-1} t_i \\cdot 2^{64i}\n\ninto an n-word residuet \\cdot R^{-1} \\bmod p with\n\nR = 2^{64n} (so R \\equiv 0 \\pmod{p}),\n\na single-word constant \\mu = -p^{-1} \\pmod{2^{64}}.\n\nThe outer loop runs once per limb (i = 0 … n−1):\n\nPick q = (t[i] \\cdot \\mu) \\bmod 2^{64} → forcest[i] + q \\cdot p \\equiv 0 \\pmod{2^{64}};\n\nAdd q \\cdot p into the running array (two inner loops);\n\nAfter the loop, the first n limbs are guaranteed zero→ drop them (divide by R);\n\nFinal conditional subtraction ensures the result < p.\n\nThe code below is a pseudocode, parameterised by the constants we set up in the previous cell.\n\ndef mont_redc(c_words: List[int]) -> List[int]:\n    \"\"\"\n    Classic Montgomery reduction, limb-for-limb.\n    Expects c_words to have length 2*N (little-endian).\n    Returns an n-word little-endian list < p.\n    \"\"\"\n    assert len(c_words) == 2 * N\n    t = c_words.copy()\n\n    for i in range(N):\n        q = (t[i] * MU) & MASK\n\n        # ---- multiply  p * q  ----\n        pq   = [0] * (N + 1)\n        carry = 0\n        for j in range(N):\n            prod   = q * P_WORDS[j] + carry\n            pq[j]  = prod & MASK\n            carry  = prod >> WORD_BITS\n        pq[N] = carry\n\n        # ---- add pq into t[i + ..] ----\n        carry = 0\n        for j in range(N + 1):\n            s        = t[i + j] + pq[j] + carry\n            t[i + j] = s & MASK\n            carry    = s >> WORD_BITS\n\n        # ---- propagate carry further if needed ----\n        k = i + N + 1\n        while carry and k < 2 * N:\n            s     = t[k] + carry\n            t[k]  = s & MASK\n            carry = s >> WORD_BITS\n            k    += 1\n\n    # t now starts with n zeros; slice off the high half\n    lhs = t[N : 2 * N]\n\n    if gte(lhs, P_WORDS[:N]):\n        lhs = sub(lhs, P_WORDS[:N])\n    return lhs\n\n\n","type":"content","url":"/math/reductions/logjump-reduction#classic-montgomery-redc","position":3},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea"},"type":"lvl1","url":"/math/reductions/logjump-reduction#from-redc-to-logjump-sos-the-big-idea","position":4},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea"},"content":"Classic Montgomery spends one full outer loop per limb.LogJump collapses n − 1 of those loops into just three ρ-jumps and leaves only a single Montgomery iteration at the end.","type":"content","url":"/math/reductions/logjump-reduction#from-redc-to-logjump-sos-the-big-idea","position":5},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea","lvl2":"1 Pre-compute a “magic” vector ρ"},"type":"lvl2","url":"/math/reductions/logjump-reduction#id-1-pre-compute-a-magic-vector","position":6},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea","lvl2":"1 Pre-compute a “magic” vector ρ"},"content":"For an n = 4 limb modulus, let\\rho = 2^{-64} \\bmod p, \\qquad\n\\rho = (\\rho_0, \\rho_1, \\rho_2, \\rho_3)_{\\text{le}} .\n\nBecause 2^{64} \\cdot \\rho \\equiv 1 \\pmod{p}, multiplying the low word of any value by \\rho and adding that in at a one-word offset both:\n\ncancels the low word, and\n\nshifts the whole number one limb to the right(the carry serves as the “lost” high word).\n\nThis is exactly what each Montgomery outer loop did — but now we get theshift for free once \\rho is available.","type":"content","url":"/math/reductions/logjump-reduction#id-1-pre-compute-a-magic-vector","position":7},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea","lvl2":"2 Do three jumps instead of three REDC loops"},"type":"lvl2","url":"/math/reductions/logjump-reduction#id-2-do-three-jumps-instead-of-three-redc-loops","position":8},{"hierarchy":{"lvl1":"From REDC to LogJump/SOS — the big idea","lvl2":"2 Do three jumps instead of three REDC loops"},"content":"For a 256-bit number we need to zero and discard the first three limbs:\n\n","type":"content","url":"/math/reductions/logjump-reduction#id-2-do-three-jumps-instead-of-three-redc-loops","position":9},{"hierarchy":{"lvl1":"LogJump Example – Execution Breakdown"},"type":"lvl1","url":"/math/reductions/logjump-reduction#logjump-example-execution-breakdown","position":10},{"hierarchy":{"lvl1":"LogJump Example – Execution Breakdown"},"content":"c = [ c0 c1 c2 c3 c4 c5 c6 c7 ]\n     ↓\nstep1: ρ·c0 added one limb up → shift 1  \nstep2: ρ·(new)low added one limb up → shift 1  \nstep3: ρ·(new)low added one limb up → shift 1After those three jumps the array looks like[ 0 0 0 r1 r2 r3 r4 ]\n\nSo we have already divided by 2^{64 \\cdot 3}.","type":"content","url":"/math/reductions/logjump-reduction#logjump-example-execution-breakdown","position":11},{"hierarchy":{"lvl1":"LogJump Example – Execution Breakdown","lvl2":"3 Finish with one standard Montgomery iteration"},"type":"lvl2","url":"/math/reductions/logjump-reduction#id-3-finish-with-one-standard-montgomery-iteration","position":12},{"hierarchy":{"lvl1":"LogJump Example – Execution Breakdown","lvl2":"3 Finish with one standard Montgomery iteration"},"content":"Only limb 0 of the remaining slice may still be non-zero.One ordinary REDC loop (with the usual constant \\mu) clears it and divides by the final \n\n264, leaving exactly four words.A compare-and-subtract with p is the last step.\n\nResult: multiplies saved\n\nClassic REDC → n^2 + n word-multiplies\n\nLogJump/SOS → n^2 + 1 word-multiplies\n\nFor secp256k1 (n = 4), that is:20 → 17 multiplies — a ~15% cut.\n\nThe next cells turn this description into code.\n\n","type":"content","url":"/math/reductions/logjump-reduction#id-3-finish-with-one-standard-montgomery-iteration","position":13},{"hierarchy":{"lvl1":"LogJump/SOS implementation (64-bit limbs, n = 4)"},"type":"lvl1","url":"/math/reductions/logjump-reduction#logjump-sos-implementation-64-bit-limbs-n-4","position":14},{"hierarchy":{"lvl1":"LogJump/SOS implementation (64-bit limbs, n = 4)"},"content":"Below we define:\n\ncalc_m(low) – multiplies a single limb low by the pre-computed\nρ-vector and returns the 5-word result.\n\nmul_logjumps_sos(c) – performs three ρ-jumps followed by one\nclassic Montgomery iteration, returning a 4-limb residue < p.\n\nAll constants (ρ, μ, p) come from the setup cell so that the\ncomparison with mont_redc is apples-to-apples.\n\ndef calc_m(low: int) -> list[int]:\n    carry, m = 0, [0]*6\n    for i in range(5):\n        prod   = low * RHO_WORDS[i] + carry\n        m[i]   = prod & MASK\n        carry  = prod >> WORD_BITS\n    m[5] = carry\n    return m\n\ndef mul_logjumps_sos(c_words: list[int]) -> list[int]:\n    R = [0]*8\n\n    # jump #1\n    m, carry = calc_m(c_words[0]), 0\n    for i in range(6):\n        s = c_words[i+1] + m[i] + carry\n        R[i], carry = s & MASK, s >> WORD_BITS\n    s = c_words[6] + carry\n    R[5], carry = s & MASK, s >> WORD_BITS\n    s = c_words[7] + carry\n    R[6], carry = s & MASK, s >> WORD_BITS\n    R[7] = carry\n\n    # jump #2\n    m, carry = calc_m(R[0]), 0\n    for i in range(6):\n        s = R[i+1] + m[i] + carry\n        R[i], carry = s & MASK, s >> WORD_BITS\n    s = R[6] + carry\n    R[5], carry = s & MASK, s >> WORD_BITS\n    R[6] = carry\n    R[7] = 0\n\n    # jump #3\n    m, carry = calc_m(R[0]), 0\n    for i in range(6):\n        s = R[i+1] + m[i] + carry\n        R[i], carry = s & MASK, s >> WORD_BITS\n    R[5] = carry\n    R[6] = R[7] = 0\n\n    # one Montgomery iteration\n    q = (R[0] * MU) & MASK\n    pq, carry = [0]*6, 0\n    for i in range(5):\n        prod      = q * U64_P[i] + carry\n        pq[i], carry = prod & MASK, prod >> WORD_BITS\n    pq[5] = carry\n\n    carry = 0\n    for i in range(6):\n        s = R[i] + pq[i] + carry\n        R[i], carry = s & MASK, s >> WORD_BITS\n    idx = 6\n    while carry and idx < 8:\n        s = R[idx] + carry\n        R[idx], carry = s & MASK, s >> WORD_BITS\n        idx += 1\n\n    out = R[1:5]\n    if gte(out, P_WORDS[:4]):\n        out = sub(out, P_WORDS[:4])\n    return out\n\n\n","type":"content","url":"/math/reductions/logjump-reduction#logjump-sos-implementation-64-bit-limbs-n-4","position":15},{"hierarchy":{"lvl1":"LogJump/SOS implementation (64-bit limbs, n = 4)","lvl3":"Official limb-loops vs. big-int back-end"},"type":"lvl3","url":"/math/reductions/logjump-reduction#official-limb-loops-vs-big-int-back-end","position":16},{"hierarchy":{"lvl1":"LogJump/SOS implementation (64-bit limbs, n = 4)","lvl3":"Official limb-loops vs. big-int back-end"},"content":"Variant\n\nWhat it actually does\n\nOfficial limb loops\n\nmanipulates four 64-bit limbs with explicit Python for-loops, carry handling, and per-limb multiplies.\n\nBig-int / GMP\n\nTreats the same 256-bit number as one large Python int (or gmpy2.mpz). Each limb step becomes a single C-level bigint multiply, so the benchmark measures only the difference in multiplication count.\n\nLimb loops incur heavy interpreter overhead, hiding LogJump’s saving.Big-int mode pushes work into optimized C code, revealing the relative speed-up.\n\nimport random, timeit, importlib.util\nfrom typing import List\n\nWORD_BITS = 64\nMASK = (1 << WORD_BITS) - 1\nP = 0x30644e72e131a029b85045b68181585d97816a916871ca8d3c208c16d87cfd47\nN, R = 4, 1 << (WORD_BITS * 4)\nMU0 = (-pow(P, -1, 1 << WORD_BITS)) & MASK\n\nhas_gmp = bool(importlib.util.find_spec(\"gmpy2\"))\n_int = __import__(\"gmpy2\").mpz if has_gmp else int\n\nchoice = input(\"Choose implementation: 1 = official limb loops, 2 = big-int > \").strip()\nuse_official = (choice != \"2\")\n\ndef to_words(x, m=2*N): return [(x >> (WORD_BITS*i)) & MASK for i in range(m)]\ndef from_words(ws): return sum(w << (WORD_BITS*i) for i, w in enumerate(ws))\ndef gte(a, b): return a[::-1] >= b[::-1]\ndef sub(a, b):\n    out, borrow = [], 0\n    for ai, bi in zip(a, b):\n        t = ai - bi - borrow\n        out.append((t + (1<<WORD_BITS)) & MASK if t < 0 else t & MASK)\n        borrow = t < 0\n    return out\n\nP_WORDS = to_words(P, N) + [0]\nRHO_WORDS = to_words(pow(2, -WORD_BITS, P), N) + [0]\n\ndef mont_redc(c):\n    t = c.copy()\n    for i in range(N):\n        q = (t[i] * MU0) & MASK\n        pq, carry = [0]*(N+1), 0\n        for j in range(N):\n            prod = q * P_WORDS[j] + carry\n            pq[j], carry = prod & MASK, prod >> WORD_BITS\n        pq[N] = carry\n        carry = 0\n        for j in range(N+1):\n            s = t[i+j] + pq[j] + carry\n            t[i+j], carry = s & MASK, s >> WORD_BITS\n        k = i+N+1\n        while carry and k < 2*N:\n            s = t[k] + carry\n            t[k], carry = s & MASK, s >> WORD_BITS\n            k += 1\n    lhs = t[N:2*N]\n    if gte(lhs, P_WORDS[:N]): lhs = sub(lhs, P_WORDS[:N])\n    return lhs\n\ndef calc_m(low):\n    carry, m = 0, [0]*6\n    for i in range(5):\n        prod = low * RHO_WORDS[i] + carry\n        m[i], carry = prod & MASK, prod >> WORD_BITS\n    m[5] = carry\n    return m\n\ndef mul_logjumps_sos(c):\n    Rv = [0]*8\n    m, carry = calc_m(c[0]), 0\n    for i in range(6):\n        s = c[i+1] + m[i] + carry\n        Rv[i], carry = s & MASK, s >> WORD_BITS\n    s = c[6] + carry; Rv[5], carry = s & MASK, s >> WORD_BITS\n    s = c[7] + carry; Rv[6], carry = s & MASK, s >> WORD_BITS\n    Rv[7] = carry\n    for _ in range(2):\n        m, carry = calc_m(Rv[0]), 0\n        for i in range(6):\n            s = Rv[i+1] + m[i] + carry\n            Rv[i], carry = s & MASK, s >> WORD_BITS\n        s = Rv[6] + carry; Rv[5], carry = s & MASK, s >> WORD_BITS\n        Rv[6], Rv[7] = carry, 0\n    q = (Rv[0] * MU0) & MASK\n    pq, carry = [0]*6, 0\n    for i in range(5):\n        prod = q * P_WORDS[i] + carry\n        pq[i], carry = prod & MASK, prod >> WORD_BITS\n    pq[5] = carry\n    carry = 0\n    for i in range(6):\n        s = Rv[i] + pq[i] + carry\n        Rv[i], carry = s & MASK, s >> WORD_BITS\n    idx = 6\n    while carry and idx < 8:\n        s = Rv[idx] + carry\n        Rv[idx], carry = s & MASK, s >> WORD_BITS\n        idx += 1\n    out = Rv[1:5]\n    if gte(out, P_WORDS[:4]): out = sub(out, P_WORDS[:4])\n    return out\n\ndef mont_big(x):\n    for _ in range(N):\n        m = (x & MASK) * MU0 & MASK\n        x = (x + m * P) >> WORD_BITS\n    return x - P if x >= P else x\n\ndef logj_big(x):\n    for _ in range(N-1):\n        m = (x & MASK) * MU0 & MASK\n        x = (x + m * P) >> WORD_BITS\n    m = (x & MASK) * MU0 & MASK\n    x = (x + m * P) >> WORD_BITS\n    return x - P if x >= P else x\n\nif use_official:\n    mont_reduce_int  = lambda z: from_words(mont_redc(to_words(int(z), 2*N)))\n    logjump_reduce_int = lambda z: from_words(mul_logjumps_sos(to_words(int(z), 2*N)))\nelse:\n    mont_reduce_int, logjump_reduce_int = mont_big, logj_big\n\nprint(f\"Backend    : {'GMP (gmpy2)' if has_gmp else 'Python int'}\")\nprint(f\"Variant    : {'official limb loops' if use_official else 'big-int'}\\n\")\n\nrng = random.SystemRandom()\nxs = [_int(rng.randrange(0, P * R)) for _ in range(10_000)]\nsingle_m = timeit.timeit(\"mont_reduce_int(xs[0])\", globals=globals(), number=100_000)\nsingle_l = timeit.timeit(\"logjump_reduce_int(xs[0])\", globals=globals(), number=100_000)\n\nsetup = \"from __main__ import mont_reduce_int, logjump_reduce_int, xs\"\nstmtm = \"for x in xs: mont_reduce_int(x)\"\nstmtl = \"for x in xs: logjump_reduce_int(x)\"\nbest_m = min(timeit.repeat(stmtm, setup=setup, repeat=10, number=1))\nbest_l = min(timeit.repeat(stmtl, setup=setup, repeat=10, number=1))\nns_m = best_m * 1e9 / len(xs)\nns_l = best_l * 1e9 / len(xs)\n\nprint(f\"[micro]    Mont  {single_m*1e6/100_000:8.3f} µs   LogJump {single_l*1e6/100_000:8.3f} µs\")\nprint(f\"[batch]    Mont  {ns_m:8.1f} ns/op   LogJump {ns_l:8.1f} ns/op   speed-up {ns_m/ns_l:4.2f}×\")","type":"content","url":"/math/reductions/logjump-reduction#official-limb-loops-vs-big-int-back-end","position":17},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction"},"type":"lvl1","url":"/math/reductions/montgomery-reduction","position":0},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction"},"content":"","type":"content","url":"/math/reductions/montgomery-reduction","position":1},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"1. Introduction: The Problem with Modular Division"},"type":"lvl2","url":"/math/reductions/montgomery-reduction#id-1-introduction-the-problem-with-modular-division","position":2},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"1. Introduction: The Problem with Modular Division"},"content":"Modular arithmetic often involves computing remainders: for integers a and n, we find q and r such that:a = qn + r,\\quad \\text{where} \\quad 0 \\leq r < |n|\n\nHere, r = a mod n. While this is straightforward for small numbers, it becomes inefficient at scale, especially in cryptographic computations with large integers.\n\nFor example, to compute:(12 \\times 15) \\mod 7 = ((12 \\mod 7) \\times (15 \\mod 7)) \\mod 7 \\\\\n= (5 \\times 1) \\mod 7 \\\\\n= 5\n\nWe still need to perform mod operations (i.e., division), which are costly for large numbers. Since cryptographic systems rely heavily on modular multiplication, this repeated division becomes a bottleneck.\n\nMontgomery reduction addresses this by avoiding direct division, making modular multiplication more efficient for large integers.\n\n","type":"content","url":"/math/reductions/montgomery-reduction#id-1-introduction-the-problem-with-modular-division","position":3},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"2. The Core Idea: A New Domain for Faster Math"},"type":"lvl2","url":"/math/reductions/montgomery-reduction#id-2-the-core-idea-a-new-domain-for-faster-math","position":4},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"2. The Core Idea: A New Domain for Faster Math"},"content":"Montgomery reduction speeds up modular arithmetic by moving calculations into a special Montgomery domain, avoiding costly division by n.\n\nThis is done using a new modulus R, typically a power of 2 (like  \n\n232 or \n\n264).","type":"content","url":"/math/reductions/montgomery-reduction#id-2-the-core-idea-a-new-domain-for-faster-math","position":5},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Why Use a Power of 2?","lvl2":"2. The Core Idea: A New Domain for Faster Math"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#why-use-a-power-of-2","position":6},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Why Use a Power of 2?","lvl2":"2. The Core Idea: A New Domain for Faster Math"},"content":"Because computers handle powers of 2 efficiently:\n\nDivision by R → simple right shift\n\nModulo R → fast bitwise AND\n\nThis makes reductions much faster than regular division.\n\n","type":"content","url":"/math/reductions/montgomery-reduction#why-use-a-power-of-2","position":7},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"type":"lvl2","url":"/math/reductions/montgomery-reduction#id-3-the-montgomery-algorithm-setup-and-multiplication","position":8},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"content":"With R as a power of 2, we set up the Montgomery system through a one-time preparation:","type":"content","url":"/math/reductions/montgomery-reduction#id-3-the-montgomery-algorithm-setup-and-multiplication","position":9},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Setup Phase","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#setup-phase","position":10},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Setup Phase","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"content":"Choose R:A power of 2 greater than n, enabling fast bitwise operations.\n\nCompute R^{-1}:The modular inverse of R such that:R \\cdot R^{-1} \\equiv 1 \\pmod{n}\n\nCompute n':The modular inverse of -n modulo R, satisfying:-n \\cdot n' \\equiv 1 \\pmod{R}\n\nThe values R^{-1} and n' are precomputed once and used in all Montgomery operations.\n\n","type":"content","url":"/math/reductions/montgomery-reduction#setup-phase","position":11},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Conversion to Montgomery Form","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#conversion-to-montgomery-form","position":12},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Conversion to Montgomery Form","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"content":"To work in the Montgomery domain, convert a number a to its Montgomery form a':a' = a \\cdot R \\pmod{n}\n\nThis step is a one-time, regular modular multiplication—our “entry fee” to faster computation.","type":"content","url":"/math/reductions/montgomery-reduction#conversion-to-montgomery-form","position":13},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Multiplication in the Montgomery Domain","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#multiplication-in-the-montgomery-domain","position":14},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Multiplication in the Montgomery Domain","lvl2":"3. The Montgomery Algorithm: Setup and Multiplication"},"content":"Given a' and b' in Montgomery form, their product c' is:c' = a' \\cdot b' \\cdot R^{-1} \\pmod{n}\n\nWhile this still looks like it needs division by n, the Montgomery Reduction (REDC) algorithm efficiently handles this without actual division.\n\n","type":"content","url":"/math/reductions/montgomery-reduction#multiplication-in-the-montgomery-domain","position":15},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"4. The REDC Algorithm and Final Conversion"},"type":"lvl2","url":"/math/reductions/montgomery-reduction#id-4-the-redc-algorithm-and-final-conversion","position":16},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl2":"4. The REDC Algorithm and Final Conversion"},"content":"To compute the Montgomery productc' = a' · b' · R⁻¹ mod(n),we use the REDC function, which efficiently calculatesT \\cdot R^{-1} \\mod nfor T = a' \\cdot b'.","type":"content","url":"/math/reductions/montgomery-reduction#id-4-the-redc-algorithm-and-final-conversion","position":17},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"The REDC Algorithm","lvl2":"4. The REDC Algorithm and Final Conversion"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#the-redc-algorithm","position":18},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"The REDC Algorithm","lvl2":"4. The REDC Algorithm and Final Conversion"},"content":"Compute m:m = T \\bmod R \\cdot n’ \\bmod R\n\nCompute t:t = \\frac{T + m \\cdot n}{R}\n\nFinal correction:\\text{If } t \\geq n, \\quad \\text{then } t = t - n\n\nReturn t as the result.\n\nThe speed boost comes from dividing by R (a power of 2), which is just a fast bit shift.","type":"content","url":"/math/reductions/montgomery-reduction#the-redc-algorithm","position":19},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Conversion Back to Standard Form","lvl2":"4. The REDC Algorithm and Final Conversion"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#conversion-back-to-standard-form","position":20},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"Conversion Back to Standard Form","lvl2":"4. The REDC Algorithm and Final Conversion"},"content":"After all operations, the result c' is still in Montgomery form.To convert back, apply one last REDC:c = \\text{REDC}(c')\n\nclass Montgomery:\n    def __init__(self, n):\n        if n % 2 == 0:\n            raise ValueError(\"Modulus n must be odd.\")\n        \n        self.n = n\n        self.logR = n.bit_length()\n        self.R = 1 << self.logR\n        self.R_mask = self.R - 1\n        \n        n_inv_R = self._modinv(self.n, self.R)\n        self.n_prime = self.R - n_inv_R\n\n    def _egcd(self, a, b):\n        if a == 0:\n            return (b, 0, 1)\n        g, y, x = self._egcd(b % a, a)\n        return (g, x - (b // a) * y, y)\n\n    def _modinv(self, a, m):\n        g, x, y = self._egcd(a, m)\n        if g != 1:\n            raise ValueError('Modular inverse does not exist')\n        return x % m\n        \n    def _reduce(self, T):\n        m = ((T & self.R_mask) * self.n_prime) & self.R_mask\n        t = (T + m * self.n) >> self.logR\n        \n        if t >= self.n:\n            return t - self.n\n        else:\n            return t\n\n    def convert_in(self, x):\n        return (x * self.R) % self.n\n\n    def convert_out(self, x_mont):\n        return self._reduce(x_mont)\n\n    def multiply(self, a_mont, b_mont):\n        T = a_mont * b_mont\n        return self._reduce(T), T\n\n# --- Usage Example ---\n# 1. One-time setup\nmonty_system = Montgomery(n=13)\nprint(f\"System Initialized for n={monty_system.n}\")\nprint(f\"Calculated R={monty_system.R}, n'={monty_system.n_prime}\")\nprint(\"-\" * 30)\n\n# 2. Convert numbers to Montgomery form\na = 7\nb = 8\na_mont = monty_system.convert_in(a)\nb_mont = monty_system.convert_in(b)\nprint(f\"{a} in Montgomery form is: {a_mont}\")\nprint(f\"{b} in Montgomery form is: {b_mont}\")\nprint(\"-\" * 30)\n\n# 3. Perform multiplication in the Montgomery domain\nproduct_mont, intermediate_T = monty_system.multiply(a_mont, b_mont)\nprint(f\"Intermediate product T = {a_mont} * {b_mont} = {intermediate_T}\")\nprint(f\"REDC(T) -> Product in Montgomery form: {product_mont}\")\nprint(\"-\" * 30)\n\n# 4. Convert the result back to a standard number\nfinal_result = monty_system.convert_out(product_mont)\nprint(f\"Final Result (after converting back): {final_result}\")\nprint(f\"Standard Check: (7 * 8) % 13 = {(7 * 8) % 13}\")\n\n","type":"content","url":"/math/reductions/montgomery-reduction#conversion-back-to-standard-form","position":21},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"A Quick Clarification: Why Does Montgomery Seem Slower at First?","lvl2":"4. The REDC Algorithm and Final Conversion"},"type":"lvl3","url":"/math/reductions/montgomery-reduction#a-quick-clarification-why-does-montgomery-seem-slower-at-first","position":22},{"hierarchy":{"lvl1":"Understanding Montgomery Reduction","lvl3":"A Quick Clarification: Why Does Montgomery Seem Slower at First?","lvl2":"4. The REDC Algorithm and Final Conversion"},"content":"Question:Montgomery starts with divisions like 112 mod 13 or 128 mod 13, which look slower than a simple 56 mod 13. So why is it still preferred—and actually faster—when working with large numbers?\n\nFor a single, small calculation, the setup cost of Montgomery reduction makes it slower than the standard method.\n\nThe performance boost isn’t for one-off calculations; it’s for chains of multiplications performed with the same modulus, which is extremely common in cryptography. The classic use case is modular exponentiation (a^e mod n), which is the core of RSA and the example we will explore next.\n\n# Re-using our clean Montgomery class\nclass Montgomery:\n    def __init__(self, n):\n        if n % 2 == 0: raise ValueError(\"Modulus n must be odd.\")\n        self.n = n\n        self.logR = n.bit_length()\n        self.R = 1 << self.logR\n        self.R_mask = self.R - 1\n        n_inv_R = self._modinv(self.n, self.R)\n        self.n_prime = self.R - n_inv_R\n\n    def _egcd(self, a, b):\n        if a == 0: return (b, 0, 1)\n        g, y, x = self._egcd(b % a, a)\n        return (g, x - (b // a) * y, y)\n\n    def _modinv(self, a, m):\n        g, x, y = self._egcd(a, m)\n        if g != 1: raise ValueError('Modular inverse does not exist')\n        return x % m\n        \n    def _reduce(self, T):\n        m = ((T & self.R_mask) * self.n_prime) & self.R_mask\n        t = (T + m * self.n) >> self.logR\n        return t - self.n if t >= self.n else t\n\n    def convert_in(self, x):\n        return (x * self.R) % self.n\n\n    def convert_out(self, x_mont):\n        return self._reduce(x_mont)\n\n    def multiply(self, a_mont, b_mont):\n        return self._reduce(a_mont * b_mont)\n\n# --- Method 1: Standard Exponentiation Trace  ---\ndef trace_standard_pow_clean(base, exp, mod):\n    print(\"--- Starting Standard Modular Exponentiation ---\")\n    expensive_ops = 0\n    res = 1\n    binary_exp = bin(exp)[2:]\n    print(f\"Executing for exponent {exp} (binary: {binary_exp})\\n\")\n    \n    for i, bit in enumerate(binary_exp):\n        # Squaring step\n        res_old = res\n        res = (res * res) % mod\n        expensive_ops += 1\n        print(f\"Step {i+1} (Square): ({res_old}*{res_old}) % {mod} -> {res} (EXPENSIVE)\")\n        \n        # Multiplication step if bit is 1\n        if bit == '1':\n            res_old = res\n            res = (res * base) % mod\n            expensive_ops += 1\n            print(f\"Step {i+1} (Mult):   ({res_old}*{base}) % {mod} -> {res} (EXPENSIVE)\")\n            \n    print(f\"\\nFinal Result: {res}\")\n    print(f\"Total Expensive (mod n) Operations: {expensive_ops}\\n\")\n\n# --- Method 2: Montgomery Exponentiation Trace ---\ndef trace_montgomery_pow_clean(base, exp, n):\n    print(\"--- Starting Montgomery Modular Exponentiation ---\")\n    expensive_ops = 0\n    \n    # 1. Setup & Conversion\n    print(\"Step 1: Setup & Initial Conversion\")\n    monty = Montgomery(n)\n    res_mont = monty.convert_in(1)\n    expensive_ops += 1\n    base_mont = monty.convert_in(base)\n    expensive_ops += 1\n    print(f\"  - Converting 1 -> {res_mont} (EXPENSIVE OP #{expensive_ops-1})\")\n    print(f\"  - Converting {base} -> {base_mont} (EXPENSIVE OP #{expensive_ops})\\n\")\n\n    # 2. Main Loop\n    print(\"Step 2: Main loop with FAST operations\")\n    binary_exp = bin(exp)[2:]\n    for i, bit in enumerate(binary_exp):\n        # Squaring step\n        res_old = res_mont\n        res_mont = monty.multiply(res_mont, res_mont)\n        print(f\"Loop {i+1} (Square): REDC({res_old}*{res_old}) -> {res_mont} (FAST)\")\n\n        # Multiplication step if bit is 1\n        if bit == '1':\n            res_old = res_mont\n            res_mont = monty.multiply(res_mont, base_mont)\n            print(f\"Loop {i+1} (Mult):   REDC({res_old}*{base_mont}) -> {res_mont} (FAST)\")\n\n    # 3. Final Conversion\n    final_res = monty.convert_out(res_mont)\n    print(f\"\\nStep 3: Final conversion -> REDC({res_mont}) -> {final_res} (FAST)\")\n    \n    print(f\"\\nFinal Result: {final_res}\")\n    print(f\"Total Expensive (mod n) Operations: {expensive_ops}\\n\")\n\n# --- Run the cleaned traces ---\nbase, exp, mod = 5, 10, 13\ntrace_standard_pow_clean(base, exp, mod)\nprint(\"=\"*50)\ntrace_montgomery_pow_clean(base, exp, mod)","type":"content","url":"/math/reductions/montgomery-reduction#a-quick-clarification-why-does-montgomery-seem-slower-at-first","position":23},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration"},"type":"lvl1","url":"/math/unipoly-div","position":0},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nLast updated: 2025-06-24\n\nTraditional polynomial division with remainder \n\nSynthetic Division requires O(n^2) computational complexity. This section introduces a fast division algorithm using Newton Iteration, with complexity matching polynomial multiplication at only O(M(n)), where M(n) represents the complexity of polynomial multiplication. This algorithm is described in Chapter 9.1 of “Modern Computer Algebra”.\n\nBefore introducing Newton’s method, we’ll first discuss the simple concept of reversed polynomials, and then examine their relationship with power series.","type":"content","url":"/math/unipoly-div","position":1},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Reversed Polynomials"},"type":"lvl2","url":"/math/unipoly-div#reversed-polynomials","position":2},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Reversed Polynomials"},"content":"Given a finite field F, for polynomials f(X) and g(X) in F[X], since F[X] is a Euclidean Domain, f(X) and g(X) satisfy the following division with remainder equation:f(X) = g(X)\\cdot q(X) + r(X), \\qquad\\text{and} \\deg(r)<\\deg(g)\n\nFor convenience, we denote n-1 as the length of the coefficient vector of f(X), with \\deg(f)=n-1, and m-1 as the length of the coefficient vector of g(X), with \\deg(g)=m-1.\n\nLet’s first introduce what a reversed polynomial is. If f(X) is expressed in coefficient form as:f(X) = a_0 + a_1X + \\cdots + a_{n-1}X^{n-1}\n\nThen its reversed polynomial is:\\mathsf{rev}(f) = a_{n-1} + a_{n-2}X + \\cdots + a_1X^{n-2} + a_0X^{n-1}\n\nSimply put, \\mathsf{rev}(f) reverses the order of coefficients in f, making the highest degree coefficient of the original polynomial the constant term of the reversed polynomial; likewise, the second-highest coefficient becomes the X term coefficient, and so on. Here’s the formal definition of the reversal transformation: \\mathsf{rev}_k(f):F[X]\\to F[X]:\\mathsf{rev}_k(f): f(X) \\longmapsto X^kf\\left(\\frac{1}{X}\\right)\n\nIf k=m=\\deg(f)+1, then \\mathsf{rev}_k(f) represents f’s reversed polynomial, and we omit the subscript k, denoting it as \\mathsf{rev}_k(f). Let’s expand the definition of \\mathsf{rev}_k(f):\\begin{aligned}\n\\mathsf{rev}_k(f) & = X^kf\\left(\\frac{1}{X}\\right) \\\\\n& = X^k\\left(a_0 + \\frac{a_{1}}{X} + \\cdots + \\frac{a_{n-2}}{X^{n-2}} + \\frac{a_{n-1}}{X^{n-1}}\\right) \\\\\n& = a_0X^k + a_1X^{k-1} + \\cdots + a_{n-1}X^{k-n+1}  \\\\\n& = X^{k-n+1}\\big( a_{n-1} + a_{n-2}X + \\cdots + a_1X^{n-2} + a_0X^{n-1} \\big)\n\\end{aligned}\n\nIf k=n-1, then \\mathsf{rev}_k(f) is exactly the reversed polynomial of f. It’s easy to verify that \\mathsf{rev}_k satisfies additive homomorphism:\\mathsf{rev}_k(f+g) = \\mathsf{rev}_k(f) + \\mathsf{rev}_k(g)\n\nAnd \\mathsf{rev}_k also satisfies these two multiplicative relational equations:\\mathsf{rev}_k(f\\cdot g) = \\mathsf{rev}_j(f)\\cdot \\mathsf{rev}_{k-j}(g)\\mathsf{rev}_k(f) = X^d \\cdot \\mathsf{rev}_{k-d}(f)\n\nSubstituting f(X)'s divisional decomposition equation into the definition of \\mathsf{rev}(f), we get:\\begin{aligned}\n\\mathsf{rev}_{n}(f) & = \\mathsf{rev}_{n}(q\\cdot g + r) \\\\\n& = \\mathsf{rev}_{n}(q\\cdot g) + \\mathsf{rev}_{n}(r) \\\\\n& = \\mathsf{rev}_{n-m}(q)\\cdot \\mathsf{rev}_{m}(g) + X^{n-m+1}\\mathsf{rev}_{m-1}(r)\n\\end{aligned}\n\nThen we can derive the following equation:\\mathsf{rev}_{n}(f) \\equiv \\mathsf{rev}_{n-m}(q)\\cdot \\mathsf{rev}_{m}(g) \\mod{X^{n-m+1}}\n\nNote that in the above equation, the reversed polynomial of the remainder polynomial r(X) is lifted by X^{n-m+1} to become a higher-degree term, so we can eliminate it using polynomial modular arithmetic.\n\nThe purpose of this treatment is that polynomial division operations no longer need to consider the remainder polynomial r(X); we only need to calculate the reversed polynomial \\mathsf{rev}(q) of the quotient polynomial q(X) using the equation above.\n\nWe can further transform the modular equation to get a formula for \\mathsf{rev}_{n-m}(q):\\mathsf{rev}_{n-m}(q) \\equiv \\mathsf{rev}_{n}(f)\\cdot \\mathsf{rev}_{m}(g)^{-1} \\mod{X^{n-m+1}}\n\nThis way, calculating the quotient polynomial q(X) further depends on a polynomial inverse operation under modular arithmetic, specifically how to calculate \\mathsf{rev}_{m}(g)^{-1} such that \\mathsf{rev}_{m}(g)\\cdot \\mathsf{rev}_{m}(g)^{-1} \\equiv 1\\mod{X^{n-m+1}}.\n\nIf we can successfully calculate \\mathsf{rev}_{n-m}(q), then q(X) can be solved as:q = \\mathsf{rev}_{n-m}(\\mathsf{rev}_{n-m}(q))\n\nThe next question is, is it easier to compute a polynomial’s multiplicative inverse in a polynomial equation with modular arithmetic? In other words, how do we calculate the multiplicative inverse in the quotient ring F[X]/\\langle X^{n-m+1} \\rangle?\n\nFor convenience, let’s introduce a new constant: l=n-m+1.","type":"content","url":"/math/unipoly-div#reversed-polynomials","position":3},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Existence of Multiplicative Inverses in Quotient Rings"},"type":"lvl2","url":"/math/unipoly-div#existence-of-multiplicative-inverses-in-quotient-rings","position":4},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Existence of Multiplicative Inverses in Quotient Rings"},"content":"We know that only unit elements in a ring have multiplicative inverses. So for elements in the quotient ring F[X]/\\langle X^l\\rangle, which polynomials have multiplicative inverses?\n\nWe can deduce that polynomials with non-zero constant terms in F[X]/\\langle X^l\\rangle definitely have multiplicative inverses. This is because for any Euclidean Domain R, if two coprime elements a, m\\in R exist, i.e., \\gcd(a, m)=1, then using the Extended Euclidean Algorithm we can find s, t\\in R that satisfy the Bezout equation:s a + t m = 1\n\nor equivalently:\\begin{aligned}\ns &\\equiv a^{-1} \\mod{m} \\\\\nt &\\equiv m^{-1} \\mod{a}\n\\end{aligned}\n\nFor any polynomial f\\in F[X]/\\langle X^l\\rangle modulo X^l, as long as its constant term is non-zero, it satisfies \\gcd(f, X^l)=1, meaning we can calculate the Inverse of f mod X^l using the Extended Euclidean Algorithm, though this requires O(n^2) complexity.\n\nFor the problem we need to solve, calculating h in g\\cdot h \\equiv 1 \\mod{X^l}, since g is the reversed polynomial of some polynomial, its constant term must be non-zero, especially if the original polynomial is Monic (leading coefficient is one), then its constant term is 1, i.e., g(0)=1.","type":"content","url":"/math/unipoly-div#existence-of-multiplicative-inverses-in-quotient-rings","position":5},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Power Series Rings and Polynomial Inversion"},"type":"lvl2","url":"/math/unipoly-div#power-series-rings-and-polynomial-inversion","position":6},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Power Series Rings and Polynomial Inversion"},"content":"To calculate the multiplicative inverse of a polynomial, we need to introduce an important concept, the Formal Power Series Ring. If we extend polynomials in F[X] by adding infinitely many non-zero higher-degree terms to any polynomial, we get a formal power series ring F[[X]]:p(X) = \\sum_{i=0}^{\\infty} a_iX^i \\in F[[X]]\n\nFor any p_1, p_2\\in F[[X]], the ring’s addition and multiplication are defined as follows:p_1(X) + p_2(X) = \\sum_{i=0}^{\\infty}(a_i + b_i)X^ip_1(X)\\cdot p_2(X) = \\sum_{i=0}^{\\infty}\\Big(\\sum_{j=0}^{i}a_j b_{i-j}\\Big)X^i\n\nAdditionally, any non-zero p(X) can be uniquely factored as X^n p_0(X), where p_0(X) has a non-zero constant term. F[[X]] is a Unique Factorization Domain (UFD). Defining function \\delta(p) = n, F[[X]] is a Euclidean Domain.\n\nFor any polynomial f(X)\\in F[X], we can view it as a power series with higher-degree term coefficients of zero, i.e.,f(X) = \\sum_{i=0}^{d}a_iX^i + \\sum_{i=d+1}^{\\infty}0X^i \\in F[[X]]\n\nWe can also map any power series p(X)\\in F[[X]] to a polynomial in F[X] through polynomial modular arithmetic:p(X) = \\sum_{i=0}^{d}a_iX^i + O(X^{d+1}) \\equiv f(X) \\mod{X^d}\n\nHere O(X^{d+1}) represents all terms of X^{d+1} and higher. By marking terms higher than X^d as unconcerned tail terms, we get an approximate representation of power series p(X), where d indicates the precision of the approximation.\n\nThus, F[X] is a subring of F[[X]], and we can embed elements from F[X] into F[[X]] through a monomorphism:\\begin{aligned}\n\\iota: & F[X] \\to F[[X]] \\\\\n& f(X) \\mapsto \\sum_{i=0}^{d}a_iX^i + \\sum_{i=d+1}^{\\infty}0X^i\n\\end{aligned}\n\nWhy introduce power series rings? Because they have a very useful property:\n\nAny power series p(X) with a non-zero constant term has a multiplicative inverse \\tilde{p}(X) such that p(X)\\cdot \\tilde{p}(X) = 1\n\nThis conclusion seems a bit magical. Let’s look at a concrete example:p_1(X) = 1 + 2X + 3X^2 + 2X^3 \\in \\mathbb{F}_7[X]\n\nIts multiplicative inverse is a potentially infinite power series. For convenience, we’ll take only its first ten terms, i.e., with precision 10:\\tilde{p}_1(X) = 1 + 5X + X^2 + 2X^3 + 4X^4 + 5X^5 + 2X^6 + X^7 + 3X^8 + X^9 + O(X^{10})\n\nThe tail term O(X^{10}) represents the sum of all higher-degree terms after X^{10}.\n\nLet’s try to take \\tilde{p}_1(X) without the tail term as a polynomial and multiply it by p_1(X) to see the result:\\begin{aligned}\n& p_1(X)\\cdot (1 + 5X + X^2 + 2X^3 + 4X^4 + 5X^5 + 2X^6 + X^7 + 3X^8 + X^9) \\\\\n= & 1 + 6X^{10} + 2X^{11} + 2X^{12}\n\\end{aligned}\n\nAs we can see, this is a polynomial approximately equal to one. In other words, the product result only has terms with degrees greater than or equal to 10. If we consider precision 10, this product result is approximately equal to one.\n\nHow do we calculate this potentially infinite \\tilde{p}(X)? Let’s derive the calculation formula. Assume the multiplicative inverse \\tilde{p}(X) of p(X) is:\\tilde{p}(X)=\\sum_{i=0}^{\\infty}b_iX^i\n\nsatisfying:\\Big(\\sum_{i=0}^{\\infty}b_iX^i\\Big)\\Big(\\sum_{i=0}^{\\infty}a_iX^i\\Big) = 1\n\nSince a_0\\neq 0, we have b_0 = \\frac{1}{a_0}, because a_0b_0 = 1. Then considering the coefficient of the first-degree term in the product, since the coefficient of X is zero:a_1b_0 + a_0b_1 = 0\n\nSo we can get:b_1 = -\\frac{a_1b_0}{a_0}\n\nThrough similar derivation, we can get the expression for b_2:b_2 = -\\frac{a_1b_1 + a_2b_0}{a_0}\n\nThis pattern can be generalized to any b_k, resulting in a recursive calculation formula:b_k = -\\frac{a_1b_{k-1} + a_2b_{k-2} + \\cdots a_kb_0 }{a_0} = \\frac{-1}{a_0}\\Big(\\sum_{j=1}^{k}a_{j}b_{k-j}\\Big)\n\nThis recursion can continue, calculating from low-degree coefficient to high-degree coefficient of \\tilde{p}(X), until the required computational precision is reached.\n\nThe formal power series ring F[[X]] is actually a local ring, meaning it has only one maximal ideal \\langle X\\rangle. All power series outside this maximal ideal (elements with non-zero constant terms) are unit elements, meaning they have multiplicative inverses.\n\nBack to our problem: given a polynomial g\\in F[X], we need to find its multiplicative inverse h\\in F[X] satisfying g\\cdot h \\equiv 1 \\mod X^l. Since g is the reversed polynomial of some polynomial, we can be sure its constant term is non-zero. Thus, according to the algorithm above, we can find a \\tilde{g}\\in F[[X]] satisfying \\tilde{g}\\cdot g = 1. Of course, we don’t need the exact result of \\tilde{g}, since \\tilde{g} is an infinitely long power series; we just need its approximate solution, where the approximation precision only needs to be greater than X^l to stop the recursive calculation.h(X) = b_0 + b_1X + b_2X^2 + \\cdots + b_{l-1}X^{l-1}\n\nAt this point, we already have an algorithm for polynomial division, but its complexity is still O(l^2).\n\nNext, we’ll introduce how to use Newton’s method (Newton Iteration) to accelerate the calculation of a polynomial’s multiplicative inverse, with algorithm complexity O(M(l)), where M(l) represents the computational cost of polynomial multiplication. If the finite field F is Algebraic-FFT friendly (meaning F contains a multiplicative subgroup of size 2^\\kappa large enough), then the complexity of O(M(l)) is O(l\\log{l}).","type":"content","url":"/math/unipoly-div#power-series-rings-and-polynomial-inversion","position":7},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Newton’s Method"},"type":"lvl2","url":"/math/unipoly-div#newtons-method","position":8},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Newton’s Method"},"content":"Newton’s method is an iterative algorithm in mathematical analysis for finding polynomial roots by successive approximation. For example, for a differentiable function \\phi: \\mathbb{R}\\to \\mathbb{R} in the real domain, to find \\alpha satisfying \\phi(\\alpha)=0, we first guess an initial value x=\\alpha_0, then successively solving for \\alpha_1, \\alpha_2, \\cdots, \\alpha_k, until \\alpha_k\\cong\\alpha, or until the precision meets requirements. The idea of successive approximation can be simply illustrated by the figure:\n\nAssume the slope of \\phi(x) at x=\\alpha_i is \\phi'(\\alpha_i), and the intersection of the tangent line with the x-axis is denoted as \\alpha_{i+1}, then they satisfy:\\frac{\\phi(\\alpha_i)}{\n\\alpha_i - \\alpha_{i+1}\n} = \\phi'(\\alpha_{i})\n\nAfter simple formula transformation, we get the recursive expression for \\alpha_{i+1}:\\alpha_{i+1} = \\alpha_i - \\frac{\\phi(\\alpha_i)}{\\phi'(\\alpha_i)}\n\nStarting from \\alpha_0, through repeated iterations k times, we can get a fast converging approximate solution \\alpha_k\\cong\\alpha.\n\nSimilarly, to solve g\\cdot h \\equiv 1 \\mod X^l, we can construct a function \\Phi: F[[X]] \\to F[[X]] mimicking the real function \\phi(x):\\Phi(Y) = \\frac{1}{Y} - g\n\nIts derivative function is denoted as \\Phi'(Y):\\Phi'(Y) = (\\frac{1}{Y} - g)' = -\\frac{1}{Y^2}\n\nThe root Y = \\tilde{g}\\in F[[X]] of this function will satisfy \\tilde{g}\\cdot g = 1:\\Phi(\\tilde{g}) = \\frac{1}{\\tilde{g}} - g = g - g = 0\n\nPlease note again that since \\tilde{g} is a power series with infinitely many terms, we only need an approximate solution h\\approx\\tilde{g}, meaning:\\Phi(h) = \\frac{1}{h} - g \\approx 0\n\nThis exact solution \\tilde{g} has non-zero coefficients for X^l or higher terms X^{>l}, while the approximate solution h only needs to have the same coefficients as \\tilde{g} for lower-degree terms:\\tilde{g} = h + O(X^l)\n\nThen clearly h only needs to include all terms lower than X^l, making h an element of the polynomial ring F[X]/\\langle X^l \\rangle:h = \\sum_{i=0}^{l-1}b_iX^i \\quad \\in F[X]/\\langle X^l \\rangle\n\nand satisfying:g\\cdot h = g \\cdot (\\tilde{g} - O(X^l)) = 1 - g\\cdot O(X^l) \\equiv 1 \\mod{X^l}\n\nNow let’s try using Newton’s method to solve for h, with the recursive formula:h_{i+1} = h_i - \\frac{\\Phi(h_i)}{\\Phi'(h_i)} = h_i - \\frac{\\frac{1}{h_i} - g}{-\\frac{1}{h_i^2}} = 2h_i - g\\cdot h_i^2\n\nFirst, let’s assume the constant term of g is one, so g=1+O(X). Then let’s guess an initial value for iteration h_0 = 1. When we substitute Y=h_0 into \\Phi(Y):\\Phi(h_0) = \\frac{1}{h_0} - g = 1 - g = O(X)\n\nWe can use (\\bmod{X}) modular arithmetic to eliminate the tail term O(X) on the right side of the equation, yielding:\\Phi(h_0) = O(X) \\equiv 0 \\mod{X}\n\nThis equation can be interpreted as: h_0 is an approximate root of \\Phi(Y) with precision X.\n\nThen using Newton’s iterative formula for the first step, we get h_1:h_1 = h_0 - \\frac{\\Phi(h_0)}{\\Phi'(h_0)} = 2h_0 - g\\cdot h_0^2 = 2 - g\n\nLet’s check how far g\\cdot h_1 is from 1:1 - g\\cdot h_1 = 1- g(2-g) = (1 - g)^2 = (O(X))^2 = O(X^2) \\equiv 0 \\mod{X^2}\n\nNow we can consider: h_1 is an approximate root of the \\Phi(Y) function with precision X^2.\n\nContinuing with the iteration for h_2:h_2 = h_1 - \\frac{\\Phi(h_1)}{\\Phi'(h_1)} = 2h_1 - g\\cdot h_1^2 = 4 - 2g - g(2-g)^2 = 4 - 2g - 4g + 4g^2 - g^3\n\nContinuing to test how far g\\cdot h_2 is from one, assuming g = 1 + e_0X + e_1X^2 + e_2X^3 + O(X^4), then:\\begin{aligned}\n1 - g\\cdot h_2 &= 1- g(4-6g+4g^2-g^3) \\\\\n& = 1 - 4g + 6g^2 - 4g^3 + g^4 \\\\\n\\end{aligned}\n\nSubstituting g = 1 + e_0X + e_1X^2 + e_2X^3 + O(X^4) into the right side, skipping the tedious calculation steps, we find that all coefficients of X, X^2, X^3 terms are eliminated to zero, finally getting:1 - g\\cdot h_2 = \\cdots = O(X^4) \\equiv 0 \\mod{X^4}\n\nSo, h_2 is an approximate root of \\Phi(X) with precision X^4.\n\nThrough observation, it’s not hard to see that each time Newton’s method is used, the calculation result progressively approaches the root of \\Phi(Y), and (1-g\\cdot h_i) only retains terms of X^{2^i} and higher. We can conjecture:g\\cdot h_i \\equiv 1 \\mod{X^{2^i}}\n\nwhere h_i is calculated as follows:h_{i} = h_{i-1} - \\frac{\\Phi(h_{i-1})}{\\Phi'(h_{i-1})} \\equiv 2\\cdot h_{i-1} - g\\cdot h_{i-1}^2 \\mod{X^{2^{i}}}\n\nWe use mathematical induction to prove this conjecture more rigorously:\n\nIf i = 0, then h_0 = 1, clearly g\\cdot h_0 = 1 + O(X) \\equiv 1 \\mod X, so the theorem holds when i=0.\n\nAssuming g\\cdot h_i \\equiv 1 \\mod X^{2^i} holds, let’s consider h_{i+1}, expand 1-g\\cdot h_{i+1}, and similarly prove that h_{i+1} satisfies the theorem equation:\\begin{aligned}\n1 - g\\cdot h_{i+1} & = 1 - g\\cdot \\Big(2\\cdot h_{i} - g\\cdot h_{i}^2\\Big) \\\\\n& = 1 - 2g\\cdot h_{i} + g^2\\cdot h_{i}^2 \\\\\n& = (1 - g\\cdot h_{i})^2 \\\\\n& \\equiv 0 \\mod{(X^{2^{i}})^2}\n\\end{aligned}\n\nNow we can understand why we initially wanted to use the \\mathsf{rev}(g) function to get g’s reversed polynomial: this is because when computing with the reversed polynomial as a power series, its constant term is stable. Though high-degree terms produce many cross-terms, they can be eliminated by modular arithmetic.","type":"content","url":"/math/unipoly-div#newtons-method","position":9},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Polynomial Inversion Algorithm"},"type":"lvl2","url":"/math/unipoly-div#polynomial-inversion-algorithm","position":10},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Polynomial Inversion Algorithm"},"content":"Let’s detail the calculation process for h_k:\\begin{aligned}\nh_{k+1}(X) & \\equiv 2\\cdot h_k(X) - f(X)\\cdot h^2_{k}(X) \\mod X^{2^k} \\\\\nh_{k}(X) & \\equiv 2\\cdot h_{k-1}(X) - f(X)\\cdot h^2_{k-1}(X) \\mod X^{2^{k-1}}\\\\\n\\vdots \\quad & \\equiv \\qquad \\vdots \\\\\nh_{2}(X) & \\equiv 2\\cdot h_{1}(X) - f(X)\\cdot h^2_{1}(X) \\mod X^{2}\\\\\nh_{1}(X) & \\equiv 2\\cdot h_{0}(X) - f(X)\\cdot h^2_{0}(X) \\mod X\\\\\n\\end{aligned}\n\nIf we assume l is a power of two, i.e., l=2^k, then we can get the following polynomial inversion algorithm (Python code), where the first parameter is a known polynomial g(X)\\in F[X], and the algorithm returns a polynomial h\\in F[X]/\\langle X^l \\rangle satisfying g\\cdot h \\equiv 1 \\mod X^l:def poly_inverse(g: list[F], l: int):\n    assert (g[0] == F.one())\n    r = log_2_floor(l)\n    h = [F(1)]\n    for i in range(1, k+1):\n        h_sq = poly_mul(h, h)\n        g_h_sq = poly_mul(h_sq, g)\n        h = poly_sub(poly_smul(h, F(2)), g_h_sq[:2**i])\n    return h\n\ndef poly_sub(f, g):\n    return [f[i] - g[i] for i in range(len(f))]\n\ndef poly_smul(f, c):\n    return [f[i] * c for i in range(len(f))]\n\ndef poly_mul(f, g):\n    ''' skip the implementation '''\n    ...\n\nWhat if l is not a power of two? How should we handle it?\n\nWe have two methods. The first is from book [GG13] Exercise 9.6 (page 287). The second, from paper [CC11], is simpler and more straightforward: if k is the power of two rounding up of l, so l\\leq k, we can have:X^k\\mid (1 - gh) \\Rightarrow  X^l\\mid (1 - gh)\n\nThis is easy to prove. Intuitively, if a power series computes to zero at precision X^k, it must also be zero at lower precision, just like (1 + O(X^k)) can certainly be expressed as (1 + O(X^l)). So we only need to calculate h^* polynomial satisfying X^k|1-gh^*, then through modular arithmetic get h = h^* \\mod X^l, which certainly satisfies:X^l\\mid (1 - gh)\n\nAnother detail to handle is considering that g(0)'s constant term may not be one. This is common since g’s reversed polynomial is likely not a monic polynomial (leading coefficient is one). Although we have simple ways to convert a non-monic polynomial into a monic one, this conversion requires O(l) additional finite field multiplications. However, we can directly remove this constraint by setting the initial guess h_0 to g(0)^{-1}, then continue with Newton’s method.\n\nA third improvement is that in each polynomial multiplication operation, we can first truncate polynomials based on precision to reduce length and computational cost. For example, when calculating g\\cdot h_{i-1}^2, we need only compute (g \\mod X^{2^i})\\cdot h_{i-1}^2, first truncating polynomial g to precision, then performing multiplication.\n\nHere’s the modified Python code:def poly_inverse_rev1(g: list[F], l: int):\n    k = log_2_floor(l)\n    h = [g[0].inverse()]\n    for i in range(1, k+1):\n        h_sq = poly_mul(h[:2**(i-1)], h[:2**(i-1)])\n        g_h_sq = poly_mul(h_sq, g[:2**i])\n        h = poly_sub(poly_smul(h, 2), g_h_sq[:2**i])\n    \n    h_sq = poly_mul(h, h)\n    g_h_sq = poly_mul(h_sq[:l+1], g[:l+1])\n    h = poly_sub(poly_smul(h[:l+1], 2), g_h_sq[:l+1])\n    return h","type":"content","url":"/math/unipoly-div#polynomial-inversion-algorithm","position":11},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl3":"Complexity Analysis","lvl2":"Polynomial Inversion Algorithm"},"type":"lvl3","url":"/math/unipoly-div#complexity-analysis","position":12},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl3":"Complexity Analysis","lvl2":"Polynomial Inversion Algorithm"},"content":"The time complexity of this polynomial inversion algorithm is 3M(l) + 2l.\n\nFirst, each round of polynomial multiplication has two operations: one for h_i^2, with complexity M(2^{i-1}), and one for h_i^2\\cdot g, with complexity M(2^i). Computing (2h_i - g\\cdot h_i^2) has complexity 2^i.\n\nThe total cost of one iteration is:M(2^i) + M(2^{i-1}) + 2^i \\leq = \\frac{3}{2}M(2^{i}) + 2^i\n\nThe total cost of r iterations is:\\sum_{1\\leq i \\leq r} \\Big(\\frac{3}{2}M(2^i) + 2^i\\Big) \\lt 3M(2^r) + 2^{r+1}","type":"content","url":"/math/unipoly-div#complexity-analysis","position":13},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Fast Polynomial Division Algorithm"},"type":"lvl2","url":"/math/unipoly-div#fast-polynomial-division-algorithm","position":14},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Fast Polynomial Division Algorithm"},"content":"Now that we know how to calculate a polynomial’s multiplicative inverse, let’s outline the steps for the polynomial division algorithm. Given two polynomials f(X), g(X)\\in F[X], where f has degree n-1, g has degree m-1, and n\\geq m, the algorithm returns the quotient polynomial q of degree n-m and the remainder polynomial r of degree strictly less than m-1, satisfying:f(X) = q(X)\\cdot g(X) + r(X)\n\nStep 1: Check that the highest degree coefficients of parameters f(X) and g(X) are non-zero, then calculate their degree difference, denoted as d=\\deg(f)-\\deg(g), assuming \\deg(f)\\geq\\deg(g).\n\nStep 2: Calculate the reversed polynomials of f(X) and g(X), denoted as \\mathsf{rev}_n(f) and \\mathsf{rev}_m(g), where \\deg(f)=n-1, \\deg(g)=m-1.\n\nStep 3: Calculate the multiplicative inverse of \\mathsf{rev}_m(g) in the power series ring F[[X]] with precision X^{d+1}, denote the result as \\tilde{g}_d=\\mathsf{rev}_m(g)^{-1}.\n\nStep 4: Calculate q^* \\equiv \\mathsf{rev}_m(f)\\cdot \\tilde{g}_d \\mod X^{d+1}, denoted as \\mathsf{rev}_m(q).\n\nStep 5: Calculate \\mathsf{rev}_{n-m}(q^*)=\\mathsf{rev}_{n-m}(\\mathsf{rev}_{n-m}(q))=q(X).\n\nStep 6: Calculate the remainder polynomial r(X) = f(X) - q(X)\\cdot g(X).\n\nHere’s the Python code implementation, followed by complexity analysis:def poly_div_rem(f: list[F], g: list[F]):\n    assert(f[-1] != Fp(0))\n    assert(g[-1] != Fp(0))\n    f_deg = len(f) - 1\n    g_deg = len(g) - 1\n    if f_deg < g_deg:\n        return [Fp(0)], f\n    \n    d = f_deg - g_deg\n    \n    rev_g = g[::-1]\n    rev_f = f[::-1]\n\n    rev_g_inv_prec_d_plus_1 = poly_inverse_rev1(rev_g, d+1)\n\n    q = poly_mul(rev_f[:d+1], rev_g_inv_prec_d_plus_1)\n    q = q[:d+1]\n\n    q.reverse()\n\n    r = poly_sub(f, poly_mul(q,g))\n    remove_leading_zeros(r)\n    return q, r","type":"content","url":"/math/unipoly-div#fast-polynomial-division-algorithm","position":15},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl3":"Complexity Analysis","lvl2":"Fast Polynomial Division Algorithm"},"type":"lvl3","url":"/math/unipoly-div#complexity-analysis-1","position":16},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl3":"Complexity Analysis","lvl2":"Fast Polynomial Division Algorithm"},"content":"In step 3, 3M(l) + 2l polynomial multiplication operations are needed; step 4 requires one polynomial multiplication with complexity M(l); step 6 requires one polynomial multiplication with complexity M(l) and one polynomial subtraction with complexity n. Thus, the total time complexity is:5M(l) + 2l + n","type":"content","url":"/math/unipoly-div#complexity-analysis-1","position":17},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Conclusion"},"type":"lvl2","url":"/math/unipoly-div#conclusion","position":18},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"Conclusion"},"content":"This is a beautiful algorithm that reduces the time complexity of polynomial division from O(l^2) to O(l\\log{l}). Newton’s method is a classic and powerful technique; [GG13] has more examples of algorithms using Newton iteration, recommended for readers interested in deeper exploration.","type":"content","url":"/math/unipoly-div#conclusion","position":19},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"References"},"type":"lvl2","url":"/math/unipoly-div#references","position":20},{"hierarchy":{"lvl1":"Fast Polynomial Division Based on Newton Iteration","lvl2":"References"},"content":"[GG13] Von Zur Gathen, Joachim, and Jürgen Gerhard. Modern computer algebra. Cambridge university press, 2003.\n\n[CC11] Zhengjun Cao, Hanyue Cao. Note on fast division algorithm for polynomials using Newton iteration. 2011. \n\nhttps://​arxiv​.org​/pdf​/1112​.4014\n\nhttps://​cs​.uwaterloo​.ca​/​~r5olivei​/courses​/2021​-winter​-cs487​/lec5​-ref​.pdf\n\nhttps://​math​.stackexchange​.com​/questions​/710252​/multiplicative​-inverse​-of​-a​-power​-series","type":"content","url":"/math/unipoly-div#references","position":21},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size"},"type":"lvl1","url":"/mercury/mercury-01","position":0},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nMercury [EG25] is a multivariate linear polynomial commitment scheme based on KZG10, where the Prover proves to the Verifier that a multivariate linear polynomial \\tilde{f}(X_0,X_1,\\ldots,X_{n-1}) evaluates to v at a public point \\vec{u} = (u_0,u_1,\\ldots,u_{n-1}). Let N = 2^n, which represents the size of \\tilde{f}. Compared to other KZG10-based schemes like ph23 [PH23], zeromorph [BCHO23], and gemini [KT23], Mercury achieves constant proof size while maintaining the Prover’s linear O(N) finite field operations, rather than the logarithmic O(\\log N) size of other approaches. A concurrent work, SamaritanPCS [GPS25], also achieves similar performance. These two protocols share similar ideas and both represent significant breakthroughs in pairing-based multivariate linear polynomial commitment schemes. This series of articles will explain in detail how Mercury achieves this.\n\nMercury’s ability to achieve constant proof size stems from its insight into the relationship between univariate polynomial decomposition and multivariate linear polynomial evaluation. It can transform the evaluation of a univariate polynomial at a random point into the evaluation of a multivariate linear polynomial at a specific point, which can be converted into an inner product proof with constant proof size, while the univariate polynomial decomposition proof also requires only constant proof size.\n\nMercury’s overall approach shares some similarities with the Hyrax [WTSTW16] protocol. It first arranges the N values of \\tilde{f} on the boolean hypercube \\mathbf{B} = \\{0,1\\}^n into a \\sqrt{N} \\times \\sqrt{N} matrix, then “flattens” this matrix column by column. Let b = \\sqrt{N}, t = \\log b. This “flattening” operation is equivalent to first substituting the first half of values from \\vec{u} and summing, which means computing \\tilde{h}(X_t, \\ldots, X_{n - 1}):=\\tilde{f}(u_0, \\ldots, u_{t - 1}, X_t, \\ldots, X_{n - 1}), and then computing \\tilde{h}(u_t, \\ldots, u_{n - 1}) = \\tilde{f}(u_0, \\ldots, u_{t - 1}, u_t, \\ldots, u_{n - 1}) to prove it equals v. This two-part division has the advantage of reducing the computation scale from N to \\sqrt{N}. Some of the Prover’s computation complexity was originally N \\log N, but with the reduced scale, the complexity becomes at most \\sqrt{N} \\log \\sqrt{N} = O(N), which is an important reason why Mercury maintains the Prover’s linear O(N) complexity.","type":"content","url":"/mercury/mercury-01","position":1},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Representation of Multivariate Linear Polynomials"},"type":"lvl2","url":"/mercury/mercury-01#representation-of-multivariate-linear-polynomials","position":2},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Representation of Multivariate Linear Polynomials"},"content":"A multivariate linear polynomial \\tilde{f}(X_0, X_1,\\ldots, X_{n-1}) can be represented by its values on the boolean hypercube \\mathbf{B}_n = \\{0,1\\}^n:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) = \\sum_{i=0}^{2^n - 1} f_i \\cdot \\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1}))\n\nwhere \\mathsf{bits}(i) = (i_0, i_1, \\ldots, i_{n-1}) is the binary representation of i, with i_0 being the least significant bit, satisfying i = \\sum_{j = 0}^{n - 1}i_j \\cdot 2^j. \\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1})) can be thought of as a Lagrange interpolation function on \\mathbf{B} = \\{0,1\\}^n, with the specific expression:\\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1})) = \\prod_{j = 0}^{n-1} ((1- i_j)(1- X_j) + i_j \\cdot X_j)\n\nWhen (X_0, X_1, \\ldots, X_{n-1}) \\in \\mathbf{B}_n, if \\mathsf{bits}(i) = (X_0, X_1, \\ldots, X_{n-1}), then each component of these two vectors is equal, so (1- i_j)(1- X_j) + i_j \\cdot X_j = 1, and the result of the \\tilde{eq} function is 1. When \\mathsf{bits}(i) \\neq (X_0, X_1, \\ldots, X_{n-1}), the result of the \\tilde{eq} function is 0.\n\nSince the \\tilde{eq} function is actually a product of n terms, and products have associative property, we can decompose the \\tilde{eq} function. Let n = 2 \\cdot t, b = 2^t = \\sqrt{N}. Divide the vector \\mathsf{bits}(i) into two equal parts, \\mathsf{bits}(i) = ((i_0,\\ldots,i_{t-1}), (i_t,\\ldots,i_{n-1})), and similarly divide (X_0, X_1, \\ldots, X_{n-1}) into two parts, (X_0, X_1, \\ldots, X_{n-1}) = ((X_0, \\ldots, X_{t-1}), (X_t, \\ldots, X_{n-1})) := (\\vec{X}_1, \\vec{X}_2). Then:\\begin{align}\n\\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1})) & = \\prod_{j = 0}^{n-1} ((1- i_j)(1- X_j) + i_j \\cdot X_j) \\\\\n & = \\left(\\prod_{j = 0}^{t-1} ((1- i_j)(1- X_j) + i_j \\cdot X_j) \\right)  \\cdot \\left(\\prod_{j = t}^{n-1} ((1- i_j)(1- X_j) + i_j \\cdot X_j) \\right)  \\\\\n & = \\tilde{eq}((i_0,\\ldots,i_{t-1}),\\vec{X}_1) \\cdot \\tilde{eq}((i_t,\\ldots,i_{n-1}),\\vec{X}_2)\n\\end{align}\n\nBecause the \\tilde{eq} function has this decomposition property, we can more conveniently decompose the evaluation of \\tilde{f}.","type":"content","url":"/mercury/mercury-01#representation-of-multivariate-linear-polynomials","position":3},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Decomposition"},"type":"lvl2","url":"/mercury/mercury-01#decomposition","position":4},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Decomposition"},"content":"Now we want to prove that the value of \\tilde{f} at point \\vec{u} = (u_0, u_1, \\ldots, u_{n-1}) is v, i.e., prove:\\tilde{f}(u_0,u_1,\\ldots, u_{n-1}) = \\sum_{i=0}^{2^n - 1} f_i \\cdot \\tilde{eq}(\\mathsf{bits}(i),(u_0, u_1, \\ldots, u_{n-1})) = v \\tag{1}\n\nThis involves summing N terms, which we’ll divide into two calculation steps.\n\nWe split the variable \\vec{u} = (\\vec{u}_1, \\vec{u}_2) into two equal-length vectors, \\vec{u}_1 = (u_0,\\ldots,u_{t-1}), \\vec{u_2} = (u_t,\\ldots,u_{n - 1}). We also divide the values of \\tilde{f} on \\mathbf{B}_n, (f_0,f_1, \\ldots, f_{2^n - 1}), into b groups, representing them with two subscripts:(f_0,f_1, \\ldots, f_{2^n - 1}) = (f_{0,0}, f_{0,1}, \\ldots, f_{0,b-1}, \\ldots, f_{b-1,0}, \\ldots, f_{b-1,b-1})\n\nArranged in a matrix form:M_f = \\begin{bmatrix}\n f_{0,0} &  f_{0,1}   & \\cdots  & f_{0,b-1} \\\\\n f_{1,0} &  f_{1,1}   & \\cdots  & f_{1,b-1} \\\\\n \\vdots &  \\vdots   & \\  & \\vdots  \\\\\nf_{b-1,0} &  f_{b-1,1}   & \\cdots  & f_{b-1,b-1}\n\\end{bmatrix}\n\nBased on the previously introduced decomposition of the \\tilde{eq} function, we get:\\begin{align}\n\\tilde{f}(\\vec{u}_1, \\vec{u}_2)  & = \\sum_{i=0}^{2^n - 1} f_i \\cdot \\tilde{eq}(\\mathsf{bits}(i),(\\vec{u}_1, \\vec{u}_2)) \\\\\n & = \\sum_{i=0}^{2^t - 1} \\sum_{j=0}^{2^t - 1} f_{i,j} \\cdot \\tilde{eq}((\\mathsf{bits}(j),\\mathsf{bits}(i)),(\\vec{u}_1, \\vec{u}_2))\\\\\n & = \\sum_{i=0}^{2^t - 1} \\sum_{j=0}^{2^t - 1} f_{i,j} \\cdot \\tilde{eq}(\\mathsf{bits}(j),\\vec{u}_1) \\cdot \\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_2)  \\\\\n & = \\sum_{i=0}^{2^t - 1} \\left(\\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_2)  \\cdot\\left(\\sum_{j=0}^{2^t - 1} f_{i,j} \\cdot \\tilde{eq}(\\mathsf{bits}(j),\\vec{u}_1) \\right) \\right)\n\\end{align}\n\nThis is more intuitive in matrix form:\\begin{align}\n& \\tilde{f}(\\vec{u}_1, \\vec{u}_2)  \\\\\n& =\\begin{bmatrix}\n\\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_2)  & \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_2)  & \\cdots  & \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_2) \n\\end{bmatrix}\n\\begin{bmatrix}\n f_{0,0} &  f_{0,1}   & \\cdots  & f_{0,b-1} \\\\\n f_{1,0} &  f_{1,1}   & \\cdots  & f_{1,b-1} \\\\\n \\vdots &  \\vdots   & \\  & \\vdots  \\\\\nf_{b-1,0} &  f_{b-1,1}   & \\cdots  & f_{b-1,b-1}\n\\end{bmatrix} \n\\begin{bmatrix}\n\\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_1)   \\\\\n \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_1)   \\\\\n\\cdots   \\\\\n\\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1) \n\\end{bmatrix} \\\\\n& \\stackrel{\\triangle}{=}  \\vec{v}_2^{\\intercal} \\cdot (M_f \\cdot \\vec{v}_1)\n\\end{align}\n\nFor example, with n = 2, the decomposition process is shown in the following figure:\n\nTo prove \\tilde{f}(\\vec{u}_1, \\vec{u}_2) = v, we can divide it into two parts:\n\nProve M_f \\cdot \\vec{v}_1 = \\vec{b}, corresponding to first calculating a multivariate linear polynomial \\tilde{h}(\\vec{X}_2) := \\tilde{f}(\\vec{u}_1, \\vec{X}_2)\n\nProve \\vec{v}_2^{\\intercal} \\cdot \\vec{b} = v, corresponding to calculating \\tilde{h}(\\vec{u}_2) = \\tilde{f}(\\vec{u}_1, \\vec{u_2}) and proving its result is v.\n\nSo far, we have transformed the summation of N terms in equation (1) into two calculation steps: first substituting \\vec{u}_1 for partial summation, then substituting \\vec{u_2} to get the final result v. Next, we’ll introduce the conversion of multivariate linear polynomials to univariate polynomials, and then use the KZG10 commitment scheme for univariate polynomials to conduct the proof.","type":"content","url":"/mercury/mercury-01#decomposition","position":5},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"From Multivariate Linear Polynomials to Univariate Polynomials"},"type":"lvl2","url":"/mercury/mercury-01#from-multivariate-linear-polynomials-to-univariate-polynomials","position":6},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"From Multivariate Linear Polynomials to Univariate Polynomials"},"content":"For a multivariate linear polynomial:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) = \\sum_{i=0}^{2^n - 1} f_i \\cdot \\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1}))\n\nWe directly use its values f_i on the boolean hypercube \\mathbf{B}_n as the coefficients of a univariate polynomial, resulting in:f(X) = \\sum_{i = 0}^{2^n - 1} f_i \\cdot X^i\n\nFor any multivariate linear polynomial, we convert it to a univariate polynomial in this way, by directly using the values of the multivariate linear polynomial on the boolean hypercube as the coefficients of the univariate polynomial.\n\nUsing n = 2 as an example, the conversion from multivariate linear polynomial to univariate polynomial can be understood as simply changing the basis. The basis for the multivariate linear polynomial is the Lagrange basis, (\\tilde{eq}(\\mathsf{bits}(0), \\vec{X}), \\tilde{eq}(\\mathsf{bits}(1), \\vec{X}), \\tilde{eq}(\\mathsf{bits}(2), \\vec{X}), \\tilde{eq}(\\mathsf{bits}(3), \\vec{X})), while the basis for the univariate polynomial is (1, X, X^2, X^3).\n\nFor the previously mentioned \\tilde{h}(\\vec{X}_2) = \\tilde{f}(\\vec{u}_1, \\vec{X}_2), expressed in matrix form:\\begin{align}\n& \\tilde{h}(\\vec{X}_2) = \\tilde{f}(\\vec{u}_1, \\vec{X}_2)  \\\\\n& =\\begin{bmatrix}\n\\tilde{eq}(\\mathsf{bits}(0), \\vec{X}_2)  & \\tilde{eq}(\\mathsf{bits}(1), \\vec{X}_2)  & \\cdots  & \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{X}_2) \n\\end{bmatrix}\n\\begin{bmatrix}\n f_{0,0} &  f_{0,1}   & \\cdots  & f_{0,b-1} \\\\\n f_{1,0} &  f_{1,1}   & \\cdots  & f_{1,b-1} \\\\\n \\vdots &  \\vdots   & \\  & \\vdots  \\\\\nf_{b-1,0} &  f_{b-1,1}   & \\cdots  & f_{b-1,b-1}\n\\end{bmatrix} \n\\begin{bmatrix}\n\\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_1)   \\\\\n \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_1)   \\\\\n\\cdots   \\\\\n\\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1) \n\\end{bmatrix} \\\\\n& = \\begin{bmatrix}\n\\tilde{eq}(\\mathsf{bits}(0), \\vec{X}_2)  & \\tilde{eq}(\\mathsf{bits}(1), \\vec{X}_2)  & \\cdots  & \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{X}_2) \n\\end{bmatrix}\n\\begin{bmatrix}\n f_{0,0} \\cdot \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_1) + f_{0,1} \\cdot \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_1) + \\ldots +  f_{0,b-1} \\cdot \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1)\\\\\n f_{1,0} \\cdot \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_1) + f_{1,1} \\cdot \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_1) + \\ldots +  f_{1,b-1} \\cdot \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1) \\\\\n \\vdots   \\\\\nf_{b-1,0} \\cdot \\tilde{eq}(\\mathsf{bits}(0), \\vec{u}_1) + f_{b-1,1} \\cdot \\tilde{eq}(\\mathsf{bits}(1), \\vec{u}_1) + \\ldots +  f_{b-1,b-1} \\cdot \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1)\n\\end{bmatrix}  \n\\end{align}\n\nWe can see that each item in the right column vector is actually the value of \\tilde{h}(\\vec{X}_2) on the boolean hypercube \\mathbf{B}_t = \\{0,1\\}^t, corresponding to the univariate polynomial h(X). The i-th row of the right column vector is the coefficient before the X^i term in h(X).h(X) = \\sum_{i = 0}^{b - 1} \\left(\\left( \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot \\tilde{eq}(\\mathsf{bits}(j), \\vec{u}_1)\\right) \\cdot X^i \\right)\n\nWe can see that the coefficient of each term in h(X), \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot \\tilde{eq}(\\mathsf{bits}(j), \\vec{u}_1), is also in the form of a number multiplied by \\tilde{eq}. We can also use f_{i,j} as coefficients of univariate polynomials, with each column of the M_f matrix serving as coefficients for a univariate polynomial:\\begin{align}\n & f_0(X) = f_{0,0} + f_{1,0}  X + \\ldots + f_{b-1, 0} X^{b-1} \\\\\n & f_1(X) = f_{0,1} + f_{1,1}  X + \\ldots + f_{b-1, 1} X^{b-1} \\\\\n & \\qquad \\qquad \\qquad \\qquad \\ldots \\\\\n & f_{b-1}(X) = f_{0,b-1} + f_{1,b-1}  X + \\ldots + f_{b-1, b-1} X^{b-1}\n\\end{align}\n\nThe advantage of using each column of M_f as coefficients for univariate polynomials is that elements in the i-th row of the M_f matrix correspond exactly to the coefficients of X^i. For example, for the first row of matrix M_f, its elements (f_{1,0}, f_{1,1}, \\ldots, f_{1,b-1}) are the coefficients of X^1 in f_0(X), f_1(X), \\ldots, f_{b-1}(X) respectively.\n\nf_i(X) can also be seen as a decomposition of f(X):\\begin{align}\nf(X)  & = \\sum_{i = 0}^{2^n - 1} f_i \\cdot X^i = \\sum_{i = 0}^{b - 1} \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot X^{i \\cdot b + j} \\\\\n & = \\sum_{i = 0}^{b - 1} \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot (X^{b})^i \\cdot X^{j}  \\\\\n& = \\sum_{j = 0}^{b - 1} \\sum_{i = 0}^{b - 1} f_{i,j} \\cdot (X^{b})^i \\cdot X^{j} \\\\\n& = \\sum_{j = 0}^{b - 1} f_j(X^b) \\cdot X^j \\\\\n& = \\sum_{i = 0}^{b - 1} f_i(X^b) \\cdot X^i\n\\end{align}\n\nThat is, f(X) is decomposed into the sum of b polynomials:f(X) = f_0(X^b) + X \\cdot f_1(X^b) + \\ldots + X^{b-1} \\cdot f_{b-1}(X^b) \\tag{2}\n\nThis decomposition of the univariate polynomial is consistent with the decomposition of the multivariate polynomial evaluation \\tilde{f}(\\vec{u}_1,\\vec{u}_2) discussed in the previous section. The multivariate linear polynomial utilizes the decomposability of the \\tilde{eq} function, while X^i in the univariate polynomial can also be decomposed. Using n = 2 as an example, comparing the two:\n\nNow h(X) can be expressed as:\\begin{align}\nh(X)  & = \\sum_{i = 0}^{b - 1} \\left(\\left( \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot \\tilde{eq}(\\mathsf{bits}(j), \\vec{u}_1)\\right) \\cdot X^i \\right) \\\\\n & = \\sum_{j = 0}^{b - 1} \\left(\\left( \\sum_{i = 0}^{b - 1} f_{i,j} \\cdot X^i \\right) \\cdot \\tilde{eq}(\\mathsf{bits}(j), \\vec{u}_1) \\right) \\\\\n & = \\sum_{j = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(j), \\vec{u}_1) \\cdot f_j(X) \\\\\n& = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(X)\n\\end{align}\n\nUsing n = 2 as an example, the following figure shows the decomposition process of h(X):\n\nSo h(X)= \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(X) corresponds to the multivariate linear polynomial \\tilde{h}(\\vec{X}) = \\tilde{f}(\\vec{u}_1, \\vec{X}). This is equivalent to replacing the first t variables in \\tilde{f} at once. We can commit to the univariate polynomial \\mathsf{cm}(h(X)) corresponding to \\tilde{f}(\\vec{u}_1, \\vec{X}), and then prove \\tilde{h}(\\vec{u}_2) = v to complete the proof. This corresponds to the two parts of the proof discussed in the previous section:\n\nProve M_f \\cdot \\vec{v}_1 = \\vec{b}, corresponding to first calculating a multivariate linear polynomial \\tilde{h}(\\vec{X}_2) := \\tilde{f}(\\vec{u}_1, \\vec{X}_2)\n\nProve \\vec{v}_2^{\\intercal} \\cdot \\vec{b} = v, corresponding to calculating \\tilde{h}(\\vec{u}_2) = \\tilde{f}(\\vec{u}_1, \\vec{u_2}) and proving its result is v.\n\nPart 2 is actually proving the inner product of two vectors, which can be transformed into an inner product proof.\n\nIs this proof complete? How do we achieve constant-sized proof? There’s a key aspect that needs to be proven: the Verifier must believe that the construction of h(X) is correct, that is:h(X)= \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(X) \\tag{3}\n\nThe Prover needs to prove to the Verifier that h(X) is indeed constructed this way, not just arbitrarily sent. To prove equation (3) is correctly constructed, the Verifier can randomly issue a challenge value r, and the Prover must prove:h(r^b)= \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(r^b) \\tag{4}\n\nf_i(r^b) is related to the values of f. According to the decomposition formula (2) of f(X):f(X) = f_0(X^b) + X \\cdot f_1(X^b) + \\ldots + X^{b-1} \\cdot f_{b-1}(X^b) ,\n\nLet \\omega^b = 1, then:\\begin{align}\n & f(r) = f_0(r^b) + r \\cdot f_1(r^b) + \\ldots + r^{b-1} \\cdot f_{b-1}(r^b) \\\\\n & f(\\omega r) = f_0(r^b) + \\omega r \\cdot f_1(r^b) + \\ldots + (\\omega r)^{b-1} \\cdot f_{b-1}(r^b)  \\\\\n & \\ldots \\\\\n & f(\\omega^{b-1} r) = f_0(r^b) + \\omega^{b-1} r \\cdot f_1(r^b) + \\ldots + (\\omega^{b-1} r)^{b-1} \\cdot f_{b-1}(r^b)\n\\end{align}\n\nThis is equivalent to a linear system of equations with b unknowns f_i(r^b) and b equations. By solving this system, we can calculate the values of f_i(r^b) from \\{f(r), f(\\omega r), \\ldots, f(\\omega^{b - 1} r) \\}. The Prover could send h(r^b) and \\{f(r), f(\\omega r), \\ldots, f(\\omega^{b - 1} r) \\} along with corresponding opening proofs, letting the Verifier calculate the values of f_i(r^b) themselves to verify if equation (4) holds. The problem with this approach is that the proof size would be O(b), not constant.\n\nIs there a method that can achieve both constant proof size and prove the correctness of equation (4)? Mercury cleverly transforms the proof of evaluating a univariate polynomial h(r^b) in equation (4) into a proof of evaluating a multivariate linear polynomial at a point.","type":"content","url":"/mercury/mercury-01#from-multivariate-linear-polynomials-to-univariate-polynomials","position":7},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Achieving Constant Proof Size"},"type":"lvl2","url":"/mercury/mercury-01#achieving-constant-proof-size","position":8},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"Achieving Constant Proof Size"},"content":"The Prover’s goal is to prove with constant proof size:h(r^b)= \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(r^b) \\tag{4}\n\nLet’s set \\alpha = r^b.\n\nDefine g(X) = f(X) \\mod{X^b - \\alpha}, with the quotient polynomial q(X), then:f(X) = q(X) \\cdot (X^b - \\alpha) + g(X) \\tag{5}\n\nSubstituting the condition X^b = \\alpha into this equation, we get:g(X) = f(X) = \\sum_{i = 0}^{b - 1} f_i(X^b) \\cdot X^i = \\sum_{i = 0}^{b - 1} f_i(\\alpha) \\cdot X^i\n\nWe can see that the coefficients of g(X) are f_i(\\alpha), and the corresponding multivariate linear polynomial would be:\\tilde{g}(X_0, \\ldots , X_{b-1}) = \\sum_{i = 0}^{b - 1} f_i(\\alpha) \\cdot \\tilde{eq}(\\mathsf{bits}(i), (X_0, \\ldots , X_{b-1}))\n\nSo equation (4) is transformed into:h(\\alpha)= \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(\\alpha) = \\tilde{g}(\\vec{u}_1)\n\nTo prove equation (4) is correct now means proving:h(\\alpha) = \\tilde{g}(\\vec{u}_1) \\tag{6}\n\nUsing n = 2 as an example, the proof transformation process is shown in the following figure:\n\nThis transforms the evaluation of a univariate polynomial h(\\alpha) into the evaluation of a multivariate linear polynomial at a point \\tilde{g}(\\vec{u}_1). Now we need to prove that the construction of g(X) corresponding to \\tilde{g} is correct, i.e., that equation (5) holds. The Prover can commit to q(X) and g(X), and the Verifier can select a random point \\zeta to verify that equation (5) holds. This only requires constant proof size, which is the core of how the Mercury protocol achieves constant proof size.\n\nAdditionally, to prevent the Prover from cheating, we need to restrict \\deg(g) < b.\n\nThus, the two proofs mentioned in the previous section:\n\nProve M_f \\cdot \\vec{v}_1 = \\vec{b}, corresponding to first calculating a multivariate linear polynomial \\tilde{h}(\\vec{X}_2) := \\tilde{f}(\\vec{u}_1, \\vec{X}_2)\n\nProve \\vec{v}_2^{\\intercal} \\cdot \\vec{b} = v, corresponding to calculating \\tilde{h}(\\vec{u}_2) = \\tilde{f}(\\vec{u}_1, \\vec{u_2}) and proving its result is v.\n\nAre transformed into four proofs:\n\nf(X) = q(X) \\cdot (X^b - \\alpha) + g(X)\n\n\\deg(g) < b\n\n\\tilde{g}(\\vec{u}_1) = h(\\alpha)\n\n\\tilde{h}(\\vec{u}_2) = v\n\nFor the first proof, the Prover can first send commitments to q(X) and g(X), then the Verifier sends a random point \\zeta, asking the Prover to open at that point by sending q(\\zeta) and g(\\zeta). The Prover just needs to prove that the quotient polynomial\\frac{f(X) - q(\\zeta) \\cdot (\\zeta^b - \\alpha) + g(\\zeta)}{X - \\zeta}\n\nexists, which proves that the equation in item 1 holds. This proof requires only constant proof size.\n\nFor the second item, it’s a degree bound proof, which can also be implemented with constant proof size.\n\nFor the third and fourth items, both involve proving the evaluation of multivariate linear polynomials with b variables at certain points, which can be transformed into inner product proofs. For example, for the multivariate linear polynomial \\tilde{g}:\\begin{align}\n\\tilde{g}(\\vec{u}_1)  & = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(\\alpha)  \\\\\n\\end{align}\n\nThis is actually calculating the inner product of vectors \\vec{a}_1 = (\\tilde{eq}(\\mathsf{bits}(0),\\vec{u}_1),\\ldots, \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1) and \\vec{b}_1 = (f_0(\\alpha),\\ldots, f_{b-1}(\\alpha)). A similar approach applies to \\tilde{h}(\\vec{u}_2). Thus, the third and fourth items can be transformed into two inner product proofs, which can be aggregated into one proof using random numbers, also achieving constant proof size.\n\nTherefore, all four parts of Mercury’s proof have constant proof size. Since the length of the multivariate linear polynomials in the third and fourth items is b, even if some calculations require O(b \\log b) complexity for the Prover, this is still within O(N) complexity, maintaining the Prover’s linear computational complexity.\n\nThis article has explained how Mercury achieves constant proof size while maintaining the Prover’s linear complexity. In the next article, we will detail how Mercury proves these four items.","type":"content","url":"/mercury/mercury-01#achieving-constant-proof-size","position":9},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"References"},"type":"lvl2","url":"/mercury/mercury-01#references","position":10},{"hierarchy":{"lvl1":"Mercury Notes: Implementing Constant Proof Size","lvl2":"References"},"content":"[EG25] Eagen, Liam, and Ariel Gabizon. “MERCURY: A multilinear Polynomial Commitment Scheme with constant proof size and no prover FFTs.” Cryptology ePrint Archive (2025). \n\nhttps://​eprint​.iacr​.org​/2025​/385\n\n[GPS25] Ganesh, Chaya, Sikhar Patranabis, and Nitin Singh. “Samaritan: Linear-time Prover SNARK from New Multilinear Polynomial Commitments.” Cryptology ePrint Archive (2025). \n\nhttps://​eprint​.iacr​.org​/2025​/419\n\n[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284\n\n[BCHO23] Bootle, Jonathan, Alessandro Chiesa, Yuncong Hu, and Michele Orru. “Gemini: Elastic SNARKs for diverse environments.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pp. 427-457. Cham: Springer International Publishing, 2022. \n\nhttps://​eprint​.iacr​.org​/2022​/420\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[WTSTW16] Riad S. Wahby, Ioanna Tzialla, abhi shelat, Justin Thaler, and Michael Walfish. “Doubly-efficient zkSNARKs without trusted setup.”  In 2018 IEEE Symposium on Security and Privacy (SP), pp. 926-943. IEEE, 2018.  \n\nhttps://​eprint​.iacr​.org​/2016​/263","type":"content","url":"/mercury/mercury-01#references","position":11},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG"},"type":"lvl1","url":"/mercury/mercury-02","position":0},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG"},"content":"Jade Xie  \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nIn the previous article, we introduced the fundamental reason why the Mercury protocol can achieve constant proof size while maintaining the Prover’s linear complexity. We also explained that proving the value of a multivariate linear polynomial \\tilde{f} at a point (\\vec{u}_1, \\vec{u}_2) is v can be converted into the following four proofs:\n\nThis article will detail these four proofs performed by Mercury and provide the complete protocol description for Mercury’s integration with the univariate polynomial commitment scheme KZG.","type":"content","url":"/mercury/mercury-02","position":1},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Inner Product Proof"},"type":"lvl2","url":"/mercury/mercury-02#inner-product-proof","position":2},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Inner Product Proof"},"content":"First, let’s look at proving the third item \\tilde{g}(\\vec{u}_1) = h(\\alpha) and the fourth item \\tilde{h}(\\vec{u}_2) = v, which essentially requires proving two inner products.\\begin{align}\n\\tilde{g}(\\vec{u}_1)  & = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(\\alpha)  \\\\\n\\end{align}\n\nLet \\vec{a}_1 = (\\tilde{eq}(\\mathsf{bits}(0),\\vec{u}_1),\\ldots, \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_1)) and \\vec{b}_1 = (f_0(\\alpha),\\ldots, f_{b-1}(\\alpha)), then we need to prove \\tilde{g}_1(\\vec{u_1}) = \\langle \\vec{a}_1, \\vec{b}_1\\rangle, which is the inner product of two vectors.\\tilde{h}(\\vec{u}_2) = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_2) \\cdot h_i\n\nWhere h_i represents the coefficients of h(X), satisfying h(X) = \\sum_{i = 0}^{b-1} h_i \\cdot X^i. Let \\vec{a}_2 = (\\tilde{eq}(\\mathsf{bits}(0),\\vec{u}_2),\\ldots, \\tilde{eq}(\\mathsf{bits}(b-1), \\vec{u}_2)) and \\vec{b}_2 = (h_0,\\ldots, h_{b-1}), then we need to prove \\tilde{h}(\\vec{u_2}) = \\langle \\vec{a}_2, \\vec{b}_2\\rangle, which is the inner product of two vectors.\n\nWe can observe that in both inner products, the components of the first vector \\vec{a}_i are all in the form of \\tilde{eq}(\\mathsf{bits}(j),\\vec{u}_i), and the second vector \\vec{b}_i consists of coefficients of a univariate polynomial: \\vec{b}_1 contains the coefficients of g(X), and \\vec{b}_2 contains the coefficients of h(X). Similarly, we can represent the components of vector \\vec{a}_i as coefficients of a univariate polynomial by defining:P_{u_1}(X) = \\sum_{i = 0}^{b-1} \\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_1) \\cdot X^iP_{u_2}(X) = \\sum_{i = 0}^{b-1} \\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_2) \\cdot X^i\n\nWe define the inner product of two univariate polynomials as the inner product of their coefficient vectors. For simplicity, we’ll only consider cases where both polynomials have the same degree. Then the two inner products we need to prove can be transformed into:\\langle P_{u_1}, g\\rangle = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(\\alpha) = \\tilde{g}(\\vec{u}_1) = h(\\alpha) \\tag{1}\\langle P_{u_2}, h\\rangle = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_2) \\cdot h_i = \\tilde{h}(\\vec{u}_2) = v\\tag{2}\n\nLet’s first introduce how to prove the inner product of two polynomials of the same degree in the general case, then discuss how to apply this method to proving equations (1) and (2).\n\nGenerally, for two polynomials of degree less than n:a(X) = \\sum_{i = 0}^{n - 1} a_i \\cdot X^ib(X) = \\sum_{i = 0}^{n - 1} b_i \\cdot X^i\n\nWe want to prove that their inner product is:\\langle a, b \\rangle = \\sum_{i = 0}^{n - 1} a_i \\cdot b_i = c\n\nInner product proofs can use various methods, such as univariate sumcheck or grand sum methods. Here, we’ll introduce the method provided in the Mercury paper.\n\nFrom the expressions of a(X) and b(X), we can observe:\\begin{aligned}\na(X) b(1/X) & = (a_0 + a_1 X + \\ldots + a_{n - 1} X^{n - 1})(b_0 + b_1 X^{-1} + \\ldots + b_{n-1} X^{-(n - 1)}) \\\\\n& = {\\color{blue}{a_0b_0}} + {\\color{orange}a_0\\cdot (b_1 X^{-1} + \\ldots + b_{n-1} X^{-( n - 1)})} \\\\\n& \\quad + {\\color{red}a_1 b_0 \\cdot X} + {\\color{blue}a_1 b_1} + {\\color{orange}a_1 (b_2 X^{-1} + \\ldots + b_{n - 1}X^{-(n - 1) + 1})} \\\\\n& \\quad + {\\color{red} a_2b_0 \\cdot X^2 + a_2 b_1 X} + {\\color{blue} a_2b_2} + {\\color{orange} a_2 \\cdot (b_3 X^{-1} + \\ldots + b_{n - 1} X^{-(n - 1) + 2})} \\\\\n& \\quad + \\ldots \\\\\n& = {\\color{red} X \\cdot S_1(X)}  + {\\color{blue} a_0b_0 + a_1 b_1 + \\ldots + a_{n-1}b_{n-1}} + {\\color{orange} 1/X \\cdot S_2(1/X)}\n\\end{aligned}\n\nThe constant term of a(X)b(1/X) is exactly the inner product \\langle a, b \\rangle. Therefore, proving \\langle a, b \\rangle = c is equivalent to proving there exist S_1(X) and S_2(X) satisfying:a(X) b(1/X) = X \\cdot S_1(X) + c + 1/X \\cdot S_2(1/X) \\tag{3}\n\nIn this case, the Prover sends commitments to S_1(X) and S_2(X). The Verifier selects a random value \\zeta and asks the Prover to open a(\\zeta), b(1/\\zeta), S_1(\\zeta), S_2(1/\\zeta) and send the corresponding opening proofs. The Verifier can then use these values to verify if equation (3) holds, thereby completing the inner product proof with a constant-sized proof.\n\nThe construction in equation (3) requires sending commitments to two polynomials, S_1(X) and S_2(X). However, we can observe that the constant term of a(1/X)b(X) is also the inner product \\langle a, b \\rangle, which is symmetric to a(X)b(1/X). By substituting X with 1/X in equation (3), we get:a(1/X)b(X) = 1/X \\cdot S_1(1/X) + c + X \\cdot S_2(X) \\tag{4}\n\nAdding equations (3) and (4), we obtain:a(X) b(1/X) + a(1/X)b(X) = X \\cdot (S_1(X) + S_2(X)) + 2c + 1/X \\cdot (S_1(1/X) + S_2(1/X))\n\nLet S(X) := S_1(X) + S_2(X), then:a(X) b(1/X) + a(1/X)b(X) = X \\cdot S(X) + 2c + 1/X \\cdot S(1/X) \\tag{5}\n\nProving the inner product \\langle a, b \\rangle = c is now equivalent to proving there exists a polynomial S(X) satisfying equation (5). This optimization reduces the construction from committing to two polynomials S_1(X) and S_2(X) to just one polynomial S(X), thus decreasing the proof size.\n\nIf we have another inner product proof \\langle a', b' \\rangle = c' where a'(X) = \\sum_{i = 0}^{n-1} a'_i \\cdot X^i and b'(X) = \\sum_{i = 0}^{n-1} b'_i \\cdot X^i, we can aggregate these two inner product proofs into one. The Verifier can select a random number \\gamma \\stackrel{\\$}{\\leftarrow} \\mathbb{F}, and the Prover proves there exists a polynomial S(X) satisfying:a(X) b(1/X) + a(1/X)b(X) + \\gamma \\cdot (a'(X) b'(1/X) + a'(1/X)b'(X)) = X \\cdot S(X) + 2(c + \\gamma \\cdot c') + 1/X \\cdot S(1/X)\n\nReturning to the Mercury protocol, the two inner products we want to prove are:\\langle P_{u_1}, g\\rangle = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(\\alpha) = \\tilde{g}(\\vec{u}_1) = h(\\alpha) \\tag{6}\\langle P_{u_2}, h\\rangle = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_2) \\cdot h_i = \\tilde{h}(\\vec{u}_2) = v \\tag{7}\n\nUsing the inner product proof method described above, the Verifier sends a random number \\gamma, and these two inner product proofs can be batched into one. The Prover proves there exists a polynomial S(X) satisfying:\\begin{aligned}\ng(X) P_{u_1}(1/X) & + g(1/X) P_{u_1}(X) + \\gamma \\cdot (h(X) P_{u_2}(1/X) + h(1/X) P_{u_2}(X)) \\\\\n& = 2(h(\\alpha) + \\gamma \\cdot v) + X \\cdot S(X) + (1/X) \\cdot S(1/X)  \n\\end{aligned} \\tag{8}\n\nThe Prover first commits to g(X), h(X), S(X). The Verifier randomly selects an opening point \\zeta. The Prover sends the values g(\\zeta), g(1/\\zeta), h(\\zeta), h(1/\\zeta), S(\\zeta), S(1/\\zeta) and corresponding opening proofs. Since \\vec{u}_1 and \\vec{u}_2 are public, the Verifier can calculate P_{u_1}(X) and P_{u_2}(X) themselves and compute P_{u_1}(\\zeta), P_{u_1}(1/\\zeta), and P_{u_2}(\\zeta), P_{u_2}(1/\\zeta) to verify equation (3)'s correctness.\n\nWe now understand that proofs for items 3 and 4 can be transformed into two inner product proofs, which can be batched together using equation (3) for a constant-sized proof.","type":"content","url":"/mercury/mercury-02#inner-product-proof","position":3},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Correct Decomposition"},"type":"lvl2","url":"/mercury/mercury-02#correct-decomposition","position":4},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Correct Decomposition"},"content":"For the proof of the first item:\n\nf(X) = q(X) \\cdot (X^b - \\alpha) + g(X)\n\nThis can be directly interfaced with the univariate polynomial KZG commitment scheme. Since the Prover has already sent the value g(\\zeta) when proving items 3 and 4, we can verify the correctness of the first item at the same random point. The Prover constructs a quotient polynomial:H(X) = \\frac{f(X) - (\\zeta^b - \\alpha)q(X) - g(\\zeta)}{X - \\zeta}\n\nThe Prover commits to this quotient polynomial [H] = \\mathsf{cm}(H) and sends it to the Verifier. Since f(\\zeta) = (\\zeta^b - \\alpha) \\cdot q(\\zeta) - g(\\zeta), this quotient polynomial exists. Conversely, the existence of this quotient polynomial confirms that the first item’s construction is correct. After receiving \\mathsf{cm}(H), the Verifier can verify using pairing:e(\\mathsf{cm}(f) - (\\zeta^b - \\alpha) \\cdot [q] - g(\\zeta) \\cdot [1]_1 , [1]_2) \\stackrel{?}{=} e(\\mathsf{cm}(H), [x - \\zeta]_2)\n\nIn summary, to prove the correctness of the first item, the Prover needs to prove the existence of a quotient polynomial H(X), and the Verifier verifies the correctness of this quotient polynomial at a random point.","type":"content","url":"/mercury/mercury-02#correct-decomposition","position":5},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Degree Bound"},"type":"lvl2","url":"/mercury/mercury-02#degree-bound","position":6},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Degree Bound"},"content":"For the proof of the second item:\n\n\\deg(g) < b\n\nWe can also use the g(1/X) function. To prove \\deg(g(X)) < b, it’s equivalent to proving that:D(X) = X^{b - 1} \\cdot g(1/X) \\tag{9}\n\nis a polynomial. Assuming \\deg(g) = d:\\begin{aligned}\nD(X) & = X^{b-1} \\cdot g(1/X) \\\\\n& = X^{b - 1}(g_0  + g_1 \\cdot X^{-1} + \\ldots + g_{d} \\cdot X^{-d}) \\\\\n& = g_{d} \\cdot X^{b - d - 1} + \\ldots + g_1 \\cdot X^{b - 2} + g_0 \\cdot X^{b - 1}\n\\end{aligned}\n\nWe can see that only when d < b is the degree of the first term g_d \\cdot X^{b-d-1} non-negative (b-d-1 \\geq 0). Otherwise, this term would have a negative degree, which can’t form a polynomial. Assuming the random number selected in the KZG Setup phase is \\tau, the SRS is:\\{[1], [\\tau], \\ldots, [\\tau^{N-1}], [1]_2, [\\tau]_2\\}\n\nIt doesn’t include negative powers of \\tau. So only when D(X) is a polynomial can it be correctly committed, indicating that the degree of g(X) is less than b. The Prover first commits to polynomial D(X) and sends \\mathsf{cm}(D(X)) = [D]. To verify equation (9)'s correctness, the Verifier needs to select a random point to open values of D(X) and g(1/X) at that point. To reduce the proof size, since the value of g(1/\\zeta) was already sent during the inner product proof, we can choose to open at point \\zeta for the degree bound proof as well. The Prover sends the value of D(\\zeta) and its opening proof, and the Verifier verifies:D(\\zeta) \\stackrel{?}{=} \\zeta^{b-1} \\cdot g(1/\\zeta)\n\nThe proof for the second item is shown in the following diagram:\n\nNow we understand how to prove all four items mentioned earlier.","type":"content","url":"/mercury/mercury-02#degree-bound","position":7},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Aggregating Multiple Polynomials Opening Proofs at Multiple Points"},"type":"lvl2","url":"/mercury/mercury-02#aggregating-multiple-polynomials-opening-proofs-at-multiple-points","position":8},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Aggregating Multiple Polynomials Opening Proofs at Multiple Points"},"content":"The four items to be proven are:\n\nf(X) = q(X) \\cdot (X^b - \\alpha) + g(X)\n\n\\deg(g) < b\n\n\\tilde{g}(\\vec{u}_1) = h(\\alpha)\n\n\\tilde{h}(\\vec{u}_2) = v\n\nLet’s review the values of polynomials that the Prover sends during the process of proving these four items:\n\nThe values sent by the Prover are:g(\\zeta), g(1/\\zeta), h(\\alpha), h(\\zeta), h(1/\\zeta), S(\\zeta), S(1/\\zeta), D(\\zeta)\n\nThe Prover needs to use KZG10 to prove that these values are correct. For a value like g(\\zeta), the Prover can construct a quotient polynomial:q_{g_\\zeta} = \\frac{g(X) - g(\\zeta)}{X - \\zeta}\n\nThe Prover first commits to this quotient polynomial \\mathsf{cm}(q_{g_\\zeta}), and the Verifier can verify using a pairing operation on the elliptic curve:e(\\mathsf{cm}(g) - g(\\zeta) \\cdot [1]_1 , [1]_2) \\stackrel{?}{=} e(\\mathsf{cm}(q_{g_\\zeta}), [\\tau - \\zeta]_2)\n\nHowever, we have 8 values to prove here. If each value is proven as above, it would increase the proof size and the Verifier’s computational load. We observe that there are 4 different polynomials involved, and the opening points for each polynomial are not entirely consistent, making it inappropriate to use schemes for opening multiple polynomials at the same point. Section 4 of [BDFG20] provides a scheme that can prove openings of different polynomials at different points, with a proof size of just 2 points on the elliptic curve \\mathbb{G}_1. The Verifier only needs to compute two pairing operations on the elliptic curve. Let’s introduce this scheme.\n\nLet sets S_1, S_2, S_3, S_4 represent the sets of points where polynomials g(X), h(X), S(X), D(X) need to be opened:\\begin{align}\nS_1 = \\{\\zeta, 1/\\zeta\\},  & \\qquad S_2 = \\{\\alpha, \\zeta, 1/\\zeta\\} \\\\\nS_3 = \\{\\zeta, 1/\\zeta\\},  & \\qquad S_4 = \\{\\zeta\\}\n\\end{align}\n\nConstruct a set T that can include all elements in S_i:T = \\{\\zeta, 1/\\zeta, \\alpha\\}\n\nSo S_i \\subset T \\subset \\mathbb{F}.\n\nTaking set S_1 and polynomial g(X) as an example, the vanishing polynomials on sets S_1, T, T \\setminus S_1 are:\\begin{align}\n & Z_{S_1}(X) = (X - \\zeta)(X - 1/\\zeta) \\\\\n & Z_{T}(X) = (X - \\zeta)(X - 1 /\\zeta)(X -\\alpha) \\\\\n & Z_{T \\setminus S_{1}}(X) = X - \\alpha \n\\end{align}\n\nLet g^*(X) be the interpolation polynomial of (g(\\zeta), g(1/\\zeta)) on (\\zeta, 1/\\zeta), satisfying \\deg(g^*) = 1 and:g^*(\\zeta) = g(\\zeta), \\qquad g^*(1/\\zeta) = g(1/\\zeta)\n\nThen we can find that the vanishing polynomial Z_{S_1}(X) divides the polynomial g(X) - g^*(X), i.e., there exists a quotient polynomial:q_g(X) = \\frac{g(X) - g^*(X)}{Z_{S_1}(X)}\n\nSince:Z_{S_1}(X) = \\frac{Z_T(X)}{Z_{T \\setminus S_1}(X)}\n\nThen:q_g(X) = \\frac{Z_{T \\setminus S_1}(X)(g(X) - g^*(X))}{Z_T(X)}\n\nThis means that Z_{S_1}(X) dividing polynomial g(X) - g^*(X) is equivalent to Z_T(X) dividing polynomial Z_{T \\setminus S_1}(X)(g(X) - g^*(X)). The benefit of converting from Z_{S_1}(X) division to Z_{T}(X) division is that Z_T(X) is the same for all other polynomials h(X), S(X), D(X), allowing us to aggregate the proofs of opening different polynomials at different points. Let’s see the specific proof process.\n\nVerifier sends a random number \\beta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates interpolation polynomials:\n\ng^*(X) is the interpolation polynomial of (g(\\zeta), g(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nh^*(X) is the interpolation polynomial of (h(\\alpha), h(\\zeta), h(1/\\zeta)) on (\\alpha, \\zeta, 1/\\zeta)\n\nS^*(X) is the interpolation polynomial of (S(\\zeta), S(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nD^*(X) is the interpolation polynomial of (D(\\zeta)) on (\\zeta)\n\nProver calculates the polynomial:\\begin{align}\n m(X)  & = Z_{T \\setminus S_1}(X)(g(X) - g^*(X)) + \\beta \\cdot Z_{T \\setminus S_2}(X)(h(X) - h^*(X))  \\\\\n & \\quad + \\beta^2 \\cdot Z_{T \\setminus S_3}(X)(S(X) - S^*(X)) +  \\beta^3 \\cdot Z_{T \\setminus S_4}(X)(D(X) - D^*(X))\n\\end{align}\n\nProver calculates and sends the commitment to q_m(X) = m(X) / Z_T(X):C_{q_{m}} = [q_m(\\tau)]_1\n\nVerifier sends a random opening point z \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates the linearization polynomial:L(X) = m_z(X) - Z_T(z) \\cdot q_{\\mathsf{batch}}(X)\n\nWhere:\\begin{align}\n m_z(X)  & = Z_{T \\setminus S_1}(z)(g(X) - g^*(z)) + \\beta \\cdot Z_{T \\setminus S_2}(z)(h(X) - h^*(z))  \\\\\n & \\quad + \\beta^2 \\cdot Z_{T \\setminus S_3}(z)(S(X) - S^*(z)) +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z)(D(X) - D^*(z))\n\\end{align}\n\nThen L(z) = m_z(z) - Z_T(z) \\cdot q_m(z) = 0, so (X - z) divides L(X).\n\nProver calculates and sends the commitment to L(X)/(X - z):C_{q_L} = \\left[\\frac{L(\\tau)}{\\tau - z}\\right]_1\n\nVerifier calculates interpolation polynomials:\n\ng^*(X) is the interpolation polynomial of (g(\\zeta), g(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nh^*(X) is the interpolation polynomial of (h(\\alpha), h(\\zeta), h(1/\\zeta)) on (\\alpha, \\zeta, 1/\\zeta)\n\nS^*(X) is the interpolation polynomial of (S(\\zeta), S(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nD^*(X) is the interpolation polynomial of (D(\\zeta)) on (\\zeta)\n\nVerifier calculates the values of interpolation polynomials at z by computing g^*(z), h^*(z), S^*(z), D^*(z)\n\nVerifier calculates the values of vanishing polynomials at z by computing Z_T(z), Z_{T \\setminus S_1}(z), Z_{T \\setminus S_2}(z), Z_{T \\setminus S_3}(z), Z_{T \\setminus S_4}(z)\n\nVerifier calculates:\\begin{align}\n F & = Z_{T \\setminus S_1}(z) \\cdot C_g + \\beta \\cdot Z_{T \\setminus S_2}(z) \\cdot C_h + \\beta^2 \\cdot Z_{T \\setminus S_3}(z) \\cdot C_S  +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z) \\cdot C_D \\\\\n & \\quad - (Z_{T \\setminus S_1}(z) \\cdot g^*(z) + \\beta \\cdot Z_{T \\setminus S_2}(z) \\cdot h^*(z) + \\beta^2 \\cdot Z_{T \\setminus S_3}(z) \\cdot S^*(z)  +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z) \\cdot D^*(z)) \\cdot [1]_1 \\\\\n & \\quad - Z_T(z) \\cdot C_{q_m}\n\\end{align}\n\nVerifier verifies:e(F + z \\cdot C_{q_L}, [1]_2) \\stackrel{?}{=} e(C_{q_L}, [\\tau]_2)\n\nNow we have clarified all the proof parts for integrating with KZG commitments.\n\n![Mercury Full Diagram]\n\nLet’s combine all these parts to see the complete Mercury protocol.","type":"content","url":"/mercury/mercury-02#aggregating-multiple-polynomials-opening-proofs-at-multiple-points","position":9},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Protocol"},"type":"lvl2","url":"/mercury/mercury-02#protocol","position":10},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"Protocol"},"content":"","type":"content","url":"/mercury/mercury-02#protocol","position":11},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Setup","lvl2":"Protocol"},"type":"lvl3","url":"/mercury/mercury-02#setup","position":12},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Setup","lvl2":"Protocol"},"content":"Choose a random number \\tau \\in \\mathbb{F} and generate SRS parameters:\\{[1], [\\tau], \\ldots, [\\tau^{N-1}], [1]_2, [\\tau]_2\\}","type":"content","url":"/mercury/mercury-02#setup","position":13},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Commit","lvl2":"Protocol"},"type":"lvl3","url":"/mercury/mercury-02#commit","position":14},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Commit","lvl2":"Protocol"},"content":"For a multivariate linear polynomial:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) = \\sum_{i=0}^{2^n - 1} f_i \\cdot \\tilde{eq}(\\mathsf{bits}(i),(X_0, X_1, \\ldots, X_{n-1}))\n\nLet N = 2^n. Divide its coefficients f_i into b = \\sqrt{N} groups: (f_0,f_1, \\ldots, f_{2^n - 1}) = (f_{0,0}, f_{0,1}, \\ldots, f_{0,b-1}, \\ldots, f_{b-1,0}, \\ldots, f_{b-1,b-1}). The univariate polynomial corresponding to \\tilde{f} is:f(X) = \\sum_{i = 0}^{b - 1} \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot X^{i \\cdot b + j}\n\nThe commitment to the multivariate linear polynomial is the commitment to f(X):C_f = \\sum_{i = 0}^{b - 1} \\sum_{j = 0}^{b - 1} f_{i,j} \\cdot [\\tau^{i \\cdot b + j}]","type":"content","url":"/mercury/mercury-02#commit","position":15},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl3","url":"/mercury/mercury-02#evaluation","position":16},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl3":"Evaluation","lvl2":"Protocol"},"content":"The Prover wants to prove to the Verifier that the value of \\tilde{f}(X_0, \\ldots, X_{n-1}) at a public point \\vec{u} = (u_0, \\ldots, u_{n-1}) is v, namely:\\tilde{f}(\\vec{u}) = v","type":"content","url":"/mercury/mercury-02#evaluation","position":17},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Common Input","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#common-input","position":18},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Common Input","lvl3":"Evaluation","lvl2":"Protocol"},"content":"Commitment C_f to the multivariate linear polynomial \\tilde{f}\n\nEvaluation point \\vec{u}\n\nValue v of \\tilde{f} at \\vec{u}","type":"content","url":"/mercury/mercury-02#common-input","position":19},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Witness","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#witness","position":20},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Witness","lvl3":"Evaluation","lvl2":"Protocol"},"content":"Coefficients (f_{0,0}, f_{0,1}, \\ldots, f_{0,b-1}, \\ldots, f_{b-1,0}, \\ldots, f_{b-1,b-1}) of the multivariate linear polynomial \\tilde{f}.","type":"content","url":"/mercury/mercury-02#witness","position":21},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 1","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-1","position":22},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 1","lvl3":"Evaluation","lvl2":"Protocol"},"content":"This round commits to the summing function h(X).\n\nSplit \\vec{u} into two equal-length vectors. Let n = 2t, then b = 2^t, so:\\vec{u}_1 = (u_0, \\ldots, u_{t - 1})\\vec{u}_2 = (u_t, \\ldots, u_{n - 1})\n\nProver calculates h(X):h(X) = \\sum_{i = 0}^{b - 1} \\tilde{eq}(\\mathsf{bits}(i), \\vec{u}_1) \\cdot f_i(X)\n\nWhere:\\begin{align}\n & f_0(X) = f_{0,0} + f_{1,0}  X + \\ldots + f_{b-1, 0} X^{b-1} \\\\\n & f_1(X) = f_{0,1} + f_{1,1}  X + \\ldots + f_{b-1, 1} X^{b-1} \\\\\n & \\qquad \\qquad \\qquad \\qquad \\ldots \\\\\n & f_{b-1}(X) = f_{0,b-1} + f_{1,b-1}  X + \\ldots + f_{b-1, b-1} X^{b-1}\n\\end{align}\n\nProver calculates and sends the commitment to h(X): C_h = [h(\\tau)]_1","type":"content","url":"/mercury/mercury-02#round-1","position":23},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 2","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-2","position":24},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 2","lvl3":"Evaluation","lvl2":"Protocol"},"content":"This round commits to the “folded” polynomial g(X).\n\nVerifier sends a random number \\alpha \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates polynomials g(X) and q(X), satisfying:f(X) = q(X) \\cdot (X^b - \\alpha) + g(X)\n\nAnd \\deg(g) < b, where:g(X) = \\sum_{i = 0}^{b - 1} f_i(\\alpha) \\cdot X^i\n\nProver calculates and sends commitments to g(X) and q(X): C_g = [g(\\tau)]_1, C_q = [q(\\tau)]_1\n\nProver calculates and sends h(\\alpha)","type":"content","url":"/mercury/mercury-02#round-2","position":25},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 3","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-3","position":26},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 3","lvl3":"Evaluation","lvl2":"Protocol"},"content":"This round sends two inner product proofs and the degree bound proof for g(X).\n\nVerifier sends a random number \\gamma \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates S(X), satisfying:\\begin{aligned}\ng(X) P_{u_1}(1/X) & + g(1/X) P_{u_1}(X) + \\gamma \\cdot (h(X) P_{u_2}(1/X) + h(1/X) P_{u_2}(X)) \\\\\n& = 2(h(\\alpha) + \\gamma \\cdot v) + X \\cdot S(X) + (1/X) \\cdot S(1/X)  \n\\end{aligned}\n\nWhere:P_{u_1}(X) = \\sum_{i = 0}^{b-1} \\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_1) \\cdot X^iP_{u_2}(X) = \\sum_{i = 0}^{b-1} \\tilde{eq}(\\mathsf{bits}(i),\\vec{u}_2) \\cdot X^i\n\nProver sends the commitment to S(X): C_S = [S(\\tau)]_1\n\nProver calculates polynomial D(X):D(X) = X^{b - 1} g(1/X)\n\nProver sends the commitment to D(X): C_D = [D(\\tau)]_1","type":"content","url":"/mercury/mercury-02#round-3","position":27},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 4","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-4","position":28},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 4","lvl3":"Evaluation","lvl2":"Protocol"},"content":"Verifier sends a random number \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates and sends g(\\zeta), g(1/\\zeta), h(\\zeta), h(1/\\zeta), S(\\zeta), S(1/\\zeta), D(\\zeta)\n\nProver calculates the quotient polynomial:H(X) = \\frac{f(X) - (\\zeta^b - \\alpha)q(X) - g(\\zeta)}{X - \\zeta}\n\nProver calculates and sends the commitment to H(X): C_H = [H(\\tau)]_1","type":"content","url":"/mercury/mercury-02#round-4","position":29},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 5","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-5","position":30},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 5","lvl3":"Evaluation","lvl2":"Protocol"},"content":"This round sends the aggregated proof of opening g(X), h(X), S(X), D(X) at multiple points.\n\nVerifier sends a random number \\beta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates interpolation polynomials:\n\ng^*(X) is the interpolation of (g(\\zeta), g(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nh^*(X) is the interpolation of (h(\\alpha), h(\\zeta), h(1/\\zeta)) on (\\alpha, \\zeta, 1/\\zeta)\n\nS^*(X) is the interpolation of (S(\\zeta), S(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nD^*(X) is the interpolation of (D(\\zeta)) on (\\zeta)\n\nProver calculates the polynomial:\\begin{align}\n m(X)  & = Z_{T \\setminus S_1}(X)(g(X) - g^*(X)) + \\beta \\cdot Z_{T \\setminus S_2}(X)(h(X) - h^*(X))  \\\\\n & \\quad + \\beta^2 \\cdot Z_{T \\setminus S_3}(X)(S(X) - S^*(X)) +  \\beta^3 \\cdot Z_{T \\setminus S_4}(X)(D(X) - D^*(X))\n\\end{align}\n\nWhere:\\begin{align}\nS_1 = \\{\\zeta, 1/\\zeta\\},  & \\qquad S_2 = \\{\\alpha, \\zeta, 1/\\zeta\\} \\\\\nS_3 = \\{\\zeta, 1/\\zeta\\},  & \\qquad S_4 = \\{\\zeta\\}\n\\end{align}T = \\{\\zeta, 1/\\zeta, \\alpha\\}\n\nZ_T(X), Z_{T \\setminus S_1}(X), Z_{T \\setminus S_2}(X), Z_{T \\setminus S_3}(X), Z_{T \\setminus S_4}(X) are the vanishing polynomials on sets T, T \\setminus S_1, T \\setminus S_2, T \\setminus S_3, T \\setminus S_4.\n\nProver calculates and sends the commitment to q_m(X) = m(X) / Z_T(X):C_{q_{m}} = [q_m(\\tau)]_1","type":"content","url":"/mercury/mercury-02#round-5","position":31},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 6","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#round-6","position":32},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Round 6","lvl3":"Evaluation","lvl2":"Protocol"},"content":"Verifier sends a random opening point z \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates the linearization polynomial:L(X) = m_z(X) - Z_T(z) \\cdot q_m(X)\n\nWhere:\\begin{align}\n m_z(X)  & = Z_{T \\setminus S_1}(z)(g(X) - g^*(z)) + \\beta \\cdot Z_{T \\setminus S_2}(z)(h(X) - h^*(z))  \\\\\n & \\quad + \\beta^2 \\cdot Z_{T \\setminus S_3}(z)(S(X) - S^*(z)) +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z)(D(X) - D^*(z))\n\\end{align}\n\nThen L(z) = m_z(z) - Z_T(z) \\cdot q_m(z) = 0, so (X - z) divides L(X).\n\nProver calculates and sends the commitment to L(X)/(X - z):C_{q_L} = \\left[\\frac{L(\\tau)}{\\tau - z}\\right]_1","type":"content","url":"/mercury/mercury-02#round-6","position":33},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Proof Representation","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#proof-representation","position":34},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Proof Representation","lvl3":"Evaluation","lvl2":"Protocol"},"content":"The proof sent by the Prover is:\\pi = (C_h, C_g, C_q, C_S, C_D, C_H, C_{q_m}, C_{q_L}, g(\\zeta), g(1/\\zeta), h(\\zeta), h(1/\\zeta), h(\\alpha), S(\\zeta), S(1/\\zeta), D(\\zeta))\n\nThe proof size is 8 ~ \\mathbb{G}_1 + 8~ \\mathbb{F}.","type":"content","url":"/mercury/mercury-02#proof-representation","position":35},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Verification","lvl3":"Evaluation","lvl2":"Protocol"},"type":"lvl4","url":"/mercury/mercury-02#verification","position":36},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl4":"Verification","lvl3":"Evaluation","lvl2":"Protocol"},"content":"Verifier calculates P_{u_1}(\\zeta), P_{u_1}(1/\\zeta), P_{u_2}(\\zeta), P_{u_2}(1/\\zeta). The Verifier can calculate these values in O(\\log N) time using:P_{u_1} (X) = \\prod_{i = 0}^{t - 1} (u_i X^{2^{i}} + 1 - u_i)P_{u_2} (X) = \\prod_{i = t}^{n - 1} (u_i X^{2^{i-t}} + 1 - u_i)\n\nVerifier verifies the two inner product proofs:\\begin{aligned}\ng(\\zeta) P_{u_1}(1/\\zeta) & + g(1/\\zeta) P_{u_1}(\\zeta) + \\gamma \\cdot (h(\\zeta) P_{u_2}(1/\\zeta) + h(1/\\zeta) P_{u_2}(\\zeta)) \\\\\n& \\stackrel{?}{=} 2(h(\\alpha) + \\gamma \\cdot v) + \\zeta \\cdot S(\\zeta) + (1/\\zeta) \\cdot S(1/\\zeta)  \n\\end{aligned}\n\nVerifier verifies if the degree of g(X) is less than b:D(\\zeta) \\stackrel{?}{=} \\zeta^{b - 1} g(1/\\zeta)\n\nVerifier verifies if the construction of g(X) is correct:e(C_f - (\\zeta^b - \\alpha) \\cdot C_q - g(\\zeta) \\cdot [1]_1, [1]_2) \\stackrel{?}{=} e(C_H, [\\tau - \\zeta]_2)\n\nVerifier calculates interpolation polynomials:\n\ng^*(X) is the interpolation of (g(\\zeta), g(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nh^*(X) is the interpolation of (h(\\alpha), h(\\zeta), h(1/\\zeta)) on (\\alpha, \\zeta, 1/\\zeta)\n\nS^*(X) is the interpolation of (S(\\zeta), S(1/\\zeta)) on (\\zeta, 1/\\zeta)\n\nD^*(X) is the interpolation of (D(\\zeta)) on (\\zeta)\n\nVerifier calculates values of interpolation polynomials at z: g^*(z), h^*(z), S^*(z), D^*(z)\n\nVerifier calculates values of vanishing polynomials at z: Z_T(z), Z_{T \\setminus S_1}(z), Z_{T \\setminus S_2}(z), Z_{T \\setminus S_3}(z), Z_{T \\setminus S_4}(z)\n\nVerifier calculates:\\begin{align}\n F & = Z_{T \\setminus S_1}(z) \\cdot C_g + \\beta \\cdot Z_{T \\setminus S_2}(z) \\cdot C_h + \\beta^2 \\cdot Z_{T \\setminus S_3}(z) \\cdot C_S  +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z) \\cdot C_D \\\\\n & \\quad - (Z_{T \\setminus S_1}(z) \\cdot g^*(z) + \\beta \\cdot Z_{T \\setminus S_2}(z) \\cdot h^*(z) + \\beta^2 \\cdot Z_{T \\setminus S_3}(z) \\cdot S^*(z)  +  \\beta^3 \\cdot Z_{T \\setminus S_4}(z) \\cdot D^*(z)) \\cdot [1]_1 \\\\\n & \\quad - Z_T(z) \\cdot C_{q_m}\n\\end{align}\n\nVerifier verifies:e(F + z \\cdot C_{q_L}, [1]_2) \\stackrel{?}{=} e(C_{q_L}, [\\tau]_2)","type":"content","url":"/mercury/mercury-02#verification","position":37},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"References"},"type":"lvl2","url":"/mercury/mercury-02#references","position":38},{"hierarchy":{"lvl1":"Mercury Notes: Integration with KZG","lvl2":"References"},"content":"[EG25] Eagen, Liam, and Ariel Gabizon. “MERCURY: A multilinear Polynomial Commitment Scheme with constant proof size and no prover FFTs.” Cryptology ePrint Archive (2025). \n\nhttps://​eprint​.iacr​.org​/2025​/385\n\n[BDFG20] Boneh, Dan, Justin Drake, Ben Fisch, and Ariel Gabizon. “Efficient polynomial commitment schemes for multiple points and polynomials.” Cryptology ePrint Archive (2020). \n\nhttps://​eprint​.iacr​.org​/2020​/081","type":"content","url":"/mercury/mercury-02#references","position":39},{"hierarchy":{"lvl1":"Content with notebooks"},"type":"lvl1","url":"/notebooks","position":0},{"hierarchy":{"lvl1":"Content with notebooks"},"content":"You can also create content with Jupyter Notebooks. This means that you can include\ncode blocks and their outputs in your book.","type":"content","url":"/notebooks","position":1},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"Markdown + notebooks"},"type":"lvl2","url":"/notebooks#markdown-notebooks","position":2},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"Markdown + notebooks"},"content":"As it is markdown, you can embed images, HTML, etc into your posts!\n\nYou can also add_{math} andmath^{blocks}\n\nor\\begin{aligned}\n\\mbox{mean} la_{tex} \\\\ \\\\\nmath blocks\n\\end{aligned}\n\nBut make sure you $Escape $your $dollar signs $you want to keep!","type":"content","url":"/notebooks#markdown-notebooks","position":3},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"MyST markdown"},"type":"lvl2","url":"/notebooks#myst-markdown","position":4},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"MyST markdown"},"content":"MyST markdown works in Jupyter Notebooks as well. For more information about MyST markdown, check\nout \n\nthe MyST guide in Jupyter Book,\nor see \n\nthe MyST markdown documentation.","type":"content","url":"/notebooks#myst-markdown","position":5},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"Code blocks and outputs"},"type":"lvl2","url":"/notebooks#code-blocks-and-outputs","position":6},{"hierarchy":{"lvl1":"Content with notebooks","lvl2":"Code blocks and outputs"},"content":"Jupyter Book will also embed your code blocks and output in your book.\nFor example, here’s some sample Matplotlib code:\n\nfrom matplotlib import rcParams, cycler\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.ion()\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nN = 10\ndata = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)]\ndata = np.array(data).T\ncmap = plt.cm.coolwarm\nrcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))\n\n\nfrom matplotlib.lines import Line2D\ncustom_lines = [Line2D([0], [0], color=cmap(0.), lw=4),\n                Line2D([0], [0], color=cmap(.5), lw=4),\n                Line2D([0], [0], color=cmap(1.), lw=4)]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nlines = ax.plot(data)\nax.legend(custom_lines, ['Cold', 'Medium', 'Hot']);\n\nThere is a lot more that you can do with outputs (such as including interactive outputs)\nwith your book. For more information about this, see \n\nthe Jupyter Book documentation","type":"content","url":"/notebooks#code-blocks-and-outputs","position":7},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)"},"type":"lvl1","url":"/ph23/ph23-pcs-01","position":0},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)"},"content":"In the paper “Improving logarithmic derivative lookups using GKR” ([PH23]), the authors presented an idea to convert MLE into a Univariate Polynomial. Although the paper didn’t provide a complete protocol description, this protocol demonstrates advantages in certain aspects, such as supporting Shift Arguments with arbitrary offsets.\n\nThe main advantage of this scheme is its ability to support Shift Arguments with arbitrary offsets (see Appendix A.2 of the paper). Additionally, when interfacing with KZG10, the proof of this PCS Adaptor only includes a constant number of \\mathbb{G}_1 elements and a logarithmic number of \\mathbb{F}_r elements. This is superior to Gemini-PCS and Zeromorph-PCS (KT23), which require a logarithmic number of \\mathbb{G}_1 elements.\n\nThe approach of this protocol is similar to Virgo-PCS in that they both view MLE polynomial operations as a summation and use the Univariate Sumcheck protocol to complete the “sum proof”. However, PH23-PCS also requires the Prover to prove the value of the MLE Lagrange Polynomial at the evaluation point, thus reducing the burden on the Verifier; while Virgo-PCS uses the GKR protocol to achieve this. Another difference is that Virgo-PCS requires the MLE polynomial to be represented in Coefficient Form, so Virgo-PCS uses the GKR circuit to prove the correctness of the computation of converting the MLE polynomial from Evaluation Form to Coefficient Form.\n\nThis article series completes the description of PH23-PCS in the [PH23] paper and provides a simplified protocol for PH23-KZG10 to help everyone understand the basic idea of this protocol.\n\nThis article first introduces the basic principles of PH23-PCS-Adaptor in detail, and then provides a simple protocol implementation of PH23-KZG10.","type":"content","url":"/ph23/ph23-pcs-01","position":1},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"1. Principle Overview"},"type":"lvl2","url":"/ph23/ph23-pcs-01#id-1-principle-overview","position":2},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"1. Principle Overview"},"content":"Before explaining how the Prover proves the Evaluation of an MLE polynomial \\tilde{f}(\\vec{X}), let’s recall the definition of an MLE polynomial:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nHere N=2^n.\nWhen calculating the value of \\tilde{f}(\\vec{X}) at \\vec{X}=(u_0, u_1, \\ldots, u_{n-1}), we need to calculate \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u}). To facilitate explanation, we introduce a new vector \\vec{c}, where each element c_i = \\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u}).\n\nIf n=3 and N=8, then all values of \\vec{c} can be enumerated:\\begin{array}{cccc}\nc_0 &= &(1-u_0)&(1-u_1)&(1-u_2) \\\\\nc_1 &= &u_0&(1-u_1)&(1-u_2)  \\\\\nc_2 &= &(1-u_0) &u_1 &(1-u_2) \\\\\nc_3 &= &u_0 &u_1 &(1-u_2) \\\\\nc_4 &= &(1-u_0)&(1-u_1) & u_2 \\\\\nc_5 &= &u_0&(1-u_1)&u_2  \\\\\nc_6 &= &(1-u_0) &u_1 &u_2 \\\\\nc_7 &= &u_0 &u_1 &u_2 \\\\\n\\end{array}\n\nIt can be seen that the elements of \\vec{c} are defined with certain patterns. For example, c_i is the product of s values, and these values also have certain patterns. Here (1-u_i) represents binary 0, while u_i represents binary 1. For instance, c_7 is the product of three numbers, u_0, u_1, u_2, which represents (111), exactly the binary representation of 7. Another example is c_5, which is the product of three numbers, u_0, (1-u_1), u_2, representing (101), which is the binary representation of 5.\n\nThe key idea of PH23 is whether the Prover can first commit to the vector \\vec{c}, and then prove that each element of \\vec{c} is correctly defined according to the binary pattern above. If possible, the Prover can then prove an Inner Product relationship, i.e., prove \\langle \\vec{a}, \\vec{c} \\rangle = v, which is equivalent to proving \\tilde{f}(\\vec{X})=v.\n\nTherefore, the proof protocol of PH23 is divided into two parts:\n\nProve the Well-Formedness of vector \\vec{c}.\n\nProve \\langle \\vec{a}, \\vec{c} \\rangle = v.","type":"content","url":"/ph23/ph23-pcs-01#id-1-principle-overview","position":3},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"2. Well-Formedness of \\vec{c}"},"type":"lvl2","url":"/ph23/ph23-pcs-01#id-2-well-formedness-of-vec-c","position":4},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"2. Well-Formedness of \\vec{c}"},"content":"Continuing with the example of \\vec{c} where s=3,\\begin{array}{cccc}\nc_0 &= &(1-u_0)&(1-u_1)&(1-u_2) \\\\\nc_1 &= &u_0&(1-u_1)&(1-u_2)  \\\\\nc_2 &= &(1-u_0) &u_1 &(1-u_2) \\\\\nc_3 &= &u_0 &u_1 &(1-u_2) \\\\\nc_4 &= &(1-u_0)&(1-u_1) & u_2 \\\\\nc_5 &= &u_0&(1-u_1)&u_2  \\\\\nc_6 &= &(1-u_0) &u_1 &u_2 \\\\\nc_7 &= &u_0 &u_1 &u_2 \\\\\n\\end{array}\n\nWe observe that\\frac{c_0}{c_4} = \\frac{1-u_2}{u_2}\n\nThus, if c_0 is correct, we can prove that c_4 is correct by proving the following constraint equation:c_0\\cdot u_2 - c_4\\cdot (1-u_2) = 0\n\nNext, we observe\\frac{c_0}{c_2} = \n\\frac{c_4}{c_6} = \\frac{1-u_1}{u_1} \\\\\n\nFrom this, we can infer that if c_0 is correct, then the following two constraint equations ensure that c_2 and c_6 are correct:\\begin{split}\nc_0\\cdot u_1 - c_2\\cdot (1-u_1) = 0 \\\\\nc_4\\cdot u_1 - c_6\\cdot (1-u_1) = 0 \\\\\n\\end{split}\n\nNext, we can prove that c_1, c_3, c_5, c_7 are correct because they can be derived from c_0, c_2, c_4, c_6, which have been proven correct in the previous step:\\begin{split}\nc_0 \\cdot u_0 - c_1 \\cdot (1-u_0) = 0 \\\\\nc_2 \\cdot u_0 - c_3 \\cdot (1-u_0) = 0 \\\\\nc_4 \\cdot u_0 - c_5 \\cdot (1-u_0) = 0 \\\\\nc_6 \\cdot u_0 - c_7 \\cdot (1-u_0) = 0 \\\\\n\\end{split}\n\nThe final conclusion is that through the above 1+2+4 constraint equations, we can prove that c_1, c_2, c_3, c_4, c_5, c_6, c_7 are all correct, assuming c_0 is known to be correct. The inference relationship between the elements of vector \\vec{c} is shown in the following diagram:\\begin{array}{ccccc}\nc_4 & &  & & \\\\\nc_2 & c_6 &  \\\\\nc_1& c_3 & c_5& c_7 \\\\\n\\cdots  & & \\\\\n\\end{array}\n\nAssume H is a multiplicative subgroup of size 8 in the finite field \\mathbb{F}_p, H=\\{1, \\omega, \\omega^2, \\omega^3, \\omega^4, \\omega^5, \\omega^6, \\omega^7\\}, where \\omega\\in \\mathbb{F}_p is an 8th root of unity. And let \\{L_i(X)\\}_{i=0}^{N-1} denote the Lagrange Basis polynomials on H.\n\nThen we can introduce c(X) as the polynomial encoding of \\vec{c} according to the Lagrange Basis:c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nIt’s easy to verify that c(\\omega^i) = c_i, where i=0,1,2,\\ldots, N-1.\n\nFurthermore, the four constraint equations proving c_1, c_3, c_5, c_7 can be combined into one polynomial constraint equation:(X-\\omega)(X-\\omega^3)(X-\\omega^5)(X-\\omega^7)\\cdot \\big(c(X)u_0 - c(\\omega\\cdot X)(1-u_0)\\big) = 0, \\quad X\\in H\n\nWe can substitute X=\\omega^2, and the above constraint equation corresponds to:c(\\omega^2) \\cdot u_0 - c(\\omega^3) \\cdot (1-u_0) = c_2 \\cdot u_0 - c_3 \\cdot (1-u_0) = 0 \\\\\n\nBy substituting X=\\omega, X=\\omega^4, X=\\omega^6 respectively, we can obtain the constraint equations proving the correctness of c_1, c_5, c_7.\n\nThe polynomial (X-\\omega)(X-\\omega^3)(X-\\omega^5)(X-\\omega^7) looks like a Selector polynomial, filtering out X values that don’t satisfy the condition.\n\nUsing this method, we can use n=\\log{N} polynomial constraints to prove the Well-Formedness of \\vec{c}.\n\nFor the example of N=8, we need to introduce 3 Selector polynomials s_0(X), s_1(X), s_2(X),s_i(X) = \\frac{v_H(X)}{v_{H_i}(X)}, \\qquad i=0,1,2\n\nwhere v_H(X) and v_{H_i}(X) are the Vanishing polynomials of Domain H and H_i respectively. And H_i is a subgroup of H, satisfying the following Group Tower relationship:\\{1\\} = H_0 \\sub H_1 \\sub H_2 \\sub H_3 = H\n\nThey are defined as follows:\\begin{split}\nH = H_3 &= (1, \\omega, \\omega^2, \\omega^3, \\omega^4, \\omega^5, \\omega^6, \\omega^7) \\\\\nH_2 & = (1, \\omega^2, \\omega^4, \\omega^6)\\\\\nH_1 & = (1, \\omega^4)\\\\\nH_0 & = (1)\\\\\n\\end{split}\n\nNaturally, the representations of Selector polynomials s_0(X), s_1(X), s_2(X) are as follows:\\begin{split}\ns_0(X) &= (X-\\omega)(X-\\omega^2)(X-\\omega^3)(X+1)(X+\\omega)(X+\\omega^2)(X+\\omega^3) \\\\\ns_1(X) &= (X-\\omega)(X-\\omega^2)(X-\\omega^3)(X+\\omega)(X+\\omega^2)(X+\\omega^3) \\\\\ns_2(X) &= (X-\\omega)(X-\\omega^3)(X+\\omega)(X+\\omega^3) \\\\\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-01#id-2-well-formedness-of-vec-c","position":5},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Polynomial Constraint Equations","lvl2":"2. Well-Formedness of \\vec{c}"},"type":"lvl3","url":"/ph23/ph23-pcs-01#polynomial-constraint-equations","position":6},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Polynomial Constraint Equations","lvl2":"2. Well-Formedness of \\vec{c}"},"content":"The constraint equation ensuring the correctness of c_0 can be expressed as the following polynomial constraint:s_0(X)\\cdot \\big(c(X) - (1-u_0)(1-u_1)(1-u_{2})\\big) = 0, \\quad X\\in H\n\nThe constraint equation ensuring the correctness of c_4 can be expressed as the following polynomial constraint:s_0(X)\\cdot \\big(c(X)u_2 - c(\\omega^4\\cdot X)(1-u_2)\\big) = 0, \\quad X\\in H\n\nThe following are the constraint equations ensuring the correctness of c_2, c_6:s_1(X)\\cdot \\big(c(X)u_1 - c(\\omega^2\\cdot X)(1-u_1)\\big) = 0, \\quad X\\in H\n\nFinally, the constraint equation ensuring the correctness of c_1,c_3,c_5,c_7:s_2(X)\\cdot \\big(c(X)u_0 - c(\\omega\\cdot X)(1-u_0)\\big) = 0, \\quad X\\in H","type":"content","url":"/ph23/ph23-pcs-01#polynomial-constraint-equations","position":7},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"3. Proving Inner Product"},"type":"lvl2","url":"/ph23/ph23-pcs-01#id-3-proving-inner-product","position":8},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"3. Proving Inner Product"},"content":"The second part of the proof is to prove \\langle \\vec{a}, \\vec{c} \\rangle = v. Assuming a(X) is the encoding of vector \\vec{a}, i.e., a(X)\\mid_H=\\vec{a}, then a(X) is committed as [a(\\tau)]_1, along with the commitment of c(X), [c(\\tau)]_1, we can use the Univariate Sumcheck protocol to prove the inner product.","type":"content","url":"/ph23/ph23-pcs-01#id-3-proving-inner-product","position":9},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Univariate Sumcheck","lvl2":"3. Proving Inner Product"},"type":"lvl3","url":"/ph23/ph23-pcs-01#univariate-sumcheck","position":10},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Univariate Sumcheck","lvl2":"3. Proving Inner Product"},"content":"Let’s first look at a theorem (Remark 5.6 in [BCRSVW19], Sec.3 in [RZ21], Sec.5.1 in [CHMMVW19]): For any P(X)\\in \\mathbb{F}[X], a multiplicative subgroup H\\sub \\mathbb{F}, P(X) can be decomposed as:P(X) = q(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N)\n\nHere v is the sum of P(X) over H, i.e.,\\sum_{\\omega\\in H}P(\\omega)=v\n\nTherefore, we can use this theorem to prove the inner product of two vectors. If a(X)\\cdot c(X) can be expressed as the following equation,a(X)\\cdot c(X) = q(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N), \\quad \\deg(g)<N-1\n\nthen \\langle \\vec{a}, \\vec{c}\\rangle = v.\n\nThe Prover can send commitments of q(X) and g(X), then the Verifier challenges with \\zeta, the Prover sends the evaluations of related polynomials at X=\\zeta, and then the Verifier verifies if the above equation holds:a(\\zeta)\\cdot c(\\zeta) \\overset{?}{=} q(\\zeta)\\cdot v_H(\\zeta) + \\zeta\\cdot g(\\zeta) + (v/N)\n\nThe Prover and Verifier then use a univariate polynomial commitment scheme, such as KZG10, to prove the correctness of a(\\zeta), c(\\zeta), q(\\zeta), g(\\zeta).\n\nAt the same time, the KZG10 protocol can also prove the Degree Bound of g(X), i.e., \\deg(g(X))\\lt N-1.","type":"content","url":"/ph23/ph23-pcs-01#univariate-sumcheck","position":11},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Grand Sum","lvl2":"3. Proving Inner Product"},"type":"lvl3","url":"/ph23/ph23-pcs-01#grand-sum","position":12},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Grand Sum","lvl2":"3. Proving Inner Product"},"content":"In fact, we can also use the Grand Sum protocol to prove the inner product of two vectors.\n\nOne problem with Univariate Sumcheck is that it includes a Degree Bound constraint, i.e., \\deg(g(X))\\lt N-1. This requires additional processing for the underlying KZG10 protocol, which increases the complexity of the protocol and introduces too many Pairing operations. The Grand Sum protocol avoids introducing Degree Bound constraints.\n\nThe idea of the Grand Sum protocol comes from the Grand Product Argument in Plonk [GWC19], and this protocol was first proposed by [BG12].\n\nSuppose a polynomial f(X) encodes the values of vector \\vec{f}, then we construct an auxiliary vector \\vec{z}, satisfying:\\begin{split}\nz_0 &= a_0\\cdot c_0 \\\\\nz_1 &= z_0 + a_1\\cdot c_1 \\\\\nz_2 &= z_1 + a_2\\cdot c_2 \\\\\n\\cdots \\\\\nz_{N-1} &= z_{N-2} + a_{N-1}\\cdot c_{N-1} \\\\\n\\end{split}\n\nOr expressed more concisely with a recursive formula:\\begin{split}\nz_0 &= a_0\\cdot c_0 \\\\\nz_i &= z_{i-1} + a_i\\cdot c_i, \\quad i=1,\\ldots, N-1 \\\\\n\\end{split}\n\nWe can encode \\vec{z} using polynomial z(X), i.e.,z(X) = \\sum_{i=0}^{N-1} z_i \\cdot L_i(X)\n\nwhere L_i(X) are the Lagrange Basis polynomials for H defined above.\n\nThen we can use the following three polynomial constraints to represent the recursive formula of z_i, thus ensuring the correctness of z(X):\\begin{split}\nL_0(X)\\cdot\\big(z(X) - a(X)\\cdot c_0\\big) = 0 \\\\\n(X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) =0  \\\\\nL_{N-1}(X)\\cdot\\big( z(X) - v \\big) = 0 \\\\\n\\end{split}\n\nNote that we use z(\\omega^{-1}\\cdot X) to represent z_{i-1}. And the third polynomial constraint ensures that the sum of the result equals the final polynomial operation result v.\n\nUsing a Univariate PCS protocol, such as KZG10 or FRI, we can prove the correctness of these polynomial constraint equations.","type":"content","url":"/ph23/ph23-pcs-01#grand-sum","position":13},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"4. Interfacing with KZG10"},"type":"lvl2","url":"/ph23/ph23-pcs-01#id-4-interfacing-with-kzg10","position":14},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"4. Interfacing with KZG10"},"content":"To prove the polynomial constraints listed above, we can implement a proof of these polynomial equations based on the KZG10 protocol. In summary, we have two types of polynomial constraints. The first type is constraints about the correctness of c(X), and the second type is constraints about the correctness of z(X).\\begin{aligned}\np_0(X) = &s_0(X)\\cdot \\big(c(X) - (1-u_0)(1-u_1)\\cdots(1-u_{n-1})\\big)      \\\\\np_1(X) = &s_0(X)\\cdot \\big(c(X)u_{n-1} - c(\\omega^{2^{n-1}}\\cdot X)(1-u_{n-1})\\big) \\\\\np_2(X) = &s_1(X)\\cdot \\big(c(X)u_{n-2} - c(\\omega^{2^{n-2}}\\cdot X)(1-u_{n-2})\\big)  \\\\\n\\cdots & \\quad\\cdots \\\\\np_{n}(X) = &s_{n-1}(X)\\cdot \\big(c(X)u_0 - c(\\omega\\cdot X)(1-u_0)\\big) \\\\\n\\end{aligned}\n\nThe second type of polynomials are\\begin{aligned}\nh_0(X) = &L_0(X)\\cdot\\big(z(X) - a_0\\cdot c_0\\big) \\\\\nh_1(X) = &(X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) = &L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{aligned}\n\nWe can use an \\alpha provided by the Verifier to aggregate these polynomials into a large polynomial, denoted as h(X):\\begin{aligned}\nh(X) &= p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\\\\  & + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{aligned}\n\nIf \\vec{c} and \\vec{z} are correct, then the values of h(X) at X\\in H are all zero, and then we know that h(X) is zero everywhere on H, so it must contain the Vanishing polynomial v_H(X) of H as a factor, i.e., there exists a Quotient polynomial t(X), such thath(X) = t(X) \\cdot v_H(X)\n\nThen the Verifier challenges a random point \\zeta, requiring the Prover to calculate and send the values of a(X), c(X) and z(X) t(X) at \\zeta, as well as the value of z(X) at X=\\zeta\\cdot\\omega^{-1}, and the values of c(X) at X=\\zeta\\cdot\\omega, \\zeta\\cdot\\omega^2, \\ldots, \\zeta\\cdot\\omega^{2^{n-1}}. And provide KZG10 Evaluation proofs \\pi_{kzg10} for these evaluations.\\begin{split}\n\\pi_e &= \\big(a(\\zeta), c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), z(\\zeta), z(\\zeta\\cdot\\omega^{-1}), t(\\zeta)\\big) \\\\\n\\pi_{kzg10} &= \\big(\\pi_{a(\\zeta)}, \\pi_{c(\\zeta)}, \\pi_{c(\\zeta\\cdot\\omega)}, \\pi_{c(\\zeta\\cdot\\omega^2)}, \\ldots, \\pi_{c(\\zeta\\cdot\\omega^{2^{n-1}})}, \\pi_{z(\\zeta)}, \\pi_{z(\\zeta\\cdot\\omega^{-1})}, \\pi_{t(\\zeta)}\\big)\n\\end{split}\n\nThe Verifier first verifies the correctness of \\pi_{kzg10},\nthen the Verifier calculates the values of the following public polynomials at X=\\zeta:s_0(\\zeta) = \\frac{v_H(\\zeta)}{v_{H_0}(\\zeta)} \\quad\ns_1(\\zeta) = \\frac{v_H(\\zeta)}{v_{H_1}(\\zeta)} \\quad\ns_2(\\zeta) = \\frac{v_H(\\zeta)}{v_{H_2}(\\zeta)} \\quad\n\\cdots  \\quad\ns_{n-1}(\\zeta) = \\frac{v_H(\\zeta)}{v_{H_{n-1}}(\\zeta)}\n\nAnd\\begin{split}\nL_0(\\zeta) &= \\frac{v_H(\\zeta)}{N(\\zeta - 1)} \\\\\nL_{N-1}(\\zeta) &= \\frac{v_H(\\zeta)}{N(\\omega\\cdot\\zeta - 1)} \\\\\n\\end{split}\n\nThen calculate the following values using all the polynomial operation values included in \\pi_e:\\begin{split}\np_0(\\zeta) &= s_0(\\zeta)\\cdot \\big(c(\\zeta) - (1-u_0)(1-u_1)\\cdots(1-u_{n-1})\\big) \\\\\np_1(\\zeta) &= s_0(\\zeta)\\cdot \\big(c(\\zeta)u_{n-1} - c(\\omega^{2^{n-1}}\\cdot\\zeta)(1-u_{n-1})\\big) \\\\\np_2(\\zeta) &= s_1(\\zeta)\\cdot \\big(c(\\zeta)u_1 - c(\\omega^{2^{n-2}}\\cdot\\zeta)(1-u_1)\\big) \\\\\n\\cdots \\\\\np_{n}(\\zeta) &= s_{n-1}(\\zeta)\\cdot \\big(c(\\zeta)u_0 - c(\\omega\\cdot\\zeta)(1-u_0)\\big) \\\\\nh_1(\\zeta) &= L_0(\\zeta)\\cdot\\big(z(\\zeta) - a_0\\cdot c_0\\big) \\\\\nh_2(\\zeta) &= (\\zeta-1)\\cdot\\big(z(\\zeta)-z(\\omega^{-1}\\cdot\\zeta)-a(\\zeta)\\cdot c(\\zeta)\\big) \\\\\nh_3(\\zeta) &= L_{N-1}(\\zeta)\\cdot\\big( z(\\zeta) - v \\big) \\\\\n\\end{split}\n\nUse \\alpha to aggregate these polynomial evaluations to get h(\\zeta):\\begin{split}\nh(\\zeta) &= p_0(\\zeta) + \\alpha\\cdot p_1(\\zeta) + \\alpha^2\\cdot p_2(\\zeta) + \\cdots + \\alpha^{n}\\cdot p_{n}(\\zeta) \\\\\n&+ \\alpha^{n+1} \\cdot h_0(\\zeta) + \\alpha^{n+2} \\cdot h_1(\\zeta) + \\alpha^{n+3} \\cdot h_2(\\zeta)\n\\end{split}\n\nFinally, check if the following equation holds:\\begin{split}\nh(\\zeta) \\overset{?}{=} t(\\zeta)\\cdot v_H(\\zeta) \\\\\n\\end{split}\n\nwhere v_H(\\zeta) is the value of the Vanishing polynomial of H at \\zeta, calculated by the Verifier itself.","type":"content","url":"/ph23/ph23-pcs-01#id-4-interfacing-with-kzg10","position":15},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl2","url":"/ph23/ph23-pcs-01#id-5-ph23-kzg10-protocol-flow-simplified-version","position":16},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"In this article, we don’t consider the performance of the protocol, but only show a simple and direct implementation of PH23+KZG10. The protocol is mainly divided into three parts:\n\nCommit of MLE polynomial\n\nProof process of MLE polynomial’s Evaluation Argument\n\nVerification process of MLE polynomial’s Evaluation Argument","type":"content","url":"/ph23/ph23-pcs-01#id-5-ph23-kzg10-protocol-flow-simplified-version","position":17},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Commit","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-01#commit","position":18},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Commit","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"If we want to commit to an MLE polynomial with n unknowns, \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}), defined as follows:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nHere \\vec{a}=(a_0,a_1,\\ldots,a_{N-1}) represents the values of this MLE polynomial on the \\{0, 1\\}^n Boolean HyperCube.\n\nFirst, the Prover constructs a univariate polynomial, denoted as a(X), such that its values on Domain H are exactly \\vec{a}, and the size of H is exactly N. Moreover, H is generally a multiplication-friendly subgroup, which can be generated by an N-th Root of Unity (denoted as \\omega) as the generator:H = (1, \\omega, \\omega^2, \\omega^3,\\ldots, \\omega^{N-1})\n\nThen a(X) is defined as follows:a(X) = a_0\\cdot L_0(X) + a_1\\cdot L_1(X) + \\cdots + a_{N-1}\\cdot L_{N-1}(X)\n\nHere \\{L_i\\}_{i=0}^{N-1} represents the Lagrange polynomials based on H, defined as follows:L_i(X) = \\frac{\\omega_i\\cdot v_H(X)}{N\\cdot(X-\\omega_i)}, \\qquad i=0,1,\\ldots, N-1\n\nHere v_H(X) is the Vanishing polynomial on H, defined as follows:v_H(X) = (X-1)(X-\\omega)(X-\\omega^2)\\cdots(X-\\omega^{N-1})\n\nThen the Prover calculates the coefficient form of a(X), uses the SRS of KZG10 to calculate the commitment of a(X), denoted as C_aC_a = b_0\\cdot [1]_1 + b_1\\cdot [\\tau]_1 + b_2 \\cdot\n[\\tau^2]_1 + \\cdots + b_{N-1}\\cdot[\\tau^{N-1}]_1\n\nHere (b_0, b_1, \\ldots, b_{N-1}) is the coefficient form of a(X):a(X) = b_0 + b_1X+b_2X^2 + \\cdots + b_{N-1}X^{N-1}","type":"content","url":"/ph23/ph23-pcs-01#commit","position":19},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-01#evaluation-proof-protocol","position":20},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"The so-called Evaluation Argument refers to the Prover proving to the Verifier that an MLE polynomial \\tilde{f} corresponding to a polynomial commitment C_f with n unknowns takes a value at a public point (u_0, u_1, \\ldots, u_{n-1}):\\tilde{f}(u_0, u_1, u_2, \\ldots, u_{n-1}) = v\n\nHere, we also assume that the value of the polynomial at the public point is also a public value, denoted as v.","type":"content","url":"/ph23/ph23-pcs-01#evaluation-proof-protocol","position":21},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Common Input","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-01#common-input","position":22},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Common Input","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"C_a=[a(\\tau)]_1:  A univariate polynomial commitment for vector \\vec{a}=(a_0, a_1, \\ldots, a_{N-1}). Where \\vec{a} equals the values of MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) on the n-dimensional Boolean Hypercube, which is also the values of a(X) on H.\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1}): The evaluation point of MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1})\n\nv: The value of MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) at \\vec{u}.","type":"content","url":"/ph23/ph23-pcs-01#common-input","position":23},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-01#round-1","position":24},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"Prover:\n\nConstruct polynomial c(X), where c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nCalculate the commitment of c(X), C_c = [c(\\tau)]_1, and send C_cC_c =  [c(\\tau)]_1 = \\mathsf{KZG10.commit}(\\vec{c})","type":"content","url":"/ph23/ph23-pcs-01#round-1","position":25},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-01#round-2","position":26},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"Verifier: Send random number \\alpha\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:\n\nConstruct Selector polynomials s_0(X), s_1(X), \\ldots, s_{n-1}(X)s_i(X) = \\frac{v_H(X)}{z_{H_i}(X)} = \\frac{X^{2^n}-1}{X^{2^i}-1}, \\qquad i=0,1,\\ldots, n-1\n\nConstruct constraint polynomials p_0(X),\\ldots, p_{n-1}(X) for \\vec{c}\np_0(X) = s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big)p_k(X) = s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X)   - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big), \\qquad k = 1\\ldots n\n\nConstruct accumulation polynomial z(X), satisfying\\begin{split}\nz(1) &= a_0\\cdot c_0 \\\\\nz(\\omega_{i}) - z(\\omega_{i-1}) &= a(\\omega_{i})\\cdot c(\\omega_{i}), \\quad i=1,\\ldots, N-1 \\\\ \nz(\\omega^{N-1}) &= v \\\\\n\\end{split}\n\nConstruct constraint polynomials h_0(X), h_1(X), h_2(X), satisfying\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - c_0\\cdot a(X) \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{split}\n\nConstruct aggregation polynomial h(X):\\begin{split}\nh(X) &= p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X) \\\\\n&+ \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{split}\n\nCalculate Quotient polynomial t(X), satisfyingt(X)\\cdot v_H(X) = h(X)\n\nCalculate polynomial commitments C_t=[t(\\tau)]_1, C_z=[z(\\tau)]_1, and send C_t and C_z","type":"content","url":"/ph23/ph23-pcs-01#round-2","position":27},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-01#round-3","position":28},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"Verifier: Send random evaluation point \\zeta\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:\n\nCalculate the values of s_i(X) at \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nDefine a new Domain D, containing n+1 elements:D = \\{\\omega, \\omega^2,\\omega^4, \\ldots, \\omega^{2^{n-1}}, 1\\}\n\nIts coset D'=\\zeta D is the set of evaluation points for c(X) that the Prover needs to calculate.D'=\\zeta D = \\{\\zeta\\omega, \\zeta\\omega^2,\\zeta\\omega^4, \\ldots, \\zeta\\omega^{2^{n-1}}, \\zeta\\}\n\nCalculate c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), c(\\zeta)\n\nCalculate z(\\zeta), z(\\omega^{-1}\\cdot\\zeta), t(\\zeta), a(\\zeta)\n\nSend these polynomial values:\\big(a(\\zeta), c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), z(\\zeta), z(\\zeta\\cdot\\omega^{-1}), t(\\zeta)\\big)\n\nSend KZG10 evaluation proofs\\pi_{kzg10} = \\big(\\pi_{a(\\zeta)}, \\pi_{c(\\zeta)}, \\pi_{c(\\zeta\\cdot\\omega)}, \\pi_{c(\\zeta\\cdot\\omega^2)}, \\ldots, \\pi_{c(\\zeta\\cdot\\omega^{2^{n-1}})}, \\pi_{z(\\zeta)}, \\pi_{z(\\zeta\\cdot\\omega^{-1})}, \\pi_{t(\\zeta)}\\big)","type":"content","url":"/ph23/ph23-pcs-01#round-3","position":29},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-01#verification","position":30},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"5. PH23+KZG10 Protocol Flow (Simplified Version)"},"content":"The proof \\pi contains the following elements:\\pi = \\left(\\begin{array}{c}\nC_t, C_z, C_c, a(\\zeta), z(\\zeta), z(\\zeta\\cdot\\omega^{-1}), t(\\zeta), \\\\[1.5ex]\nc(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), \\\\[1.5ex]\n\\pi_{a(\\zeta)}, \\pi_{z(\\zeta)}, \\pi_{z(\\zeta\\cdot\\omega^{-1})}, \\pi_{t(\\zeta)}, \\\\[1.5ex]\n\\pi_{c(\\zeta)}, \\pi_{c(\\zeta\\cdot\\omega)}, \\pi_{c(\\zeta\\cdot\\omega^2)}, \\ldots, \\pi_{c(\\zeta\\cdot\\omega^{2^{n-1}})}, \n\\end{array}\\right)\n\nThe Verifier first calculates s_0(\\zeta), \\ldots, s_{n-1}(\\zeta), v_H(\\zeta), L_0(\\zeta), L_{N-1}(\\zeta).\n\nThe Verifier needs to verify the following equations to complete the verification process of the evaluation proofs for polynomials a(X), c(X), t(X), z(X):\\begin{aligned}\n\\mathsf{KZG.Verify}(C_a, \\zeta, a(\\zeta), \\pi_{a(\\zeta)}) &\\overset{?}{=} 1  \\\\\n\\mathsf{KZG.Verify}(C_t, \\zeta, t(\\zeta), \\pi_{t(\\zeta)}) &\\overset{?}{=} 1  \\\\\n\\mathsf{KZG.Verify}(C_z, \\zeta, z(\\zeta), \\pi_{z(\\zeta)}) &\\overset{?}{=} 1  \\\\\n\\mathsf{KZG.Verify}(C_z, \\zeta\\omega^{-1}, z(\\zeta\\omega^{-1}), \\pi_{z(\\zeta\\omega^{-1})}) &\\overset{?}{=} 1  \\\\\n\\mathsf{KZG.Verify}(C_c, \\zeta, c(\\zeta), \\pi_{c(\\zeta)}) &\\overset{?}{=} 1  \\\\\n\\mathsf{KZG.Verify}(C_c, \\zeta\\omega, c(\\zeta\\omega), \\pi_{c(\\zeta\\omega)}) &\\overset{?}{=} 1  \\\\ \n\\mathsf{KZG.Verify}(C_c, \\zeta\\omega^2, c(\\zeta\\omega^2), \\pi_{c(\\zeta\\omega^2)}) &\\overset{?}{=} 1  \\\\\n\\cdots \\\\\n\\mathsf{KZG.Verify}(C_c, \\zeta\\omega^{2^{n-1}}, c(\\zeta\\omega^{2^{n-1}}), \\pi_{c(\\zeta\\omega^{2^{n-1}})}) &\\overset{?}{=} 1  \\\\\n\\end{aligned}\n\nThe Verifier uses the polynomial values at X=\\zeta to verify the following constraint equation:t(\\zeta)\\cdot v_H(\\zeta) \\overset{?}{=} \\Big(\\sum_{i=0}^{n} \\alpha^i\\cdot p_i(\\zeta)\\Big) + \\alpha^{n+1}\\cdot h_0(\\zeta) + \\alpha^{n+2}\\cdot h_1(\\zeta) + \\alpha^{n+3}\\cdot h_2(\\zeta)\n\nHere p_0(\\zeta),\\ldots, p_{n}(\\zeta), h_0(\\zeta), h_1(\\zeta), h_2(\\zeta) are defined as follows:\\begin{split}\np_0(\\zeta) &= s_0(\\zeta)\\cdot \\big(c(\\zeta) - (1-u_0)(1-u_1)\\cdots(1-u_{n-1})\\big) \\\\\np_1(\\zeta) &= s_0(\\zeta)\\cdot \\big(c(\\zeta)u_{n-1} - c(\\omega^{2^{n-1}}\\cdot\\zeta)(1-u_{n-1})\\big) \\\\\np_2(\\zeta) &= s_1(\\zeta)\\cdot \\big(c(\\zeta)u_1 - c(\\omega^{2^{n-2}}\\cdot\\zeta)(1-u_1)\\big) \\\\\n\\cdots \\\\\np_{n}(\\zeta) &= s_{n-1}(\\zeta)\\cdot \\big(c(\\zeta)u_0 - c(\\omega\\cdot\\zeta)(1-u_0)\\big) \\\\\nh_0(\\zeta) &= L_0(\\zeta)\\cdot\\big(z(\\zeta) - a_0\\cdot c_0\\big) \\\\\nh_1(\\zeta) &= (\\zeta-1)\\cdot\\big(z(\\zeta)-z(\\zeta\\omega^{-1})-a(\\zeta)\\cdot c(\\zeta)\\big) \\\\\nh_2(\\zeta) &= L_{N-1}(\\zeta)\\cdot\\big( z(\\zeta) - v \\big) \\\\\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-01#verification","position":31},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"Summary"},"type":"lvl2","url":"/ph23/ph23-pcs-01#summary","position":32},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"Summary"},"content":"The PH23 PCS Adaptor maps the Evaluations of MLE polynomials to the Evaluations of a univariate polynomial, and then uses “sum proof” to ensure the correctness of MLE polynomial operations.\n\nWe will optimize and improve this protocol in subsequent articles.","type":"content","url":"/ph23/ph23-pcs-01#summary","position":33},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"References"},"type":"lvl2","url":"/ph23/ph23-pcs-01#references","position":34},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 1)","lvl2":"References"},"content":"[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[ZXZS20] Jiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent polynomial delegation and its applications to zero knowledge proof.” 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020. \n\nhttps://​eprint​.iacr​.org​/2019​/1482\n\n[KZG10] Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. “Constant-size commitments to polynomials and their applications.” Advances in Cryptology-ASIACRYPT 2010: 16th International Conference on the Theory and Application of Cryptology and Information Security, Singapore, December 5-9, 2010. Proceedings 16. Springer Berlin Heidelberg, 2010.\n\n[CHMMVW19] Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Psi Vesely, and Nicholas Ward. “Marlin: Preprocessing zkSNARKs with universal and updatable SRS.” Advances in Cryptology–EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part I 39. Springer International Publishing, 2020.\n\n[BCRSVW19] Eli Ben-Sasson, Alessandro Chiesa, Michael Riabzev, Nicholas Spooner, Madars Virza, and Nicholas P. Ward. “Aurora: Transparent succinct arguments for R1CS.” Advances in Cryptology–EUROCRYPT 2019: 38th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Darmstadt, Germany, May 19–23, 2019, Proceedings, Part I 38. Springer International Publishing, 2019.\n\n[RZ21] Carla Ràfols and Arantxa Zapico. An algebraic framework for universal and updatable SNARKs. In Tal Malkin and Chris Peikert, editors, CRYPTO 2021, Part I, volume 12825 of LNCS, pages 774–804, Virtual Event, August 2021. Springer, Heidelberg.","type":"content","url":"/ph23/ph23-pcs-01#references","position":35},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)"},"type":"lvl1","url":"/ph23/ph23-pcs-02","position":0},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)"},"content":"This article provides the complete optimized protocol for PH23-KZG10.","type":"content","url":"/ph23/ph23-pcs-02","position":1},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"1. Protocol Framework and Optimization"},"type":"lvl2","url":"/ph23/ph23-pcs-02#id-1-protocol-framework-and-optimization","position":2},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"1. Protocol Framework and Optimization"},"content":"First, let’s review the simple process of the Evaluation Argument in the PH23+KZG10 protocol, and then we’ll look at areas for optimization.\n\nP: Send commitment C_c of c(X)\nV: Send random number \\alpha to aggregate constraint equations for multiple polynomials\nP: Calculate the set of public polynomials \\{s_i(X)\\}\nP: Calculate the aggregated constraint polynomial h(X)h(X) = G(c(X), s_0(X), s_1(X),\\ldots, s_{n-1}(X), z(X), z(\\omega^{-1}X), X)\n\nP: Calculate commitment C_t of quotient polynomial t(X), commitment C_z of z(X)t(X) = \\frac{h(X)}{v_H(X)}\n\nV: Send random evaluation point \\zeta\n\nP: Calculate c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), c(\\zeta), and z(\\zeta), z(\\omega^{-1}\\cdot\\zeta), t(\\zeta), a(\\zeta); Send KZG10 Evaluation Arguments for the above polynomial evaluations\n\nV: Verify all KZG10 Evaluation Arguments, then verify the following equation:\\begin{split}\nh(\\zeta) \\overset{?}{=} t(\\zeta)\\cdot v_H(\\zeta) \\\\\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-02#id-1-protocol-framework-and-optimization","position":3},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Optimization of Multi-point Evaluation Proof for c^*(X)","lvl2":"1. Protocol Framework and Optimization"},"type":"lvl3","url":"/ph23/ph23-pcs-02#optimization-of-multi-point-evaluation-proof-for-c-x","position":4},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Optimization of Multi-point Evaluation Proof for c^*(X)","lvl2":"1. Protocol Framework and Optimization"},"content":"In the proof, the Prover needs to prove the Evaluation of polynomial c(X) at n+1 points, namelyc(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), c(\\zeta)\n\nUsing the technique from the [BDFG20] paper, if a polynomial f(X) has Evaluation \\vec{v}=(v_0, v_1, \\ldots, v_{m-1}) at m points D=(z_0,z_1,\\ldots,z_{m-1}), define f^*(X) as the interpolation polynomial of \\vec{v} on D, i.e., \\deg(f^*(X)) = m-1, and f^*(z_i) = f(z_i), \\forall i\\in[0,m)v_D(X) = \\prod_{i=0}^{m-1} (X-z_i)\n\nThen f(X) satisfies the following equation:f(X) - f^*(X) = q(X)\\cdot (X-z_0)(X-z_1)\\cdots(X-z_{m-1})\n\nThis equation is easy to verify because when X=z_i, the left side of the equation equals zero, so f(X)-f^*(X) can be divided by (X-z_i). For all i=0,1,\\ldots,m-1, f(X)-f^*(X) can be divided by v_D(X),v_D(X) = \\prod_{i=0}^{m-1} (X-z_i)\n\nIn this way, the Prover only needs to prove to the Verifier that there exists q(X) such that f(X) - f^*(X) = q(X)\\cdot v_D(X), then the Evaluation of f(X) on D equals \\vec{v}. This equation can be verified by the Verifier providing a random challenge point X=\\xi, where v_D(\\xi) and f^*(\\xi) can be calculated by the Verifier, and f(\\xi) and q(\\xi) can be proven through KZG10’s Evaluation Argument.","type":"content","url":"/ph23/ph23-pcs-02#optimization-of-multi-point-evaluation-proof-for-c-x","position":5},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Optimization of c^*(X) Polynomial Calculation","lvl2":"1. Protocol Framework and Optimization"},"type":"lvl3","url":"/ph23/ph23-pcs-02#optimization-of-c-x-polynomial-calculation","position":6},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Optimization of c^*(X) Polynomial Calculation","lvl2":"1. Protocol Framework and Optimization"},"content":"The Prover can construct polynomial c^*(X), which is the interpolation polynomial of the following vector on \\zeta D. The advantage of doing this is to allow the Prover to prove the Evaluation of c(X) at multiple different points at once, denoted as \\vec{c^*}:c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), c(\\zeta)\n\nWe introduce D satisfying |D|=n+1, defined asD = \\big(\\omega,\\ \\omega^2,\\ \\omega^4,\\ \\ldots,\\ \\omega^{2^{n-1}}, \\omega^{2^n}=1\\big)\n\nThen the Evaluation Domain of c^*(X) can be expressed as \\zeta D,D'=D\\zeta = \\big(\\omega\\cdot\\zeta,\\ \\omega^2\\cdot\\zeta,\\ \\omega^4\\cdot\\zeta,\\ \\ldots,\\ \\omega^{2^{n-1}}\\cdot\\zeta,\\ \\zeta\\big)\n\nIts Vanishing polynomial v_{D'}(X) is defined as follows:v_{D'}(X) = (X-\\omega\\zeta)(X-\\omega^2\\zeta)(X-\\omega^4\\zeta)\\cdots(X-\\omega^{2^n}\\zeta)\n\nThe Lagrange polynomial on D' can be defined as follows:L^{D'}_j(X) = \\hat{d}_j\\cdot\\frac{v_{D'}(X)}{X-\\omega^{2^j}\\zeta}, \\qquad j=0,1,\\ldots, n\n\nwhere \\hat{d}_j are the Bary-Centric Weights on D', defined as\\hat{d}_j = \\prod_{l\\neq j} \\frac{1}{\\zeta\\cdot\\omega^{2^j} - \\zeta\\cdot\\omega^{2^l}} = \\frac{1}{\\zeta^n}\\cdot\\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}} = \\frac{1}{\\zeta^n}\\cdot\\hat{w}_j\n\nHere \\hat{w}_j are the Bary-Centric Weights on D, and their definition is only related to D, independent of \\zeta. Therefore, we can precompute \\hat{w}_j and then use \\hat{w}_j to calculate c^*(X):c^*(X) = c^*_0 \\cdot L^{D'}_0(X) + c^*_1 \\cdot L^{D'}_1(X) + \\cdots + c^*_n \\cdot L^{D'}_n(X)\n\nThe above equation can be further optimized by dividing the right side by a constant polynomial g(X)=1g(X) = 1 \\cdot L^{D'}_0(X) + 1 \\cdot L^{D'}_1(X) + \\cdots + 1 \\cdot L^{D'}_n(X)\n\nWe can get:c^*(X) = \\frac{c^*(X)}{g(X)} = \\frac{c^*_0 \\cdot L^{D'}_0(X) + c^*_1 \\cdot L^{D'}_1(X) + \\cdots + c^*_n \\cdot L^{D'}_n(X)}{g(X)} \\\\\n\nExpanding g(X) and L^{D'}_i(X), we can get:c^*(X) = \\frac{c^*_0 \\cdot \\hat{d}_0 \\cdot \\frac{z_{D'}(X)}{X-\\omega\\zeta} + c^*_1 \\cdot \\hat{d}_1 \\cdot \\frac{z_{D'}(X)}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\hat{d}_n \\cdot \\frac{z_{D'}(X)}{X-\\omega^{2^n}\\zeta}}{\n  1 \\cdot \\hat{d}_0 \\cdot \\frac{z_{D'}(X)}{X-\\omega\\zeta} + 1 \\cdot \\hat{d}_1 \\cdot \\frac{z_{D'}(X)}{X-\\omega^2\\zeta} + \\cdots + 1 \\cdot \\hat{d}_n \\cdot \\frac{z_{D'}(X)}{X-\\omega^{2^n}\\zeta}\n  }\n\nCanceling out z_{D'}(X) in both numerator and denominator, we can getc^*(X) = \\frac{c^*_0 \\cdot \\hat{d}_0 \\cdot \\frac{1}{X-\\omega\\zeta} + c^*_1 \\cdot \\hat{d}_1 \\cdot \\frac{1}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\hat{d}_n \\cdot \\frac{1}{X-\\omega^{2^n}\\zeta}}{\n  1 \\cdot \\hat{d}_0 \\cdot \\frac{1}{X-\\omega\\zeta} + 1 \\cdot \\hat{d}_1 \\cdot \\frac{1}{X-\\omega^2\\zeta} + \\cdots + 1 \\cdot \\hat{d}_n \\cdot \\frac{1}{X-\\omega^{2^n}\\zeta}\n  }\n\nExpanding the definition of \\hat{d}_i and canceling out \\frac{1}{\\zeta^n} in both numerator and denominator, we can getc^*(X) = \\frac{c^*_0 \\cdot \\frac{\\hat{w}_0}{X-\\omega\\zeta} + c^*_1 \\cdot \\frac{\\hat{w}_1}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}}{\n   \\frac{\\hat{w}_0}{X-\\omega\\zeta} + \\frac{\\hat{w}_1}{X-\\omega^2\\zeta} + \\cdots + \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}\n  }\n\nThe Prover can use the precomputed Bary-Centric Weights \\{\\hat{w}_i\\} on D to quickly calculate c^*(X), if n is fixed. Nevertheless, the computational complexity of c^*(X) is still O(n\\log^2(n)). However, considering that n=\\log(N), the computational complexity of c^*(X) is logarithmic.c^*(X) = \\sum_{j=0}^{n-1} \\frac{{\\hat{w}}_j}{\\zeta^n} \\cdot \\frac{z_{D_\\zeta}(X)}{X-\\zeta\\cdot\\omega^{2^j}}\n\nThe definition of the precomputed \\hat{w}_j is\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nMoreover, the Verifier needs to calculate the value of c^*(X) at a certain challenge point, such as X=\\xi. The Verifier can use the above equation to calculate c^*(\\xi) based on \\vec{c^*} provided by the Prover with a time complexity of O(\\log{N}).","type":"content","url":"/ph23/ph23-pcs-02#optimization-of-c-x-polynomial-calculation","position":7},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl2","url":"/ph23/ph23-pcs-02#id-2-ph23-kzg10-protocol-optimized-version","position":8},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"For the KZG10 protocol, because its Commitment has additive homomorphism.","type":"content","url":"/ph23/ph23-pcs-02#id-2-ph23-kzg10-protocol-optimized-version","position":9},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Precomputation","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#precomputation","position":10},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Precomputation","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Precompute s_0(X),\\ldots, s_{n-1}(X) and v_H(X)v_H(X) = X^N -1s_i(X) = \\frac{v_H(X)}{v_{H_i}(X)} = \\frac{X^N-1}{X^{2^i}-1}\n\nPrecompute Bary-Centric Weights \\{\\hat{w}_i\\} on D=(1, \\omega, \\omega^2, \\ldots, \\omega^{2^{n-1}}). This can accelerate\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nPrecompute KZG10 SRS of Lagrange Basis A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1","type":"content","url":"/ph23/ph23-pcs-02#precomputation","position":11},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Common inputs","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#common-inputs","position":12},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Common inputs","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"C_a=[\\hat{f}(\\tau)]_1:  the (uni-variate) commitment of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1})\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1}): evaluation point\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1}): computation value of MLE polynomial \\tilde{f} at \\vec{X}=\\vec{u}","type":"content","url":"/ph23/ph23-pcs-02#common-inputs","position":13},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Commit Calculation Process","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#commit-calculation-process","position":14},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Commit Calculation Process","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Prover constructs univariate polynomial a(X) such that its Evaluation form equals \\vec{a}=(a_0, a_1, \\ldots, a_{N-1}), where a_i = \\tilde{f}(\\mathsf{bits}(i)), which is the value of \\tilde{f} on the Boolean Hypercube \\{0,1\\}^n.a(X) = a_0\\cdot L_0(X) + a_1\\cdot L_1(X) + a_2\\cdot L_2(X)\n+ \\cdots + a_{N-1}\\cdot L_{N-1}(X)\n\nProver calculates commitment C_a of \\hat{f}(X) and sends C_aC_{a} = a_0\\cdot A_0 + a_1\\cdot A_1 + a_2\\cdot A_2 + \\cdots + a_{N-1}\\cdot A_{N-1} = [\\hat{f}(\\tau)]_1\n\nwhere A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1 have been obtained in the precomputation process.","type":"content","url":"/ph23/ph23-pcs-02#commit-calculation-process","position":15},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#evaluation-proof-protocol","position":16},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Recall the constraint of polynomial computation to be proved:\\tilde{f}(u_0, u_1, u_2, \\ldots, u_{n-1}) = v\n\nHere \\vec{u}=(u_0, u_1, u_2, \\ldots, u_{n-1}) is a public challenge point.","type":"content","url":"/ph23/ph23-pcs-02#evaluation-proof-protocol","position":17},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-02#round-1","position":18},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Prover:\n\nCalculate vector \\vec{c}, where each element c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})\n\nConstruct polynomial c(X), whose computation result on H is exactly \\vec{c}.c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nCalculate commitment C_c= [c(\\tau)]_1 of c(X) and send C_cC_c = \\mathsf{KZG10.Commit}(\\vec{c})  =  [c(\\tau)]_1","type":"content","url":"/ph23/ph23-pcs-02#round-1","position":19},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-02#round-2","position":20},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Verifier: Send challenge number \\alpha\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:\n\nConstruct constraint polynomials p_0(X),\\ldots, p_{n}(X) about \\vec{c}\\begin{split}\np_0(X) &= s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\np_k(X) &= s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big) , \\quad k=1\\ldots n\n\\end{split}\n\nAggregate \\{p_i(X)\\} into one polynomial p(X)p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\n\nConstruct accumulation polynomial z(X), satisfying\\begin{split}\nz(1) &= a_0\\cdot c_0 \\\\\nz(\\omega_{i}) - z(\\omega_{i-1}) &=  a(\\omega_{i})\\cdot c(\\omega_{i}), \\quad i=1,\\ldots, N-1 \\\\ \nz(\\omega^{N-1}) &= v \\\\\n\\end{split}\n\nConstruct constraint polynomials h_0(X), h_1(X), h_2(X), satisfying\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - c_0\\cdot a(X) \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{split}\n\nAggregate p(X) and h_0(X), h_1(X), h_2(X) into one polynomial h(X), satisfying\\begin{split}\nh(X) &= p(X) + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{split}\n\nCalculate Quotient polynomial t(X), satisfyingh(X) =t(X)\\cdot v_H(X)\n\nCalculate C_t=[t(\\tau)]_1, C_z=[z(\\tau)]_1, and send C_t and C_z\\begin{split}\nC_t &= \\mathsf{KZG10.Commit}(t(X)) = [t(\\tau)]_1 \\\\\nC_z &= \\mathsf{KZG10.Commit}(z(X)) = [z(\\tau)]_1\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-02#round-2","position":21},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-02#round-3","position":22},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Verifier: Send random evaluation point \\zeta\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver:\n\nCalculate the values of s_i(X) at \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nHere the Prover can efficiently calculate s_i(\\zeta). From the formula of s_i(X), we get\\begin{aligned}\n  s_i(\\zeta) & = \\frac{\\zeta^N - 1}{\\zeta^{2^i} - 1} \\\\\n  & = \\frac{(\\zeta^N - 1)(\\zeta^{2^i} +1)}{(\\zeta^{2^i} - 1)(\\zeta^{2^i} +1)} \\\\\n  & = \\frac{\\zeta^N - 1}{\\zeta^{2^{i + 1}} - 1} \\cdot (\\zeta^{2^i} +1) \\\\\n  & = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1)\n\\end{aligned}\n\nTherefore, the value of s_i(\\zeta) can be calculated from s_{i + 1}(\\zeta), ands_{n-1}(\\zeta) = \\frac{\\zeta^N - 1}{\\zeta^{2^{n-1}} - 1} = \\zeta^{2^{n-1}} + 1\n\nThus, we can get an O(n) algorithm to calculate s_i(\\zeta), and it doesn’t contain division operations. The calculation process is: s_{n-1}(\\zeta) \\rightarrow s_{n-2}(\\zeta) \\rightarrow \\cdots \\rightarrow s_0(\\zeta).\n\nDefine evaluation Domain D', containing n+1 elements:D'=D\\zeta = \\{\\zeta, \\omega\\zeta, \\omega^2\\zeta,\\omega^4\\zeta, \\ldots, \\omega^{2^{n-1}}\\zeta\\}\n\nCalculate and send the values of c(X) on D'c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})\n\nCalculate and send z(\\omega^{-1}\\cdot\\zeta)\n\nCalculate Linearized Polynomial l_\\zeta(X)\\begin{split}\nl_\\zeta(X) =& \\Big(s_0(\\zeta) \\cdot (c(\\zeta) - c_0) \\\\\n& + \\alpha\\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\\\\n  & + \\alpha^2\\cdot s_1(\\zeta) \\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta)) \\\\\n  & + \\cdots \\\\\n  & + \\alpha^{n-1}\\cdot s_{n-2}(\\zeta)\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\\\\n  & + \\alpha^n\\cdot s_{n-1}(\\zeta)\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta)) \\\\\n  & + \\alpha^{n+1}\\cdot (L_0(\\zeta)\\cdot\\big(z(X) - c_0\\cdot a(X))\\\\\n  & + \\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot a(X) ) \\\\\n  & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(X) - v) \\\\\n  & - v_H(\\zeta)\\cdot t(X)\\ \\Big)\n\\end{split}\n\nObviously, l_\\zeta(\\zeta)= 0, so this computation value doesn’t need to be sent to the Verifier, and [l_\\zeta(\\tau)]_1 can be constructed by the Verifier themselves.\n\nConstruct polynomial c^*(X), which is the interpolation polynomial of the following vector on D\\zeta\\vec{c^*}= \\Big(c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), c(\\zeta)\\Big)\n\nThe Prover can use the precomputed Bary-Centric Weights \\{\\hat{w}_i\\} on D to quickly calculate c^*(X),c^*(X) = \\frac{c^*_0 \\cdot \\frac{\\hat{w}_0}{X-\\omega\\zeta} + c^*_1 \\cdot \\frac{\\hat{w}_1}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}}{\n   \\frac{\\hat{w}_0}{X-\\omega\\zeta} + \\frac{\\hat{w}_1}{X-\\omega^2\\zeta} + \\cdots + \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}\n  }\n\nHere \\hat{w}_j are precomputed values:\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nBecause l_\\zeta(\\zeta)= 0, there exists a Quotient polynomial q_\\zeta(X) satisfyingq_\\zeta(X) = \\frac{1}{X-\\zeta}\\cdot l_\\zeta(X)\n\nConstruct vanishing polynomial z_{D_{\\zeta}}(X) on D\\zetaz_{D_{\\zeta}}(X) = (X-\\zeta\\omega)\\cdots (X-\\zeta\\omega^{2^{n-1}})(X-\\zeta)\n\nConstruct Quotient polynomial q_c(X):q_c(X) = \\frac{(c(X) - c^*(X))}{(X-\\zeta)(X-\\omega\\zeta)(X-\\omega^2\\zeta)\\cdots(X-\\omega^{2^{n-1}}\\zeta)}\n\nConstruct Quotient polynomial q_{\\omega\\zeta}(X)q_{\\omega\\zeta}(X) = \\frac{z(X) - z(\\omega^{-1}\\cdot\\zeta)}{X - \\omega^{-1}\\cdot\\zeta}\n\nSend \\big(Q_c = [q_c(\\tau)]_1, Q_\\zeta=[q_\\zeta(\\tau)]_1, Q_{\\omega\\zeta}=[q_{\\omega\\zeta}(\\tau)]_1,  \\big)","type":"content","url":"/ph23/ph23-pcs-02#round-3","position":23},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 4.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-02#round-4","position":24},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl4":"Round 4.","lvl3":"Evaluation Proof Protocol","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Verifier sends the second random challenge point \\xi\\leftarrow_{\\$}\\mathbb{F}_p\n\nProver constructs the third Quotient polynomial q_\\xi(X)q_\\xi(X) = \\frac{c(X) - c^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot q_c(X)}{X-\\xi}\n\nProver calculates and sends Q_\\xiQ_\\xi = \\mathsf{KZG10.Commit}(q_\\xi(X)) = [q_\\xi(\\tau)]_1","type":"content","url":"/ph23/ph23-pcs-02#round-4","position":25},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Proof","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#proof","position":26},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Proof","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"7\\cdot\\mathbb{G}_1, (n+1)\\cdot\\mathbb{F}\\begin{aligned}\n\\pi_{eval} &= \\big(z(\\omega^{-1}\\cdot\\zeta), c(\\zeta)，c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), \\\\\n& \\qquad C_{c}, C_{t}, C_{z}, Q_c, Q_\\zeta, Q_\\xi, Q_{\\omega\\zeta}\\big)\n\\end{aligned}","type":"content","url":"/ph23/ph23-pcs-02#proof","position":27},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Verification Process","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-02#verification-process","position":28},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl3":"Verification Process","lvl2":"2. PH23+KZG10 Protocol (Optimized Version)"},"content":"Verifier calculates c^*(\\xi) using precomputed Barycentric Weights \\{\\hat{w}_i\\}c^*(\\xi)=\\frac{\\sum_i c_i^*\\frac{\\hat{w}_i}{\\xi-x_i}}{\\sum_i \\frac{\\hat{w}_i}{\\xi-x_i}}\n\nThen compute the corresponding commitment C^(\\xi)=[c^(\\xi)]_1.\"\n\nVerifier calculates v_H(\\zeta), L_0(\\zeta), L_{N-1}(\\zeta)v_H(\\zeta) = \\zeta^N - 1L_0(\\zeta) = \\frac{1}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-1}L_{N-1}(\\zeta) = \\frac{\\omega^{N-1}}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-\\omega^{N-1}}\n\nVerifier calculates s_0(\\zeta), \\ldots, s_{n-1}(\\zeta), which can be calculated using the recursive method mentioned earlier.\n\nVerifier calculates z_{D_\\zeta}(\\xi) ，z_{D_{\\zeta}}(\\xi) = (\\xi-\\zeta\\omega)\\cdots (\\xi-\\zeta\\omega^{2^{n-1}})(\\xi-\\zeta)\n\nVerifier calculates the commitment of the linearized polynomial C_l\\begin{split}\nC_l & = \n\\Big( \\Big((c(\\zeta) - c_0)s_0(\\zeta) \\\\\n& + \\alpha \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\cdot s_0(\\zeta)\\\\\n  & + \\alpha^2\\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta))\\cdot s_1(\\zeta)  \\\\\n  & + \\cdots \\\\\n  & + \\alpha^{n-1}\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\cdot s_{n-2}(\\zeta)\\\\\n  & + \\alpha^n\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta))\\cdot s_{n-1}(\\zeta) \\Big) \\cdot [1]_1 \\\\\n  & + \\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot(C_z - c_0\\cdot C_a)\\\\\n  & + \\alpha^{n+2}\\cdot (\\zeta-1)\\cdot\\big(C_z - z(\\omega^{-1}\\cdot \\zeta) \\cdot [1]_1 -c(\\zeta)\\cdot C_{a} ) \\\\\n  & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(C_z - v \\cdot [1]_1) \\\\\n  & - v_H(\\zeta)\\cdot C_t \\Big)\n\\end{split}\n\nVerifier generates a random number \\eta to merge the following Pairing verifications:\\begin{split}\ne(C_l + \\zeta\\cdot Q_\\zeta, [1]_2)\\overset{?}{=}e(Q_\\zeta, [\\tau]_2)\\\\\ne(C_c - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi, [1]_2) \\overset{?}{=} e(Q_\\xi, [\\tau]_2)\\\\\ne(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1, [1]_2) \\overset{?}{=} e(Q_{\\omega\\zeta}, [\\tau]_2)\\\\\n\\end{split}\n\nAfter merging, the verification only needs two Pairing operations.\\begin{split}\nP &= \\Big(C_l + \\zeta\\cdot Q_\\zeta\\Big) \\\\\n& + \\eta\\cdot \\Big(C_c - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi\\Big) \\\\\n& + \\eta^2\\cdot\\Big(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1\\Big)\n\\end{split}e\\Big(P, [1]_2\\Big) \\overset{?}{=} e\\Big(Q_\\zeta + \\eta\\cdot Q_\\xi + \\eta^2\\cdot Q_{\\omega\\zeta}, [\\tau]_2\\Big)","type":"content","url":"/ph23/ph23-pcs-02#verification-process","position":29},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"3. Optimized Performance Analysis"},"type":"lvl2","url":"/ph23/ph23-pcs-02#id-3-optimized-performance-analysis","position":30},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"3. Optimized Performance Analysis"},"content":"Proof size:  7~\\mathbb{G}_1 + (n+1)~\\mathbb{F}\n\nProver’s cost\n\nCommit phase: O(N\\log N)~\\mathbb{F} + \\mathbb{G}_1\n\nEvaluation phase: O(N\\log N)~\\mathbb{F} + 7~\\mathbb{G}_1\n\nVerifier’s cost: 4~\\mathbb{F} + O(n)~\\mathbb{F}+ 3~\\mathbb{G}_1 + 2~P","type":"content","url":"/ph23/ph23-pcs-02#id-3-optimized-performance-analysis","position":31},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"References"},"type":"lvl2","url":"/ph23/ph23-pcs-02#references","position":32},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 2)","lvl2":"References"},"content":"[BDFG20] Dan Boneh, Justin Drake, Ben Fisch, and Ariel Gabizon. “Efficient polynomial commitment schemes for multiple points and polynomials”. Cryptology {ePrint} Archive, Paper 2020/081. \n\nhttps://​eprint​.iacr​.org​/2020​/081.","type":"content","url":"/ph23/ph23-pcs-02#references","position":33},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)"},"type":"lvl1","url":"/ph23/ph23-pcs-fri-01","position":0},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis paper presents the PH23 protocol coupled with the univariate polynomial commitment scheme FRI-PCS.","type":"content","url":"/ph23/ph23-pcs-fri-01","position":1},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"Integrating with FRI"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-01#integrating-with-fri","position":2},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"Integrating with FRI"},"content":"Let’s first review the PH23 protocol. For an n-variable MLE polynomial \\tilde{f}(X_0,X_1, \\ldots, X_{n - 1}), it can be written in point-value form over the hypercube \\{0,1\\}^n:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nwhere N = 2^n. When proving that this MLE polynomial evaluates to v at a point \\vec{u} = (u_0, u_1, \\ldots, u_{n-1}), we have:\\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (u_0, u_1, \\ldots, u_{n-1})) = v\n\nLet c_i = \\overset{\\sim}{eq}(\\mathsf{bits}(i), (u_0, u_1, \\ldots, u_{n-1})), then the evaluation proof is transformed into proving an inner product:\\sum_{i = 0}^{N - 1} a_i \\cdot c_i = \\langle \\vec{a}, \\vec{c} \\rangle =  v\n\nSince \\vec{c} is provided by the Prover, to prevent cheating, we need to prove:\n\nThat \\vec{c} is well-formed.\n\nThat the inner product \\langle \\vec{a}, \\vec{c} \\rangle = v.\n\nTo prove the correctness of point 1, we need to prove that the following n + 1 polynomials evaluate to 0 on a multiplicative subgroup H = \\{\\omega^0, \\omega^1, \\ldots, \\omega^{N - 1}\\}.\\begin{aligned}\np_0(X) = &s_0(X)\\cdot \\big(c(X) - (1-u_0)(1-u_1)\\cdots(1-u_{n-1})\\big)      \\\\\np_1(X) = &s_0(X)\\cdot \\big(c(X)u_{n-1} - c(\\omega^{2^{n-1}}\\cdot X)(1-u_{n-1})\\big) \\\\\np_2(X) = &s_1(X)\\cdot \\big(c(X)u_{n-2} - c(\\omega^{2^{n-2}}\\cdot X)(1-u_{n-2})\\big)  \\\\\n\\cdots & \\quad\\cdots \\\\\np_{n}(X) = &s_{n-1}(X)\\cdot \\big(c(X)u_0 - c(\\omega\\cdot X)(1-u_0)\\big) \\\\\n\\end{aligned}\n\nTo prove the correctness of the inner product in point 2, we use the Grand Sum method, constructing a polynomial z(X) and constraining it with the following polynomials that must evaluate to 0 on H:\\begin{aligned}\nh_0(X) = &L_0(X)\\cdot\\big(z(X) - a_0\\cdot c_0\\big) \\\\\nh_1(X) = &(X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) = &L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{aligned}\n\nUsing a random value \\alpha provided by the Verifier, we can aggregate the above n + 4 polynomials into a single polynomial h(X):\\begin{aligned}\nh(X) &= p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\\\\  & + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X) \n\\end{aligned}\n\nNow we only need to prove that h(X) evaluates to 0 on H, which completes the proof that \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = v. Let v_H(X) be the vanishing polynomial on H, then there exists a quotient polynomial t(X) such that:h(X) = t(X) \\cdot v_H(X)\n\nTo verify the existence of this quotient polynomial, the Verifier selects a random point \\zeta, and the Prover sends t(\\zeta), h(\\zeta) to the Verifier. However, the Prover is actually committing to a(X), c(X), z(X), and t(X), so the Prover sends:\\big(a(\\zeta), c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), z(\\zeta), z(\\zeta\\cdot\\omega^{-1}), t(\\zeta)\\big)\n\nWith these values, the Verifier can calculate h(\\zeta) on their own. Since H is public, the Verifier can also calculate v_H(\\zeta), and then verify:h(\\zeta) \\stackrel{?}{=} t(\\zeta) \\cdot v_H(\\zeta)\n\nTo convince the Verifier that these values are correct, we need to use a univariate polynomial commitment scheme (PCS). Previous articles introduced using KZG10, but this paper implements it using FRI-PCS.\n\nThrough the polynomial construction process above, we know that a(X), c(X), z(X), t(X) all have degree N - 1. The polynomial a(X) needs to be opened at point \\zeta. FRI-PCS uses the DEEP technique, where Reed-Solomon encoding space \\mathsf{RS}_{k}[\\mathbb{F},D] is defined as:\\mathsf{RS}_{k}[\\mathbb{F},D] = \\{p(x)_{x \\in D} : p(X) \\in \\mathbb{F}[X], \\deg p(X) \\le k - 1 \\}\n\nThis requires the Verifier to select a random number \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F} \\setminus D. To prove the correctness of a(\\zeta), we need to prove that the quotient polynomial:q_a(X) = \\frac{a(X) - a(\\zeta)}{X - \\zeta}\n\nhas degree less than N - 1.\n\nFor c(X), which needs to be opened at n + 1 points, H_{\\zeta}' = \\{\\zeta, \\zeta\\cdot\\omega, \\zeta\\cdot\\omega^2, \\ldots, \\zeta\\cdot\\omega^{2^{n-1}} \\}, we use the method introduced in the [H22] paper’s “Multi-point queries” section to open multiple points simultaneously. The quotient polynomial is:q_c(X) = \\sum_{x \\in H_\\zeta'} \\frac{c(X) - c(x)}{X - x} = \\frac{c(X) - c(\\zeta)}{X - \\zeta} + \\frac{c(X) - c(\\zeta \\cdot \\omega)}{X - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(X) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{X - \\zeta \\cdot \\omega^{2^{n-1}}}\n\nWe now need to prove that q_c(X) has degree less than N - 1.\n\nSimilarly for z(X), we prove that the quotient polynomial:q_z(X) = \\frac{z(X) - z(\\zeta)}{X - \\zeta} + \\frac{z(X) - z(\\zeta \\cdot \\omega^{-1})}{X - \\zeta \\cdot \\omega^{-1}}\n\nhas degree less than N - 1.\n\nFor t(X), we prove that the quotient polynomial:q_t(X) = \\frac{t(X) - t(\\zeta)}{X - \\zeta}\n\nhas degree less than N - 1.\n\nAt this point, the Verifier provides a random number r \\stackrel{\\$}{\\leftarrow} \\mathbb{F}, and we can batch the four quotient polynomials together:q'(X) = q_a(X) + r \\cdot q_c(X) + r^2 \\cdot q_z(X) + r^3 \\cdot q_t(X)\n\nThis way, we only need to call FRI’s low degree test once to prove that \\deg(q'(X)) < N - 1. To interface with the FRI low degree test protocol, we need to align the degree of q'(X) to a power of 2, by getting another random number \\lambda from the Verifier and proving:q(X) = (1 + \\lambda \\cdot X) q'(X)\n\nhas degree less than N.\n\n📝 Remark:\nWhen batching different polynomials above, we could also select three different random numbers r_1, r_2, r_3 from \\mathbb{F} and set:> q'(X) = q_a(X) + r_1 \\cdot q_c(X) + r_2 \\cdot q_z(X) + r_3 \\cdot q_t(X)\n>\n\nThis method provides slightly higher security than using powers of a single random number for batching. ([BCIKS20])\n\nOne more point to note: since we use the DEEP method to construct quotient polynomials, we require that the set of evaluation points formed by the selected random number \\zeta must not intersect with the Reed-Solomon encoding group:\\{\\zeta, \\zeta\\cdot\\omega, \\zeta\\cdot\\omega^2, \\ldots, \\zeta\\cdot\\omega^{2^{n-1}}, \\zeta \\cdot \\omega^{-1}\\} \\cap D = \\emptyset","type":"content","url":"/ph23/ph23-pcs-fri-01#integrating-with-fri","position":3},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"PH23 + FRI Protocol"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-01#ph23-fri-protocol","position":4},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"PH23 + FRI Protocol"},"content":"Proof objective: For an MLE polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) with n variables, represented in point-value form:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nThe goal is to prove that \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) evaluates to v = \\tilde{f}(u_0,u_1, \\ldots, u_{n - 1}) at point \\vec{u} = (u_0,u_1, \\ldots, u_{n - 1}).","type":"content","url":"/ph23/ph23-pcs-fri-01#ph23-fri-protocol","position":5},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Commit Phase","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-01#commit-phase","position":6},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Commit Phase","lvl2":"PH23 + FRI Protocol"},"content":"For the FRI protocol, the commitment to a polynomial is computing its Reed-Solomon encoding and committing to this encoding. In the Commit phase of PCS, we need to commit to the original MLE polynomial:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nVector \\vec{a} uniquely defines an n-variable MLE polynomial, so committing to polynomial \\tilde{f} is actually committing to \\vec{a}. If using the FRI protocol, we first need to convert \\vec{a} into polynomial a(X), then commit to its Reed-Solomon encoding on D.\n\nThe Prover constructs a univariate polynomial a(X) such that its evaluation at H gives \\vec{a} = (a_{0,}a_{1},\\ldots, a_{N-1}).a(X) = a_0 \\cdot L_0(X) + a_1 \\cdot L_1(X) + \\ldots + a_{N-1} \\cdot L_{N-1}(X)\n\nThe Prover calculates the commitment C_a to polynomial a(X) and sends C_a to the Verifier:C_a = \\mathsf{cm}([a(x)|_{x \\in D}]) = \\mathsf{MT.commit}([a(x)|_{x \\in D}])","type":"content","url":"/ph23/ph23-pcs-fri-01#commit-phase","position":7},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Common Inputs","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-01#common-inputs","position":8},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Common Inputs","lvl2":"PH23 + FRI Protocol"},"content":"FRI protocol parameters: Reed-Solomon encoding regions D_n \\subset D_{n-1} \\subset \\cdots \\subset D_0 = D, rate \\rho, number of query rounds l.\n\nCommitment C_a:C_a = \\mathsf{cm}([a(x)|_{x \\in D}]) = \\mathsf{MT.commit}([a(x)|_{x \\in D}])\n\nEvaluation point \\vec{u} = (u_0,u_1, \\ldots, u_{n - 1})\n\nv = \\tilde{f}(u_0,u_1, \\ldots, u_{n - 1})","type":"content","url":"/ph23/ph23-pcs-fri-01#common-inputs","position":9},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Witness","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-01#witness","position":10},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Witness","lvl2":"PH23 + FRI Protocol"},"content":"The values of the multivariate polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) on the Boolean Hypercube \\vec{a} = (a_0,a_1, \\ldots, a_{N-1})","type":"content","url":"/ph23/ph23-pcs-fri-01#witness","position":11},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-01#evaluation-proof-protocol","position":12},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"","type":"content","url":"/ph23/ph23-pcs-fri-01#evaluation-proof-protocol","position":13},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-1","position":14},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Prover:\n\nCalculates vector \\vec{c}, where each element c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})\n\nConstructs polynomial c(X) whose evaluations on H match \\vec{c}:c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nCalculates commitment C_c to c(X) and sends C_c:C_c = \\mathsf{cm}([c(x)|_{x \\in D}]) = \\mathsf{MT.commit}([c(x)|_{x \\in D}])","type":"content","url":"/ph23/ph23-pcs-fri-01#round-1","position":15},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-2","position":16},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends challenge number \\alpha \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p\n\nProver:\n\nConstructs constraint polynomials p_0(X),\\ldots, p_{n}(X) for \\vec{c}:\\begin{split}\np_0(X) &= s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\np_k(X) &= s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big) , \\quad k=1\\ldots n\n\\end{split}\n\nAggregates \\{p_i(X)\\} into a single polynomial p(X):p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\n\nConstructs accumulator polynomial z(X) satisfying:\\begin{split}\nz(1) &= a_0\\cdot c_0 \\\\\nz(\\omega_{i}) - z(\\omega_{i-1}) &=  a(\\omega_{i})\\cdot c(\\omega_{i}), \\quad i=1,\\ldots, N-1 \\\\ \nz(\\omega^{N-1}) &= v \\\\\n\\end{split}\n\nConstructs constraint polynomials h_0(X), h_1(X), h_2(X) satisfying:\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - c_0\\cdot a(X) \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{split}\n\nAggregates p(X) and h_0(X), h_1(X), h_2(X) into polynomial h(X):\\begin{split}\nh(X) &= p(X) + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{split}\n\nCalculates Quotient polynomial t(X) satisfying:h(X) =t(X)\\cdot v_H(X)\n\nCalculates commitments C_t, C_z to t(X) and z(X) and sends them to the Verifier:\\begin{split}\nC_t &= \\mathsf{cm}([t(x)|_{x \\in D}]) = \\mathsf{MT.commit}([t(x)|_{x \\in D}]) \\\\\nC_z &= \\mathsf{cm}([z(x)|_{x \\in D}]) = \\mathsf{MT.commit}([z(x)|_{x \\in D}])\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-fri-01#round-2","position":17},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-3","position":18},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends random evaluation point \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p^* \\setminus D\n\nProver:\n\nCalculates s_i(X) evaluations at \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nThe Prover can efficiently calculate s_i(\\zeta) using the formula:\\begin{aligned}\n  s_i(\\zeta) & = \\frac{\\zeta^N - 1}{\\zeta^{2^i} - 1} \\\\\n  & = \\frac{(\\zeta^N - 1)(\\zeta^{2^i} +1)}{(\\zeta^{2^i} - 1)(\\zeta^{2^i} +1)} \\\\\n  & = \\frac{\\zeta^N - 1}{\\zeta^{2^{i + 1}} - 1} \\cdot (\\zeta^{2^i} +1) \\\\\n  & = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1)\n\\end{aligned}\n\nThis means s_i(\\zeta) can be calculated from s_{i + 1}(\\zeta), with:s_{n-1}(\\zeta) = \\frac{\\zeta^N - 1}{\\zeta^{2^{n-1}} - 1} = \\zeta^{2^{n-1}} + 1\n\nThis gives an O(n) algorithm to calculate s_i(\\zeta) without division operations. The calculation proceeds: s_{n-1}(\\zeta) \\rightarrow s_{n-2}(\\zeta) \\rightarrow \\cdots \\rightarrow s_0(\\zeta).\n\nDefines evaluation Domain H_\\zeta' with n+1 elements:H_\\zeta'=\\zeta H = \\{\\zeta, \\omega\\zeta, \\omega^2\\zeta,\\omega^4\\zeta, \\ldots, \\omega^{2^{n-1}}\\zeta\\}\n\nCalculates and sends evaluations of c(X) on H_\\zeta':c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})\n\nCalculates and sends z(\\zeta) and z(\\omega^{-1}\\cdot\\zeta)\n\nCalculates and sends t(\\zeta)\n\nCalculates and sends a(\\zeta)","type":"content","url":"/ph23/ph23-pcs-fri-01#round-3","position":19},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-4","position":20},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends random number r \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p\n\nProver:\n\nCalculates quotient polynomial q_a(X):q_a(X) = \\frac{a(X) - a(\\zeta)}{X - \\zeta}\n\nCalculates quotient polynomial q_c(X):q_c(X) = \\sum_{x \\in H_\\zeta'} \\frac{c(X) - c(x)}{X - x} = \\frac{c(X) - c(\\zeta)}{X - \\zeta} + \\frac{c(X) - c(\\zeta \\cdot \\omega)}{X - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(X) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{X - \\zeta \\cdot \\omega^{2^{n-1}}}\n\nCalculates quotient polynomial q_z(X):q_z(X) = \\frac{z(X) - z(\\zeta)}{X - \\zeta} + \\frac{z(X) - z(\\zeta \\cdot \\omega^{-1})}{X - \\zeta \\cdot \\omega^{-1}}\n\nCalculates quotient polynomial q_t(X):q_t(X) = \\frac{t(X) - t(\\zeta)}{X - \\zeta}\n\nBatches the four quotient polynomials using powers of random number r:q'(X) = q_a(X) + r \\cdot q_c(X) + r^2 \\cdot q_z(X) + r^3 \\cdot q_t(X)","type":"content","url":"/ph23/ph23-pcs-fri-01#round-4","position":21},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-5","position":22},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"This round aligns the quotient polynomial q'(X) to a power of 2 to interface with the FRI protocol:\n\nVerifier sends random number \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver calculates:q(X) = (1 + \\lambda \\cdot X) q'(X)\n\non domain D.","type":"content","url":"/ph23/ph23-pcs-fri-01#round-5","position":23},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-6","position":24},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Prover and Verifier interact in the FRI low degree test to prove q(X) has degree less than 2^n:\\pi_{q} = \\mathsf{FRI.LDT}(q(X), 2^n)\n\nThis includes n rounds of interaction, until the original polynomial is folded into a constant polynomial. The specific interaction process is:\n\nLet q^{(0)}(x)|_{x \\in D} := q(x)|_{x \\in D}\n\nFor i = 1,\\ldots, n:\n\nVerifier sends random number \\alpha^{(i)}\n\nFor any y \\in D_i, find x \\in D_{i - 1} such that x^2 = y, Prover calculates:q^{(i)}(y) = \\frac{q^{(i - 1)}(x) + q^{(i - 1)}(-x)}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(x) - q^{(i - 1)}(-x)}{2x}\n\nIf i < n, Prover sends Merkle Tree commitment to [q^{(i)}(x)|_{x \\in D_{i}}]:\\mathsf{cm}(q^{(i)}(X)) = \\mathsf{cm}([q^{(i)}(x)|_{x \\in D_{i}}]) = \\mathsf{MT.commit}([q^{(i)}(x)|_{x \\in D_{i}}])\n\nIf i = n, choose any x_0 \\in D_{n}, Prover sends the value of q^{(i)}(x_0).\n\n📝 Notes\n\nIf the folding count r < n, then the polynomial won’t fold to a constant, so the Prover will send a Merkle Tree commitment in round r rather than sending a value.","type":"content","url":"/ph23/ph23-pcs-fri-01#round-6","position":25},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#round-7","position":26},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"This round continues the FRI protocol’s low degree test query phase. The Verifier repeats the query l times, each time selecting a random number from D_0 and having the Prover send the folded values and corresponding Merkle Paths to verify the correctness of each folding round.\n\nRepeat l times:\n\nVerifier randomly selects s^{(0)} \\stackrel{\\$}{\\leftarrow} D_0\n\nProver opens commitments to a(s^{(0)}), a(-s^{(0)},c(s^{(0)}),c(-s^{(0)}),z(s^{(0)}),z(-s^{(0)}),t(s^{(0)}),t(-s^{(0)}), sending these values and their Merkle Paths:(a(s^{(0)}), \\pi_{a}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([a(x)|_{x \\in D_0}], s^{(0)})(a(-s^{(0)}), \\pi_{a}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([a(x)|_{x \\in D_0}], -s^{(0)})(c(s^{(0)}), \\pi_{c}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([c(x)|_{x \\in D_0}], s^{(0)})(c(-s^{(0)}), \\pi_{c}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([c(x)|_{x \\in D_0}], -s^{(0)})(z(s^{(0)}), \\pi_{z}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([z(x)|_{x \\in D_0}], s^{(0)})(z(-s^{(0)}), \\pi_{z}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([z(x)|_{x \\in D_0}], -s^{(0)})(t(s^{(0)}), \\pi_{t}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([t(x)|_{x \\in D_0}], s^{(0)})(t(-s^{(0)}), \\pi_{t}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([t(x)|_{x \\in D_0}], -s^{(0)})\n\nProver computes s^{(1)} = (s^{(0)})^2\n\nFor i = 1, \\ldots, n - 1\n\nProver sends the values of q^{(i)}(s^{(i)}) and q^{(i)}(-s^{(i)}), along with their Merkle paths:\\{(q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], s^{(i)})\\{(q^{(i)}(-s^{(i)}), \\pi_{q}^{(i)}(-s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], -s^{(i)})\n\nProver computes s^{(i + 1)} = (s^{(i)})^2\n\nIf the number of folding rounds r < n, then the final step requires sending the value of q^{(r)}(s^{(r)}) along with its Merkle path.","type":"content","url":"/ph23/ph23-pcs-fri-01#round-7","position":27},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#proof","position":28},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"The proof sent by the Prover is:\\pi = (C_c,C_t, C_z, c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), z(\\zeta), z(\\omega^{-1} \\cdot \\zeta), t(\\zeta), a(\\zeta), \\pi_{q})\n\nUsing the notation \\{\\cdot\\}^l to denote the proof generated by repeating the query l times in the FRI low degree test phase, the FRI low degree test proof is:\\begin{aligned}\n  \\pi_{q} = &  ( \\mathsf{cm}(q^{(1)}(X)), \\ldots, \\mathsf{cm}(q^{(n - 1)}(X)),q^{(n)}(x_0),  \\\\\n  & \\, \\{a(s^{(0)}), \\pi_{a}(s^{(0)}), a(- s^{(0)}), \\pi_{a}(-s^{(0)}),\\\\\n  & \\quad c(s^{(0)}), \\pi_{c}(s^{(0)}), c(- s^{(0)}), \\pi_{c}(-s^{(0)}), \\\\\n  & \\quad z(s^{(0)}), \\pi_{z}(s^{(0)}), z(- s^{(0)}), \\pi_{z}(-s^{(0)}), \\\\\n  & \\quad t(s^{(0)}), \\pi_{t}(s^{(0)}), t(- s^{(0)}), \\pi_{t}(-s^{(0)}), \\\\\n  & \\quad q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)}),q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)}), \\ldots, \\\\\n  & \\quad q^{(n - 1)}(s^{(n - 1)}), \\pi_{q^{(n - 1)}}(s^{(n - 1)}),q^{(n - 1)}(-s^{(n - 1)}), \\pi_{q^{(i)}}(-s^{(n - 1)})\\}^l)\n\\end{aligned}","type":"content","url":"/ph23/ph23-pcs-fri-01#proof","position":29},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-01#verification","position":30},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier calculates s_0(\\zeta), \\ldots, s_{n-1}(\\zeta) using the recursive method mentioned earlier.\n\nVerifier calculates p_0(\\zeta), \\ldots, p_n(\\zeta)\\begin{split}\np_0(\\zeta) &= s_0(\\zeta) \\cdot \\Big( c(\\zeta) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\np_k(\\zeta) &= s_{k-1}(\\zeta) \\cdot \\Big( u_{n-k}\\cdot c(\\zeta) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot \\zeta)\\Big) , \\quad k=1\\ldots n\n\\end{split}\n\nVerifier calculates p(\\zeta)p(\\zeta) = p_0(\\zeta) + \\alpha\\cdot p_1(\\zeta) + \\alpha^2\\cdot p_2(\\zeta) + \\cdots + \\alpha^{n}\\cdot p_{n}(\\zeta)\n\nVerifier calculates v_H(\\zeta), L_0(\\zeta), L_{N-1}(\\zeta)v_H(\\zeta) = \\zeta^N - 1L_0(\\zeta) = \\frac{1}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-1}L_{N-1}(\\zeta) = \\frac{\\omega^{N-1}}{N}\\cdot \\frac{v_{H}(\\zeta)}{\\zeta-\\omega^{N-1}}\n\nVerifier calculates h_0(\\zeta), h_1(\\zeta), h_2(\\zeta)\\begin{split}\nh_0(\\zeta) &= L_0(\\zeta)\\cdot\\big(z(\\zeta) - c_0\\cdot a(\\zeta) \\big) \\\\\nh_1(\\zeta) &= (\\zeta-1)\\cdot\\big(z(\\zeta)-z(\\omega^{-1}\\cdot \\zeta)-a(\\zeta)\\cdot c(\\zeta)) \\\\\nh_2(\\zeta) & = L_{N-1}(\\zeta)\\cdot\\big( z(\\zeta) - v \\big) \\\\\n\\end{split}\n\nVerifier calculates h(\\zeta)\\begin{split}\nh(\\zeta) &= p(\\zeta) + \\alpha^{n+1} \\cdot h_0(\\zeta) + \\alpha^{n+2} \\cdot h_1(\\zeta) + \\alpha^{n+3} \\cdot h_2(\\zeta)\n\\end{split}\n\nVerifier verifies the correctness of the quotient polynomial:h(\\zeta) \\stackrel{?}{=} t(\\zeta) \\cdot v_H(\\zeta)\n\nVerifier verifies the low degree test proof for q(X):\\mathsf{FRI.LDT.verify}(\\pi_{q}, 2^n) \\stackrel{?}{=} 1\n\nThe verification process repeats the following steps l times:\n\nVerify the correctness of a(s^{(0)}), a(-s^{(0)}), c(s^{(0)}), c(-s^{(0)}), z(s^{(0)}), z(-s^{(0)}), t(s^{(0)}), t(-s^{(0)}) using Merkle tree verification:\\mathsf{MT.verify}(\\mathsf{cm}(a(X)), a(s^{(0)}), \\pi_{a}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(a(X)), a(-s^{(0)}), \\pi_{a}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(c(X)), c(s^{(0)}), \\pi_{c}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(c(X)), c(-s^{(0)}), \\pi_{c}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(z(X)), z(s^{(0)}), \\pi_{z}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(z(X)), z(-s^{(0)}), \\pi_{z}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(t(X)), t(s^{(0)}), \\pi_{t}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(t(X)), t(-s^{(0)}), \\pi_{t}(-s^{(0)})) \\stackrel{?}{=} 1\n\nBased on the verified values, the verifier calculates q^{(0)}(s^{(0)}) and q^{(0)}(-s^{(0)}). For x \\in \\{s^{(0)}, -s^{(0)} \\}, compute:\\begin{align}\nq'(x) & = \\frac{a(x) - a(\\zeta)}{x - \\zeta} + r \\cdot \\left( \\frac{c(x) - c(\\zeta)}{x - \\zeta} + \\frac{c(x) - c(\\zeta \\cdot \\omega)}{x - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(x) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{x - \\zeta \\cdot \\omega^{2^{n-1}}}\\right) \\\\ \\\\\n& \\qquad + r^2 \\cdot \\left(\\frac{z(x) - z(\\zeta)}{x - \\zeta} + \\frac{z(x) - z(\\zeta \\cdot \\omega^{-1})}{x - \\zeta \\cdot \\omega^{-1}}\\right) + r^3 \\cdot \\frac{t(x) - t(\\zeta)}{x - \\zeta}\n\\end{align}\n\nThen compute:q^{(0)}(s^{(0)}) = (1 + \\lambda \\cdot s^{(0)}) q'(s^{(0)})q^{(0)}(-s^{(0)}) = (1 - \\lambda \\cdot s^{(0)}) q'(-s^{(0)})\n\nVerify the correctness of q^{(1)}(s^{(1)}) and q^{(1)}(-s^{(1)}):\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)})) \\stackrel{?}{=} 1\n\nVerify the 1 round of folding is correct:q^{(1)}(s^{(1)}) \\stackrel{?}{=} \\frac{q^{(0)}(s^{(0)}) + q^{(0)}(- s^{(0)})}{2} + \\alpha^{(1)} \\cdot \\frac{q^{(0)}(s^{(0)}) - q^{(0)}(- s^{(0)})}{2 \\cdot s^{(0)}}\n\nFor i = 2, \\ldots, n - 1:\n\nVerify the correctness of q^{(i)}(s^{(i)}) and q^{(i)}(-s^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(-s^{(i)}), \\pi_{q^{(i)}}(-s^{(i)})) \\stackrel{?}{=} 1\n\nVerify the i-th round of folding is correct:q^{(i)}(s^{(i)}) \\stackrel{?}{=} \\frac{q^{(i-1)}(s^{(i - 1)}) + q^{(i - 1)}(- s^{(i - 1)})}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(s^{(i - 1)}) - q^{(i - 1)}(- s^{(i - 1)})}{2 \\cdot s^{(i - 1)}}\n\nVerify that the polynomial has been folded down to a constant:q^{(n)}(x_0) \\stackrel{?}{=} \\frac{q^{(n-1)}(s^{(n - 1)}) + q^{(n - 1)}(- s^{(n - 1)})}{2} + \\alpha^{(n)} \\cdot \\frac{q^{(n - 1)}(s^{(n - 1)}) - q^{(n - 1)}(- s^{(n - 1)})}{2 \\cdot s^{(n - 1)}}","type":"content","url":"/ph23/ph23-pcs-fri-01#verification","position":31},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"Summary"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-01#summary","position":32},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"Summary"},"content":"This paper implements MLE polynomial PCS by connecting the PH23 protocol with the FRI protocol. The inner product proof is implemented through Grand Sum. This protocol can also be implemented through Univariate Sumcheck, which will be specifically described in the next article, and compared with this protocol.","type":"content","url":"/ph23/ph23-pcs-fri-01#summary","position":33},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"References"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-01#references","position":34},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 4)","lvl2":"References"},"content":"[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284\n\n[H22] Haböck, Ulrich. “A summary on the FRI low degree test.” Cryptology ePrint Archive (2022).\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.","type":"content","url":"/ph23/ph23-pcs-fri-01#references","position":35},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)"},"type":"lvl1","url":"/ph23/ph23-pcs-fri-02","position":0},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nIn the previous article The Missing Protocol PH23-PCS (Four) in the Connecting to FRI section, we reviewed that the PH23 protocol proof is divided into two parts:\n\nProving that \\vec{c} is Well-Formedness.\n\nProving that the inner product \\langle \\vec{a}, \\vec{c} \\rangle = v.\n\nTo prove the correctness of part 1, we need to prove that the following n + 1 polynomials all evaluate to 0 on a multiplicative subgroup H = \\{\\omega^0, \\omega^1, \\ldots, \\omega^{N - 1}\\}.\\begin{aligned}\np_0(X) = &s_0(X)\\cdot \\big(c(X) - (1-u_0)(1-u_1)\\cdots(1-u_{n-1})\\big)      \\\\\np_1(X) = &s_0(X)\\cdot \\big(c(X)u_{n-1} - c(\\omega^{2^{n-1}}\\cdot X)(1-u_{n-1})\\big) \\\\\np_2(X) = &s_1(X)\\cdot \\big(c(X)u_{n-2} - c(\\omega^{2^{n-2}}\\cdot X)(1-u_{n-2})\\big)  \\\\\n\\cdots & \\quad\\cdots \\\\\np_{n}(X) = &s_{n-1}(X)\\cdot \\big(c(X)u_0 - c(\\omega\\cdot X)(1-u_0)\\big) \\\\\n\\end{aligned}\n\nIn the protocol introduced in The Missing Protocol PH23-PCS (Four), the Grand Sum method was used to prove the correctness of part 2. Here, we use the Univariate Sumcheck protocol to prove the inner product. We decompose a(X) \\cdot c(X) as follows:a(X)\\cdot c(X) = q_{ac}(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N), \\quad \\deg(g)<N-1\n\nIf we prove that the above equation holds and \\deg (g) < N - 1, we have effectively proven that the inner product is correct: \\langle \\vec{a}, \\vec{c} \\rangle = v.\n\nLet’s see how the Verifier validates that the above polynomials hold and that \\deg (g) < N - 1.\n\nProving that \\vec{c} is Well-Formedness.\n\nFirst, using the random number \\alpha provided by the Verifier, we aggregate the n + 1 polynomials p_i(X) into a single polynomial p(X):p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\n\nNow we need to show that p(X) evaluates to 0 on H. Let v_H(X) be the Vanishing polynomial for H, then there exists a quotient polynomial t(X) satisfying:p(X) = t(X) \\cdot v_H(X)\n\nTo verify the existence of the quotient polynomial, the Verifier selects a random point \\zeta, and the Prover sends:\\big(c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), t(\\zeta)\\big)\n\nThe Verifier can compute p(\\zeta), calculate v_H(\\zeta) independently, and verify:p(\\zeta) \\stackrel{?}{=} t(\\zeta) \\cdot v_H(\\zeta)\n\nProving the inner product \\langle \\vec{a}, \\vec{c} \\rangle = v.\n\nTo prove:a(X)\\cdot c(X) = q_{ac}(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N), \\quad \\deg(g)<N-1\n\nWe can use the same random number \\zeta, and the Prover sends:\\big(a(\\zeta), q_{ac}(\\zeta), g(\\zeta)\\big)\n\nThe Verifier checks:a(\\zeta)\\cdot c(\\zeta) \\overset{?}{=} q_{ac}(\\zeta)\\cdot v_H(\\zeta) + \\zeta\\cdot g(\\zeta) + (v/N)\n\nAdditionally, we need to prove \\deg(g)<N-1, which can be done using FRI’s low degree test.\n\nTo demonstrate that the values sent by the Prover above are correct, we need to use FRI-PCS. To integrate with the FRI protocol, let’s first analyze the degrees of these polynomials. Since a(X) and c(X) are obtained from \\vec{a} and \\vec{c} respectively:\\deg(a(X)) = N - 1, \\quad \\deg(c(X)) = N - 1\n\nAnd:s_i(X) = \\frac{v_H(X)}{v_{H_i}(X)} = \\frac{X^N-1}{X^{2^i}-1}\n\nWe can determine that \\deg(s_i) = N - 2^i, therefore:\\deg(p(X)) = \\deg(p_0(X)) = \\deg(s_0(X)) + \\deg(c(X)) = N - 1+ N - 1 = 2N - 2\\deg(t(X)) = \\deg(p(X)) - \\deg(v_H(X)) = 2N - 2 - N = N - 2\n\nFrom the decomposition of a(X) \\cdot c(X), we can deduce:\\deg(q_{ac}(X)) = \\deg(a(X) \\cdot c(X)) - \\deg(v_H(X)) = 2N - 2 - N = N - 2\\deg(g(X)) = N - 1 - 1 = N - 2\n\nTo call FRI’s low degree test only once, we first perform degree correction. We ask the Verifier for a random number r \\stackrel{\\$}{\\leftarrow} \\mathbb{F}:t'(X) = t(X) + r \\cdot X \\cdot t(X)q'_{ac}(X) = q_{ac}(X) + r \\cdot X \\cdot q_{ac}(X)g'(X) = g(X) + r \\cdot X \\cdot g(X)\n\nNow the polynomials a(X), c(X), t'(X), q'_{ac}(X), g'(X) all have degree N - 1. The values sent by the Prover are:\\big(c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), t(\\zeta),a(\\zeta), q_{ac}(\\zeta), g(\\zeta)\\big)\n\nTo prove that the values sent above are correct, a function may need to be opened at multiple points simultaneously. We adopt the same method of constructing quotient polynomials as introduced in the Connecting to FRI section of this article.\n\nFor a(X), prove that the quotient polynomialq_a(X) = \\frac{a(X) - a(\\zeta)}{X - \\zeta}\n\nhas a degree less than N - 1.\n\nFor c(X), prove that the quotient polynomialq_c(X) = \\sum_{x \\in H_\\zeta'} \\frac{c(X) - c(x)}{X - x} = \\frac{c(X) - c(\\zeta)}{X - \\zeta} + \\frac{c(X) - c(\\zeta \\cdot \\omega)}{X - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(X) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{X - \\zeta \\cdot \\omega^{2^{n-1}}}\n\nhas a degree less than N - 1.\n\nFor t(X), use the quotient polynomial of t'(X) to prove thatq_{t'}(X) = \\frac{t'(X) - t'(\\zeta)}{X - \\zeta}\n\nhas a degree less than N - 1.\n\nFor q_{ac}(X), use the quotient polynomial of q_{ac}'(X) to prove thatq_{q_{ac}'}(X) = \\frac{q_{ac}'(X) - q_{ac}'(\\zeta)}{X - \\zeta}\n\nhas a degree less than N - 1.\n\nFor g(X), use the quotient polynomial of g'(X) to prove thatq_{g'}(X) = \\frac{g'(X) - g'(\\zeta)}{X - \\zeta}\n\nhas a degree less than N - 1. This naturally proves that \\deg(g(X)) < N - 1.\n\nNext, we batch these 5 low degree tests into a single low degree test proof using powers of the random number r:q'(X) = q_a(X) + r \\cdot q_c(X) + r^2 \\cdot q_{t'}(X) + r^4 \\cdot q_{q_{ac}'}(X) + r^6 \\cdot q_{g'}(X)\n\nNote that since t'(X), q_{ac}'(X), g'(X) polynomials have already used the random number r for degree correction, to achieve the effect of multiple random numbers with powers of a single random number, the batch powers are not increasing naturally as (1, r, r^2, r^3, r^4) but rather as (1, r, r^2, r^4, r^6).\n\nNow we just need to use FRI’s low degree test to prove that \\deg(q'(X)) < N - 1. Finally, to interface with the FRI low degree test protocol, we need to align the degree of q'(X) to a power of 2 by asking the Verifier for a random number \\lambda and proving that:q(X) = (1 + \\lambda \\cdot X) q'(X)\n\nhas a degree less than N.","type":"content","url":"/ph23/ph23-pcs-fri-02","position":1},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"PH23 + FRI Protocol"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-02#ph23-fri-protocol","position":2},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"PH23 + FRI Protocol"},"content":"","type":"content","url":"/ph23/ph23-pcs-fri-02#ph23-fri-protocol","position":3},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Commit Phase","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-02#commit-phase","position":4},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Commit Phase","lvl2":"PH23 + FRI Protocol"},"content":"For the FRI protocol, the commitment to a polynomial is the commitment to its Reed-Solomon encoding. In the PCS Commit phase, we need to commit to the original MLE polynomial:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nVector \\vec{a} uniquely defines an n-variable MLE polynomial. To commit to the MLE polynomial \\tilde{f}, we essentially commit to \\vec{a}. If using the FRI protocol, we first convert \\vec{a} into a polynomial a(X), then commit to its Reed-Solomon encoding on D.\n\nThe Prover constructs a univariate polynomial a(X) such that its evaluations on H are \\vec{a} = (a_{0,}a_{1},\\ldots, a_{N-1}).a(X) = a_0 \\cdot L_0(X) + a_1 \\cdot L_1(X) + \\ldots + a_{N-1} \\cdot L_{N-1}(X)\n\nThe Prover computes the commitment C_a to polynomial a(X) and sends C_a to the Verifier:C_a = \\mathsf{cm}([a(x)|_{x \\in D}]) = \\mathsf{MT.commit}([a(x)|_{x \\in D}])","type":"content","url":"/ph23/ph23-pcs-fri-02#commit-phase","position":5},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Public Input","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-02#public-input","position":6},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Public Input","lvl2":"PH23 + FRI Protocol"},"content":"FRI protocol parameters: Reed-Solomon encoding domains D_n \\subset D_{n-1} \\subset \\cdots \\subset D_0 = D, code rate \\rho, number of query rounds l.\n\nCommitment C_a:C_a = \\mathsf{cm}([a(x)|_{x \\in D}]) = \\mathsf{MT.commit}([a(x)|_{x \\in D}])\n\nEvaluation point \\vec{u} = (u_0,u_1, \\ldots, u_{n - 1})\n\nv = \\tilde{f}(u_0,u_1, \\ldots, u_{n - 1})","type":"content","url":"/ph23/ph23-pcs-fri-02#public-input","position":7},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Witness","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-02#witness","position":8},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Witness","lvl2":"PH23 + FRI Protocol"},"content":"Values of the multivariate polynomial \\tilde{f}(X_0, X_1, \\ldots, X_{n - 1}) on the Boolean Hypercube: \\vec{a} = (a_0,a_1, \\ldots, a_{N-1})","type":"content","url":"/ph23/ph23-pcs-fri-02#witness","position":9},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl3","url":"/ph23/ph23-pcs-fri-02#evaluation-proof-protocol","position":10},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"","type":"content","url":"/ph23/ph23-pcs-fri-02#evaluation-proof-protocol","position":11},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-1","position":12},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Prover:\n\nComputes vector \\vec{c}, where each element c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})\n\nConstructs polynomial c(X) such that its evaluations on H are exactly \\vec{c}.c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nComputes the commitment C_c to c(X) and sends C_c:C_c = \\mathsf{cm}([c(x)|_{x \\in D}]) = \\mathsf{MT.commit}([c(x)|_{x \\in D}])\n\nDecomposes a(X) \\cdot c(X) to compute q_{ac}(X) and g(X) satisfying:a(X)\\cdot c(X) = q_{ac}(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N)\n\nComputes the commitment C_{q_{ac}} to q_{ac}(X) and sends C_{q_{ac}}:C_{q_{ac}} = \\mathsf{cm}([q_{ac}(x)|_{x \\in D}]) = \\mathsf{MT.commit}([q_{ac}(x)|_{x \\in D}])\n\nComputes the commitment C_g to g(X) and sends C_{g}:C_{g} = \\mathsf{cm}([g(x)|_{x \\in D}]) = \\mathsf{MT.commit}([g(x)|_{x \\in D}])","type":"content","url":"/ph23/ph23-pcs-fri-02#round-1","position":13},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-2","position":14},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends challenge number \\alpha \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p\n\nProver:\n\nConstructs constraint polynomials p_0(X),\\ldots, p_{n}(X) for \\vec{c}:\\begin{split}\n\tp_0(X) &= s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\n\tp_k(X) &= s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big) , \\quad k=1\\ldots n\n\t\\end{split}\n\nAggregates \\{p_i(X)\\} into a single polynomial p(X):p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\n\nComputes the Quotient polynomial t(X) satisfying:p(X) =t(X)\\cdot v_H(X)\n\nComputes the commitment C_t to t(X) and sends it to the Verifier:\\begin{split}\n\tC_t &= \\mathsf{cm}([t(x)|_{x \\in D}]) = \\mathsf{MT.commit}([t(x)|_{x \\in D}])\n\t\\end{split}","type":"content","url":"/ph23/ph23-pcs-fri-02#round-2","position":15},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-3","position":16},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends random evaluation point \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p^* \\setminus D\n\nProver:\n\nComputes the evaluations of s_i(X) at \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nThe Prover can efficiently compute s_i(\\zeta) using the formula for s_i(X):\\begin{aligned}\n\t  s_i(\\zeta) & = \\frac{\\zeta^N - 1}{\\zeta^{2^i} - 1} \\\\\n\t  & = \\frac{(\\zeta^N - 1)(\\zeta^{2^i} +1)}{(\\zeta^{2^i} - 1)(\\zeta^{2^i} +1)} \\\\\n\t  & = \\frac{\\zeta^N - 1}{\\zeta^{2^{i + 1}} - 1} \\cdot (\\zeta^{2^i} +1) \\\\\n\t  & = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1)\n\t\\end{aligned}\n\nThus, s_i(\\zeta) can be computed from s_{i + 1}(\\zeta), and:s_{n-1}(\\zeta) = \\frac{\\zeta^N - 1}{\\zeta^{2^{n-1}} - 1} = \\zeta^{2^{n-1}} + 1\n\nThis gives us an O(n) algorithm to compute s_i(\\zeta) without division operations. The computation sequence is: s_{n-1}(\\zeta) \\rightarrow s_{n-2}(\\zeta) \\rightarrow \\cdots \\rightarrow s_0(\\zeta).\n\nDefines the evaluation Domain H_\\zeta' containing n+1 elements:H_\\zeta'=\\zeta H = \\{\\zeta, \\omega\\zeta, \\omega^2\\zeta,\\omega^4\\zeta, \\ldots, \\omega^{2^{n-1}}\\zeta\\}\n\nComputes and sends the evaluations of c(X) on H_\\zeta':c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})\n\nComputes and sends t(\\zeta)\n\nComputes and sends a(\\zeta)\n\nComputes and sends q_{ac}(\\zeta)\n\nComputes and sends g(\\zeta)","type":"content","url":"/ph23/ph23-pcs-fri-02#round-3","position":17},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-4","position":18},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier: Sends random number r \\stackrel{\\$}{\\leftarrow} \\mathbb{F}_p\n\nProver:\n\nPerforms degree correction, computing polynomials t'(X), q'_{ac}(X), g'(X):t'(X) = t(X) + r \\cdot X \\cdot t(X)q'_{ac}(X) = q_{ac}(X) + r \\cdot X \\cdot q_{ac}(X)g'(X) = g(X) + r \\cdot X \\cdot g(X)\n\nComputes quotient polynomial q_a(X):q_a(X) = \\frac{a(X) - a(\\zeta)}{X - \\zeta}\n\nComputes quotient polynomial q_c(X):q_c(X) = \\sum_{x \\in H_\\zeta'} \\frac{c(X) - c(x)}{X - x} = \\frac{c(X) - c(\\zeta)}{X - \\zeta} + \\frac{c(X) - c(\\zeta \\cdot \\omega)}{X - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(X) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{X - \\zeta \\cdot \\omega^{2^{n-1}}}\n\nComputes quotient polynomial q_{t'}(X):q_{t'}(X) = \\frac{t'(X) - t'(\\zeta)}{X - \\zeta}\n\nComputes quotient polynomial q_{q_{ac}'}(X):q_{q_{ac}'}(X) = \\frac{q_{ac}'(X) - q_{ac}'(\\zeta)}{X - \\zeta}\n\nComputes quotient polynomial q_{g'}(X):q_{g'}(X) = \\frac{g'(X) - g'(\\zeta)}{X - \\zeta}\n\nBatches the 5 quotient polynomials using powers of r:q'(X) = q_a(X) + r \\cdot q_c(X) + r^2 \\cdot q_{t'}(X) + r^4 \\cdot q_{q_{ac}'}(X) + r^6 \\cdot q_{g'}(X)","type":"content","url":"/ph23/ph23-pcs-fri-02#round-4","position":19},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-5","position":20},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"This round aligns the quotient polynomial q'(X) to a power of 2 to interface with the FRI protocol.\n\nVerifier sends random number \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver computesq(X) = (1 + \\lambda \\cdot X) q'(X)\n\non D.","type":"content","url":"/ph23/ph23-pcs-fri-02#round-5","position":21},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-6","position":22},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"The Prover and Verifier interact for FRI’s low degree test to prove that q(X) has degree less than 2^n.\\pi_{q} = \\mathsf{FRI.LDT}(q(X), 2^n)\n\nThis involves n rounds of interaction, ultimately folding the original polynomial into a constant polynomial. Using i to denote the i-th round, the interaction process is:\n\nDefine q^{(0)}(x)|_{x \\in D} := q(x)|_{x \\in D}\n\nFor i = 1,\\ldots, n:\n\nVerifier sends random number \\alpha^{(i)}\n\nFor any y \\in D_i, find x \\in D_{i-1} such that y^2 = x, Prover computes:q^{(i)}(y) = \\frac{q^{(i - 1)}(x) + q^{(i - 1)}(-x)}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(x) + q^{(i - 1)}(-x)}{2x}\n\nIf i < n, Prover sends the Merkle Tree commitment of [q^{(i)}(x)|_{x \\in D_{i}}],\\mathsf{cm}(q^{(i)}(X)) = \\mathsf{cm}([q^{(i)}(x)|_{x \\in D_{i}}]) = \\mathsf{MT.commit}([q^{(i)}(x)|_{x \\in D_{i}}])\n\nIf i = n, choose any x_0 \\in D_{n}, Prover sends the value of q^{(i)}(x_0).\n\n📝 Notes\n\nIf the folding count r < n, the final result won’t be a constant polynomial. In that case, the Prover sends a Merkle Tree commitment in round r instead of sending a value.","type":"content","url":"/ph23/ph23-pcs-fri-02#round-6","position":23},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#round-7","position":24},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"This round continues the FRI protocol’s low degree test query phase. The Verifier repeats queries l times, each time selecting a random number from D_0 and asking the Prover to send the folding values from each round and corresponding Merkle Paths to verify each folding’s correctness.\n\nRepeat l times:\n\nVerifier randomly selects s^{(0)} \\stackrel{\\$}{\\leftarrow} D_0\n\nProver opens commitments to a(s^{(0)}), a(-s^{(0)},c(s^{(0)}),c(-s^{(0)}),t(s^{(0)}),t(-s^{(0)}),q_{ac}(s^{(0)}),q_{ac}(-s^{(0)}),g(s^{(0)}),g(-s^{(0)}), providing these values and their Merkle Paths:(a(s^{(0)}), \\pi_{a}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([a(x)|_{x \\in D_0}], s^{(0)})(a(-s^{(0)}), \\pi_{a}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([a(x)|_{x \\in D_0}], -s^{(0)})(c(s^{(0)}), \\pi_{c}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([c(x)|_{x \\in D_0}], s^{(0)})(c(-s^{(0)}), \\pi_{c}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([c(x)|_{x \\in D_0}], -s^{(0)})(t(s^{(0)}), \\pi_{t}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([t(x)|_{x \\in D_0}], s^{(0)})(t(-s^{(0)}), \\pi_{t}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([t(x)|_{x \\in D_0}], -s^{(0)})(q_{ac}(s^{(0)}), \\pi_{q_{ac}}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([q_{ac}(x)|_{x \\in D_0}], s^{(0)})(q_{ac}(-s^{(0)}), \\pi_{q_{ac}}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([q_{ac}(x)|_{x \\in D_0}], -s^{(0)})(g(s^{(0)}), \\pi_{g}(s^{(0)})) \\leftarrow \\mathsf{MT.open}([g(x)|_{x \\in D_0}], s^{(0)})(g(-s^{(0)}), \\pi_{g}(-s^{(0)})) \\leftarrow \\mathsf{MT.open}([g(x)|_{x \\in D_0}], -s^{(0)})\n\nProver computes s^{(1)} = (s^{(0)})^2\n\nFor i = 1, \\ldots, n - 1:\n\nProver sends values q^{(i)}(s^{(i)}), q^{(i)}(-s^{(i)}) with their Merkle Paths:\\{(q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], s^{(i)})\\{(q^{(i)}(-s^{(i)}), \\pi_{q}^{(i)}(-s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q^{(i)}(x)|_{x \\in D_i}], -s^{(i)})\n\nProver computes s^{(i + 1)} = (s^{(i)})^2\n\nIf the folding count r < n, then the last step requires sending the value q^{(r)}(s^{(r)}) with its Merkle Path.","type":"content","url":"/ph23/ph23-pcs-fri-02#round-7","position":25},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#proof","position":26},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"The proof sent by the Prover is:\\pi = (C_c,C_{q_{ac}}, C_g, C_t, c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}}), t(\\zeta), a(\\zeta), q_{ac}(\\zeta), g(\\zeta), \\pi_{q})\n\nUsing the notation \\{\\cdot\\}^l to represent the proofs generated during the l query repetitions in FRI’s low degree test, which are randomly selected, the FRI low degree test proof is:\\begin{aligned}\n  \\pi_{q} = &  ( \\mathsf{cm}(q^{(1)}(X)), \\ldots, \\mathsf{cm}(q^{(n - 1)}(X)),q^{(n)}(x_0),  \\\\\n  & \\, \\{a(s^{(0)}), \\pi_{a}(s^{(0)}), a(- s^{(0)}), \\pi_{a}(-s^{(0)}),\\\\\n  & \\quad c(s^{(0)}), \\pi_{c}(s^{(0)}), c(- s^{(0)}), \\pi_{c}(-s^{(0)}), \\\\\n  & \\quad t(s^{(0)}), \\pi_{t}(s^{(0)}), t(- s^{(0)}), \\pi_{t}(-s^{(0)}), \\\\\n  & \\quad q_{ac}(s^{(0)}), \\pi_{q_{ac}}(s^{(0)}), q_{ac}(- s^{(0)}), \\pi_{q_{ac}}(-s^{(0)}), \\\\\n  & \\quad g(s^{(0)}), \\pi_{g}(s^{(0)}), g(- s^{(0)}), \\pi_{g}(-s^{(0)}), \\\\\n  & \\quad q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)}),q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)}), \\ldots, \\\\\n  & \\quad q^{(n - 1)}(s^{(n - 1)}), \\pi_{q^{(n - 1)}}(s^{(n - 1)}),q^{(n - 1)}(-s^{(n - 1)}), \\pi_{q^{(i)}}(-s^{(n - 1)})\\}^l)\n\\end{aligned}","type":"content","url":"/ph23/ph23-pcs-fri-02#proof","position":27},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"type":"lvl4","url":"/ph23/ph23-pcs-fri-02#verification","position":28},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"PH23 + FRI Protocol"},"content":"Verifier computes s_0(\\zeta), \\ldots, s_{n-1}(\\zeta) using the recursive method mentioned earlier.\n\nVerifier computes p_0(\\zeta), \\ldots, p_n(\\zeta):\\begin{split}\n\tp_0(\\zeta) &= s_0(\\zeta) \\cdot \\Big( c(\\zeta) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\n\tp_k(\\zeta) &= s_{k-1}(\\zeta) \\cdot \\Big( u_{n-k}\\cdot c(\\zeta) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot \\zeta)\\Big) , \\quad k=1\\ldots n\n\t\\end{split}\n\nVerifier computes p(\\zeta):p(\\zeta) = p_0(\\zeta) + \\alpha\\cdot p_1(\\zeta) + \\alpha^2\\cdot p_2(\\zeta) + \\cdots + \\alpha^{n}\\cdot p_{n}(\\zeta)\n\nVerifier computes v_H(\\zeta):v_H(\\zeta) = \\zeta^N - 1\n\nVerifier checks the correctness of the quotient polynomial:p(\\zeta) \\stackrel{?}{=} t(\\zeta) \\cdot v_H(\\zeta)\n\nVerifier checks the correctness of the inner product:a(\\zeta)\\cdot c(\\zeta) \\overset{?}{=} q_{ac}(\\zeta)\\cdot v_H(\\zeta) + \\zeta\\cdot g(\\zeta) + (v/N)\n\nVerifier verifies the low degree test proof for q(X):\\mathsf{FRI.LDT.verify}(\\pi_{q}, 2^n) \\stackrel{?}{=} 1\n\nThe specific verification process repeats l times:\n\nVerify the correctness of a(s^{(0)}), a(-s^{(0)},c(s^{(0)}),c(-s^{(0)}),t(s^{(0)}),t(-s^{(0)}),q_{ac}(s^{(0)}),q_{ac}(-s^{(0)}),g(s^{(0)}),g(-s^{(0)}) by checking:\\mathsf{MT.verify}(\\mathsf{cm}(a(X)), a(s^{(0)}), \\pi_{a}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(a(X)), a(-s^{(0)}), \\pi_{a}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(c(X)), c(s^{(0)}), \\pi_{c}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(c(X)), c(-s^{(0)}), \\pi_{c}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(t(X)), t(s^{(0)}), \\pi_{t}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(t(X)), t(-s^{(0)}), \\pi_{t}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{ac}(X)), q_{ac}(s^{(0)}), \\pi_{q_{ac}}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{ac}(X)), q_{ac}(-s^{(0)}), \\pi_{q_{ac}}(-s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(g(X)), g(s^{(0)}), \\pi_{g}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(g(X)), g(-s^{(0)}), \\pi_{g}(-s^{(0)})) \\stackrel{?}{=} 1\n\nVerifier computes t'(s^{(0)}), t'(-s^{(0)}), q'_{ac}(s^{(0)}), q'_{ac}(-s^{(0)}), g'(s^{(0)}), g'(-s^{(0)}) and t'(\\zeta), q'_{ac}(\\zeta), g'(\\zeta)t'(s^{(0)}) = t(s^{(0)}) + r \\cdot s^{(0)} \\cdot t(s^{(0)}), \\qquad t'(-s^{(0)}) = t(-s^{(0)}) + r \\cdot (-s^{(0)}) \\cdot t(-s^{(0)})q_{ac}'(s^{(0)}) = q_{ac}(s^{(0)}) + r \\cdot s^{(0)} \\cdot q_{ac}(s^{(0)}), \\qquad q_{ac}'(-s^{(0)}) = q_{ac}(-s^{(0)}) + r \\cdot (-s^{(0)}) \\cdot q_{ac}(-s^{(0)})g'(s^{(0)}) = g(s^{(0)}) + r \\cdot s^{(0)} \\cdot g(s^{(0)}), \\qquad g'(-s^{(0)}) = g(-s^{(0)}) + r \\cdot (-s^{(0)}) \\cdot g(-s^{(0)})t'(\\zeta) = t(\\zeta) + r \\cdot \\zeta \\cdot t(\\zeta)q_{ac}'(\\zeta) = q_{ac}(\\zeta) + r \\cdot \\zeta \\cdot q_{ac}(\\zeta)g'(\\zeta) = g(\\zeta) + r \\cdot \\zeta \\cdot g(\\zeta)\n\nVerifier calculates q'(s^{(0)}) and q'(-s^{(0)}) using the provided values: a(s^{(0)}), a(-s^{(0)},c(s^{(0)}),c(-s^{(0)}),t(s^{(0)}),t(-s^{(0)}),q_{ac}(s^{(0)}),q_{ac}(-s^{(0)}),g(s^{(0)}),g(-s^{(0)})\\begin{align}\nq'(s^{(0)}) & = \\frac{a(s^{(0)}) - a(\\zeta)}{s^{(0)} - \\zeta} + r \\cdot \\left( \\frac{c(s^{(0)}) - c(\\zeta)}{s^{(0)} - \\zeta} + \\frac{c(s^{(0)}) - c(\\zeta \\cdot \\omega)}{s^{(0)} - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(s^{(0)}) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{s^{(0)} - \\zeta \\cdot \\omega^{2^{n-1}}}\\right) \\\\ \\\\\n& \\qquad + r^2 \\cdot \\frac{t'(s^{(0)}) - t'(\\zeta)}{s^{(0)} - \\zeta} + r^4 \\cdot \\frac{q_{ac}'(s^{(0)}) - q_{ac}'(\\zeta)}{s^{(0)} - \\zeta} + r^6 \\cdot \\frac{g'(s^{(0)}) - g'(\\zeta)}{s^{(0)} - \\zeta}\n\\end{align}\\begin{align}\nq'(-s^{(0)}) & = \\frac{a(-s^{(0)}) - a(\\zeta)}{-s^{(0)} - \\zeta} + r \\cdot \\left( \\frac{c(-s^{(0)}) - c(\\zeta)}{-s^{(0)} - \\zeta} + \\frac{c(-s^{(0)}) - c(\\zeta \\cdot \\omega)}{-s^{(0)} - \\zeta \\cdot \\omega} + \\ldots + \\frac{c(-s^{(0)}) - c(\\zeta \\cdot \\omega^{2^{n-1}})}{-s^{(0)} - \\zeta \\cdot \\omega^{2^{n-1}}}\\right) \\\\ \\\\\n& \\qquad + r^2 \\cdot \\frac{t'(-s^{(0)}) - t'(\\zeta)}{-s^{(0)} - \\zeta} + r^4 \\cdot \\frac{q_{ac}'(-s^{(0)}) - q_{ac}'(\\zeta)}{-s^{(0)} - \\zeta} + r^6 \\cdot \\frac{g'(-s^{(0)}) - g'(\\zeta)}{-s^{(0)} - \\zeta}\n\\end{align}\n\nVerifier computes:q^{(0)}(s^{(0)}) = (1 + \\lambda \\cdot s^{(0)}) q'(s^{(0)})q^{(0)}(-s^{(0)}) = (1 - \\lambda \\cdot s^{(0)}) q'(-s^{(0)})\n\nVerifier checks the correctness of q^{(1)}(s^{(1)}), q^{(1)}(-s^{(1)})\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(s^{(1)}), \\pi_{q^{(1)}}(s^{(1)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(1)}(X)), q^{(1)}(-s^{(1)}), \\pi_{q^{(1)}}(-s^{(1)})) \\stackrel{?}{=} 1\n\nVerifier checks if the first round folding is correct:q^{(1)}(s^{(1)}) \\stackrel{?}{=} \\frac{q^{(0)}(s^{(0)}) + q^{(0)}(- s^{(0)})}{2} + \\alpha^{(1)} \\cdot \\frac{q^{(0)}(s^{(0)}) - q^{(0)}(- s^{(0)})}{2 \\cdot s^{(0)}}\n\nFor i = 2, \\ldots, n - 1:\n\nVerify the correctness of q^{(i)}(s^{(i)}), q^{(i)}(-s^{(i)})\n\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(s^{(i)}), \\pi_{q^{(i)}}(s^{(i)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q^{(i)}(X)), q^{(i)}(-s^{(i)}), \\pi_{q^{(i)}}(-s^{(i)})) \\stackrel{?}{=} 1\n\nCheck if the i-th round folding is correct:q^{(i)}(s^{(i)}) \\stackrel{?}{=} \\frac{q^{(i-1)}(s^{(i - 1)}) + q^{(i - 1)}(- s^{(i - 1)})}{2} + \\alpha^{(i)} \\cdot \\frac{q^{(i - 1)}(s^{(i - 1)}) - q^{(i - 1)}(- s^{(i - 1)})}{2 \\cdot s^{(i - 1)}}\n\nCheck if the final polynomial is constant:q^{(n)}(x_0) \\stackrel{?}{=} \\frac{q^{(n-1)}(s^{(n - 1)}) + q^{(n - 1)}(- s^{(n - 1)})}{2} + \\alpha^{(n)} \\cdot \\frac{q^{(n - 1)}(s^{(n - 1)}) - q^{(n - 1)}(- s^{(n - 1)})}{2 \\cdot s^{(n - 1)}}","type":"content","url":"/ph23/ph23-pcs-fri-02#verification","position":29},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"Comparison of Two FRI Integration Approaches"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-02#comparison-of-two-fri-integration-approaches","position":30},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"Comparison of Two FRI Integration Approaches"},"content":"Comparing the protocols in The Missing Protocol PH23-PCS (Four) and this article, the main difference is in the method used to prove the inner product. We’ll refer to the protocol from The Missing Protocol PH23-PCS (Four) as Protocol 1, which uses the Grand Sum method for proving inner products and requires computing the class sum polynomial z(X), constrained by three polynomials that must evaluate to 0 on H:\\begin{aligned}\nh_0(X) = &L_0(X)\\cdot\\big(z(X) - a_0\\cdot c_0\\big) \\\\\nh_1(X) = &(X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\cdot c(X)) \\\\\nh_2(X) = &L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{aligned}\n\nProtocol 2 (this article’s protocol) uses the Univariate Sumcheck method, decomposing a(X) \\cdot c(X) to obtain q_{ac}(X) and g(X):a(X)\\cdot c(X) = q_{ac}(X)\\cdot v_H(X) + X\\cdot g(X) + (v/N), \\quad \\deg(g)<N-1\n\nLet’s compare their computational complexities:\n\nProver Computation:\n\nProtocol 1 additionally requires:\n\nComputing z(X), h_0(X), h_1(X), h_2(X)\n\nComputing commitment C_z\n\nComputing z(\\zeta), z(\\omega^{-1} \\cdot \\zeta)\n\nComputing q_z(X)\n\nProtocol 2 additionally requires:\n\nDecomposing a(X) \\cdot c(X) to get q_{ac}(X) and g(X)\n\nComputing commitments C_{q_{ac}}, C_{g}\n\nComputing q_{ac}(\\zeta), g(\\zeta)\n\nPerforming degree correction, computing t'(X), q'_{ac}(X), g'(X)\n\nComputing q_{q'_{ac}}(X), q_{g'}(X)\n\nBy comparison, the difference in Prover computational complexity is not particularly significant.\n\nProof Size:\n\nProtocol 2 additionally sends:\n\nOne more polynomial commitment, C_g\n\nIn the FRI query phase, repeated l times, it sends values and Merkle Paths for two more points\n\nProtocol 2 has a larger proof size, requiring additional hash values and field elements, with the quantity related to the repetition count l.\n\nVerifier Computation:\n\nProtocol 1 additionally requires:\n\nComputing L_0(\\zeta), L_{N-1}(\\zeta)\n\nComputing h_0(\\zeta), h_1(\\zeta), h_2(\\zeta)\nProtocol 1’s additional computation complexity is 2 ~ \\mathbb{F}_{\\mathsf{inv}} + 9 ~\\mathbb{F}_{\\mathsf{mul}}.\n\nProtocol 2 additionally requires:\n\nVerifying a(\\zeta)\\cdot c(\\zeta) \\overset{?}{=} q_{ac}(\\zeta)\\cdot v_H(\\zeta) + \\zeta\\cdot g(\\zeta) + (v/N)\n\nRepeating l times: verifying 2 more opening points, requiring hash calculations\n\nRepeating l times: computing degree-corrected polynomial values at corresponding points. For x \\in \\{s^{(0)}, -s^{(0)}, \\zeta\\}, computing t'(x), g'(x), q_{ac}'(x) from t(x), g(x), q_{ac}(x). Computing one value costs 2 ~ \\mathbb{F}_{\\mathsf{mul}}, so the total complexity is 18l ~ \\mathbb{F}_{\\mathsf{mul}}.\n\nProtocol 1’s Verifier computation is more efficient than Protocol 2’s.\n\nIn summary, Protocol 2 requires handling 5 polynomials when interfacing with FRI: a(X),c(X),t(X),q_{ac}(X),g(X), which is one more than Protocol 1. Additionally, these polynomials have inconsistent degrees, requiring degree correction for t(X),q_{ac}(X),g(X) to reach degree N-1. Since the protocol initially commits to the original polynomials and requires additional operations for degree correction, this increases complexity during the FRI low degree test.\n\nIn the query phase, Protocol 2 needs to send proofs for one more polynomial at query points, repeated l times, increasing the proof size. For the verifier, this means verifying more query point proofs and computing degree-corrected function values at query points, both related to l, increasing verification computational complexity.\n\nOur analysis shows that the complexity of interfacing with the FRI protocol depends on the number of polynomials to handle and the number of opening points. Protocol 2 deals with more polynomials, resulting in larger proof sizes and higher verifier computational complexity compared to Protocol 1.","type":"content","url":"/ph23/ph23-pcs-fri-02#comparison-of-two-fri-integration-approaches","position":31},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"References"},"type":"lvl2","url":"/ph23/ph23-pcs-fri-02#references","position":32},{"hierarchy":{"lvl1":"The Missing Protocol PH23-PCS (Part 5)","lvl2":"References"},"content":"[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284\n\n[H22] Haböck, Ulrich. “A summary on the FRI low degree test.” Cryptology ePrint Archive (2022).\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.","type":"content","url":"/ph23/ph23-pcs-fri-02#references","position":33},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)"},"type":"lvl1","url":"/ph23/ph23-pcs-zk","position":0},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)"},"content":"This article adds support for Zero-knowledge to the PH23-KZG10 protocol.","type":"content","url":"/ph23/ph23-pcs-zk","position":1},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"1. How to Support ZK"},"type":"lvl2","url":"/ph23/ph23-pcs-zk#id-1-how-to-support-zk","position":2},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"1. How to Support ZK"},"content":"To make the PH23-KZG10 protocol support ZK, we need to modify two parts of the protocol. First, we need to support Hiding in the KZG10 sub-protocol, which means that no information other than the evaluation will be leaked in any Evaluation proof. Second, we need to ensure that no information about the Witness vector \\vec{a} is leaked in the PH23 protocol.\n\nFirst, we need a Perfect Hiding KZG10 protocol that can guarantee that no information other than the polynomial evaluation is leaked after each opening of the polynomial. The following is the KZG10 protocol from [KT23], with its main ideas derived from [PST13], [ZGKPP17], and [XZZPS19].","type":"content","url":"/ph23/ph23-pcs-zk#id-1-how-to-support-zk","position":3},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Hiding KZG10","lvl2":"1. How to Support ZK"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#hiding-kzg10","position":4},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Hiding KZG10","lvl2":"1. How to Support ZK"},"content":"SRS = ([1]_1, [\\tau]_1, [\\tau^2]_1, [\\tau^3]_1, \\ldots, [\\tau^D]_1, {\\color{red}[\\gamma]_1}, [1]_2, [\\tau]_2, {\\color{red}[\\gamma]_2})\n\nThe commitment of a polynomial f(X)\\in\\mathbb{F}[X] is defined as:C_f=\\mathsf{KZG.Commit}(f(X);\\rho_f) = f_0 \\cdot [1]_1 + f_1 \\cdot [\\tau]_1 + \\cdots + f_d \\cdot [\\tau^d]_1 + {\\color{blue}\\rho_f} \\cdot {\\color{red}[\\gamma]_1}\n\nAccording to the properties of polynomial rings, f(X) can be decomposed as:f(X) = q(X)\\cdot(X-z) + f(z)\n\nThe commitment of the quotient polynomial is calculated as follows, also requiring a Blinding Factor {\\color{blue}\\rho_q} to protect the commitment of q(X).\\begin{aligned}\nQ = \\mathsf{KZG.Commit}(q(X); {\\color{blue}\\rho_q}) & = q_0 \\cdot [1]_1 + q_1 \\cdot [\\tau]_1 + \\cdots + q_d \\cdot [\\tau^{d-1}]_1 + {\\color{blue}\\rho_q} \\cdot {\\color{red}[\\gamma]_1} \\\\\n& = [q(\\tau)]_1 + {\\color{blue}\\rho_q}\\cdot{\\color{red}[\\gamma}]_1\n\\end{aligned}\n\nThe Prover also needs to calculate an additional \\mathbb{G}_1 element below to balance the verification formula:\\color{blue}E = \\rho_f \\cdot [1]_1 - \\rho_q \\cdot [\\tau]_1 + (\\rho_q\\cdot z)\\cdot [1]_1\n\nThen, the Evaluation proof consists of two \\mathbb{G}_1 elements:\\pi = (Q, {\\color{blue}E})\n\nThus, the Verifier can verify using the following formula:e\\Big(C_f - f(z)\\cdot[1]_1,\\ [1]_2\\Big) = e\\Big(Q,\\ [\\tau]_2 - z\\cdot[1]_2\\Big) + {\\color{blue}e\\Big(E,\\ {\\color{red}[\\gamma]_2}\\Big)}","type":"content","url":"/ph23/ph23-pcs-zk#hiding-kzg10","position":5},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#zk-for-sum-proof","position":6},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"In the process where the Prover uses the accumulation polynomial z(X) to prove the sum value, information about the \\vec{z} vector, including information about the Witness \\vec{a}, would also be leaked. Therefore, we need a ZK version of the sum proof protocol.\n\nWe have a multiplicative subgroup H\\subset\\mathbb{F} of order N:H=(1, \\omega, \\omega^2, \\ldots, \\omega^{N-1})\n\nWe denote \\{L_i(X)\\}_{i=0}^{N-1} as the Lagrange polynomials with respect to H, and v_H(X)=X^N-1 is the vanishing polynomial on H.\n\nSuppose we have a vector \\vec{a}=(a_0, a_1, \\ldots, a_{N-1}) with N elements, and we want to prove \\sum_i a_i = v. The Prover has actually computed the commitment of \\vec{a}, denoted as C_a.C_a = \\mathsf{KZG10.Commit}(a(X); {\\color{blue}\\rho_a}) = [a(\\tau)]_1 + {\\color{blue}\\rho_a\\cdot[\\gamma]_1}","type":"content","url":"/ph23/ph23-pcs-zk#zk-for-sum-proof","position":7},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 1","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-1","position":8},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 1","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"First, we need to determine how many times z(X) will be opened, for example, z(X) will be opened at \\zeta and \\omega^{-1}\\cdot\\zeta. Then we introduce a random polynomial: r(X),r(X) = r_0\\cdot L_0(X) + r_1\\cdot L_1(X) + r_2\\cdot L_2(X) + r_3\\cdot L_3(X)\n\nThis polynomial contains four random factors. Why four? We’ll see later.\n\nThe Prover then computes the commitment of r(X) and introduces an additional Blinding Factor {\\color{blue}\\rho_r}:C_r = \\mathsf{KZG10.Commit}(r(X); {\\color{blue}\\rho_r}) = [r(\\tau)]_1 + {\\color{blue}\\rho_r\\cdot[\\gamma]_1}\n\nThe Prover computes a new sum \\sum_i r_i:v_r = r_0 + r_1 + r_2 + r_3\n\nThe Prover sends C_r and v_r to the Verifier.","type":"content","url":"/ph23/ph23-pcs-zk#round-1","position":9},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 2","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-2","position":10},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 2","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"The Verifier sends a random challenge \\beta\\leftarrow_{\\$}\\mathbb{F} to the Prover.\n\nThe Prover constructs a new polynomial a'(X) satisfying{\\color{blue}a'(X) = a(X) + \\beta\\cdot r(X)}\n\nThe Prover sends a mixed sum value v' to the Verifier:{\\color{blue}v' = v_r + \\beta\\cdot v}\n\nAt this point, the Prover and Verifier convert the sum proof target \\sum_i a_i=v into \\sum_i (a_i + \\beta\\cdot r_i) = v + \\beta\\cdot v_r.","type":"content","url":"/ph23/ph23-pcs-zk#round-2","position":11},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 3","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-3","position":12},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 3","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"The Verifier sends another random number \\alpha\\leftarrow_{\\$}\\mathbb{F} to the Prover.\n\nThe Prover constructs constraint polynomials h_0(X), h_1(X), h_2(X) satisfying\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - a(X) \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)-a(X)\\big) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - v \\big) \\\\\n\\end{split}\n\nThe Prover constructs polynomial h(X) satisfying\\begin{split}\nh(X) &= h_0(X) + \\alpha \\cdot h_1(X) + \\alpha^2 \\cdot h_2(X)\n\\end{split}\n\nThe Prover computes the quotient polynomial t(X) satisfyingh(X) =t(X)\\cdot v_H(X)\n\nThe Prover computes the commitment of z(X), C_z, and sends C_zC_z = \\mathsf{KZG10.Commit}(z(X); \\rho_z) = [z(\\tau)]_1 + {\\color{blue}\\rho_z\\cdot[\\gamma]_1}\n\nThe Prover computes the commitment of t(X), C_t, and sends C_tC_t = \\mathsf{KZG10.Commit}(t(X); \\rho_t) = [t(\\tau)]_1 + {\\color{blue}\\rho_t\\cdot[\\gamma]_1}","type":"content","url":"/ph23/ph23-pcs-zk#round-3","position":13},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 4","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-4","position":14},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 4","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"The Verifier sends a random evaluation point \\zeta\\leftarrow_{\\$}\\mathbb{F}\n\nThe Prover constructs quotient polynomials q_{a}(X), q_z(X), q_t(X), and q'_z(X) satisfyingq_{a}(X) = \\frac{a'(X) - a'(\\zeta)}{X-\\zeta}q_t(X) = \\frac{t(X) - t(\\zeta)}{X-\\zeta}q_z(X) = \\frac{z(X) - z(\\zeta)}{X-\\zeta}q'_z(X) = \\frac{z(X) - z(\\omega^{-1}\\cdot\\zeta)}{X-\\omega^{-1}\\cdot\\zeta}\n\nThe Prover computes the commitments of the four quotient polynomials and introduces corresponding Blinding Factors {\\color{blue}\\rho_{q_a}}, {\\color{blue}\\rho_{q_z}}, {\\color{blue}\\rho_{q_t}}, {\\color{blue}\\rho_{q'_z}}\\begin{split}\nQ_{a} &= \\mathsf{KZG10.Commit}(q_{a}(X); {\\color{blue}\\rho_{q_{a}}}) = [q_{a}(\\tau)]_1 + {\\color{blue}\\rho_{q_{a}}\\cdot[\\gamma]_1} \\\\\nQ_z &= \\mathsf{KZG10.Commit}(q_z(X); {\\color{blue}\\rho_{q_z}}) = [q_z(\\tau)]_1 + {\\color{blue}\\rho_{q_z}\\cdot[\\gamma]_1} \\\\\nQ_t &= \\mathsf{KZG10.Commit}(q_t(X); {\\color{blue}\\rho_{q_t}}) = [q_t(\\tau)]_1 + {\\color{blue}\\rho_{q_t}\\cdot[\\gamma]_1} \\\\\nQ'_z &= \\mathsf{KZG10.Commit}(q'_z(X); {\\color{blue}\\rho_{q'_z}}) = [q'_z(\\tau)]_1 + {\\color{blue}\\rho_{q'_z}\\cdot[\\gamma]_1} \\\\\n\\end{split}\n\nThe Prover also needs to construct four corresponding Blinding Factor commitments and send them to the Verifier:\\begin{split}\nE_a &= (\\rho_{a} + \\beta\\cdot\\rho_{r})\\cdot[1]_1 - \\rho_{q_a}\\cdot[\\tau]_1 + (\\rho_{q_a}\\cdot\\zeta)\\cdot[1]_1 \\\\\nE_z &= \\rho_{z}\\cdot[1]_1 - \\rho_{q_z}\\cdot[\\tau]_1 + (\\rho_{q_z}\\cdot\\zeta)\\cdot[1]_1 \\\\\nE_t &= \\rho_{t}\\cdot[1]_1 - \\rho_{q_t}\\cdot[\\tau]_1 + (\\rho_{q_t}\\cdot\\zeta)\\cdot[1]_1 \\\\\nE'_z &= \\rho_{z}\\cdot[1]_1 - \\rho_{q'_z}\\cdot[\\tau]_1 + (\\rho_{q'_z}\\cdot\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1 \\\\\n\\end{split}\n\nHere we can see that during the proof process, the Prover needs to evaluate four polynomials, and the evaluations of these four polynomials would all leak information about \\vec{a}. Therefore, the Prover adds a random polynomial r(X) containing two additional random factors in Round 1. This way, all polynomial evaluations in the proof process are performed on a'(X), rather than directly computing and evaluating a(X).","type":"content","url":"/ph23/ph23-pcs-zk#round-4","position":15},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Proof","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#proof","position":16},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Proof","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"\\pi = (C_r, v_r, C_z, C_t, a'(\\zeta), z(\\zeta), t(\\zeta), z(\\omega^{-1}\\cdot\\zeta), Q_a, Q_z, Q_t, Q'_z, E_a, E_z, E_t, E'_z)","type":"content","url":"/ph23/ph23-pcs-zk#proof","position":17},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Verification","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#verification","position":18},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Verification","lvl3":"ZK for Sum Proof","lvl2":"1. How to Support ZK"},"content":"The Verifier first checks the following equation:h(\\zeta) = t(\\zeta)\\cdot v_H(\\zeta)\n\nwhere v_H(\\zeta) is computed by the Verifier, and h(\\zeta) is calculated using the following equation:\\begin{aligned}\nh(\\zeta) &= L_0(\\zeta)\\cdot\\big(z(\\zeta) - a'(\\zeta) \\big) \\\\\n&+ \\alpha\\cdot(\\zeta-1)\\cdot\\big(z(\\zeta)-z(\\omega^{-1}\\cdot\\zeta)-a'(\\zeta)\\big) \\\\\n&+ \\alpha^2\\cdot L_{N-1}(\\zeta)\\cdot\\big( z(\\zeta) - (v_r + \\beta\\cdot v) \\big)\n\\end{aligned}\n\nThen the Verifier checks the correctness of a'(\\zeta), z(\\zeta), t(\\zeta), z(\\omega^{-1}\\cdot\\zeta):\\begin{aligned}\ne\\Big(C_{a'} - a'(\\zeta)\\cdot[1]_1,\\ [1]_2\\Big) &= e\\Big(Q_a,\\ [\\tau]_2 - \\zeta\\cdot[1]_2\\Big) + e\\Big(E_a,\\ [\\gamma]_2\\Big) \\\\\ne\\Big(C_z - z(\\zeta)\\cdot[1]_1,\\ [1]_2\\Big) &= e\\Big(Q_z,\\ [\\tau]_2 - \\zeta\\cdot[1]_2\\Big) + e\\Big(E_z,\\ [\\gamma]_2\\Big) \\\\\ne\\Big(C_t - t(\\zeta)\\cdot[1]_1,\\ [1]_2\\Big) &= e\\Big(Q_t,\\ [\\tau]_2 - \\zeta\\cdot[1]_2\\Big) + e\\Big(E_t,\\ [\\gamma]_2\\Big) \\\\\ne\\Big(C_z - (\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1,\\ [1]_2\\Big) &= e\\Big(Q'_z,\\ [\\tau]_2 - \\omega^{-1}\\cdot\\zeta\\cdot[1]_2\\Big) + e\\Big(E'_z,\\ [\\gamma]_2\\Big) \\\\\n\\end{aligned}","type":"content","url":"/ph23/ph23-pcs-zk#verification","position":19},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl2","url":"/ph23/ph23-pcs-zk#id-2-zk-ph23-kzg10-protocol-optimized-version","position":20},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"Below is the complete PH23-KZG10 protocol supporting Zero-knowledge.","type":"content","url":"/ph23/ph23-pcs-zk#id-2-zk-ph23-kzg10-protocol-optimized-version","position":21},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Precomputation","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#precomputation","position":22},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Precomputation","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"Precompute s_0(X),\\ldots, s_{n-1}(X) and v_H(X)v_H(X) = X^N -1s_i(X) = \\frac{v_H(X)}{v_{H_i}(X)} = \\frac{X^N-1}{X^{2^i}-1}\n\nPrecompute the Barycentric Weights \\{\\hat{w}_i\\} on D=(1, \\omega, \\omega^2, \\ldots, \\omega^{2^{n-1}}). This can accelerate\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nPrecompute the KZG10 SRS for Lagrange Basis A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1","type":"content","url":"/ph23/ph23-pcs-zk#precomputation","position":23},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Commit Computation Process","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#commit-computation-process","position":24},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Commit Computation Process","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"The Prover constructs a univariate polynomial a(X) such that its Evaluation form equals \\vec{a}=(a_0, a_1, \\ldots, a_{N-1}), where a_i = \\tilde{f}(\\mathsf{bits}(i)), which is the value of \\tilde{f} on the Boolean Hypercube \\{0,1\\}^n.a(X) = a_0\\cdot L_0(X) + a_1\\cdot L_1(X) + a_2\\cdot L_2(X)\n+ \\cdots + a_{N-1}\\cdot L_{N-1}(X)\n\nThe Prover samples a random number \\rho_a\\leftarrow_{\\$}\\mathbb{F} to protect the commitment of \\vec{a}.\n\nThe Prover computes the commitment of \\hat{f}(X), C_a, and sends C_aC_{a} = a_0\\cdot A_0 + a_1\\cdot A_1 + a_2\\cdot A_2 + \\cdots + a_{N-1}\\cdot A_{N-1} + {\\color{blue}\\rho_a\\cdot[\\gamma]_1} = [\\hat{f}(\\tau)]_1 + {\\color{blue}\\rho_a\\cdot[\\gamma]_1}\n\nwhere A_0 =[L_0(\\tau)]_1, A_1= [L_1(\\tau)]_1, A_2=[L_2(\\tau)]_1, \\ldots, A_{N-1} = [L_{2^{n-1}}(\\tau)]_1 have been obtained in the precomputation process.","type":"content","url":"/ph23/ph23-pcs-zk#commit-computation-process","position":25},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#evaluation-proof-protocol","position":26},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"","type":"content","url":"/ph23/ph23-pcs-zk#evaluation-proof-protocol","position":27},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Common inputs","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#common-inputs","position":28},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Common inputs","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"C_a=[\\hat{f}(\\tau)]_1:  the (uni-variate) commitment of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1})\n\n\\vec{u}=(u_0, u_1, \\ldots, u_{n-1}): evaluation point\n\nv=\\tilde{f}(u_0,u_1,\\ldots, u_{n-1}): The computed value of the MLE polynomial \\tilde{f} at \\vec{X}=\\vec{u}.\n\nRecall the constraint of the polynomial computation to be proven:\\tilde{f}(u_0, u_1, u_2, \\ldots, u_{n-1}) = v\n\nHere \\vec{u}=(u_0, u_1, u_2, \\ldots, u_{n-1}) is a public challenge point.","type":"content","url":"/ph23/ph23-pcs-zk#common-inputs","position":29},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-1-1","position":30},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 1.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"Prover:\n\nCompute vector \\vec{c}, where each element c_i=\\overset{\\sim}{eq}(\\mathsf{bits}(i), \\vec{u})\n\nConstruct polynomial c(X), whose evaluation results on H are exactly \\vec{c}.c(X) = \\sum_{i=0}^{N-1} c_i \\cdot L_i(X)\n\nCompute the commitment of c(X), C_c= [c(\\tau)]_1, and send C_cC_c = \\mathsf{KZG10.Commit}(\\vec{c})  =  [c(\\tau)]_1\n\nConstruct a Blinding polynomial \\color{blue}r(X) = r_0\\cdot L_0(X) + r_1\\cdot L_{1}(X), where \\{r_0, r_1\\}\\leftarrow_{\\$}\\mathbb{F}^2 are randomly sampled Blinding Factors.\n\nCompute the commitment of \\color{blue}r(X), C_r = [\\color{blue}r(\\tau)]_1, and send C_rC_r = \\mathsf{KZG10.Commit}({\\color{blue}r(X)}; \\rho_r)  =  [\\color{blue}r(\\tau)]_1 + \\rho_r\\cdot[\\gamma]_1\n\nCompute \\color{blue} v_r = \\langle \\vec{r}, \\vec{c}\\rangle, and send v_r,  where \\vec{r} is defined as:\\vec{r}\\in\\mathbb{F}^{N} = (r_0, r_1, 0, \\cdots, 0)","type":"content","url":"/ph23/ph23-pcs-zk#round-1-1","position":31},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-2-1","position":32},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 2.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"Verifier: Send challenge numbers \\alpha, {\\color{blue}\\beta}\\leftarrow_{\\$}\\mathbb{F}^2_p\n\nProver:\n\nConstruct constraint polynomials p_0(X),\\ldots, p_{n}(X) for \\vec{c}\\begin{split}\np_0(X) &= s_0(X) \\cdot \\Big( c(X) - (1-u_0)(1-u_1)...(1-u_{n-1}) \\Big) \\\\\np_k(X) &= s_{k-1}(X) \\cdot \\Big( u_{n-k}\\cdot c(X) - (1-u_{n-k})\\cdot c(\\omega^{2^{n-k}}\\cdot X)\\Big) , \\quad k=1\\ldots n\n\\end{split}\n\nAggregate \\{p_i(X)\\} into one polynomial p(X)p(X) = p_0(X) + \\alpha\\cdot p_1(X) + \\alpha^2\\cdot p_2(X) + \\cdots + \\alpha^{n}\\cdot p_{n}(X)\n\nConstruct \\color{blue}a'(X), and compute \\color{blue}\\langle \\vec{a}', \\vec{c}\\rangle=v'\\color{blue}a'(X) = a(X) + \\beta\\cdot r(X)\n\nConstruct accumulation polynomial z(X) satisfying\\begin{split}\nz(1) &= {\\color{blue}a'_0}\\cdot c_0 \\\\\nz(\\omega_{i}) - z(\\omega_{i-1}) &=  {\\color{blue}a'(\\omega_{i})}\\cdot c(\\omega_{i}), \\quad i=1,\\ldots, N-1 \\\\ \nz(\\omega^{N-1}) &= {\\color{blue}v'} \\\\\n\\end{split}\n\nConstruct constraint polynomials h_0(X), h_1(X), h_2(X) satisfying\\begin{split}\nh_0(X) &= L_0(X)\\cdot\\big(z(X) - c_0\\cdot {\\color{blue}a'(X)} \\big) \\\\\nh_1(X) &= (X-1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot X)- {\\color{blue}a'(X)}\\cdot c(X)) \\\\\nh_2(X) & = L_{N-1}(X)\\cdot\\big( z(X) - {\\color{blue}v'} \\big) \\\\\n\\end{split}\n\nAggregate p(X) and h_0(X), h_1(X), h_2(X) into one polynomial h(X) satisfying\\begin{split}\nh(X) &= p(X) + \\alpha^{n+1} \\cdot h_0(X) + \\alpha^{n+2} \\cdot h_1(X) + \\alpha^{n+3} \\cdot h_2(X)\n\\end{split}\n\nCompute the Quotient polynomial t(X) satisfyingh(X) =t(X)\\cdot v_H(X)\n\nSample \\rho_t, \\rho_z\\leftarrow_{\\$}\\mathbb{F}^2_p, compute C_t=[t(\\tau)]_1+{\\color{blue}\\rho_t\\cdot[\\gamma]_1}, C_z=[z(\\tau)]_1+{\\color{blue}\\rho_z\\cdot[\\gamma]_1}, and send C_t and C_z\\begin{split}\nC_t &= \\mathsf{KZG10.Commit}(t(X); {\\color{blue}\\rho_t}) = [t(\\tau)]_1 + {\\color{blue}\\rho_t\\cdot[\\gamma]_1} \\\\\nC_z &= \\mathsf{KZG10.Commit}(z(X); {\\color{blue}\\rho_z}) = [z(\\tau)]_1 + {\\color{blue}\\rho_z\\cdot[\\gamma]_1}\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-zk#round-2-1","position":33},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-3-1","position":34},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 3.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"Verifier: Send random evaluation point \\zeta\\leftarrow_{\\$}\\mathbb{F}\n\nProver:\n\nCompute the values of s_i(X) at \\zeta:s_0(\\zeta), s_1(\\zeta), \\ldots, s_{n-1}(\\zeta)\n\nHere the Prover can quickly compute s_i(\\zeta). From the formula of s_i(X), we have\\begin{aligned}\n  s_i(\\zeta) & = \\frac{\\zeta^N - 1}{\\zeta^{2^i} - 1} \\\\\n  & = \\frac{(\\zeta^N - 1)(\\zeta^{2^i} +1)}{(\\zeta^{2^i} - 1)(\\zeta^{2^i} +1)} \\\\\n  & = \\frac{\\zeta^N - 1}{\\zeta^{2^{i + 1}} - 1} \\cdot (\\zeta^{2^i} +1) \\\\\n  & = s_{i + 1}(\\zeta) \\cdot (\\zeta^{2^i} +1)\n\\end{aligned}\n\nTherefore, the value of s_i(\\zeta) can be calculated from s_{i + 1}(\\zeta), ands_{n-1}(\\zeta) = \\frac{\\zeta^N - 1}{\\zeta^{2^{n-1}} - 1} = \\zeta^{2^{n-1}} + 1\n\nThus, we can obtain an O(n) algorithm to compute s_i(\\zeta), and it doesn’t involve division operations. The computation process is: s_{n-1}(\\zeta) \\rightarrow s_{n-2}(\\zeta) \\rightarrow \\cdots \\rightarrow s_0(\\zeta).\n\nDefine the evaluation Domain D', which includes n+1 elements:D'=D\\zeta = \\{\\zeta, \\omega\\zeta, \\omega^2\\zeta,\\omega^4\\zeta, \\ldots, \\omega^{2^{n-1}}\\zeta\\}\n\nCompute and send the values of c(X) on D'c(\\zeta), c(\\zeta\\cdot\\omega), c(\\zeta\\cdot\\omega^2), c(\\zeta\\cdot\\omega^4), \\ldots, c(\\zeta\\cdot\\omega^{2^{n-1}})\n\nCompute and send z(\\omega^{-1}\\cdot\\zeta)\n\nCompute the Linearized Polynomial l_\\zeta(X)\\begin{split}\nl_\\zeta(X) =& \\Big(s_0(\\zeta) \\cdot (c(\\zeta) - c_0) \\\\\n& + \\alpha\\cdot s_0(\\zeta) \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\\\\n  & + \\alpha^2\\cdot s_1(\\zeta) \\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta)) \\\\\n  & + \\cdots \\\\\n  & + \\alpha^{n-1}\\cdot s_{n-2}(\\zeta)\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\\\\n  & + \\alpha^n\\cdot s_{n-1}(\\zeta)\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta)) \\\\\n  & + \\alpha^{n+1}\\cdot (L_0(\\zeta)\\cdot\\big(z(X) - c_0\\cdot {\\color{blue}a'(X)})\\\\\n  & + \\alpha^{n+2}\\cdot (\\zeta - 1)\\cdot\\big(z(X)-z(\\omega^{-1}\\cdot\\zeta)-c(\\zeta)\\cdot {\\color{blue}a'(X)} ) \\\\\n  & + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(z(X) - {\\color{blue}v'}) \\\\\n  & - v_H(\\zeta)\\cdot t(X)\\ \\Big)\n\\end{split}\n\nObviously, r_\\zeta(\\zeta)= 0, so this computed value doesn’t need to be sent to the Verifier, and [r_\\zeta(\\tau)]_1 can be constructed by the Verifier themselves.\n\nConstruct polynomial c^*(X), which is the interpolation polynomial of the following vector on D\\zeta\\alpha^{n+1}L_0(\\zeta)(\\rho_z - c_0\\cdot\\rho_a) \\\\\n+\\alpha^{n+2}(\\zeta-1)(\\rho_z - c(\\zeta)\\cdot\\rho_a) \\\\\n+ \\alpha^{n+3}L_{N-1}(\\zeta)\\cdot \\rho_z \\\\\n- v_H(\\zeta)\\cdot\\rho_t\\vec{c^*}= \\Big(c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), c(\\zeta)\\Big)\n\nThe Prover can use the pre-computed Barycentric Weights \\{\\hat{w}_i\\} on D to quickly compute c^*(X),c^*(X) = \\frac{c^*_0 \\cdot \\frac{\\hat{w}_0}{X-\\omega\\zeta} + c^*_1 \\cdot \\frac{\\hat{w}_1}{X-\\omega^{2}\\zeta} + \\cdots + c^*_n \\cdot \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}}{\n   \\frac{\\hat{w}_0}{X-\\omega\\zeta} + \\frac{\\hat{w}_1}{X-\\omega^2\\zeta} + \\cdots + \\frac{\\hat{w}_n}{X-\\omega^{2^n}\\zeta}\n  }\n\nHere \\hat{w}_j are pre-computed values:\\hat{w}_j = \\prod_{l\\neq j} \\frac{1}{\\omega^{2^j} - \\omega^{2^l}}\n\nBecause l_\\zeta(\\zeta)= 0, there exists a Quotient polynomial q_\\zeta(X) satisfyingq_\\zeta(X) = \\frac{1}{X-\\zeta}\\cdot l_\\zeta(X)\n\nCompute the commitment of q_\\zeta(X), Q_\\zeta, and simultaneously sample a random number {\\color{blue}\\rho_q}\\leftarrow_{\\$}\\mathbb{F} as the Blinding Factor for the commitment:Q_\\zeta = \\mathsf{KZG10.Commit}(q_\\zeta(X); {\\color{blue}\\rho_q}) = [q_\\zeta(\\tau)]_1 + {\\color{blue}\\rho_q\\cdot[\\gamma]_1}{\\color{blue}\n\\begin{split}\nE_\\zeta &= \\big(\\alpha^{n+1}L_0(\\zeta)(\\rho_z - c_0\\cdot(\\rho_a + \\beta\\cdot\\rho_r)) \\\\\n& \\quad +\\alpha^{n+2}(\\zeta-1)(\\rho_z - c(\\zeta)\\cdot(\\rho_a + \\beta\\cdot\\rho_r)) \\\\\n& \\quad + \\alpha^{n+3}L_{N-1}(\\zeta)\\cdot \\rho_z - v_H(\\zeta)\\cdot\\rho_t\\big)\\cdot [1]_1  \\\\\n&- \\rho_q\\cdot[\\tau]_1 + (\\zeta\\cdot\\rho_q)\\cdot[1]_1 \\\\\n\\end{split}}\n\nConstruct the vanishing polynomial z_{D_{\\zeta}}(X) on D\\zetaz_{D_{\\zeta}}(X) = (X-\\zeta\\omega)\\cdots (X-\\zeta\\omega^{2^{n-1}})(X-\\zeta)\n\nConstruct Quotient polynomial q_c(X):q_c(X) = \\frac{(c(X) - c^*(X))}{(X-\\zeta)(X-\\omega\\zeta)(X-\\omega^2\\zeta)\\cdots(X-\\omega^{2^{n-1}}\\zeta)}\n\nCompute the commitment of q_c(X), Q_c and E_c. Since c(X) doesn’t contain any private information, there’s no need to add a Blinding Factor:Q_c = \\mathsf{KZG10.Commit}(q_c(X)) = [q_c(\\tau)]_1\n\nConstruct Quotient polynomial q_{\\omega\\zeta}(X) to prove the value of z(X) at \\omega^{-1}\\cdot\\zeta:q_{\\omega\\zeta}(X) = \\frac{z(X) - z(\\omega^{-1}\\cdot\\zeta)}{X - \\omega^{-1}\\cdot\\zeta}\n\nCompute the commitment of q_{\\omega\\zeta}(X), Q_{\\omega\\zeta}, and simultaneously sample a random number {\\color{blue}\\rho_{q}'}\\leftarrow_{\\$}\\mathbb{F} as the Blinding Factor for the commitment:Q_{\\omega\\zeta} = \\mathsf{KZG10.Commit}(q_{\\omega\\zeta}(X); {\\color{blue}\\rho_{q}'}) = [q_{\\omega\\zeta}(\\tau)]_1 + {\\color{blue}\\rho_{q}'\\cdot[\\gamma]_1}{\\color{blue}E_{\\omega\\zeta} = \\rho_z\\cdot[1]_1 - \\rho_{q}'\\cdot[\\tau]_1 + (\\omega^{-1}\\cdot\\zeta\\cdot\\rho_{q}')\\cdot[1]_1}\n\nSend \\big(Q_c, Q_\\zeta, {\\color{blue}E_\\zeta}, Q_{\\omega\\zeta}, {\\color{blue}E_{\\omega\\zeta}} \\big)","type":"content","url":"/ph23/ph23-pcs-zk#round-3-1","position":35},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 4.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl4","url":"/ph23/ph23-pcs-zk#round-4-1","position":36},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl4":"Round 4.","lvl3":"Evaluation Proof Protocol","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"The Verifier sends a second random challenge point \\xi\\leftarrow_{\\$}\\mathbb{F}\n\nThe Prover constructs a third Quotient polynomial q_\\xi(X)q_\\xi(X) = \\frac{c(X) - c^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot q_c(X)}{X-\\xi}\n\nThe Prover computes and sends the commitment of q_\\xi(X), Q_\\xiQ_\\xi = \\mathsf{KZG10.Commit}(q_\\xi(X)) = [q_\\xi(\\tau)]_1","type":"content","url":"/ph23/ph23-pcs-zk#round-4-1","position":37},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Proof Representation","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#proof-representation","position":38},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Proof Representation","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"9\\cdot\\mathbb{G}_1, (n+1)\\cdot\\mathbb{F}\\begin{split}\n\\pi_{eval} &= \\big(z(\\omega^{-1}\\cdot\\zeta), c(\\zeta)，c(\\omega\\cdot\\zeta), c(\\omega^2\\cdot\\zeta), c(\\omega^4\\cdot\\zeta), \\ldots, c(\\omega^{2^{n-1}}\\cdot\\zeta), \\\\\n& C_{c}, C_{t}, C_{z}, Q_c, Q_\\zeta, {\\color{blue}E_\\zeta}, Q_\\xi, Q_{\\omega\\zeta}, {\\color{blue}E_{\\omega\\zeta}}\\big)\n\\end{split}","type":"content","url":"/ph23/ph23-pcs-zk#proof-representation","position":39},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Verification Process","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"type":"lvl3","url":"/ph23/ph23-pcs-zk#verification-process","position":40},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl3":"Verification Process","lvl2":"2. ZK-PH23-KZG10 Protocol (Optimized Version)"},"content":"The Verifier computes \\color{blue}C'_a and \\color{blue} v'\n\\color{blue}C'_a = C_a + \\beta \\cdot C_b\\color{blue} v' = v + \\beta \\cdot v_b\n\nThe Verifier computes c^*(\\xi) using pre-computed Barycentric Weights \\{\\hat{w}_i\\}\nc^*(\\xi)=\\frac{\\sum_i c_i\\frac{w_i}{\\xi-x_i}}{\\sum_i \\frac{w_i}{\\xi-x_i}}\n\nThe Verifier computes v_H(\\zeta), L_0(\\zeta), L_{N-1}(\\zeta)\nv_H(\\zeta) = \\zeta^N - 1L_0(\\zeta) = \\frac{1}{N}\\cdot \\frac{z_{H}(\\zeta)}{\\zeta-1}L_{N-1}(\\zeta) = \\frac{\\omega^{N-1}}{N}\\cdot \\frac{z_{H}(\\zeta)}{\\zeta-\\omega^{N-1}}\n\nThe Verifier computes s_0(\\zeta), \\ldots, s_{n-1}(\\zeta), which can be calculated using the recursive method mentioned earlier.\n\nThe Verifier computes the commitment of the linearization polynomial C_l\\begin{split}\nC_l & = \n\\Big( (c(\\zeta) - c_0)s_0(\\zeta) \\\\\n& + \\alpha \\cdot (u_{n-1}\\cdot c(\\zeta) - (1-u_{n-1})\\cdot c(\\omega^{2^{n-1}}\\cdot\\zeta))\\cdot s_0(\\zeta)\\\\\n& + \\alpha^2\\cdot (u_{n-2}\\cdot c(\\zeta) - (1-u_{n-2})\\cdot c(\\omega^{2^{n-2}}\\cdot\\zeta))\\cdot s_1(\\zeta)  \\\\\n& + \\cdots \\\\\n& + \\alpha^{n-1}\\cdot (u_{1}\\cdot c(\\zeta) - (1-u_{1})\\cdot c(\\omega^2\\cdot\\zeta))\\cdot s_{n-2}(\\zeta)\\\\\n& + \\alpha^n\\cdot (u_{0}\\cdot c(\\zeta) - (1-u_{0})\\cdot c(\\omega\\cdot\\zeta))\\cdot s_{n-1}(\\zeta) \\\\\n& + \\alpha^{n+1}\\cdot L_0(\\zeta)\\cdot(C_z - c_0\\cdot C_a)\\\\\n& + \\alpha^{n+2}\\cdot (\\zeta-1)\\cdot\\big(C_z - z(\\omega^{-1}\\cdot \\zeta)-c(\\zeta)\\cdot C_{a} ) \\\\\n& + \\alpha^{n+3}\\cdot L_{N-1}(\\zeta)\\cdot(C_z - {\\color{blue}v'}) \\\\\n& - v_H(\\zeta)\\cdot C_t \\Big)\n\\end{split}\n\nThe Verifier generates a random number \\eta to merge the following Pairing verifications:\\begin{aligned}\ne(C_l + \\zeta\\cdot Q_\\zeta, [1]_2) & \\overset{?}{=} e(Q_\\zeta, [\\tau]_2) + {\\color{blue}e(E_\\zeta, [\\gamma]_2)}\\\\\ne(C - C^*(\\xi) - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi, [1]_2) & \\overset{?}{=} e(Q_\\xi, [\\tau]_2)\\\\\ne(Z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1, [1]_2) &\\overset{?}{=} e(Q_{\\omega\\zeta}, [\\tau]_2) + {\\color{blue}e(E_{\\omega\\zeta}, [\\gamma]_2)}\\\\\n\\end{aligned}\n\nThe merged verification only requires two Pairing operations:\\begin{aligned}\nP &= \\Big(C_l + \\zeta\\cdot Q_\\zeta\\Big) \\\\\n&+ \\eta\\cdot \\Big(C - C^* - z_{D_\\zeta}(\\xi)\\cdot Q_c + \\xi\\cdot Q_\\xi\\Big) \\\\\n&+ \\eta^2\\cdot\\Big(C_z + \\zeta\\cdot Q_{\\omega\\zeta} - z(\\omega^{-1}\\cdot\\zeta)\\cdot[1]_1\\Big)\n\\end{aligned}e\\Big(P, [1]_2\\Big) \\overset{?}{=} e\\Big(Q_\\zeta + \\eta\\cdot Q_\\xi + \\eta^2\\cdot Q_{\\omega\\zeta}, [\\tau]_2\\Big) + {\\color{blue}e\\Big(E_\\zeta + \\eta^2\\cdot E_{\\omega\\zeta}, [\\gamma]_2\\Big)}","type":"content","url":"/ph23/ph23-pcs-zk#verification-process","position":41},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"3. Optimized Performance Analysis"},"type":"lvl2","url":"/ph23/ph23-pcs-zk#id-3-optimized-performance-analysis","position":42},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"3. Optimized Performance Analysis"},"content":"Proof size:  9~\\mathbb{G}_1 + (n+1)~\\mathbb{F}\n\nVerifier: 4~\\mathbb{F} + O(n)~\\mathbb{F}+ 3~\\mathbb{G}_1 + 2~P","type":"content","url":"/ph23/ph23-pcs-zk#id-3-optimized-performance-analysis","position":43},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"References"},"type":"lvl2","url":"/ph23/ph23-pcs-zk#references","position":44},{"hierarchy":{"lvl1":"Missing Protocol PH23-PCS (Part 3)","lvl2":"References"},"content":"[BDFG20] Dan Boneh, Justin Drake, Ben Fisch, and Ariel Gabizon. “Efficient polynomial commitment schemes for multiple points and polynomials”. Cryptology {ePrint} Archive, Paper 2020/081. \n\nhttps://​eprint​.iacr​.org​/2020​/081.\n\n[KZG10] Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. “Constant-size commitments to polynomials and their applications.” Advances in Cryptology-ASIACRYPT 2010: 16th International Conference on the Theory and Application of Cryptology and Information Security, Singapore, December 5-9, 2010. Proceedings 16. Springer Berlin Heidelberg, 2010.\n\n[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[PST13] Papamanthou, Charalampos, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” Theory of Cryptography Conference. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. \n\nhttps://​eprint​.iacr​.org​/2011​/587\n\n[ZGKPP17] “A Zero-Knowledge Version of vSQL.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2017​/1146\n\n[XZZPS19] Tiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song. “Libra: Succinct Zero-Knowledge Proofs with Optimal Prover Computation.” \n\nhttps://​eprint​.iacr​.org​/2019​/317\n\n[CHMMVW19] Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Psi Vesely, and Nicholas Ward. “Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS.” \n\nhttps://​eprint​.iacr​.org​/2019​/1047","type":"content","url":"/ph23/ph23-pcs-zk#references","position":45},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/ph23/ph23","position":0},{"hierarchy":{"lvl1":""},"content":"\n\n# Define variables\nn = 3\nN = 2^n\nX = var(['X{}'.format(i) for i in range(n)]) # Define the variables X_0, X-1,.....\nu = var(['u{}'.format(i) for i in range(n)]) # Define the variables u_0, u-1,.....\n\n# Function to get binary representation as a list of bits\ndef bits(i,n):\n    return list(map(int, format(i,'0{}b'.format(n))))\n\ndef bits_reverse(i, n):\n    bits_list = bits(i, n)\n    return list(reversed(bits_list))\n\n# Define the eq_tilde function\ndef eq_tilde(bits_i, u_vector):\n    result=1\n    for bit,u in zip(bits_i,u_vector):\n        result *= (1-bit)*(1-u) + bit*u\n    return result\n\n# Coefficients of the polynomial\na = [var('a{}'.format(i)) for i in range(N)] # Coefficients a_0, a_1, ...., a_(N-1)\n\n# MLE polynomial\nf_tilde = sum(a[i]*eq_tilde(bits(i,n), u) for i in range(N))\nshow(f_tilde)\n\n# Generate all combinations of (1-u[i]) and u[i] based on binary representation\ndef generate_c_vector(n, u):\n    c_vector = []\n    for i in range(2^n):  # Loop over all binary numbers from 0 to 2^n - 1\n        binary = list(map(int, format(i, f'0{n}b')))  # Binary representation of i\n        binary_reverse = list(reversed(binary))  # Reverse the binary representation\n        product = 1\n        for j, bit in enumerate(binary_reverse):\n            if bit == 0:\n                product *= (1 - u[j])  # Use (1 - u[j]) for 0\n            else:\n                product *= u[j]  # Use u[j] for 1\n        c_vector.append(product)\n    return c_vector\n\n# Compute the c vector\nc = generate_c_vector(n, u)\n\n# Display the c vector\nshow(c)\n\n# SageMath Implementation for Polynomial Encoding using Lagrange Basis\n\n# Define the finite field and subgroup H\np = 17  # Example prime (p must be chosen appropriately)\nF = GF(p)  # Finite field F_p\nomega = F(3)  # Example primitive 8th root of unity in F_p\nH = [omega^i for i in range(8)]  # Multiplicative subgroup H of size 8\n\n# Define Lagrange Basis Polynomials\ndef lagrange_basis(H, X):\n    basis = []\n    for i in range(len(H)):\n        Li = 1\n        for j in range(len(H)):\n            if i != j:\n                Li *= (X - H[j]) / (H[i] - H[j])  # Lagrange basis polynomial\n        basis.append(Li)\n    return basis\n\n# Symbolic variable for the polynomial\nX = polygen(F, 'X')\n\n# Compute the Lagrange basis polynomials\nL = lagrange_basis(H, X)\nshow(L)\n\n# Vector c as computed earlier\nc = [F(c_val) for c_val in [1, 2, 3, 4, 5, 6, 7, 8]]  # Example values for c vector\nshow(c)\n# Compute the polynomial encoding c(X)\nc_X = sum(c[i] * L[i] for i in range(len(c)))\n\n# Compute the polynomial encoding c(X) step by step\n# c_X = 0  # Start with an empty polynomial\n# print(\"Building c(X) step by step:\")\n# for i in range(len(c)):\n#     show(c[i])\n#     show(L[i])\n#     term = c[i] * L[i]  # Compute the current term\n#     show(term)\n#     c_X += term  # Add the current term to the polynomial\n#     show(term)\n#     '\\n'\n# #     print(f\"Step {i+1}: Adding term {term}\")\n# #     print(f\"Partial sum: {c_X}\\n\")\n\n# # Final polynomial\n# print(\"Final polynomial c(X):\")\n# show(c_X)\n\n# Display the resulting polynomial c(X)\nshow(c_X)\n\n# Verification: Check that c(omega^i) = c_i\nverification = [c_X(H[i]) == c[i] for i in range(len(H))]\nshow(verification)\n\n\n# Define subsets H0, H1, H2 based on the Group Tower relationship\nH0 = [H[0]]  # {1}\nH1 = [H[0], H[4]]  # {1, ω^4}\nH2 = [H[0], H[2], H[4], H[6]]  # {1, ω^2, ω^4, ω^6}\nH3 = H  # Full set {1, ω, ω^2, ..., ω^7}\n\n# Define the vanishing polynomials v_H(X) and v_Hi(X)\nX = polygen(F, 'X')  # Define X as a polynomial variable\n\ndef vanishing_polynomial(domain):\n    \"\"\"Compute the vanishing polynomial for a given domain.\"\"\"\n    v = 1\n    for alpha in domain:\n        v *= (X - alpha)\n    return v\n\n# Compute vanishing polynomials\nv_H = vanishing_polynomial(H3)  # Full vanishing polynomial for H\nv_H0 = vanishing_polynomial(H0)\nv_H1 = vanishing_polynomial(H1)\nv_H2 = vanishing_polynomial(H2)\n\n# Compute the selector polynomials s_0(X), s_1(X), s_2(X)\ns0 =  v_H/v_H0\ns1 = v_H/v_H1\ns2 =  v_H/v_H2\n\n# Display the selector polynomials\nshow(s0)\nshow(s1)\nshow(s2)\n\n\n# Define the finite field and its elements\np = 17  # Prime modulus\nF = GF(p)  # Finite field F_p\ngenerator = F.multiplicative_generator() # the root unity of F_p\nprint(\"the root unity of F_p: \", generator)\n\n# find the order of the generator of F_p\ndef find_order(generator, field):\n    order = 1\n    power = generator\n    while power != 1:\n        power *= generator\n        order += 1\n    return order\n\n# return the root of unity in subgroup H, where |H| = N\ndef root_of_H(generator, generator_order, N):\n    return generator**(generator_order / N)\n\ngenerator_order = find_order(generator, F)\nomega = root_of_H(generator, generator_order, 8)\nprint(\"the root unity of subgroup H: \", omega)\nH = [omega^i for i in range(8)]  # Subgroup H of size 8 (2^3)\nshow(H)\n\n# Define variables\nX = polygen(F, 'X')  # Polynomial variable\nN = len(H)  # Size of the group H\n\n# Define vanishing polynomial of H\ndef vanishing_polynomial(H):\n    v_H = F(1)\n    for h in H:\n        v_H *= (X - h)\n    return v_H\n\nv_H = vanishing_polynomial(H)\n\n# Define polynomials a(X) and c(X)\na_coeffs = [F.random_element() for _ in range(N)]  # Random coefficients for a(X)\nc_coeffs = [F.random_element() for _ in range(N)]  # Random coefficients for c(X)\n\na_X = sum(a_coeffs[i] * X^i for i in range(N))  # a(X)\nc_X = sum(c_coeffs[i] * X^i for i in range(N))  # c(X)\n\n# Compute P(X) = a(X) * c(X)\nP_X = a_X * c_X\n\nprint(\"P_X: \", P_X)\n\n# Compute the decomposition of P(X)\nq_X = P_X // v_H  # Quotient\nr_X = P_X % v_H   # Remainder\nv = sum(P_X(h) for h in H)  # v = sum of P(ω) over H\ng_X = (r_X - v / N) // X    # g(X) \n\n# Verify the decomposition\ndecomposed_P_X = q_X * v_H + X * g_X + (v / N)\nassert P_X == decomposed_P_X  # Ensure the decomposition holds\n\n# Verification at a challenge ζ\nzeta = F.random_element()  # Random challenge ζ\nlhs = a_X(zeta) * c_X(zeta)  # a(ζ) * c(ζ)\nrhs = q_X(zeta) * v_H(zeta) + zeta * g_X(zeta) + (v / N)  # q(ζ) * v_H(ζ) + ζ * g(ζ) + v/N\n\n# Output results\nprint(\"a(X):\", a_X)\nprint(\"c(X):\", c_X)\nprint(\"P(X):\", P_X)\nprint(\"q(X):\", q_X)\nprint(\"g(X):\", g_X)\nprint(\"v:\", v)\nprint(\"Verification at ζ:\")\nprint(\"LHS (a(ζ) * c(ζ)):\", lhs)\nprint(\"RHS (q(ζ) * v_H(ζ) + ζ * g(ζ) + v/N):\", rhs)\nassert lhs == rhs, \"Verification failed!\"\n\n# Define the finite field and its elements\np = 17  # Prime modulus\nF = GF(p)  # Finite field F_p\nomega = F(9)  # Primitive 8th root of unity in F_p\nH = [omega^i for i in range(8)]  # Subgroup H of size 8 (2^3)\n\n# Define the vanishing polynomial for H\ndef vanishing_polynomial(H):\n    v_H = F(1)\n    for h in H:\n        v_H *= (X - h)\n    return v_H\n\n# Lagrange basis polynomials\ndef lagrange_basis(H, X):\n    basis = []\n    for i in range(len(H)):\n        Li = F(1)\n        for j in range(len(H)):\n            if i != j:\n                Li *= (X - H[j]) / (H[i] - H[j])\n        basis.append(Li)\n    return basis\n\n# Define variables\nX = polygen(F, 'X')  # Polynomial variable\nN = len(H)\n\n# Random coefficients for a(X) and c(X)\na_coeffs = [F.random_element() for _ in range(N)]\nc_coeffs = [F.random_element() for _ in range(N)]\n\n\n# Construct a(X) and c(X)\nL = lagrange_basis(H, X)\na_X = sum(a_coeffs[i] * L[i] for i in range(N))\nc_X = sum(c_coeffs[i] * L[i] for i in range(N))\n\n# Compute the auxiliary vector z_i\nz = [a_coeffs[0] * c_coeffs[0]]  # z_0\nfor i in range(1, N):\n    z.append(z[i-1] + a_coeffs[i] * c_coeffs[i])\n\n# Compute z(X) using Lagrange basis polynomials\nz_X = sum(z[i] * L[i] for i in range(N))\n\n# Compute the final sum v\nv = z[-1]\n\n# Polynomial constraints\n# Constraint 1: L0(X) * (z(X) - a(X) * c_0)\nconstraint1 = L[0] * (z_X - a_X * c_coeffs[0])\n\n# Constraint 2: (X - 1) * (z(X) - z(ω^{-1} * X) - a(X) * c(X))\nomega_inv = omega^(-1)\nz_shifted = z_X.subs(X=omega_inv * X)\nconstraint2 = (X - 1) * (z_X - z_shifted - a_X * c_X)\n\n# Constraint 3: LN-1(X) * (z(X) - v)\nconstraint3 = L[-1] * (z_X - v)\n\n# Display the constraints\nprint(\"Constraint 1:\")\nshow(constraint1)\n\nprint(\"Constraint 2:\")\nshow(constraint2)\n\nprint(\"Constraint 3:\")\nshow(constraint3)\n\n# Evaluate constraints point-by-point in H to verify correctness\nverification_points = H  # Points in the subgroup H\n\ndef verify_constraint_at_points(constraint, points):\n    \"\"\"Check if a constraint is satisfied at all points in the given domain.\"\"\"\n    return all(constraint.subs(X=point) == 0 for point in points)\n\n# Check if constraints are satisfied at all points in H\nconstraint1_valid = verify_constraint_at_points(constraint1, verification_points)\nconstraint2_valid = verify_constraint_at_points(constraint2, verification_points)\nconstraint3_valid = verify_constraint_at_points(constraint3, verification_points)\n\n# Display results\nif constraint1_valid:\n    print(\"Constraint 1 is satisfied!\")\nelse:\n    print(\"Constraint 1 failed!\")\n\nif constraint2_valid:\n    print(\"Constraint 2 is satisfied!\")\nelse:\n    print(\"Constraint 2 failed!\")\n\nif constraint3_valid:\n    print(\"Constraint 3 is satisfied!\")\nelse:\n    print(\"Constraint 3 failed!\")\n\n# Assert that all constraints are satisfied\nassert constraint1_valid, \"Constraint 1 failed!\"\nassert constraint2_valid, \"Constraint 2 failed!\"\nassert constraint3_valid, \"Constraint 3 failed!\"\n\nprint(\"All constraints are satisfied!\")\n\n# Define the finite field and its elements\np = 17  # Prime modulus\nF = GF(p)  # Finite field F_p\nomega = F(9)  # Primitive 8th root of unity in F_p\nH = [omega^i for i in range(8)]  # Subgroup H of size 8 (2^3)\nN = len(H)\nn = log(len(H), 2)\n\n\n# Define the vanishing polynomial for H\ndef vanishing_polynomial(H):\n    v_H = F(1)\n    for h in H:\n        v_H *= (X - h)\n    return v_H\n\n# Polynomial variable\nX = polygen(F, 'X')\nv_H = vanishing_polynomial(H)\n\n# Define the lagrange basis polynomials\ndef lagrange_basis(H, X):\n    basis = []\n    for i in range(len(H)):\n        Li = F(1)\n        for j in range(len(H)):\n            if i != j:\n                Li *= (X - H[j]) / (H[i] - H[j])\n        basis.append(Li)\n    return basis\n\nL = lagrange_basis(H, X)  # Lagrange basis polynomials\n\nu = [F.random_element() for _ in range(n)]\nprint(\"u: \", u)\n\n# Random coefficients for a(X) and c(X)\na_coeffs = [F.random_element() for _ in range(len(H))]\nc_coeffs = generate_c_vector(n, u)\n# c_coeffs = [F.random_element() for _ in range(len(H))]\nprint(\"c_coeffs: \", c_coeffs)\n\n# Construct a(X) and c(X)\na_X = sum(a_coeffs[i] * L[i] for i in range(len(H)))\nc_X = sum(c_coeffs[i] * L[i] for i in range(len(H)))\n\n# Recursive auxiliary polynomial z(X)\nz = [a_coeffs[0] * c_coeffs[0]]\nfor i in range(1, len(H)):\n    z.append(z[i-1] + a_coeffs[i] * c_coeffs[i])\n\nz_X = sum(z[i] * L[i] for i in range(len(H)))\nv = z[-1]  # Final sum\n\ndef compute_s_polynomial(H, X):\n    s_polynomials = []\n    N = len(H)\n    n = log(N, 2)\n    s_polynomials.append(X**(2**(n - 1)) + F(1))\n    for i in range(n - 2, -1, -1):\n        temp = X**(2**i) + F(1)\n        s_polynomials.append(s_polynomials[-1] * temp)\n    s_polynomials.reverse()\n    return s_polynomials\n\n# # Define the polynomials p_i(X)\np = []\ns_polys = compute_s_polynomial(H, X)\n\nfor i in range(n + 1):\n    if i == 0:\n        p.append(s_polys[0] * (c_X - prod([(F(1) - u_i) for u_i in u])))\n    else:\n        temp_X = c_X.subs(X=(omega^(2^(n - i)) * X))\n        p.append(s_polys[i - 1] * (c_X * u[n - i] - temp_X * (F(1) - u[n - i])))\n\n# Define h_1(X), h_2(X), h_3(X)\nh1_X = L[0] * (z_X - a_X * c_coeffs[0])\nh2_X = (X - 1) * (z_X - z_X.subs(X=omega^(-1) * X) - a_X * c_X)\nh3_X = L[-1] * (z_X - v)\n\n# Aggregate all polynomials into h(X)\nalpha = F.random_element()  # Random alpha\nh_X = sum(alpha^i * p[i] for i in range(len(p))) + alpha^(len(p)) * h1_X + alpha^(len(p) + 1) * h2_X + alpha^(len(p) + 2) * h3_X\n\n# Verify correctness of h(X)\nt_X = h_X // v_H  # Quotient polynomial\nremainder = h_X % v_H  # Remainder\n\n# Display results\nprint(\"Polynomial h(X):\")\nshow(h_X)\n\nprint(\"Quotient polynomial t(X):\")\nshow(t_X)\n\nprint(\"Remainder:\")\nshow(remainder)\n\n# Verify that h(X) is divisible by v_H(X)\nif remainder != 0: print(\"h(X) is not divisible by v_H(X)!\")\nelse :\n    print(\"Verification successful: h(X) is divisible by v_H(X).\")\n\n\n# Generate random challenge ζ ensuring ζ is not in H\nwhile True:\n    zeta = F.random_element()\n    if zeta not in H:\n        break\n\n# Compute values of polynomials at ζ\nf_zeta = a_X(zeta) * c_X(zeta)\nc_zeta = c_X(zeta)\nz_zeta = z_X(zeta)\n\n# Compute quotient t(ζ) = h(ζ) // v_H(ζ)\nv_H_zeta = v_H(zeta)\nif v_H_zeta == 0:\n    raise ValueError(\"Vanishing polynomial evaluates to zero at ζ, choose a different ζ.\")\n\n# Compute t(ζ)\nt_zeta = h_X(zeta) // v_H_zeta\n\n# Compute s_i(ζ) values\ns = [v_H(zeta) / vanishing_polynomial(H[:2**i])(zeta) for i in range(len(H))]\n\n# Compute L_0(ζ) and L_{N-1}(ζ)\nL0_zeta = v_H(zeta) / (N * (zeta - 1))\nLN_minus1_zeta = v_H(zeta) / (N * (omega * zeta - 1))\n\n# Compute p_i(ζ) values\np_zeta = []\nfor i in range(n + 1):\n    if i == 0:\n        p_zeta.append(s[0] * (c_zeta - prod([(F(1) - u_i) for u_i in u])))\n    else:\n        p_zeta.append(s[i] * (c_zeta * u[n - i] - c_X.subs(X=(omega^(2^(n - i)) * zeta)) * (F(1) - u[n - i])))\n\n# Compute h_1(ζ), h_2(ζ), h_3(ζ)\nh1_zeta = L0_zeta * (z_zeta - a_X(zeta) * c_coeffs[0])\nh2_zeta = (zeta - 1) * (z_zeta - z_X.subs(X=omega^(-1) * zeta) - a_X(zeta) * c_X(zeta))\nh3_zeta = LN_minus1_zeta * (z_zeta - v)\n\n# Aggregate polynomial evaluations into h(ζ)\nalpha = F.random_element()\nh_zeta = sum(alpha^i * p_zeta[i] for i in range(len(p_zeta))) + alpha^len(p_zeta) * h1_zeta + alpha^(len(p_zeta) + 1) * h2_zeta + alpha^(len(p_zeta) + 2) * h3_zeta\n\n# Verify correctness\nt_zeta = h_zeta // v_H_zeta\n# remainder = h_zeta % v_H_zeta\n\n# Display results\nprint(\"h(ζ):\", h_zeta)\nprint(\"t(ζ):\", t_zeta)\nprint(\"Remainder:\", remainder)\n\n# Ensure h(ζ) is divisible by v_H(ζ)\nif remainder != 0:\n    print(\"Verification failed: h(ζ) is not divisible by v_H(ζ)!\")\nelse :\n    print(\"Verification successful: h(ζ) is divisible by v_H(ζ).\")\n\n\n# Define the finite field and subgroup H\np = 17  # Prime modulus\nF = GF(p)  # Finite field F_p\nomega = F(9)  # Primitive N-th root of unity in F_p\nN = 8  # Size of H, N = 2^n\nH = [omega^i for i in range(N)]  # Multiplicative subgroup H of size N\n\n# Polynomial variable\nX = polygen(F, 'X')\n\n# Define the vanishing polynomial v_H(X)\ndef vanishing_polynomial(H):\n    v_H = F(1)\n    for h in H:\n        v_H *= (X - h)\n    return v_H\n\nv_H = vanishing_polynomial(H)\n\n# Define the Lagrange basis polynomials L_i(X)\ndef lagrange_basis(H, v_H):\n    basis = []\n    for i in range(len(H)):\n        Li = (v_H / (X - H[i])) * (1 / v_H.derivative()(H[i]))\n        basis.append(Li)\n    return basis\n\nL = lagrange_basis(H, v_H)\n\n# Define the MLE Polynomial coefficients\na_coeffs = [F.random_element() for _ in range(N)]  # Random coefficients a_0, ..., a_{N-1}\n\n# Construct the univariate polynomial a(X)\na_X = sum(a_coeffs[i] * L[i] for i in range(N))\n\n# Display the univariate polynomial a(X)\nprint(\"Univariate Polynomial a(X):\")\nshow(a_X)\n\n# Simulate the SRS of KZG10 (Commitment scheme)\n# Example SRS: {g^1, g^2, ..., g^N}, where g is a generator\ng = F.random_element()  # Random generator in F_p\nSRS = [g^i for i in range(N)]  # SRS elements\n\n# Commit a(X)\ncommitment = sum(a_coeffs[i] * SRS[i] for i in range(N))\nprint(\"Commitment of a(X):\")\nshow(commitment)\n\n\n# Step 1: Define Common Input Struct\nclass CommonInput:\n    def __init__(self, commitment_a, evaluation_point_u, polynomial_value_v):\n        \"\"\"\n        Initialize the common input for the protocol.\n        :param commitment_a: Commitment of a(X) (Ca)\n        :param evaluation_point_u: Evaluation point (u_0, u_1, ..., u_{n-1})\n        :param polynomial_value_v: Value of MLE polynomial at the evaluation point\n        \"\"\"\n        self.commitment_a = commitment_a\n        self.evaluation_point_u = evaluation_point_u\n        self.polynomial_value_v = polynomial_value_v\n\n\n# Step 2: Define the Prover Class\nclass Prover:\n    def __init__(self, a_coeffs, lagrange_basis, subgroup_H, vanishing_poly, finite_field, common_input: CommonInput):\n        \"\"\"\n        Initialize the prover with the polynomial coefficients and other required inputs.\n        :param a_coeffs: Coefficients of a(X)\n        :param lagrange_basis: Lagrange basis polynomials\n        :param subgroup_H: Subgroup H\n        :param vanishing_poly: Vanishing polynomial v_H(X)\n        \"\"\"\n        self.a_coeffs = a_coeffs\n        self.lagrange_basis = lagrange_basis\n        self.subgroup_H = subgroup_H\n        self.vanishing_poly = vanishing_poly\n        self.polynomial_value = common_input.polynomial_value_v # v = f(u)\n        self.finite_field = finite_field \n    \n    def compute_selector_polynomials(self):\n        \"\"\"\n        Construct selector polynomials s_i(X).\n        \"\"\"\n        s_polynomials = []\n        N = len(self.subgroup_H)\n        n = log(N, 2)\n        s_polynomials.append(X**(2**(n - 1)) + F(1))\n        for i in range(n - 2, -1, -1):\n            temp = X**(2**i) + F(1)\n            s_polynomials.append(s_polynomials[-1] * temp)\n        s_polynomials.reverse()\n        print(\"s_polys: \", s_polys)\n        return s_polynomials\n    \n    def compute_constraint_polynomials(self, c_X, evaluation_point_u):\n        \"\"\"\n        Construct constraint polynomials p_0(X), ..., p_n(X).\n        \"\"\"\n        p_polys = []\n        s_polys = self.compute_selector_polynomials()\n        n = len(evaluation_point_u)\n\n        # Construct p_0(X)\n        p_0 = s_polys[0] * (c_X - prod([(1 - u) for u in evaluation_point_u]))\n        p_polys.append(p_0)\n\n        # Construct p_k(X) for k = 1 to n\n        omega = self.subgroup_H[1]\n        for k in range(1, n + 1):\n            # omega_k = self.subgroup_H[k]\n            term = s_polys[k - 1] * ((evaluation_point_u[n - k] * c_X) - (1 - evaluation_point_u[n - k]) * c_X.subs(X=(omega^(2^(n - k))) * X))\n            p_polys.append(term)\n        return p_polys\n    \n    def compute_accumulation_polynomial(self, c_X):\n        \"\"\"\n        Construct accumulation polynomial z(X).\n        \"\"\"\n        # Ensure c_X is a proper polynomial\n        c_poly = c_X.numerator()  # Extract the numerator as a polynomial\n        c_coeffs = [c_poly.subs(X=x) for x in self.subgroup_H]\n    \n        # Compute z(X) coefficients\n        z_coeffs = [self.a_coeffs[0] * c_coeffs[0]]  # First term\n        for i in range(1, len(self.a_coeffs)):\n            z_coeffs.append(z_coeffs[-1] + self.a_coeffs[i] * c_coeffs[i])\n\n        # Construct z(X) as a linear combination of Lagrange basis\n        z_X = sum(z_coeffs[i] * self.lagrange_basis[i] for i in range(len(self.a_coeffs)))\n        return z_X\n    \n    def compute_constraint_h_polynomials(self, z_X, a_X, c_X):\n        \"\"\"\n        Construct constraint polynomials h_0(X), h_1(X), h_2(X).\n        \"\"\"\n        # Ensure c_X is a proper polynomial\n        c_poly = c_X.numerator()  # Extract the numerator as a polynomial\n        \n        # Construct h_0(X)\n        h0_X = self.lagrange_basis[0] * (z_X - self.a_coeffs[0] * c_poly.subs(X=1))\n    \n        # Construct h_1(X)\n        z_prev = z_X.subs(X=self.subgroup_H[-1] * X)  # z(ω^-1 * X)\n        h1_X = (X - 1) * (z_X - z_prev - a_X * c_X)\n    \n        # Construct h_2(X)\n        h2_X = self.lagrange_basis[-1] * (z_X - self.polynomial_value)\n\n        return h0_X, h1_X, h2_X \n\n    \n    def compute_aggregation_polynomial(self, constraint_polys, h_polys, alpha):\n        \"\"\"\n        Construct aggregation polynomial h(X).\n        \"\"\"\n        n = len(constraint_polys)\n        h_X = sum(alpha**i * constraint_polys[i] for i in range(n))\n        h_X += alpha**n * h_polys[0] + alpha**(n + 1) * h_polys[1] + alpha**(n + 2) * h_polys[2]\n        return h_X\n\n    def compute_t_polynomial(self, h_X):\n        \"\"\"\n        Compute quotient polynomial t(X) satisfying h(X) = t(X) * v_H(X).\n        \"\"\"\n        if self.vanishing_poly == 0:\n            raise ZeroDivisionError(\"Vanishing polynomial v_H(X) evaluated to 0.\")\n        t_X = h_X // self.vanishing_poly\n        return t_X\n    \n    def compute_c_polynomial(self, evaluation_point_u):\n        \"\"\"\n        Construct the polynomial c(X) using the provided evaluation point.\n        :param evaluation_point_u: The evaluation point (u_0, u_1, ..., u_{n-1})\n        :return: c(X)\n        \"\"\"\n        def eq_tilde(bits_i, u_vector):\n            result = F(1)\n            for bit, u in zip(bits_i, u_vector):\n                result *= (1 - bit) * (1 - u) + bit * u\n            return result\n\n        # Compute coefficients c_i\n        c_coeffs = [eq_tilde(list(reversed(list(map(int, f\"{i:03b}\")))), evaluation_point_u) for i in range(len(self.subgroup_H))]\n\n        # Construct c(X)\n        c_X = sum(c_coeffs[i] * self.lagrange_basis[i] for i in range(len(self.subgroup_H)))\n        return c_X\n\n    def commit_c_polynomial(self, c_X):\n        \"\"\"\n        Compute the commitment of c(X).\n        :param c_X: The polynomial c(X)\n        :return: Commitment of c(X)\n        \"\"\"\n        # Convert c_X to a polynomial\n        c_poly = c_X.numerator()  # Get the numerator to ensure it's a polynomial\n        g = F.random_element()  # Random generator for SRS\n        # TODO: should set SRS be a parameter\n        SRS = [g**i for i in range(len(self.subgroup_H))]\n\n        # Get the coefficients of the polynomial\n        c_coeffs = c_poly.list()  # Retrieve the list of coefficients\n        # Pad coefficients with zeros to match the length of the SRS\n        c_coeffs += [F(0)] * (len(self.subgroup_H) - len(c_coeffs))\n\n        # Compute the commitment as a linear combination with the SRS\n        commitment_c = sum(c_coeffs[i] * SRS[i] for i in range(len(self.subgroup_H)))\n        return commitment_c\n\n    def round_2(self, c_X, a_X, evaluation_point_u):\n        \"\"\"\n        Perform all steps in Round 2.\n        \"\"\"\n        # Step 1: Compute selector polynomials\n        s_polys = self.compute_selector_polynomials()\n\n        # Step 2: Compute constraint polynomials\n        constraint_polys = self.compute_constraint_polynomials(c_X, evaluation_point_u)\n\n        # Step 3: Compute accumulation polynomial z(X)\n        z_X = self.compute_accumulation_polynomial(c_X)\n        z_value = [z_X.subs(X=x) for x in self.subgroup_H]\n\n        # Step 4: Compute constraint h polynomials\n        h_polys = self.compute_constraint_h_polynomials(z_X, a_X, c_X)\n\n        # Step 5: Compute aggregation polynomial h(X)\n        alpha = F.random_element()  # Random challenge\n        h_X = self.compute_aggregation_polynomial(constraint_polys, h_polys, alpha)\n\n        # Step 6: Compute quotient polynomial t(X)\n        t_X = self.compute_t_polynomial(h_X)\n\n        return t_X, h_X, z_X\n    \n    def round_3(self, verifier, c_X, z_X, t_X, a_X):\n        \"\"\"\n        Perform Round 3 of the protocol.\n        :param verifier: Verifier instance providing ζ\n        :param c_X: Polynomial c(X)\n        :param z_X: Accumulation polynomial z(X)\n        :param t_X: Quotient polynomial t(X)\n        :param a_X: Original polynomial a(X)\n        :return: Evaluations and KZG10 proofs\n        \"\"\"\n        # Step 1: Verifier sends random evaluation point ζ\n        zeta = verifier.generate_valid_zeta()  # Verifier provides ζ\n\n        # Step 2: Calculate values of s_i(X) at ζ\n        s_polys = self.compute_selector_polynomials()\n        s_values = [s(zeta) for s in s_polys]\n\n        # Step 3: Define new domain D and coset D'\n        D = self.subgroup_H\n        D_prime = [zeta * d for d in D]\n#         for d in D_prime:\n#             if self.vanishing_poly(d) == 0:\n#                 raise ZeroDivisionError(f\"v_H({d}) = 0, cannot compute t(X).\")\n\n\n        # Step 4: Evaluate c(X), z(X), t(X), a(X) at D'\n        c_values = [c_X(d) for d in D_prime]\n        z_values = [z_X(d) for d in D_prime]\n        t_values = [t_X(d) for d in D_prime]\n        a_values = [a_X(d) for d in D_prime]\n\n        # Step 5: Send evaluations and KZG10 proofs\n        evaluations = {\n            \"c(X)\": c_values,\n            \"z(X)\": z_values,\n            \"t(X)\": t_values,\n            \"a(X)\": a_values\n        }\n\n        # Generate KZG10 proofs for the evaluations\n        kzg_proofs = {\n            \"c(X)\": self.generate_kzg_proof(c_X, D_prime),\n            \"z(X)\": self.generate_kzg_proof(z_X, [zeta, zeta / self.subgroup_H[-1]]),\n            \"t(X)\": self.generate_kzg_proof(t_X, D_prime),\n            \"a(X)\": self.generate_kzg_proof(a_X, D_prime),\n        }\n        \n        # Return evaluations and KZG proofs\n        evaluations = {\n            \"c(X)\": c_values,\n            \"z(X)\": z_values,\n            \"t(X)\": t_values,\n            \"a(X)\": a_values\n        }\n        return evaluations, kzg_proofs\n    \n    def generate_kzg_proof(self, polynomial, points):\n        \"\"\"\n        Generate KZG10 proofs for the given polynomial at the specified points.\n        :param polynomial: Polynomial for which the proof is generated\n        :param points: Points where the polynomial is evaluated\n        :return: List of proofs\n        \"\"\"\n        # Ensure the polynomial is in the proper polynomial ring\n        poly_ring = PolynomialRing(self.finite_field, 'X')\n        polynomial = poly_ring(polynomial)\n\n        # Generate SRS large enough for the polynomial's degree\n        max_degree = polynomial.degree()\n        SRS = [self.finite_field.random_element() ** i for i in range(max_degree + 1)]\n\n        proofs = []\n        for point in points:\n            # Compute the divisor\n            divisor = polynomial - polynomial(point)\n\n            # Convert the numerator into the finite field polynomial ring directly\n            numerator = divisor.numerator()\n            numerator_in_ring = poly_ring(numerator)\n\n            # Generate the proof as a sum of SRS coefficients\n            proof = sum(numerator_in_ring[i] * SRS[i] for i in range(numerator_in_ring.degree() + 1))\n            proofs.append(proof)\n\n        return proofs\n\n    \nclass Verifier:\n    def __init__(self, finite_field, vanishing_poly, subgroup_H):\n        self.finite_field = finite_field  # Finite field F_p\n        self.vanishing_poly = vanishing_poly  # Vanishing polynomial v_H(X)\n        self.subgroup_H = subgroup_H  # Subgroup H of size 2^n\n        self.random_zeta = None  # Random evaluation point ζ\n    \n    def generate_random_alpha(self):\n        \"\"\"\n        Generate and send random scalar α for aggregation.\n        \"\"\"\n        self.random_alpha = self.finite_field.random_element()\n        return self.random_alpha\n    \n    \n    def generate_valid_zeta(self):\n        while True:\n            zeta = self.finite_field.random_element()\n            if all(self.vanishing_poly(zeta * d) != 0 for d in self.subgroup_H):\n                self.random_zeta = zeta\n                return zeta\n\n    def verify_commitment(self, commitment, evaluation_point, polynomial, proof):\n        \"\"\"\n        Verify a single KZG commitment.\n        :param commitment: Commitment of the polynomial\n        :param evaluation_point: Evaluation point ζ\n        :param polynomial: Polynomial to verify\n        :param proof: KZG proof\n        :return: True if verification succeeds, False otherwise\n        \"\"\"\n        # Simulate KZG verification (replace with actual verification in implementation)\n        return hash((commitment, evaluation_point, polynomial, proof)) % 2 == 1\n\n    def verify_constraint_equation(self, t_zeta, h_polys, p_polys, alpha, v_H_zeta):\n        \"\"\"\n        Verify the final constraint equation:\n        t(ζ) * v_H(ζ) = sum of constraint evaluations.\n        \"\"\"\n        # Compute the left-hand side\n        lhs = t_zeta * v_H_zeta\n\n        # Compute the right-hand side\n        rhs = sum(alpha**i * p_polys[i] for i in range(len(p_polys)))\n        rhs += alpha**len(p_polys) * h_polys[0]\n        rhs += alpha**(len(p_polys) + 1) * h_polys[1]\n        rhs += alpha**(len(p_polys) + 2) * h_polys[2]\n\n        return lhs == rhs\n\n    def verify_proof(self, proof, zeta, alpha, v_H, lagrange_basis):\n        \"\"\"\n        Perform the verification of the proof π.\n        :param proof: Proof π containing all elements\n        :param zeta: Evaluation point ζ\n        :param alpha: Aggregation scalar α\n        :param v_H: Vanishing polynomial v_H(X)\n        :param lagrange_basis: Lagrange basis polynomials\n        :return: True if all verifications succeed, False otherwise\n        \"\"\"\n        # Parse the proof\n        C_t, C_z, C_c = proof[\"C_t\"], proof[\"C_z\"], proof[\"C_c\"]\n        a_zeta, z_zeta, z_omega_zeta, t_zeta = proof[\"a(ζ)\"], proof[\"z(ζ)\"], proof[\"z(ζ/ω)\"], proof[\"t(ζ)\"]\n        c_values = proof[\"c_values\"]\n        kzg_proofs = proof[\"kzg_proofs\"]\n\n        # Verify individual KZG commitments\n        if not self.verify_commitment(C_c, zeta, \"c(X)\", kzg_proofs[\"c(X)\"]):\n            return False\n        if not self.verify_commitment(C_z, zeta, \"z(X)\", kzg_proofs[\"z(X)\"]):\n            return False\n        if not self.verify_commitment(C_t, zeta, \"t(X)\", kzg_proofs[\"t(X)\"]):\n            return False\n\n        # Compute constraint polynomials at ζ\n        p_polys = [\n            lagrange_basis[0] * (c_values[0] - (1 - zeta)),\n            lagrange_basis[1] * (zeta - z_omega_zeta),\n            lagrange_basis[-1] * (z_zeta - a_zeta)\n        ]\n\n        # Compute h polynomials at ζ\n        h_polys = [\n            lagrange_basis[0] * (z_zeta - a_zeta),\n            (z_zeta - z_omega_zeta) - t_zeta,\n            lagrange_basis[-1] * (t_zeta - a_zeta)\n        ]\n\n        # Verify the final constraint equation\n        return self.verify_constraint_equation(t_zeta, h_polys, p_polys, alpha, v_H(zeta))\n\n\n# Initialize Parameters\np = 17  # Prime modulus\nF = GF(p)  # Finite field\nomega = F(9)  # Primitive root of unity\nN = 8  # Size of the subgroup H\nH = [omega^i for i in range(N)]  # Subgroup H\nX = polygen(F, 'X')\n\n# Compute vanishing polynomial and Lagrange basis\nv_H = vanishing_polynomial(H)\nL = lagrange_basis(H, v_H)\n\n# Random coefficients for a(X)\na_coeffs = [F.random_element() for _ in range(N)]\na_X = sum(a_coeffs[i] * L[i] for i in range(N))\n\n# Commitment of a(X)\ng = F.random_element()  # Random generator for SRS\nSRS = [g^i for i in range(N)]\ncommitment_a = sum(a_coeffs[i] * SRS[i] for i in range(N))\n\n# Define common inputs\nevaluation_point_u = [F.random_element() for _ in range(3)]  # Random evaluation point\nc_coeffs = [eq_tilde(list(reversed(list(map(int, f\"{i:03b}\")))), evaluation_point_u) for i in range(len(H))]\nprint(\"a_coeffs: \", a_coeffs)\nprint(\"c_coeffs: \", c_coeffs)\npolynomial_value_v = sum(a_coeffs[i] * c_coeffs[i] for i in range(N))\nprint(\"polynomial_value_v: \", polynomial_value_v)\n# polynomial_value_v = a_X(evaluation_point_u[0])  # Example: value of a(X) at evaluation point\ncommon_input = CommonInput(commitment_a, evaluation_point_u, polynomial_value_v)\n\n# Prover Round 1: Compute c(X) and its commitment\nprover = Prover(a_coeffs, L, H, v_H, F, common_input)\nc_X = prover.compute_c_polynomial(evaluation_point_u)\ntest_c_values = [c_X.subs(X=e) for e in H]\ncommitment_c = prover.commit_c_polynomial(c_X)\n\n# Display results\nprint(\"c(X):\")\nshow(c_X)\nprint(\"Commitment of c(X):\")\nshow(commitment_c)\n\n# Perform Round 2\nt_X, h_X, z_X = prover.round_2(c_X, a_X, evaluation_point_u)\n\n# Display results\nprint(\"Quotient Polynomial t(X):\")\nshow(t_X)\nprint(\"Aggregation Polynomial h(X):\")\nshow(h_X)\nprint(\"Accumulation Polynomial z(X):\")\nshow(z_X)\n\n# Initialize Verifier with subgroup H and vanishing polynomial\nverifier = Verifier(F, vanishing_poly=prover.vanishing_poly, subgroup_H=prover.subgroup_H)\n\n# Validate v_H(X) at D'\n# v_H_values = [prover.vanishing_poly(zeta * d) for d in prover.subgroup_H]\n# if any(v == 0 for v in v_H_values):\n#     print(\"Error: v_H(X) evaluates to 0 at some points in D'.\")\n\n# Perform Round 3 with valid zeta\ntry:\n    evaluations, kzg_proofs = prover.round_3(verifier, c_X, z_X, t_X, a_X)\n    print(\"Evaluations at coset D':\", evaluations)\n    print(\"KZG10 Proofs:\", kzg_proofs)\nexcept ZeroDivisionError as e:\n    print(\"Error during Round 3:\", e)\nexcept ValueError as e:\n    print(\"Validation Error:\", e)\n\n","type":"content","url":"/ph23/ph23","position":1},{"hierarchy":{"lvl1":"Barrett Reduction"},"type":"lvl1","url":"/reductions/barrett","position":0},{"hierarchy":{"lvl1":"Barrett Reduction"},"content":"Barrett Reduction is an optimization algorithm used in modular arithmetic calculations (finding remainders) that avoids expensive division operations. Its basic concept is quite straightforward.\n\nWhen calculating the remainder r of a \\bmod N (a modulo N), the conventional approach is to first compute the quotient q = a \\div N, and then calculate the remainder r = a - q \\times N.\n\nHowever, computing q requires a division operation, which is computationally expensive for CPUs. Barrett Reduction avoids this division by introducing a pre-computed constant.","type":"content","url":"/reductions/barrett","position":1},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Core Algorithm Concept"},"type":"lvl2","url":"/reductions/barrett#core-algorithm-concept","position":2},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Core Algorithm Concept"},"content":"Choose a sufficiently large R = 2^k such that R > N.\n\nPre-compute \\mu = \\lfloor R / N \\rfloor. (where \\lfloor \\rfloor represents the floor function)\n\nWhen you need to compute a \\bmod N, use the following steps instead:\n\nCalculate q = \\lfloor(a \\times \\mu) / R\\rfloor\n\nCalculate r = a - q \\times N\n\nIf r \\geq N, then r = r - N\n\nThe key optimization here is that since R is a power of 2, (a \\times \\mu) / R can be implemented through a simple bit shift operation: q = (a \\times \\mu) \\gg k.\n\nFor a fixed finite field, N is typically constant, so \\mu can also be pre-computed and stored.","type":"content","url":"/reductions/barrett#core-algorithm-concept","position":3},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Error Analysis"},"type":"lvl2","url":"/reductions/barrett#error-analysis","position":4},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Error Analysis"},"content":"This method may result in q being slightly smaller than the actual quotient because \\mu is a lower bound of R/N (due to the floor function). This is why we need to check if r is less than N after calculating r = a - q \\times N, and subtract N again if r is greater than or equal to N.\n\nRegarding the number of times N needs to be subtracted in the worst case:\n\nThe upper bound of the error can be proven to be 2. That is, the calculated r can be at most 2N larger than the actual remainder.\n\nTherefore, at most two subtractions of N are needed to obtain the correct remainder.","type":"content","url":"/reductions/barrett#error-analysis","position":5},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Algorithm Advantages"},"type":"lvl2","url":"/reductions/barrett#algorithm-advantages","position":6},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Algorithm Advantages"},"content":"Avoids expensive division operations, replacing them with multiplication and bit shift operations.\n\nFor a fixed modulus N, \\mu can be pre-computed, further improving efficiency.\n\nEven in the worst case, only a finite number (at most two) of additional subtractions are needed.","type":"content","url":"/reductions/barrett#algorithm-advantages","position":7},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Error Range for Computing μ"},"type":"lvl2","url":"/reductions/barrett#error-range-for-computing","position":8},{"hierarchy":{"lvl1":"Barrett Reduction","lvl2":"Error Range for Computing μ"},"content":"Let’s define q' as the actual value of a/N, and q as the value obtained through Barrett Reduction:\nq' = \\lfloor a / N \\rfloor, q = \\lfloor(a \\times \\mu) / R\\rfloor\n\nR = 2^k, where k is an integer, and R > N.\n\n\\mu = \\lfloor R / N \\rfloor\n\nAssume a < R (an important assumption that usually holds in practical applications)","type":"content","url":"/reductions/barrett#error-range-for-computing","position":9},{"hierarchy":{"lvl1":"Barrett Reduction","lvl3":"Upper Bound: q \\leq q'","lvl2":"Error Range for Computing μ"},"type":"lvl3","url":"/reductions/barrett#upper-bound-q-leq-q","position":10},{"hierarchy":{"lvl1":"Barrett Reduction","lvl3":"Upper Bound: q \\leq q'","lvl2":"Error Range for Computing μ"},"content":"We know that \\mu = \\lfloor R / N \\rfloor, so \\mu \\leq R / N.\\begin{aligned}\nq &= \\lfloor(a \\times \\mu) / R\\rfloor \\\\\n  &\\leq \\lfloor(a \\times (R/N)) / R\\rfloor \\quad \\text{(since } \\mu \\leq R/N\\text{)} \\\\\n  &= \\lfloor a / N \\rfloor \\\\\n  &= q'\n\\end{aligned}\n\nTherefore, q \\leq q'","type":"content","url":"/reductions/barrett#upper-bound-q-leq-q","position":11},{"hierarchy":{"lvl1":"Barrett Reduction","lvl3":"Lower Bound: q > q' - 2","lvl2":"Error Range for Computing μ"},"type":"lvl3","url":"/reductions/barrett#lower-bound-q-q-2","position":12},{"hierarchy":{"lvl1":"Barrett Reduction","lvl3":"Lower Bound: q > q' - 2","lvl2":"Error Range for Computing μ"},"content":"First, we know that R/N - 1 < \\mu \\leq R/N (since \\mu is the floor of R/N)\nThus:\\begin{aligned}\nq &= \\lfloor(a \\times \\mu) / R\\rfloor \\\\\n  &> \\lfloor(a \\times (R/N - 1)) / R\\rfloor \\quad \\text{(since } \\mu > R/N - 1\\text{)} \\\\\n  &= \\lfloor a/N - a/R \\rfloor\n\\end{aligned}\n\nNext, we can write:\nq > a/N - a/R - 1 (since for any x, \\lfloor x \\rfloor > x - 1)\n\nNow, using our assumption that a < R, we have a/R < 1, so:\\begin{aligned}\nq &> a/N - 1 - 1 \\\\\n  &= a/N - 2 \n\\end{aligned}\n\nBut q' = \\lfloor a/N \\rfloor, so a/N - 1 < q' \\leq a/N\nSubstituting this inequality into the expression above:\nq > (q' - 1) - 2 = q' - 3\n\nSince q is an integer, q \\geq q' - 2\n\nCombining with our previous result q \\leq q', we can conclude that q' - 2 < q \\leq q'","type":"content","url":"/reductions/barrett#lower-bound-q-q-2","position":13},{"hierarchy":{"lvl1":"Montgomery Reduction"},"type":"lvl1","url":"/reductions/montgomery","position":0},{"hierarchy":{"lvl1":"Montgomery Reduction"},"content":"In finite field operations, we frequently need to perform mod N calculations. For exponential operations involving consecutive multiplications, Montgomery Reduction provides better performance optimization.\n\nThe core idea of Montgomery Reduction is to convert operands to a special Montgomery Form, where all division operations can be significantly simplified.\n\nLet’s understand this through a concrete example:\n\nGiven modulus N = 97, we need to calculate (a \\cdot b) \\bmod N. The traditional method requires performing a modular operation on the result of a \\cdot b, which involves costly division operations.","type":"content","url":"/reductions/montgomery","position":1},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Montgomery Form Conversion"},"type":"lvl2","url":"/reductions/montgomery#montgomery-form-conversion","position":2},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Montgomery Form Conversion"},"content":"Choose R = 100 (usually selected as a power of 2; for demonstration purposes we use 100, which allows us to simply take the last digits or remove trailing zeros in base 10)\n\nFor inputs a = 15, b = 32:\n\nT_a = aR \\bmod N = 45\n\nT_b = bR \\bmod N = 96","type":"content","url":"/reductions/montgomery#montgomery-form-conversion","position":3},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Montgomery Multiplication Process"},"type":"lvl2","url":"/reductions/montgomery#montgomery-multiplication-process","position":4},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Montgomery Multiplication Process"},"content":"Calculate the product in Montgomery Form:\nT_r = T_a \\cdot T_b = 45 \\cdot 96 = 4320\n\nResult conversion:\n\nSince we’ve gone through two Montgomery Form conversions, the result contains an R^2 term, and we need to eliminate one R\n\nWe need to adjust T_r without changing its value modulo N, making it divisible by R\n\nWe can add multiples of N to T without affecting the result modulo N, giving us T + xN\n\nWe need a suitable x that makes T + xN divisible by R\n\nThrough the congruence equation T + xN \\equiv 0 \\pmod{R}, we get x = T \\cdot (-\\frac{1}{N}) \\bmod R\n\nWe denote -\\frac{1}{N} as N', where N' = -\\frac{1}{N} \\bmod R = 67 (pre-computed)\n\nThe x calculated above is the m in Montgomery Reduction, and t = T + mN\n\nSpecific calculations:\nFirst Montgomery Reduction (aR \\cdot bR/R):\n\nm = T_r \\cdot N' \\bmod R = 4320 \\cdot 67 \\bmod 100 = 289440 \\bmod 100 = 40\n(Note: mod 100 in base 10 means taking the last two digits)\n\nt = (T_r + m \\cdot N) / R = (4320 + 40 \\cdot 97) / 100 = 8200 / 100 = 82 = abR\n(Note: dividing by 100 in base 10 means removing trailing zeros)\n\nSecond Montgomery Reduction (abR/R):\n\nm' = t \\cdot N' \\bmod R = 82 \\cdot 67 \\bmod 100 = 94\n\nt' = (t + m' \\cdot N) / R = (82 + 94 \\cdot 97) / 100 = 92 = ab","type":"content","url":"/reductions/montgomery#montgomery-multiplication-process","position":5},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Advantage Analysis"},"type":"lvl2","url":"/reductions/montgomery#advantage-analysis","position":6},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Advantage Analysis"},"content":"Although Montgomery Reduction may not save much computational resources for a single operation (conversion to Montgomery Form still requires mod N operations), its advantages become very clear in scenarios requiring continuous modular multiplication (such as modular exponentiation):\n\nModular exponentiation example: calculate a^5 \\bmod N\n\nTraditional method requires mod N operation after each multiplication\n\nUsing Montgomery Form:\n\nInitial conversion: T_a = aR \\bmod N\n\nNo division needed for intermediate operations: T_a \\cdot T_a / R = (aR \\cdot aR) / R = a^2R\n\nOnly need to convert back to original form at the end\n\nMain advantages:\n\nAvoids mod N operations in intermediate steps\n\nEliminates most division operations\n\nSuitable for hardware implementation (when R is chosen as a power of 2, division and modulo can be completed using shift and bitwise AND operations)\n\nEspecially suitable for scenarios requiring continuous modular multiplications","type":"content","url":"/reductions/montgomery#advantage-analysis","position":7},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Engineering Implementation"},"type":"lvl2","url":"/reductions/montgomery#engineering-implementation","position":8},{"hierarchy":{"lvl1":"Montgomery Reduction","lvl2":"Engineering Implementation"},"content":"In practical engineering, we typically encode an element a into Montgomery Form by multiplying it by R^2. The advantages of this approach are:\n\nMontgomery Reduction and multiplication can be merged into a unified mul function:\n\n\\text{mul}(a, b) = ab/R\n\nBoth encode and decode can use the same mul function:\n\n\\text{encode}(a) = \\text{mul}(a, R^2) = aR^2/R = aR\n\n\\text{decode}(T_a) = \\text{mul}(T_a, 1) = aR/R = a\n\nThis unified implementation makes the code more concise, and when R is chosen as a power of 2, all division and modulo operations can be efficiently performed through shift and bitwise AND operations.\n\nFor modular exponentiation, we can see the advantages of Montgomery multiplication:\n\nTraditional algorithm:\na^5 = ((((a \\cdot a) \\bmod N) \\cdot a) \\bmod N) \\cdot a) \\bmod N) \\cdot a) \\bmod N\nEach step requires expensive division operations for modular reduction\n\nMontgomery algorithm:\n\nT_a = aR \\bmod N\n\nT_a \\cdot T_a / R = a^2R\n\n(a^2R \\cdot aR) / R = a^3R\n\n(a^3R \\cdot aR) / R = a^4R\n\n(a^4R \\cdot aR) / R = a^5R\n\n\\text{decode}(a^5R) = a^5\n\nAll intermediate steps only require simple shift operations, greatly improving performance.","type":"content","url":"/reductions/montgomery#engineering-implementation","position":9},{"hierarchy":{"lvl1":"Finite Field"},"type":"lvl1","url":"/src/basefold","position":0},{"hierarchy":{"lvl1":"Finite Field"},"content":"","type":"content","url":"/src/basefold","position":1},{"hierarchy":{"lvl1":"Finite Field"},"type":"lvl1","url":"/src/basefold#finite-field","position":2},{"hierarchy":{"lvl1":"Finite Field"},"content":"p = 64 * 3 + 1\n\nF193.<a> = GF(193)\nR193.<X> = F193[]\nR193\nFp=F193\nFp\n\nR_2.<X0, X1, X2, X3, X4, X5, X6, X7, X8, X9, Y0, Y1, Y2, Y3, Y4, Y5, Y6, Y7, A0, A1, A2, B0, B1, B2> = Fp[]; R_2\n\nfrom field import magic\n\nFp = magic(Fp)\n\n\ndef bits_le_with_width(i, width):\n    if i >= 2**width:\n        return \"Failed\"\n    bits = []\n    while width:\n        bits.append(i % 2)\n        i //= 2\n        width -= 1\n    return bits\n        \nbits_le_with_width(6, 5)\n\ndef is_power_of_two(n):\n    d = n\n    k = 0\n    while d > 0:\n        d >>= 1\n        k += 1\n    if n == 2**(k-1):\n        return True\n    else:\n        return False\n\ndef next_power_of_two(n):\n    assert n >= 0, \"No negative integer\"\n    if is_power_of_two(n):\n        return n\n    d = n\n    k = 0\n    while d > 0:\n        d >>= 1\n        k += 1\n    return k\n\nis_power_of_two(15), next_power_of_two(15)\n\n(1 - X0) * (1 - X1)\n\nFp(((1 - X0) * (1 - X1))(X0=9, X1=19))\n\nFp((X0 * X1)(X0=9, X1=19))\n\n","type":"content","url":"/src/basefold#finite-field","position":3},{"hierarchy":{"lvl1":"Foldable code"},"type":"lvl1","url":"/src/basefold#foldable-code","position":4},{"hierarchy":{"lvl1":"Foldable code"},"content":"\n\n# m: message\n# k0: message length of base code\n# c: blowup factor\n\ndef rep_encode(m, k0, c):\n    assert len(m) % k0 == 0, \"len(m): %d is not a multiple of k0: %d\" % (len(m), k0)\n    code = []\n    for i in range(0, c*len(m), k0):\n        for j in range(0, c):\n            code += m[i:i+k0]\n    return code\n\nrep_encode([1,2],1, 3), rep_encode([1,2,3,4], 2, 3), rep_encode([1,2,3,4], 4, 3)\n\n","type":"content","url":"/src/basefold#foldable-code","position":5},{"hierarchy":{"lvl1":"Foldable code","lvl2":"Reed-Solomon Code"},"type":"lvl2","url":"/src/basefold#reed-solomon-code","position":6},{"hierarchy":{"lvl1":"Foldable code","lvl2":"Reed-Solomon Code"},"content":"Fix a sequence \\mathbf{\\alpha} = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_n) of n distict elements from \\mathbf{F}_q. Map a message \\mathbf{m} = (m_0, m_1, \\ldots, m_{k_0 - 1}) with m_i \\in \\mathbf{F}_q to the degree k_0 - 1 polynomial.\\mathbf{m} \\mapsto f_{\\mathbf{m}}(X)\n\nwheref_{\\mathbf{m}}(X) = \\sum_{i = 0}^{k_0 - 1} m_i X^i\n\nThen\\text{RS}_q[\\mathbf{\\alpha}, k_0](\\mathbf{m}) = (f_{\\mathbf{m}}(\\alpha_1), f_{\\mathbf{m}}(\\alpha_2), \\ldots, f_{\\mathbf{m}}(\\alpha_{n}))\n\nFor simplicity, we can choose \\alpha_1 = 0, \\alpha_2 = 1, \\ldots, \\alpha_{n} = n - 1.\n\n# Reed-Solomon code for m = (m_0, ..., m_{k_0 - 1})\n# alpha: evaluation points\n# c: blowup factor\ndef rs_encode_single(m, alpha, c):\n    k0 = len(m)\n    code = [None] * (k0 * c)\n    for i in range(k0 * c): \n        # comput f_m(alpha[i])\n        code[i] = sum(m[j] * (alpha[i] ** j) for j in range(k0))\n    return code\n\n# Reed-Solomon code for m\n# m: message\n# k0: message length of base code\n# c: blowup factor\n# default alpha: [0, 1, 2, ... , k0*c - 1]\ndef rs_encode(m, k0, c):\n    assert len(m) % k0 == 0, \"len(m): %d is not a multiple of k0: %d\" % (len(m), k0)\n    code = []\n    alpha = list(range(k0 * c)) # 这里默认 alpha = [0, 1, 2, ... , k0*c - 1]\n    for i in range(0, c*len(m), k0):\n        code += rs_encode_single(m[i:i+k0], alpha, c)\n    return code\n\nrs_encode([1,2],1,3), rs_encode([1,2,3,4], 2, 3), rs_encode([1,2,3,4], 4, 3)\n\n# m: message with length `n`\n# k0: message length of code `G_0`,  G_0: k0 \\otimes ck0\n# depth: \n# c: blowup factor\n# G_0: an encoding function\n#      G_0(m, k0, c)\n# T: RM code table\n\ndef basefold_encode(m, k0, depth, c, T, G0=rep_encode, debug=False):\n    if debug: print(\">>> basefold_encode: m={}, k0={}, d={}, blowup_factor={}, T={}\".format(m, k0, depth, c, T))\n    kd = k0 * 2^depth\n    blowup_factor = c\n    assert len(m) == kd, \"len(m): %d != kd: %d\" % (len(m), kd)\n    assert len(T) == depth, \"len(T): %d != depth: %d\" % (len(T), depth)\n     \n    chunk_size = k0\n    chunk_num = 2^depth\n    \n    code = []\n    for i in range(chunk_num):\n        chunk = m[i*chunk_size: (i+1)*chunk_size]\n        chunk_code = G0(chunk, chunk_size, blowup_factor)\n        code += chunk_code\n    if debug: print(\">>> basefold_encode: code=\", code)\n\n    if depth == 0:\n        return code\n\n    chunk_size = k0 * blowup_factor\n    if debug: print(\">>> basefold_encode: chunk_size=\", chunk_size)\n    if debug: print(\">>> basefold_encode: chunk_num=\", chunk_num)\n\n\n    for i in range(0, depth):\n        table = T[i]\n        assert len(table) == chunk_size, \"table[{}] != chunk_size, len(table)={}, chunk_size={})\".format(i, len(table), chunk_size)\n        if debug: print(\">>> basefold_encode: table=\", table)\n        for c in range(0, chunk_num, 2):\n            left  = code[    c*chunk_size : (c+1)*chunk_size]\n            right = code[(c+1)*chunk_size : (c+2)*chunk_size]\n            if debug: print(\">>> basefold_encode: left={}, right={}\".format(left, right))\n\n            for j in range(0, chunk_size):\n                if debug: print(\">>> basefold_encode: i={}, c={}, j={}\".format(i, c, j))\n                rhs = right[j] * table[j]\n                lhs = left[j]\n                code[(c)*chunk_size + j] = lhs + rhs\n                code[(c+1)*chunk_size + j] = lhs - rhs\n        chunk_size = chunk_size * 2\n        chunk_num = chunk_num // 2\n    return code\n\n# basefold_encode(m, k0, depth, c, G0, T, debug=False):\n\ndef basefold_encode_rec(m, k0, depth, c, T, G0=rep_encode, debug=False):\n    if depth == 0:\n        assert len(m) == k0, \"len(m): %d != k0: %d\" % (len(m), k0)\n        return G0(m, k0, c)\n    \n    kd = len(m)\n    half = kd / 2\n    code_size = c * kd\n\n    m_left = m[:half]\n    m_right = m[half:]\n    \n    code_left = basefold_encode_rec(m_left, k0, depth-1, c, T, G0=G0, debug=debug)\n    print(\"code_left=\", code_left)\n    code_right = basefold_encode_rec(m_right, k0, depth-1, c, T, G0=G0, debug=debug)\n    print(\"code_right=\", code_right)\n\n    t = T[depth-1]\n    assert len(t) == code_size/2, \"wrong table size, len(table)={}, code_size={})\".format(len(t), code_size/2)\n    if debug: print(\"t=\", t)\n    code_half = kd * c // 2\n    code = [0 for _ in range(code_half * 2)]\n    for j in range(code_half):\n        code[j] = code_left[j] + t[j] * code_right[j]\n        code[j+code_half] = code_left[j] - t[j] * code_right[j]\n    return code\n\nbasefold_encode([1, 2], k0=1, depth=1, c=2, T=[[X0, X1]], debug=True)\n\nbasefold_encode_rec([1, 2], k0=1, depth=1, c=2, T=[[X0, X1]])\n\nbasefold_encode([1, 2], k0=1, depth=1, c=3, T=[[X0, X1, X2]], debug=True)\n\nbasefold_encode([1,2,3,4], k0=1, depth=2, c=3, T=[[X0, X1, X2], [Y0, Y1, Y2, Y3, Y4, Y5]])\n\n","type":"content","url":"/src/basefold#reed-solomon-code","position":7},{"hierarchy":{"lvl1":"Basefold IOPP"},"type":"lvl1","url":"/src/basefold#basefold-iopp","position":8},{"hierarchy":{"lvl1":"Basefold IOPP"},"content":"B^{(monomial)} = (1, \\alpha_0)\\otimes (1, \\alpha_1) \\otimes \\cdots \\otimes (1, \\alpha_{k-1})B^{(multilinear)} = ((1-\\alpha_0), \\alpha_0)\\otimes ((1-\\alpha_1), \\alpha_1) \\otimes \\cdots \\otimes ((1-\\alpha_{k-1}), \\alpha_{k-1})\n\ndef basefold_fri_monomial_basis(vs, table, alpha, debug=False):\n    assert len(table) == len(vs)/2, \"len(table) is not double len(vs), len(table) = %d, len(vs) = %d\" % (len(table), len(vs))\n    n = len(vs)\n    half = n / 2\n    new_vs = []\n    left = vs[:half]\n    right = vs[half:]\n\n    for i in range(0, half):\n        if debug: print(\"(left[i] + right[i])/2=\", (left[i] + right[i])/2)\n        if debug: print(\"(left[i] + right[i])/2=\", (left[i] + right[i])/2)\n        new_vs.append((left[i] + right[i])/2 + (alpha) * (left[i] - right[i])/(2*table[i]))\n    return new_vs\n\ndef basefold_fri_multilinear_basis(vs, table, c, debug=False):\n    assert len(table) == len(vs)/2, \"len(table) is not double len(vs), len(table) = %d, len(vs) = %d\" % (len(table), len(vs))\n    n = len(vs)\n    half = n / 2\n    new_vs = []\n    left = vs[:half]\n    right = vs[half:]\n\n    for i in range(0, half):\n        if debug: print(\"(left[i] + right[i])/2=\", (left[i] + right[i])/2)\n        if debug: print(\"(left[i] + right[i])/2=\", (left[i] + right[i])/2)\n        new_vs.append((1 - c) * (left[i] + right[i])/2 + (c) * (left[i] - right[i])/(2*table[i]))\n    return new_vs\n\npi_0 = basefold_encode([1, 2, 3, 4], k0=1, depth=2, c=2, T=[[X4, X5], [X0, X1, X2, X3]]); pi_0\n\npi_1 = basefold_fri_multilinear_basis(pi_0, [X0, X1, X2, X3], A0); pi_1\n\npi_2 = basefold_fri_multilinear_basis(pi_1, [X4, X5], A1); pi_2\n\n","type":"content","url":"/src/basefold#basefold-iopp","position":9},{"hierarchy":{"lvl1":"Basefold Evaluation Argument"},"type":"lvl1","url":"/src/basefold#basefold-evaluation-argument","position":10},{"hierarchy":{"lvl1":"Basefold Evaluation Argument"},"content":"v = f(\\mathbf{z}) = \\sum_{\\mathbf{b}}f(\\mathbf{b})\\cdot \\tilde{eq}_{\\mathbf{z}}(\\mathbf{b})\n\nby sumcheck, the statement can be reduced tov' \\overset{?}{=} f(\\mathbf{r})\\cdot \\tilde{eq}_{\\mathbf{z}}(\\mathbf{r})\n\nand we can have the following statement, since the code is linear\\mathsf{encode}(v'/\\tilde{eq}_{\\mathbf{z}}(\\mathbf{r})) \\overset{?}{=} \\mathsf{encode}\\big(f(\\mathbf{r})\\big)\n\nFortunately, the right hand side is exactly the result of FRI-folding, and the left hand side can be computed by verifier himself.\n\n","type":"content","url":"/src/basefold#basefold-evaluation-argument","position":11},{"hierarchy":{"lvl1":"Basefold Evaluation Argument","lvl2":"More features"},"type":"lvl2","url":"/src/basefold#more-features","position":12},{"hierarchy":{"lvl1":"Basefold Evaluation Argument","lvl2":"More features"},"content":"Add RS code support for G0","type":"content","url":"/src/basefold#more-features","position":13},{"hierarchy":{"lvl1":"Basefold Evaluation Argument","lvl2":"TODO"},"type":"lvl2","url":"/src/basefold#todo","position":14},{"hierarchy":{"lvl1":"Basefold Evaluation Argument","lvl2":"TODO"},"content":"Change Rnd into a transcript that absorbs messages and squeezes challenges.\n\nChange Proof into a struct, dict or class\n\nAdd inputs validation\n\nAdd error messages for assert\n\nfrom mle2 import MLEPolynomial\nfrom merkle import MerkleTree\n\ndef query_phase(transcript, first_tree: MerkleTree, first_oracle, trees: list, oracles: list, num_vars, num_verifier_queries, debug=False):\n    queries = [transcript.squeeze_byte_challenge() % num_vars for _ in range(num_verifier_queries)]\n\n    query_paths = []\n    # query paths\n    for q in queries:\n        num_vars_copy = num_vars\n        cur_path = []\n        indices = []\n        x0 = q\n        x1 = q - num_vars_copy / 2 if q >= num_vars_copy / 2 else q + num_vars_copy / 2\n        if x1 < x0:\n            x0, x1 = x1, x0\n        \n        cur_path.append((first_oracle[x0], first_oracle[x1]))\n        indices.append(x0)\n        q = x0\n        num_vars_copy >>= 1\n\n        for oracle in oracles:\n            x0 = q\n            x1 = q - num_vars_copy / 2 if q >= num_vars_copy / 2 else q + num_vars_copy / 2\n            if x1 < x0:\n                x0, x1 = x1, x0\n            \n            cur_path.append((oracle[x0], oracle[x1]))\n            if debug: print(\"x0:\", x0, \"x1:\", x1, \"num_vars:\", num_vars_copy)\n            if debug: print(\"oracle:\", oracle)\n            indices.append(x0)\n            q = x0\n            num_vars_copy >>= 1\n        \n        query_paths.append((cur_path, indices))\n\n    # merkle paths\n    merkle_paths = []\n    for _, indices in query_paths:\n        cur_query_paths = []\n        for i, idx in enumerate(indices):\n            if i == 0:\n                cur_query_paths.append(first_tree.get_authentication_path(idx))\n                if debug: print(\"mp:\", cur_query_paths[-1])\n                if debug: print(\"commit:\", first_tree.root)\n                if debug: print(\"idx:\", idx)\n            else:\n                cur_tree = trees[i - 1]\n                assert isinstance(cur_tree, MerkleTree)\n                cur_query_paths.append(cur_tree.get_authentication_path(idx))\n                if debug: print(\"mp:\", cur_query_paths[-1])\n                if debug: print(\"commit:\", first_tree.root)\n                if debug: print(\"idx:\", idx)\n        merkle_paths.append(cur_query_paths)\n\n    return query_paths, merkle_paths\n\nfrom unipolynomial import UniPolynomial\n\n# Define prove\n# f_code: Encode(f)\n# f_evals: evaluations of f\n# us: point\n# v: f(point) = v\n# k: number of variables, k = len(u)\n# T: random code table\n# blowup_factor: as named\n# transcript: as named\n\ndef prove_basefold_evaluation_arg_multilinear_basis(f_code, f_evals, us, v, k, T, blowup_factor, commit, num_verifier_queries, transcript, debug=False):\n    # TODO: check if the length of f is a power of 2\n    \n    n = len(f_evals)\n    half = n >> 1\n    assert len(T) == k, \"wrong table size, k={}, len(T)={}\".format(k, len(T))\n    f_code_copy = f_code[:]\n    f = f_evals[:]\n    eq = MLEPolynomial.eqs_over_hypercube(us)\n    \n    challenge_vec = []\n    sumcheck_sum = v\n    h_poly_vec = []\n    f_code_vec = []\n    f_code_trees = []\n    for i in range(k):\n        if debug: print(\"sumcheck round {}\".format(i))\n        f_low = f[:half]\n        f_high = f[half:]\n        eq_low = eq[:half]\n        eq_high = eq[half:]\n        \n        # print(\"low={}, high={}\".format(low, high))\n\n        h_eval_at_0 = sum([f_low[j] * eq_low[j] for j in range(half)])\n        h_eval_at_1 = sum([f_high[j] * eq_high[j] for j in range(half)])\n        h_eval_at_2 = sum([ (2 * f_high[j] - f_low[j]) * (2 * eq_high[j] - eq_low[j]) for j in range(half)])\n        h_poly_vec.append([h_eval_at_0, h_eval_at_1, h_eval_at_2])\n        \n        if debug: print(\"> sumcheck: h=[{},{},{}]\".format(h_eval_at_0, h_eval_at_1, h_eval_at_2))\n        if debug: print(\"> sumcheck: {} ?= {}\".format(h_eval_at_0 + h_eval_at_1, sumcheck_sum))\n        \n        assert h_eval_at_0 + h_eval_at_1 == sumcheck_sum, \"{} + {} != {}\" % (h_eval_at_0, h_eval_at_1, sumcheck_sum)\n\n        # Receive a random number from the verifier\n        alpha = A0 * transcript.squeeze_byte_challenge()\n        \n        challenge_vec.append(alpha)\n\n        # fold(low, high, alpha)\n        f = [(1 - alpha) * f_low[i] + alpha * f_high[i] for i in range(half)]\n        eq = [(1 - alpha) * eq_low[i] + alpha * eq_high[i] for i in range(half)]\n        if debug: print(\"> sumcheck: f_folded = {}\".format(f))\n        if debug: print(\"> sumcheck: eq_folded = {}\".format(eq))\n\n        # compute the new sum = h(alpha)\n        sumcheck_sum = UniPolynomial.uni_eval_from_evals([h_eval_at_0, h_eval_at_1, h_eval_at_2], alpha, [Fp(0),Fp(1),Fp(2)])\n        if debug: print(\"> sumcheck: sumcheck_sum = {}\".format(sumcheck_sum))\n\n        if debug: print(\"fri round {}\".format(i))\n\n        # # Basefold fri\n        f_code = basefold_fri_multilinear_basis(f_code, T[k-i-1], alpha, debug=debug)\n        if debug: print(\"> fri: f_code_folded=\", f_code)\n        f_code_vec.append(f_code)\n        f_code_trees.append(MerkleTree(f_code))\n\n        # DEBUG: consistency check (invariant)\n        # Enc(fold(message)) = fold(Enc(message)) \n        debug_f_folded_code = basefold_encode(m=f, k0=1, depth=k-i-1, c=blowup_factor, G0=rs_encode, T=T[:k-i-1], debug=debug)\n        if debug: print(\"> fri: enc({})={}\".format(f, debug_f_folded_code))\n        assert f_code == debug_f_folded_code, \"Enc(fold(message)) != fold(Enc(message))\"\n\n        transcript.write_field_elements([h_eval_at_0, h_eval_at_1, h_eval_at_2])\n        transcript.write_field_elements(f_code)\n        \n        half = half >> 1\n\n    # DEBUG:\n    f_eval_at_random = sumcheck_sum/eq[0]\n    assert rs_encode([f_eval_at_random], k0=1, c=blowup_factor) == f_code, \"❌: Encode(f(rs)) != f_code_0\"\n    if debug: print(\"end: f_code={}, sum={}, sum/eq={}\".format(f_code, sumcheck_sum, sumcheck_sum/eq[0]))\n    if debug: print(\"🙈: fold(f_code) == encode(fold(f_eq)/fold(eq(us)))\")\n\n    query_paths, merkle_paths = query_phase(transcript, commit, f_code_copy, f_code_trees, f_code_vec, len(f_code_copy), num_verifier_queries, debug)\n\n    # return (h_poly_vec, challenge_vec, f_code_vec)\n    return {\n        'h_poly_vec': h_poly_vec,\n        'f_code_vec': f_code_vec,\n        'challenge_vec': challenge_vec,\n        'f_code_trees': f_code_trees,\n        'query_paths': query_paths,\n        'merkle_paths': merkle_paths\n    }\n\nfrom merkle import verify_decommitment\nfrom transcript import Transcript\n\ndef verify_queries(commit, proof, k, num_vars, num_verifier_queries, T, debug=False):\n    transcript = Transcript()\n    transcript.write_field_elements(commit.root)\n\n    fold_challenges = []\n    for i in range(k):\n        fold_challenges.append(A0 * transcript.squeeze_byte_challenge())\n        transcript.write_field_elements(proof['h_poly_vec'][i])\n        transcript.write_field_elements(proof['f_code_vec'][i])\n\n    queries = [transcript.squeeze_byte_challenge() % num_vars for _ in range(num_verifier_queries)]\n    # query loop\n    for q, (cur_path, _), mps in zip(queries, proof['query_paths'], proof['merkle_paths']):\n        if debug: print(\"cur_path:\", cur_path)\n        num_vars_copy = num_vars\n        # fold loop\n        for i, mp in enumerate(mps):\n            x0 = q\n            x1 = q - num_vars_copy / 2 if q >= num_vars_copy / 2 else q + num_vars_copy / 2\n            if x1 < x0:\n                x0, x1 = x1, x0\n                \n            code_left, code_right = cur_path[i][0], cur_path[i][1]\n\n            if i != len(mps) - 1:\n                table = T[k-i-1]\n                if debug: print(\"table:\", table)\n                if debug: print(\"x0:\", x0)\n                if debug: print(\"x1:\", x1)\n                f_code_folded = cur_path[i + 1][0 if x0 < num_vars_copy / 4 else 1]\n                alpha = fold_challenges[i]\n                if debug: print(\"f_code_folded:\", f_code_folded)\n                if debug: print(\"expected:\", ((1 - alpha) * (code_left + code_right)/2 + (alpha) * (code_left - code_right)/(2*table[x0])))\n                if debug: print(\"code_left:\", code_left)\n                if debug: print(\"code_right:\", code_right)\n                if debug: print(\"alpha:\", alpha)\n                assert f_code_folded == ((1 - alpha) * (code_left + code_right)/2 + (alpha) * (code_left - code_right)/(2*table[x0])), \"failed to check multilinear base fri\"\n\n            if i == 0:\n                if debug: print(\"mp:\", mp)\n                if debug: print(\"commit:\", commit.root)\n                if debug: print(\"idx:\", x0)\n                assert verify_decommitment(x0, code_left, mp, commit.root)\n            else:\n                if debug: print(\"mp:\", mp)\n                if debug: print(\"commit:\", proof['f_code_trees'][i - 1].root)\n                if debug: print(\"idx:\", x0)\n                assert verify_decommitment(x0, code_left, mp, proof['f_code_trees'][i - 1].root)\n\n            num_vars_copy >>= 1\n            q = x0\n\ndef verify_basefold_evaluation_arg_multilinear_basis(f_code, commit, proof, us, v, d, k, T, blowup_factor, num_verifier_queries, Rng, debug=False):\n    # TODO: check if the length of f is a power of 2\n    \n    N = len(f_code)\n    assert k == len(us), \"k != len(us), k = %d, len(us) = %d\" % (k, len(us))\n    n = 1 << k\n    assert N == n * blowup_factor, \"N != n * blowup_factor, N = %d, n = %d, blowup_factor = %d\" % (N, n, blowup_factor)\n    \n    h_poly_vec = proof['h_poly_vec']\n    challenge_vec = proof['challenge_vec']\n    f_code_vec = proof['f_code_vec']\n    sumcheck_sum = v\n    half = n >> 1\n    f_last_code = f_code\n    eq_evals = MLEPolynomial.eqs_over_hypercube(us)\n    \n    for i in range(k):\n        if debug: print(\"sumcheck round {}\".format(i))\n        h_evals = h_poly_vec[i]\n        assert d+1 == len(h_evals), \"d+1 != len(h_evals), d+1 = %d, len(h_evals) = %d\" % (d+1, len(h_evals))\n\n        assert sumcheck_sum == h_evals[0] + h_evals[1], \"sumcheck_sum != h_evals[0] + h_evals[1], sumcheck_sum = %d, h_evals[0] = %d, h_evals[1] = %d\" % (sumcheck_sum, h_evals[0], h_evals[1])\n        # print(\"low={}, high={}\".format(low, high))\n\n        alpha = challenge_vec[i]\n\n        sumcheck_sum = UniPolynomial.uni_eval_from_evals(h_evals, alpha, [Fp(0),Fp(1),Fp(2)])\n\n        eq_low = eq_evals[:half]\n        eq_high = eq_evals[half:]\n        if debug: print(\"eq_low={}, eq_high={}\".format(eq_low, eq_high))\n        eq_evals = [(1-alpha) * eq_low[i] + alpha * eq_high[i] for i in range(half)]\n\n        if debug: print(\"fri round {}\".format(i))\n\n        table = T[k-i-1]\n        f_code_folded = f_code_vec[i]\n        assert len(f_code_folded) == half * blowup_factor, \"len(f_code_folded) != half * blowup_factor, len(f_code_folded) = %d, half = %d, blowup_factor = %d\" % (len(f_code_folded), half, blowup_factor)\n        code_left = f_last_code[:half*blowup_factor]\n        code_right = f_last_code[half*blowup_factor:]\n        if debug: print(\"code_left={}\".format(code_left))\n        if debug: print(\"code_right={}\".format(code_right))\n        if debug: print(\"f_code_folded={}\".format(f_code_folded))\n        if debug: print(\"alpha={}\".format(alpha))\n        if debug: print(\"table={}\".format(table))\n        for j in range(half*blowup_factor):\n            assert f_code_folded[j] == ((1 - alpha) * (code_left[j] + code_right[j])/2 + (alpha) * (code_left[j] - code_right[j])/(2*table[j])), \"failed to check multilinear base fri\"\n        f_last_code = f_code_folded\n        half = half >> 1\n\n    verify_queries(commit, proof, k, N, num_verifier_queries, T, debug)\n        \n    # check the final code\n    final_code = f_code_vec[i]\n    assert len(final_code) == blowup_factor, \"len(final_code) != blowup_factor, len(final_code) = %d, blowup_factor = %d\" % (len(final_code), blowup_factor)\n    for i in range(len(final_code)):\n        assert final_code[0] == final_code[i], \"final_code is not a repetition code\"\n    # check f(alpha_vec)\n    f_eval_at_random = sumcheck_sum/eq_evals[0]\n    if debug: print(\"f_eval_at_random={}\".format(f_eval_at_random))\n    if debug: print(\"rs_encode([f_eval_at_random], k0=1, c=blowup_factor)=\", rs_encode([f_eval_at_random], k0=1, c=blowup_factor))\n    assert rs_encode([f_eval_at_random], k0=1, c=blowup_factor) == f_code_folded, \"❌: Encode(f(rs)) != f_code_0\"\n    print(\"✅: Verified! fold({}) == encode(fold(f_eq)/fold(eq(us)))\".format(f_code))\n\n    return True\n\nff = [Fp(1),Fp(2),Fp(3),Fp(4)]\nff_code = basefold_encode(m=ff, k0=1, depth=2, c=2, G0=rs_encode, T=[[X4, X5], [X0, X1, X2, X3]]); ff_code\n\ncommit = MerkleTree(ff_code)\ncommit.root\n\nfrom transcript import Transcript\n\ntranscript = Transcript()\ntranscript.write_field_elements(commit.root)\nproof = prove_basefold_evaluation_arg_multilinear_basis(ff_code, ff, [3,4], 12, 2, [[X4, X5], [X0, X1, X2, X3]], 2, commit, 4, transcript); proof\n\n# verify_basefold_evaluation_arg_multilinear_basis(f_code, arg, us, v, d, k, T, blowup_factor, Rng):\n\nverify_basefold_evaluation_arg_multilinear_basis(ff_code, commit, proof, us=[3,4], v=12, d=2, k=2, T=[[X4, X5], [X0, X1, X2, X3]], blowup_factor=2, num_verifier_queries=4, Rng=[A1, A0])\n\nfrom field import Field\n\nprint(Field.get_operation_count())\nField.reset_operation_count()\nprint(Field.get_operation_count())\n\nprint(Fp.Fp)\nprint(Fp.zero())\nprint(Fp.one())\nprint(Fp.primitive_element())","type":"content","url":"/src/basefold#todo","position":15},{"hierarchy":{"lvl1":"BCHO PCS"},"type":"lvl1","url":"/src/bcho-pcs","position":0},{"hierarchy":{"lvl1":"BCHO PCS"},"content":"from group import DummyGroup\nfrom utils import log_2, pow_2\nfrom unipolynomial import UniPolynomial\nfrom mle2 import MLEPolynomial\nfrom kzg10 import Commitment, KZG10Commitment\n\nF193.<a> = GF(193)\nR193.<X> = F193[]\nR193\nFp=F193\nFp\n\nUniPolynomial.set_scalar(Fp(0), Fp)\nCommitment.set_scalar(Fp(0))\n\n# For symbolic calculation\n\nR_2.<X0, X1, X2, X3, X4, X5, X6, X7, X8, X9, Y0, Y1, Y2, Y3, Y4, Y5, Y6, Y7, A0, A1, A2, B0, B1, B2> = Fp[]; R_2\n\nFp2 = Fp.extension(2, 'b'); Fp2\n\nomega = Fp.primitive_element()^3; omega, omega^64 == 1, omega^32 == -1\n\nH16 = [(omega^4)^i for i in range(16)]; H16\n\nH8 = [(omega^8)^i for i in range(8)]; H8\n\nH4 = [(omega^16)^i for i in range(4)]; H4\n\nG1 = DummyGroup(Fp)\nG1.generator(), G1.exp(2, 3), G1.order(), \n\nG1.exp(X1, 3)\n\nG2 = DummyGroup(Fp2)\nFp2.random_element()\nG2.generator(), G2.exp(G2.generator(), 3), G2.order(), G2.field.polynomial()\n\ndef is_power_of_two(n):\n    d = n\n    k = 0\n    while d > 0:\n        d >>= 1\n        k += 1\n    if n == 2**(k-1):\n        return True\n    else:\n        return False\n\ndef next_power_of_two(n):\n    assert n >= 0, \"No negative integer\"\n    if is_power_of_two(n):\n        return n\n    d = n\n    k = 0\n    while d > 0:\n        d >>= 1\n        k += 1\n    return k\n\nis_power_of_two(15), next_power_of_two(15)\n\ndef bits_le_with_width(i, width):\n    if i >= 2**width:\n        return \"Failed\"\n    bits = []\n    while width:\n        bits.append(i % 2)\n        i //= 2\n        width -= 1\n    return bits\n        \nbits_le_with_width(6, 5)\n\n\n","type":"content","url":"/src/bcho-pcs","position":1},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Polynomial"},"type":"lvl2","url":"/src/bcho-pcs#polynomial","position":2},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Polynomial"},"content":"\n\n# \n#  m = [1]\n#  m = [1] + [1 * X0, 1 * X1]\n#  m = m + [1 * X0^2, 1 * X0 * X1, 1 * X1]\ndef mle_eval_from_coeffs(f_coeffs, point):\n    k = len(point)\n    n = len(f_coeffs)\n    assert len(f_coeffs) == 1 << k\n    \n    e = Fp(0)\n    for i in range(n):\n        i_bits = bits_le_with_width(i, k)\n        monomial = product([point[j] if i_bits[j] == 1 else 1 for j in range(k)])\n        e += f_coeffs[i] * monomial\n    \n    return e\n\nmle_eval_from_coeffs([1, 3, 2, 1], [91, 181]) == Fp((1 + 3*X0 + 2*X1 + 1*X0*X1)(X0=91, X1=181))\n\nmle_eval_from_coeffs([1, 3, 2, 1], [X0, X1])\n\ndef uni_eval_from_coeffs(coeffs, z):\n    t = Fp(1)\n    eval = Fp(0)\n    for i in range(0, len(coeffs)):\n        eval += coeffs[i] * t\n        t *= z\n    return eval\n\nuni_eval_from_coeffs([1, 3, 2, 1], 91) == Fp(1 + 3*91 + 2*91^2 + 1*91^3) == Fp(144)\n\ndef polynomial_division(dividend, divisor):\n    \"\"\"\n    Perform univariate polynomial division.\n\n    Args:\n        dividend (list): Coefficients of the dividend polynomial, in descending order of degree.\n        divisor (list): Coefficients of the divisor polynomial, in descending order of degree.\n\n    Returns:\n        tuple: (quotient, remainder), where quotient and remainder are lists of coefficients.\n\n    Raises:\n        ValueError: If the divisor is zero or of higher degree than the dividend.\n    \"\"\"\n    # Remove leading zeros\n    while dividend and dividend[0] == 0:\n        dividend = dividend[1:]\n    while divisor and divisor[0] == 0:\n        divisor = divisor[1:]\n\n    # Check for zero division\n    if not divisor:\n        raise ValueError(\"Division by zero polynomial\")\n\n    # Check degrees\n    if len(dividend) < len(divisor):\n        return [], dividend\n\n    # Initialize quotient and remainder\n    quotient = [0] * (len(dividend) - len(divisor) + 1)\n    remainder = dividend.copy()\n\n    # Perform polynomial long division\n    for i in range(len(quotient)):\n        if len(remainder) < len(divisor):\n            break\n        \n        # Compute the next term of the quotient\n        term = remainder[0] // divisor[0]\n        quotient[i] = term\n\n        # Subtract term * divisor from the remainder\n        for j in range(len(divisor)):\n            if j < len(remainder):\n                remainder[j] -= term * divisor[j]\n\n        # Remove leading zero from remainder\n        while remainder and remainder[0] == 0:\n            remainder.pop(0)\n\n    # Remove trailing zeros from quotient\n    while quotient and quotient[-1] == 0:\n        quotient.pop()\n\n    return quotient, remainder\n\n# Example usage\ndividend = [Fp(1), Fp(0), Fp(-2), Fp(0), Fp(1)]  # x^4 - 2x^2 + 1\ndivisor = [Fp(1), Fp(1)]  # x + 1\n\nquotient, remainder = polynomial_division(dividend, divisor)\n\nprint(\"Dividend:\", dividend)\nprint(\"Divisor:\", divisor)\nprint(\"Quotient:\", quotient)\nprint(\"Remainder:\", remainder)\n\n(X^4 - 2*X^2 + 1)/(X+1)\n\nkzg10 = KZG10Commitment(G1, G2, 10); \n\nkzg10.setup(secret_symbol=X0, g1_generator=1, g2_generator=1)\nkzg10.srs\n\n\nkzg10.h, kzg10.h_s\n\nkzg10.commit([3,2,1]), kzg10.commit([3,2,1])\n\nC0 = kzg10.commit([3,2,1]); C0\n\nC1 = kzg10.commit([9,10,11]); C1\n\nC0 + C1\n\nW = kzg10.prove_eval([3,2,1], 91, 167)\n\nkzg10.verify_eval(C0, W, 91, 167)\n\n\nUniPolynomial([93,1]) * UniPolynomial([-91, 1]) + UniPolynomial([167])\n\ndef log_2(x):\n    \"\"\"\n    Compute the integer part of the logarithm base 2 of x.\n\n    Args:\n        x (int): The number to compute the logarithm of. Must be a positive integer.\n\n    Returns:\n        int: The floor of the logarithm base 2 of x.\n\n    Raises:\n        ValueError: If x is not a positive integer.\n    \"\"\"\n    if not isinstance(x, int) or x <= 0:\n        raise ValueError(\"x must be a positive integer\")\n    \n    result = 0\n    while x > 1:\n        x >>= 1  # Bit shift right (equivalent to integer division by 2)\n        result += 1\n    return result\n\n","type":"content","url":"/src/bcho-pcs#polynomial","position":3},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"BCHO PCS"},"type":"lvl2","url":"/src/bcho-pcs#bcho-pcs","position":4},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"BCHO PCS"},"content":"\n\nclass ProofTranscript:\n    def __init__(self, field):\n        self.field = field\n        self.state = 0  # Initial state\n    \n    def absorb(self, element):\n        \"\"\"\n        Absorb a field element into the transcript.\n        \n        Args:\n            element: A field element to be absorbed. \n        \"\"\"\n        if isinstance(element, int):\n            element = self.field(element)\n        elif isinstance(element, sage.rings.integer.Integer):\n            element = self.field(element)\n        elif isinstance(element, Commitment):\n            element = element.value\n        elif isinstance(element, list):\n            element = sum(element)\n        elif not element.parent() == self.field:\n            raise TypeError(\"Must absorb a field element\")\n        \n        # if not isinstance(element, self.field.Element):\n        #     raise TypeError(\"Must absorb a field element\")\n        \n        # Simple hash function: multiply by a prime and add the element\n        bs = str(element).encode('utf-8')\n        n = int.from_bytes(bs, 'big')\n        self.state = (self.state * int(31) + n) % self.field.order()\n    \n    def squeeze(self):\n        \"\"\"\n        Generate a random field element based on the current state.\n        \n        Returns:\n            A field element.\n        \"\"\"\n        # Use the current state to generate a \"random\" field element\n        self.state = (self.state * 31 + 17) % self.field.order()\n        return self.field(self.state)\n    \n    def clone(self):\n        new = ProofTranscript(self.field)\n        new.state = self.state\n        return new\n\n\nFp(1).parent() == Fp\n\ntype(30)\n\n    \ntranscript = ProofTranscript(Fp)\n\n# Absorb some elements\ntranscript.absorb(10)\ntranscript.absorb(20)\n\n# Squeeze some randomness\nr1 = transcript.squeeze()\nr2 = transcript.squeeze()\n\nprint(f\"Random element 1: {r1}\")\nprint(f\"Random element 2: {r2}\")\n\n# Absorb more elements\ntranscript.absorb(30)\n\n# Squeeze more randomness\nr3 = transcript.squeeze()\n\nprint(f\"Random element 3: {r3}\")\n\ndef prove_eval_arg_naive(C, f, point, v, transcript):\n    \"\"\"\n    C: commitment to f\n    f: polynomial\n    point: evaluation point\n    v: evaluation value\n    Rng: randomness\n    \"\"\"\n    \n    n = len(f)\n    k = log_2(n)\n    if len(point) != k:\n        raise ValueError(\"Invalid evaluation point, k={}, while len(point)={}\".format(k, len(point)))\n    if not is_power_of_two(n):\n        raise ValueError(\"Invalid polynomial length\")\n    \n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    # Split and fold polynomials\n    coeffs = f[:]\n    h_uni_cm_vec = []\n    h_uni_poly_vec = [coeffs]\n    for i in range(k):\n        f_even = coeffs[::2]\n        f_odd = coeffs[1::2]\n        coeffs = [Fp(f_even[j] + point[i] * f_odd[j])  for j in range(len(f_even))]\n        h_cm = kzg10.commit(coeffs)\n\n        transcript.absorb(h_cm)\n\n        h_uni_poly_vec.append(coeffs)\n        h_uni_cm_vec.append(h_cm)\n\n    print(\"h_uni_cm_vec=\", h_uni_cm_vec)\n    print(\"h_uni_poly_vec=\", h_uni_poly_vec)\n    assert coeffs == [Fp(v)]\n    assert len(coeffs) == 1\n    print(\"coeffs=\", coeffs)\n\n    # Sample randomness and reply evaluations\n    r = transcript.squeeze()\n    print(\"r=\", r)\n    evals_pos = []\n    evals_neg = []\n    evals_sq = []\n    for i in range(k):\n        poly = h_uni_poly_vec[i]\n        evals_pos.append(uni_eval_from_coeffs(poly, r))\n        evals_neg.append(uni_eval_from_coeffs(poly, (-1) * r))\n        evals_sq.append(uni_eval_from_coeffs(poly, r^2))\n    \n    \n    # DEBUG: verify evaluations\n    dbg_evals_sq = evals_sq[:] + [v]\n    for i in range(k):\n        print(\"i={}, {}\".format(i, dbg_evals_sq[i+1] == (evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*r)))\n\n    return (h_uni_cm_vec, evals_pos, evals_neg, evals_sq)\n\ndef verify_eval_arg_naive(C, arg, point, v, transcript):\n    \"\"\"\n    C: commitment to f\n    arg: argument to be verified\n    point: evaluation point\n    v: evaluation value\n    transcript: proof transcript\n    \"\"\"\n    k = len(point)\n    n = 1 << k\n    \n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    h_uni_cm_vec, evals_pos, evals_neg, evals_sq = arg\n\n    for i in range(len(h_uni_cm_vec)):\n        transcript.absorb(h_uni_cm_vec[i])\n    \n    r = transcript.squeeze()\n    \n    evals_sq.append(v)\n    verified = True\n    # Verify evaluations\n    for i in range(k):\n        b = evals_sq[i+1] == (evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*r)\n        print(\"i={}, {}\".format(i, b))\n        verified = verified and b\n    if verified:\n        print(\"✅ Verified!\")\n    return verified\n\n\ntranscript = ProofTranscript(Fp); transcript.state\n\nff = [1, 3, 2, 1]\nff_cm = kzg10.commit(ff)\nff_cm\n\narg = prove_eval_arg_naive(ff_cm, [1, 3, 2, 1], point=[91, 181], v=123, transcript=transcript.clone())\n\nuni_eval_from_coeffs([81, 93], 150)\n\nverify_eval_arg_naive(ff_cm, arg, point=[91, 181], v=123, transcript=ProofTranscript(Fp))\n\n","type":"content","url":"/src/bcho-pcs#bcho-pcs","position":5},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Optimized version"},"type":"lvl2","url":"/src/bcho-pcs#optimized-version","position":6},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Optimized version"},"content":"\n\ndef prove_eval_arg_opt(C, f, point, v, transcript):\n    \"\"\"\n    C: commitment to f\n    f: polynomial\n    point: evaluation point\n    v: evaluation value\n    Rng: randomness\n    \"\"\"\n    \n    n = len(f)\n    k = log_2(n)\n    if len(point) != k:\n        raise ValueError(\"Invalid evaluation point, k={}, while len(point)={}\".format(k, len(point)))\n    if not is_power_of_two(n):\n        raise ValueError(\"Invalid polynomial length\")\n    \n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    # Split and fold polynomials\n    coeffs = f[:]\n    h_uni_cm_vec = []\n    h_uni_poly_vec = [coeffs]\n    for i in range(k-1):\n        f_even = coeffs[::2]\n        f_odd = coeffs[1::2]\n        coeffs = [Fp(f_even[j] + point[i] * f_odd[j])  for j in range(len(f_even))]\n        h_cm = kzg10.commit(coeffs)\n\n        transcript.absorb(h_cm)\n\n        h_uni_poly_vec.append(coeffs)\n        h_uni_cm_vec.append(h_cm)\n\n    print(\"h_uni_cm_vec=\", h_uni_cm_vec)\n    print(\"h_uni_poly_vec=\", h_uni_poly_vec)\n    assert Fp(v) == coeffs[0] + point[k-1] * coeffs[1]\n    assert len(coeffs) == 2\n    print(\"coeffs=\", coeffs)\n\n    # Sample randomness and reply evaluations\n    r = transcript.squeeze()\n    print(\"r=\", r)\n    evals_pos = []\n    evals_neg = []\n    evals_sq = []\n    for i in range(k):\n        poly = h_uni_poly_vec[i]\n        poly_at_r = uni_eval_from_coeffs(poly, r)\n        poly_at_neg_r = uni_eval_from_coeffs(poly, (-1) * r)\n        poly_at_r_sq = uni_eval_from_coeffs(poly, r^2)\n        evals_pos.append(poly_at_r)\n        evals_neg.append(poly_at_neg_r)\n        evals_sq.append(poly_at_r_sq)\n        transcript.absorb(poly_at_r)\n        transcript.absorb(poly_at_neg_r)\n        transcript.absorb(poly_at_r_sq)\n    \n    print(\"evals_pos=\", evals_pos)\n    print(\"evals_neg=\", evals_neg)\n    print(\"evals_sq=\", evals_sq)\n    \n    gamma = transcript.squeeze()\n    # gamma = 1\n    print(\"gamma=\", gamma)\n\n    h_agg = [0] * n\n    for i in range(n):\n        for j in range(k):\n            poly = h_uni_poly_vec[j]\n            if i < len(poly):\n                h_agg[i] += poly[i] * (gamma**j)\n    print(\"h_agg=\", h_agg)\n    \n    h_agg_poly_at_r = uni_eval_from_coeffs(h_agg, r)\n    h_agg_poly_at_neg_r = uni_eval_from_coeffs(h_agg, -r)\n    h_agg_poly_at_r_sq = uni_eval_from_coeffs(h_agg, r^2)\n\n    print(\"h_agg_poly_at_r={}, h_agg_poly_at_neg_r={}, h_agg_poly_at_r_sq={}\".format(h_agg_poly_at_r, h_agg_poly_at_neg_r, h_agg_poly_at_r_sq))\n\n    # DEBUG: verify h_agg\n    assert h_agg_poly_at_r == sum([evals_pos[i]*(gamma**i) for i in range(k)])\n\n\n    # DEBUG: verify evaluations\n    dbg_evals_sq = evals_sq[:] + [v]\n    for i in range(k):\n        print(\"dbg_evals_sq[i+1]={}, interp_and_eval_line(evals_pos[i], evals_neg[i], r, point[i])={}\".format(dbg_evals_sq[i+1], (evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*r)))\n        print(\"i={}, {}\".format(i, dbg_evals_sq[i+1] == (evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*r)))\n\n    interp_poly = UniPolynomial.interpolate([h_agg_poly_at_r, h_agg_poly_at_neg_r, h_agg_poly_at_r_sq], [r, -r, r^2])\n    print(\"interp_poly=\", interp_poly)\n    vanishing_poly = UniPolynomial([-r, 1])*UniPolynomial([r, 1]) * UniPolynomial([-r*r, 1])\n    print(\"vanishing_poly=\", vanishing_poly)\n\n    h_agg_poly = UniPolynomial(h_agg)\n\n    quo_poly, rem = divmod(h_agg_poly-interp_poly, vanishing_poly)\n    print(\"quo={}, rem={}\".format(quo_poly, rem))\n    # assert rem == UniPolynomial([0])\n    print(\"quo_poly={}\".format(quo_poly))\n    \n    g_poly = h_agg_poly - interp_poly\n    print(\"g_poly=\", g_poly.coeffs)\n    print(\"rem={}\".format(g_poly % vanishing_poly))\n    print(\"quo={}\".format(g_poly // vanishing_poly))\n    \n    Cq = kzg10.commit(quo_poly.coeffs)\n    transcript.absorb(Cq)\n\n    zeta = transcript.squeeze()\n    print(\"zeta={}\".format(zeta))\n    \n    interp_at_zeta =interp_poly.evaluate(zeta)\n    print(\"interp_at_zeta={}\".format(interp_at_zeta))\n    \n    r_poly = h_agg_poly - UniPolynomial([interp_at_zeta]) - quo_poly * ((zeta - r) * (zeta + r) * (zeta - r*r))\n    print(\"h_agg_poly - UniPolynomial([interp_at_zeta])=\", h_agg_poly - UniPolynomial([interp_at_zeta]))\n    print(\"quo_poly * ((zeta - r) * (zeta + r) * (zeta - r*r))=\", quo_poly * ((zeta - r) * (zeta + r) * (zeta - r*r)))\n    print(\"r_poly=\", r_poly)\n\n    print(\"r(zeta)={}\".format(r_poly.evaluate(zeta)))\n\n    Cr = kzg10.commit(r_poly.coeffs)\n\n    transcript.absorb(Cr)\n\n    arg_r_poly_at_zeta =kzg10.prove_eval(r_poly.coeffs, point=zeta, v=0)\n\n    # DEBUG: verify r_poly argument\n    assert kzg10.verify_eval(Cr, arg_r_poly_at_zeta, point=zeta, v=0)\n\n    return (h_uni_cm_vec, evals_pos, evals_neg, evals_sq, Cq, arg_r_poly_at_zeta)\n\ndef verify_eval_arg_opt(C, arg, point, v, transcript):\n    \"\"\"\n    Verify an optimized evaluation argument proof.\n\n    Args:\n    C: commitment to f\n    arg: argument to be verified (output of prove_eval_arg_opt)\n    point: evaluation point\n    v: claimed evaluation value\n    transcript: proof transcript\n    \"\"\"\n    k = len(point)\n    n = 1 << k\n    \n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    print(\"C=\", C)\n\n    print(\"arg=\", arg)\n\n    h_uni_cm_vec, evals_pos, evals_neg, evals_sq, Cq, Cw = arg\n\n    for i in range(len(h_uni_cm_vec)):\n        transcript.absorb(h_uni_cm_vec[i])\n    \n    beta = transcript.squeeze()\n    print(\"beta=\", beta)\n\n    for i in range(k):\n        transcript.absorb(evals_pos[i])\n        transcript.absorb(evals_neg[i])\n        transcript.absorb(evals_sq[i])\n    \n    gamma = transcript.squeeze()\n    print(\"gamma=\", gamma)\n\n    Ch = C\n    for i in range(0, k-1):\n        Ch = Ch + (gamma**(i+1)) * h_uni_cm_vec[i]\n    print(\"Ch=\", Ch)\n    \n    h_agg_poly_at_beta = sum([evals_pos[i]*(gamma**i) for i in range(k)])\n    h_agg_poly_at_neg_beta = sum([evals_neg[i]*(gamma**i) for i in range(k)])\n    h_agg_poly_at_beta_sq = sum([evals_sq[i]*(gamma**i) for i in range(k)])\n    print(\"h_agg_poly_at_beta={}, h_agg_poly_at_neg_beta={}, h_agg_poly_at_beta_sq={}\".format(h_agg_poly_at_beta, h_agg_poly_at_neg_beta, h_agg_poly_at_beta_sq))\n\n    # vanishing_poly = UniPolynomial([-beta, h_agg_poly_at_beta]) * UniPolynomial([beta, h_agg_poly_at_neg_beta]) * UniPolynomial([-beta*beta, h_agg_poly_at_beta_sq])\n    interp_poly = UniPolynomial.interpolate(\n        [h_agg_poly_at_beta, h_agg_poly_at_neg_beta, h_agg_poly_at_beta_sq], \n        [beta, -beta, beta**2])\n\n    transcript.absorb(Cq)\n\n    zeta = transcript.squeeze()\n    print(\"zeta={}\".format(zeta))\n    \n    # vanishing_poly_at_zeta= vanishing_poly.evaluate(zeta)\n    interp_at_zeta = interp_poly.evaluate(zeta)\n    print(\"interp_at_zeta={}\".format(interp_at_zeta))\n\n    Cr = Ch - kzg10.commit([interp_at_zeta]) - Cq * ((zeta - beta) * (zeta + beta) * (zeta - beta*beta))\n\n    print(\"Cr=\", Cr)\n    print(\"Cq=\", Cq)\n\n    transcript.absorb(Cr)\n\n    # Verify r_poly argument\n    if not kzg10.verify_eval(Cr, Cw, point=zeta, v=0):\n        return False\n\n    # Verify evaluations\n    evals_sq = evals_sq + [v]\n    for i in range(k):\n        if evals_sq[i+1] != (evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*beta):\n            return False\n\n    return True\n\nUniPolynomial.polynomial_division_with_remainder([46, 25, 25, 1], [123, 128, 170])\n\nUniPolynomial([126, 151]) * UniPolynomial([123, 128, 170]) + UniPolynomial([181, 64])\n\ndivmod( UniPolynomial([46, 25, 25, 1]),  UniPolynomial([123, 128, 170]))\n\nUniPolynomial([46, 25, 25, 1]) % UniPolynomial([123, 128, 170])\n\ntype(ff_cm)\n\narg = prove_eval_arg_opt(ff_cm, [1, 3, 2, 1], point=[91, 181], v=123, transcript=transcript.clone()); arg\n\nverify_eval_arg_opt(ff_cm, arg, point=[91, 181], v=123, transcript=transcript.clone())\n\ngamma=50\n(X0^3 + 2*X0^2 + 3*X0 + 1) + (93*X0 + 81) * gamma \n\nuni_eval_from_coeffs([1, 3, 2, 1], 174), uni_eval_from_coeffs([81, 93], 174)\n\ntype(transcript.squeeze())\n\n\n\nf1 = UniPolynomial([1, 2, 3])\nf2 = UniPolynomial([4, 5, 6])\nf3 = f1 + f2\nf3\n\nf1 = UniPolynomial([1, 2, 3])\nf2 = UniPolynomial([4, 5, 6])\nf3 = f1 * f2\nf3\n\ndef poly(cof):\n    return sum([cof[i] * X^i for i in range(len(cof))])\n\npoly([1, 2, 3])\n\n\npoly([1, 2, 3]) * poly([4, 5, 6])\n\nf1 = UniPolynomial([1, 2, 3])\nf2 = UniPolynomial([4, 5])\ndivmod(f1, f2)\n\n(155*X + 108) * poly([4, 5]) + poly([148])\n\nz4 = UniPolynomial.vanishing_polynomial([Fp(1), Fp(2), Fp(3), Fp(4)]); z4\n\nz4 // (UniPolynomial([-1, 1])), z4 % (UniPolynomial([-1, 1]))\n\nz4.division_by_linear_divisor(Fp(1))\n\ndivmod(z4, UniPolynomial([-1, 1]))\n\nUniPolynomial.polynomial_division_with_remainder([24, 143, 35, 183, 1], [Fp(-1), Fp(1)])\n\ndivmod(z4, UniPolynomial([-1, 1]))\n\n\n(X-1)*(X-2)*(X-3)*(X-4)\n\ncof = UniPolynomial.compute_coeffs_from_evals_fast([5, 3, 2, 4], [1, 2, 3, 4]); cof\n\n[poly(cof).subs(X=i+1) for i in range(4)]\n\nUniPolynomial.compute_evals_from_coeffs_fast(cof, [1, 2, 3, 4])\n\n\n\ncof = UniPolynomial.ntt_coeffs_from_evals([5, 3, 2, 4], 2, H4[1]); cof\n\nUniPolynomial.ntt_evals_from_coeffs(cof, 2, H4[1])\n\n","type":"content","url":"/src/bcho-pcs#optimized-version","position":7},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Optimization"},"type":"lvl2","url":"/src/bcho-pcs#optimization","position":8},{"hierarchy":{"lvl1":"BCHO PCS","lvl2":"Optimization"},"content":"Prover doesn’t need to send h^{(i)}(\\beta^2)\n\n## Interpolate two points into a polynomial and evaluate it at X=r\n#\n#   f(X) = [(-x, a), (x, b)] -> (a+b)/2 + ((a-b) / (2*x)) * X\n#\n#   f(r) = (a+b)/2 + r * (a-b) / (2*x)\n#\n# \n#(evals_pos[i] + evals_neg[i])/2 + point[i] * (evals_pos[i] - evals_neg[i])/(2*r)))\n\n\ndef interp_and_eval_line(a, b, r, x):\n    \"\"\"\n    Interpolate two points into a polynomial and evaluate it at X=r\n\n        f(X) = [(-x, a), (x, b)] -> (a+b)/2 + ((a-b) / (2*x)) * X\n\n        f(r) = (a+b)/2 + r * (a-b) / (2*x)\n    Args:\n        a (int): The evaluation at -x\n        b (int): The evaluation at x\n        r (int): The point at which to evaluate the polynomial\n        x (int): The x-coordinate of the point\n\n    Returns:\n        int: The result of evaluating the polynomial at X=r\n    \"\"\"\n    return ((a + b) / 2 + r * (a - b) / (2 * x))\n\ndef prove_eval_arg_opt_opt(C, f, point, v, transcript, debug=0):\n    \"\"\"\n    C: commitment to f\n    f: polynomial\n    point: evaluation point\n    v: evaluation value\n    Rng: randomness\n    \"\"\"\n    \n    n = len(f)\n    k = log_2(n)\n    if len(point) != k:\n        raise ValueError(\"Invalid evaluation point, k={}, while len(point)={}\".format(k, len(point)))\n    if not is_power_of_two(n):\n        raise ValueError(\"Invalid polynomial length\")\n    \n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    # Split and fold polynomials\n    coeffs = f[:]\n    h_uni_cm_vec = []\n    h_uni_poly_vec = [coeffs]\n    for i in range(k-1):\n        f_even = coeffs[::2]\n        f_odd = coeffs[1::2]\n        coeffs = [Fp(f_even[j] + point[i] * f_odd[j])  for j in range(len(f_even))]\n        h_cm = kzg10.commit(coeffs)\n\n        transcript.absorb(h_cm)\n\n        h_uni_poly_vec.append(coeffs)\n        h_uni_cm_vec.append(h_cm)\n\n    if debug > 0: \n        coeffs = [coeffs[i] + point[k-1] * coeffs[i+1] for i in range(0, len(coeffs), 2)]\n        print(\"check: fully folded polynomial must be equal to the claimed evaluation: {} = {}\".format(coeffs[0], v))\n        assert (coeffs[0] == v)\n        assert len(coeffs) == 1\n        print(\"coeffs=\", coeffs)\n        print(\"h_uni_cm_vec=\", h_uni_cm_vec)\n        print(\"h_uni_poly_vec=\", h_uni_poly_vec)\n\n    # Sample randomness and reply evaluations\n    beta = transcript.squeeze()\n    print(\"beta=\", beta)\n    evals_pos = []\n    evals_neg = []    \n    evals_sq = []\n    for i in range(k):\n        poly = h_uni_poly_vec[i]\n        poly_at_beta = uni_eval_from_coeffs(poly, beta)\n        poly_at_neg_beta = uni_eval_from_coeffs(poly, (-1) * beta)\n        poly_at_beta_sq = uni_eval_from_coeffs(poly, beta^2)\n        evals_pos.append(poly_at_beta)\n        evals_neg.append(poly_at_neg_beta)\n        evals_sq.append(poly_at_beta_sq)\n        transcript.absorb(poly_at_beta)\n        transcript.absorb(poly_at_neg_beta)\n\n    transcript.absorb(evals_sq[0])\n\n    print(\"evals_pos=\", evals_pos)\n    print(\"evals_neg=\", evals_neg)\n    print(\"evals_sq=\", evals_sq)\n\n    gamma = transcript.squeeze()\n    \n    if debug > 0: \n        print(\"gamma=\", gamma)\n\n    h_agg = [0] * n\n    for i in range(n):\n        for j in range(k):\n            poly = h_uni_poly_vec[j]\n            if i < len(poly):\n                h_agg[i] += poly[i] * (gamma**j)\n    if debug > 1: \n        print(\"h_agg=\", h_agg)\n    \n    h_agg_poly_at_beta = uni_eval_from_coeffs(h_agg, beta)\n    h_agg_poly_at_neg_beta = uni_eval_from_coeffs(h_agg, -beta)\n    h_agg_poly_at_beta_sq = uni_eval_from_coeffs(h_agg, beta^2)\n\n    if debug > 1: \n        print(\"h_agg_poly_at_beta={}, h_agg_poly_at_neg_beta={}, h_agg_poly_at_beta_sq={}\".format(h_agg_poly_at_beta, h_agg_poly_at_neg_beta, h_agg_poly_at_beta_sq))\n\n    # DEBUG: verify h_agg\n    if debug > 0:\n        print(\"check: aggregated h(X) must be equal to sum(h_i(X)) at (beta, -beta, beta^2)\")\n        assert h_agg_poly_at_beta == sum([evals_pos[i]*(gamma**i) for i in range(k)])\n        assert h_agg_poly_at_neg_beta == sum([evals_neg[i]*(gamma**i) for i in range(k)])\n        assert h_agg_poly_at_beta_sq == sum([evals_sq[i]*(gamma**i) for i in range(k)])\n\n    # DEBUG: verify evaluations\n    if debug > 0:\n        dbg_evals_sq = evals_sq[:] + [v]\n        print(\"dbg_evals_sq=\", dbg_evals_sq)\n        for i in range(k):\n            print(\"check: h_{i+1}(r^2) = h_i(r) + h_i(-r))/2 + x_i * (h_i(r) - h_i(-r))/(2*r)\")\n            print(\"i={}, {}, {}\".format(i, \n                dbg_evals_sq[i+1], interp_and_eval_line(evals_pos[i], evals_neg[i], point[i], beta)))\n\n    h_agg_poly = UniPolynomial(h_agg)\n    interp_poly = UniPolynomial.interpolate([h_agg_poly_at_beta, h_agg_poly_at_neg_beta, h_agg_poly_at_beta_sq], [beta, -beta, beta^2])\n    vanishing_poly = UniPolynomial([-beta, 1])*UniPolynomial([beta, 1]) * UniPolynomial([-beta*beta, 1])\n    g_poly = h_agg_poly - interp_poly\n    quo_poly, rem = divmod(g_poly, vanishing_poly)\n\n    if debug > 0:\n        # assert rem == UniPolynomial([0])\n        pass\n\n    if debug > 1:\n        print(\"interp_poly=\", interp_poly)\n        print(\"vanishing_poly=\", vanishing_poly)    \n        print(\"quo={}, rem={}\".format(quo_poly, rem))\n        print(\"quo_poly={}\".format(quo_poly))\n        print(\"rem={}\".format(rem))\n        print(\"quo={}\".format(quo_poly))\n    \n    Cq = kzg10.commit(quo_poly.coeffs)\n    transcript.absorb(Cq)\n\n    zeta = transcript.squeeze()\n    print(\"zeta={}\".format(zeta))\n    \n    interp_at_zeta =interp_poly.evaluate(zeta)\n    print(\"interp_at_zeta={}\".format(interp_at_zeta))\n    \n    r_poly = h_agg_poly - UniPolynomial([interp_at_zeta]) - quo_poly * ((zeta - beta) * (zeta + beta) * (zeta - beta*beta))\n    print(\"h_agg_poly - UniPolynomial([interp_at_zeta])=\", h_agg_poly - UniPolynomial([interp_at_zeta]))\n    print(\"quo_poly * ((zeta - beta) * (zeta + beta) * (zeta - beta*beta))=\", quo_poly * ((zeta - beta) * (zeta + beta) * (zeta - beta*beta)))\n    print(\"r_poly=\", r_poly)\n\n    print(\"r(zeta)={}\".format(r_poly.evaluate(zeta)))\n\n    Cr = kzg10.commit(r_poly.coeffs)\n\n    transcript.absorb(Cr)\n\n    arg_r_poly_at_zeta =kzg10.prove_eval(r_poly.coeffs, point=zeta, v=0)\n\n    # DEBUG: verify r_poly argument\n    if debug > 0:\n        print(\"check: `r(z)=0` argument must hold\")\n        assert kzg10.verify_eval(Cr, arg_r_poly_at_zeta, point=zeta, v=0)\n\n    return (h_uni_cm_vec, evals_pos, evals_neg, evals_sq[:1], Cq, arg_r_poly_at_zeta)\n\ndef verify_eval_arg_opt_opt(C, arg, point, v, transcript, debug=0):\n    \"\"\"\n    Verify an optimized evaluation argument proof.\n\n    Args:\n    C: commitment to f\n    arg: argument to be verified (output of prove_eval_arg_opt)\n    point: evaluation point\n    v: claimed evaluation value\n    transcript: proof transcript\n    \"\"\"\n    k = len(point)\n    n = 1 << k\n    \n    if debug > 1:\n        print(\"C={}, arg={}, point={}, v={}\".format(C, arg, point, v))\n\n    transcript.absorb(C)\n    transcript.absorb(point)\n    transcript.absorb(v)\n\n    h_uni_cm_vec, evals_pos, evals_neg, evals_sq, Cq, Cw = arg\n\n    for i in range(len(h_uni_cm_vec)):\n        transcript.absorb(h_uni_cm_vec[i])\n    \n    beta = transcript.squeeze()\n    if debug > 1:\n        print(\"> beta = {}\".format(beta))\n\n    for i in range(k):\n        transcript.absorb(evals_pos[i])\n        transcript.absorb(evals_neg[i])\n        evals_sq.append(interp_and_eval_line(evals_pos[i], evals_neg[i], point[i], beta))\n    transcript.absorb(evals_sq[0])\n\n    if debug > 1:\n        print(\"> evals_sq = {}\".format(evals_sq))\n    \n    # 1st check:\n    if debug > 0:\n        print(\"🛡️: check if the evaluation f({})={} is correct\".format(point, v))\n        print(\"    by computing the folded polynomial h_{}({}^2) ?= {}\".format(k, beta, v))\n        if (evals_sq[-1] == v): \n            print(\"😀 ✅\")\n    if not (evals_sq[-1] == v):\n        return False\n    \n    gamma = transcript.squeeze()\n    if debug > 1:\n        print(\"> gamma = {}\".format(gamma))\n\n    Ch = C\n    for i in range(0, k-1):\n        Ch = Ch + (gamma**(i+1)) * h_uni_cm_vec[i]\n\n    if debug > 1:\n        print(\"> Commitment of h(X): Ch={}\".format(Ch))\n    \n    # Compute h_agg(beta), h_agg(-beta), h_agg(beta^2)\n    h_agg_poly_at_beta = sum([evals_pos[i]*(gamma**i) for i in range(k)])\n    h_agg_poly_at_neg_beta = sum([evals_neg[i]*(gamma**i) for i in range(k)])\n    h_agg_poly_at_beta_sq = sum([evals_sq[i]*(gamma**i) for i in range(k)])\n\n    if debug > 1:\n        print(\"> h_agg_poly_at_beta={}\".format(h_agg_poly_at_beta))\n        print(\"> h_agg_poly_at_neg_beta={}\".format(h_agg_poly_at_neg_beta))\n        print(\"> h_agg_poly_at_beta_sq={}\".format(h_agg_poly_at_beta_sq))\n\n    # Interpolate c(X) from three points: \n    #\n    #    [(beta, h_agg(beta)), (-beta, h_agg(-beta)), (beta^2, h_agg(beta^2))]\n\n    interp_poly = UniPolynomial.interpolate(\n        [h_agg_poly_at_beta, h_agg_poly_at_neg_beta, h_agg_poly_at_beta_sq], \n        [beta, -beta, beta**2])\n\n    # h_agg(X) - c(X) = q(X) * (X-beta)(X+beta)(X-beta^2)\n    # \n    # Receive Cq = Commit(c(X)) \n    transcript.absorb(Cq)\n\n    # Sample randomness\n    zeta = transcript.squeeze()\n\n    if debug > 1:\n        print(\"> zeta = {}\".format(zeta))\n    \n    # vanishing_poly_at_zeta= vanishing_poly.evaluate(zeta)\n    interp_at_zeta = interp_poly.evaluate(zeta)\n    vanishing_poly_at_zeta = ((zeta - beta) * (zeta + beta) * (zeta - beta*beta))\n\n    print(\"interp_at_zeta={}\".format(interp_at_zeta))\n\n    Cr = Ch - kzg10.commit([interp_at_zeta]) - Cq * vanishing_poly_at_zeta\n\n    if debug > 1:\n        print(\"> Cr={}\".format(Cr))\n        print(\"> Cq={}\".format(Cq))\n\n    transcript.absorb(Cr)\n\n    # Verify r_poly argument\n    \n    # 2. Check if r(z)=0\n    Cw_verified = kzg10.verify_eval(Cr, Cw, point=zeta, v=0)\n\n    if debug > 0:\n        if Cw_verified: print(\"🛡️: check if r(z)=0 😀 ✅\")\n        else: print(\"🛡️: check if r(z)=0 😢 ❌\")\n\n    if not Cw_verified:\n        return False\n\n    return True\n\narg = prove_eval_arg_opt_opt(ff_cm, [1, 3, 2, 1], \n            point=[91, 181], v=123, transcript=transcript.clone(), debug=1); arg\n\nverify_eval_arg_opt_opt(ff_cm, arg, point=[91, 181], v=123, transcript=transcript.clone(), debug=1)","type":"content","url":"/src/bcho-pcs#optimization","position":9},{"hierarchy":{"lvl1":"Circle Group"},"type":"lvl1","url":"/src/circle","position":0},{"hierarchy":{"lvl1":"Circle Group"},"content":"","type":"content","url":"/src/circle","position":1},{"hierarchy":{"lvl1":"Circle Group"},"type":"lvl1","url":"/src/circle#circle-group","position":2},{"hierarchy":{"lvl1":"Circle Group"},"content":"Before we start building circle FFT, let’s first build the circle group, which will be used as the domain of circle fft.\n\nWe choose a basic field GF(31), which has a order of 31. And then we extend it to a 2-degree extension field GF(\n\n312), to get a multiplicative subgroup of order 32.\n\nF31 = GF(31)\nR31.<X> = F31[]\nR31\n\nNow we extend F31 to a 2-degree extension field GF(\n\n312).\n\nC31.<i> = F31.extension(X^2 + 1)\nC31\n\nWe want to find a group element of order 32, so we take the generator of GF(\n\n312) and get 30th power of it.\n\nFact: For any prime field \\mathbb{F}_p, where p=2^n - 1, the 2-degree extension over \\mathbb{F}_p is\nK=\\mathbb{F}_{p^2}, then K has a multiplicative subgroup with order 2^n.\n\nProof:p^2 - 1 = (2^n - 1) * (2^n - 1) - 1 = 2^{2n} - 2^{n+1} = 2^n(2^n - 2)\n\nIn our case, F31.order() - 1 is 2^n - 2.\n\n\ng = C31.multiplicative_generator()^(F31.order() - 1)\ng\n\nDefine a function to calculate the order of an element in the circle group.\n\ndef group_ord(elem):\n    K = elem.parent()\n\n    for i in range(1, K.order()):\n        # An order of an element is defined by how much power we need to raise the element to get the identity element\n        if elem**i == K.one():\n            return i\n    return \"False\"\n\nCheck that g is of order 32.\n\ngroup_ord(g)\n\nDefine the group multiplication, which is similar to complex number multiplication.\n\ndef group_mul(g1, g2):\n    x1, y1 = g1\n    x2, y2 = g2\n    return (x1 * x2 - y1 * y2) + (x1 * y2 + x2 * y1) * i \n\nDefine the group inverse, which is also the J mapping in the paper.\n\ndef group_inv(g1):\n    x1, y1 = g1\n    return (x1 - y1 * i)\n\nDefine show function to print group.\n\ndef show(GG): \n    for j in range(0, len(GG)):\n        t = GG[j]\n        t0 = t[0]\n        t1 = t[1]\n        if t1 > F31(15) and t1 <= F31(30):\n            t1_str = \"-\" + str(31-t1)\n        else:\n            t1_str = str(t1)\n        print(\"i={}, log={}, ord={}, ({},{})\".format(j, log_gen(g, t), group_ord(t), t0, t1_str))\n\nDefine log_gen function to calculate the power of an element in the circle group.\n\ndef log_gen(gen, t):\n    K = gen.parent()\n    for i in range(1, K.order()):\n        if gen^i == t:\n            return i\n    return \"Fail\"\n\n","type":"content","url":"/src/circle#circle-group","position":3},{"hierarchy":{"lvl1":"Jump over the circle"},"type":"lvl1","url":"/src/circle#jump-over-the-circle","position":4},{"hierarchy":{"lvl1":"Jump over the circle"},"content":"Here we define the generator for Circle Group as chosen by Vitalik \n\nhttps://​vitalik​.eth​.limo​/general​/2024​/07​/23​/circlestarks​.html\n\n# optional\ng = 10 + 5*i\ngroup_ord(g)\n\nThen we generate G5, which is a normal multiplicative group of order \n\n25.\n\nG5’s elements must be on the circle x^2 + y^2 = 1, due to:a^{p + 1} = 1, a = x + y * ia * a ^ p = (x + y * i) * (x - y * i) = x^2 + y^2 = 1\n\nSince a^p is a Frobenius map, a^p equals to the conjugate of a, which is x - y * i.\n\nG5 = [g^i for i in range(0, 2^5)]\n# check that G5's elements are on the circle x^2 + y^2 = 1\nfor t in G5:\n    x, y = t\n    assert x^2 + y^2 == 1\nG5\n\nPrint information of G5’s elements.\n\nfor j in range(0, 32):\n    t = g^j\n    print(\"i={}, log={}, ord={}, ({},{})\".format(j, log_gen(g, t), group_ord(t), t[0], t[1]))\n\nCheck elements’ operation on G5.\n\nassert G5[1] * G5[1] == G5[2]\nassert G5[3] * G5[4] == G5[7]\nassert group_mul(G5[1], G5[2]) == G5[3]\nassert group_inv(G5[1]) ==  G5[31]\nassert G5[1] * group_inv(G5[1]) == 1\n\nGenerate G4 from G5 by squaring each element.\n\nG4 = [t^2 for t in G5[:16]]\nG4\n\nGenerate G3 from G4 by squaring each element.\n\nG3 = [t^2 for t in G4[:8]]\nG3\n\n","type":"content","url":"/src/circle#jump-over-the-circle","position":5},{"hierarchy":{"lvl1":"Standard Position Coset"},"type":"lvl1","url":"/src/circle#standard-position-coset","position":6},{"hierarchy":{"lvl1":"Standard Position Coset"},"content":"Let Q be the generator of G_{n+1}, D is a standard position coset of size 2^nD = Q\\cdot G_n\n\nwhere ord(Q)=2^{n+1}.\n\nAnd D is also a twin-coset of size 2^n defined with G_{n-1}:D = Q\\cdot G_{n-1} \\uplus Q^{-1}\\cdot G_{n-1}\n\nThe rotation Q^2 is exactly the generator of G_n.\n\nHere we construct a series of cosets, including standard position cosets and twin cosets, where we can randomly find a coset for circle fft.\n\n# Define a coset or G4 from a rotation Q, ord(Q) = 32\n#\n# Let Q = g (generator of G, whose order is 32)\n\nD_standard_coset_4 = [g * t for t in G4]; show(D_standard_coset_4)\n\n# D_twin_coset_4 is equal to D_standard_coset_4 in the set-theoretic sense, with different order\n\nD_twin_coset_4 = [g * t for t in G3] + [group_inv(g) * t for t in G3]; show(D_standard_coset_4)\n\n# `log` specifies the order of an element in the circle\n\n# This is also a twin-coset of size 16\n\nD_twin_coset_4_1 = [G5[3] * t for t in G3] + [group_inv(G5[3]) * t for t in G3]; show(D_twin_coset_4_1)\n\n# Again\n\nD_twin_coset_4_2 = [G5[5] * t for t in G3] + [group_inv(G5[5]) * t for t in G3]; show(D_twin_coset_4_2)\n\n# Fact:\n#   g^2 is the generator of G4\n#   g * G4 is the standard position coset\n\nG4 == [(g^2)^i for i in range(0, 16)]\n\n# Define a coset or G3 from a rotation Q, ord(Q) = 16\n#\n# Let Q = g^2 (g is the generator of G5, whose order is 32)\n\nD_standard_coset_3 = [g^2 * t for t in G3]; show(D_standard_coset_3)\n\n","type":"content","url":"/src/circle#standard-position-coset","position":7},{"hierarchy":{"lvl1":"Squaring map"},"type":"lvl1","url":"/src/circle#squaring-map","position":8},{"hierarchy":{"lvl1":"Squaring map"},"content":"Lemma: For any twin-coset D of size N=2^n, the image of \\pi(D) must be a twin-coset of size 2^{n-1}.D = Q\\cdot G_{n-1} \\uplus Q^{-1}\\cdot G_{n-1}\\pi(D) = \\pi(Q)\\cdot G_{n-2} \\uplus \\pi(Q)^{-1}\\cdot G_{n-2}\n\nFact: If D is a standard position coset, so is \\pi(D).\n\ndef sq(D):\n    rs = []\n    for t in D:\n        if t^2 not in rs:\n            # x' == 2 * x^2 - 1\n            rs += [t^2]\n    return rs\n\n# This is exactly the same coset we generated from G4 before\n\nD_standard_coset_3 = sq(D_standard_coset_4); show(D_standard_coset_3)\n\n","type":"content","url":"/src/circle#squaring-map","position":9},{"hierarchy":{"lvl1":"Circle FFT"},"type":"lvl1","url":"/src/circle#circle-fft","position":10},{"hierarchy":{"lvl1":"Circle FFT"},"content":"\n\nTo apply circle fft, let’s generate a random polynomial’s evaluation based on D_twin_coset_4_1 (or any other twin coset you prefer).\n\nf = {}\nfrom random import randint\n\nfor t in D_twin_coset_4_1:\n    f[t] = C31(randint(0, 100))\n\nf\n\nDefine the pi mapping applying to point’s x coordinate.\n\ndef pi(t):\n    # x^2 - y^2 == 2 * x^2 - 1 (x^2 + y^2 = 1)\n    return C31(2 * t^2 - 1)\n\nIn the first step of ifft, we try to eliminate y coordinate to simplifier our computations in later steps.\n\nWe extract y from the polynomial by dividing polynomial in to 2 parts, f0 and f1, which correspond to the equation f[t] == f0[x] + y * f1[x].\n\nt is a group element, x and y are the x and y coordinate of t respectively.\n\nSo that f0 and f1 are only functions of x coordinate.\n\ndef ifft_first_step(f):\n    f0 = {}\n    f1 = {}\n    for t in f:\n        x, y = t\n\n        f0[x] = (f[t] + f[group_inv(t)]) / C31(2)\n        f1[x] = (f[t] - f[group_inv(t)]) / (C31(2) * y)\n\n        # Check that f is divided into 2 parts correctly\n        assert f[t] == f0[x] + y * f1[x]\n\n        # print(\"{}: {} = {} + {} * {}\".format(t, f[t], f0[x], f1[x], y))\n\n    return f0, f1\n\nIn normal steps, we deal with the polynomial exactly the same as the classical polynomial ifft, deeming the polynomial as a univariate polynomial, except that x is applied by the pi mapping.\n\nBasically, we divide the polynomial into 2 parts, f_0 and f_1, which correspond to the equation:f[x] = f_0[x] + x * f_1[x]\n\nThusf_0[x] = \\frac{f[x] + f[-x]}{2}f_1[x] = \\frac{f[x] - f[-x]}{2x}\n\ndef ifft_normal_step(f):\n\n    if len(f) == 1:\n        res = []\n        for x in f:\n            res.append(f[x])\n        return res\n\n    f0 = {}\n    f1 = {}\n\n    for x in f:\n        assert x != 0, \"f should be on coset\"\n        f0[pi(x)] = (f[x] + f[-x]) / C31(2)\n        f1[pi(x)] = (f[x] - f[-x]) / (C31(2) * x)\n\n        # Check that f is divided into 2 parts correctly\n        assert f[x] == f0[pi(x)] + x * f1[pi(x)]\n\n    return ifft_normal_step(f0) + ifft_normal_step(f1)\n\nNow we have ifft_first_step and ifft_normal_step, we can define the ifft function.\n\ndef ifft(f):\n    f0, f1 = ifft_first_step(f)\n    f0 = ifft_normal_step(f0)\n    f1 = ifft_normal_step(f1)\n\n    return f0 + f1\n\nApply ifft to our random polynomial and see what we get.\n\ncoeffs = ifft(f)\ncoeffs\n\nNow we try to define the fft function.\n\nBefore that, we define pie_group function to generate next domain of a domain, which is equivalent to sq function in the x dimension (you can check that by focusing on the x coordinate in sq operation).\n\ndef pie_group(D):\n    D_new = []\n    for x in D:\n        x_new = pi(x)\n        if x_new not in D_new:\n            D_new.append(x_new)\n\n    # Check that the new domain is exactly half size of the old domain\n    assert len(D_new) * 2 == len(D), \"len(D_new) * 2 != len(D), {} * 2 != {}, D_new={}, D={}\".format(len(D_new), len(D), D_new, D)\n    \n    return D_new\n\nDefine the first step of fft, which contains the same logic as ifft_first_step.\n\nThe basis of coefficients is like:1, v_1(x), v_2(x), v_1(x)v_2(x), y, yv_1(x), yv_2(x), yv_1(x)v_2(x) \\quad (N = 8) \\tag{1}\n\nwherev_1(x) = x, v_2(x) = 2x^2-1, v_3(x) = 2(2x^2-1)^2-1, ...\n\nSo to divide the polynomial into 2 parts, we just need to divide the coefficients into 2 parts, it’s basis are also divided into 2 parts as follows:1, v_1(x), v_2(x), v_1(x)v_2(x) \\tag{2}y, yv_1(x), yv_2(x), yv_1(x)v_2(x) \\tag{3}\n\nNotice that basis (2) is bit reversed order compared to normal basis, and basis (3) is like basis (2) but multiplied by y.\n\nTo eliminate y in normal steps’ computation, we just consider basis of the second part of coefficients (which is actually basis (3)) as same as basis (2), and supply the y at the end of fft.\n\nFirst, we define the first step of fft.\n\ndef fft_first_step(f, D):\n    # Check that the polynomial and the domain have the same length\n    assert len(f) == len(D), \"len(f) != len(D), {} != {}, f={}, D={}\".format(len(f), len(D), f, D)\n\n    # divide the polynomial into 2 parts\n    len_f = len(f)\n    f0 = f[:len_f//2]\n    f1 = f[len_f//2:]\n\n    # halve the domain by simply removing the y coordinate\n    D_new = []\n    for t in D:\n        x, _ = t\n        if x not in D_new:\n            D_new.append(x)\n\n    # Check that the new domain is exactly half size of the old domain\n    assert len(D_new) * 2 == len(D), \"len(D_new) * 2 != len(D), {} * 2 != {}, D_new={}, D={}\".format(len(D_new), len(D), D_new, D)\n\n    return f0, f1, D_new\n\nIn normal steps, we deal with the polynomial exactly the same as the classical polynomial fft, deeming the polynomial as a univariate polynomial, except that x is applied by the pi mapping.\n\nDivide the polynomial into 2 parts, f_0 and f_1, which correspond to the equation:f[x] = f_0[x] + x * f_1[x]\n\nMerge f_0 and f_1 using x from the domain, which corresponds to the equation above too.\n\ndef fft_normal_step(f, D):\n\n    if len(f) == 1:\n        return {D[0]: f[0]}\n    \n    next_domain = pie_group(D)\n\n    # Check that the new domain is exactly half size of the old domain\n    assert len(next_domain) * 2 == len(D), \"len(next_domain) * 2 != len(D), {} * 2 != {}, next_domain={}, D={}\".format(len(next_domain), len(D), next_domain, D)\n    # Check that the polynomial and the domain have the same length\n    assert len(f) == len(D), \"len(f) != len(D), {} != {}, f={}, D={}\".format(len(f), len(D), f, D)\n\n    f0 = fft_normal_step(f[:len(f)//2], next_domain)\n    f1 = fft_normal_step(f[len(f)//2:], next_domain)\n\n    f_new = {}\n    for x in D:\n        f_new[x] = f0[pi(x)] + f1[pi(x)] * x\n\n    # Check that f is divided into 2 parts correctly\n    for x in D:\n        if x != 0:\n            assert f0[pi(x)] == (f_new[x] + f_new[-x]) / C31(2), \"f0[pi(x)] = {}\".format(f0[pi(x)])\n            assert f1[pi(x)] == (f_new[x] - f_new[-x]) / (C31(2) * x), \"f1[pi(x)] = {}\".format(f1[pi(x)])\n        else:\n            assert f0[pi(x)] == f_new[x], \"f0[pi(x)] = {}\".format(f0[pi(x)])\n\n    # Check that the polynomial and the domain have the same length\n    assert len(f) == len(f_new), \"len(f) != len(f_new), {} != {}, f={}, f_new={}, D={}\".format(len(f), len(f_new), f, f_new, D)\n\n    # Check that ifft and fft are correct inverse operations\n    assert ifft_normal_step(f_new) == f, \"ifft(f_new) != f, {} != {}\".format(ifft_normal_step(f_new), f)\n        \n    return f_new\n\nWe have fft_first_step and fft_normal_step, we can define the fft function.\n\ndef fft(f, D):\n\n    # Check that the polynomial and the domain have the same length\n    assert len(f) == len(D), \"len(f) != len(D), {} != {}, f={}, D={}\".format(len(f), len(D), f, D)\n\n    D_copy = D[:]\n    f0, f1, D = fft_first_step(f, D)\n\n    # Check that the polynomial and the domain have the same length\n    assert len(f0) == len(D), \"len(f0) != len(D), {} != {}, f0={}, D={}\".format(len(f0), len(D), f0, D)\n    assert len(f1) == len(D), \"len(f1) != len(D), {} != {}, f1={}, D={}\".format(len(f1), len(D), f1, D)\n\n    f0 = fft_normal_step(f0, D)\n    f1 = fft_normal_step(f1, D)\n\n    f = {}\n    # supply y to the polynomial\n    for t in D_copy:\n        x, y = t\n        f[t] = f0[x] + f1[x] * y\n\n    return f\n\n\nCheck that ifft and fft are correct inverse operations.\n\nf_prime = fft(coeffs, D_twin_coset_4_1)\n\nfor t, s in zip(f, f_prime):\n    assert t == s\n\nfor t in f:\n    print(\"f[{}]={}, f_prime[{}]={}\".format(t, f[t], t, f_prime[t]))\n    assert f[t] == f_prime[t], \"f[{}] != f_prime[{}], {} != {}\".format(t, t, f[t], f_prime[t])\n\n","type":"content","url":"/src/circle#circle-fft","position":11},{"hierarchy":{"lvl1":"Circle FRI"},"type":"lvl1","url":"/src/circle#circle-fri","position":12},{"hierarchy":{"lvl1":"Circle FRI"},"content":"Basis:1, v_1(x), v_2(x), v_1(x)v_2(x), y, yv_1(x), yv_2(x), yv_1(x)v_2(x) \\quad (N = 8)\n\n# Inputs:\n#   f is the polynomial to be folded\n#   D is the domain of f\n#   r is the random number for random linear combination\n# Outputs:\n#   The first return value is the folded polynomial\n#   The second return value is the new domain\ndef fold(f, D, r, debug=False):\n    assert len(f) == len(D), \"len(f) != len(D), {} != {}, f={}, D={}\".format(len(f), len(D), f, D)\n\n    # divide\n    N = len(f)\n    # left is the first half of f, of x from 1 to g^(N/2)\n    left = f[:N//2]\n    # right is the second half of f, of x from g^(N-1) to g^(N/2), which corresponds to minus x in left\n    right = f[:N//2-1:-1]\n    assert len(left) == len(right), \"len(left) != len(right), {} != {}, left={}, right={}\".format(len(left), len(right), left, right)\n\n    for i, x in enumerate(D[:N//2]):\n        # f == f0 + x * f1\n        f0 = (left[i] + right[i]) / C31(2)\n        f1 = (left[i] - right[i]) / (C31(2) * x)\n        # f[:N//2] stores the folded polynomial\n        if debug: print(f\"f[{i}] = {f[i]} = ({left[i]} + {right[i]})/2 + {r} * ({left[i]} - {right[i]})/(2 * {x})\")\n        f[i] = f0 + r * f1\n        # if debug: print(f\"{f[i]} = ({left[i]} + {right[i]})/2 + {r} * ({left[i]} - {right[i]})/(2 * {x})\")\n        # reuse f[N//2:] to store new domain\n        f[N//2 + i] = 2 * x^2 - 1\n\n    # return the folded polynomial and the new domain\n    return f[:N//2], f[N//2:]\n\nfrom utils import log_2\nfrom merlin.merlin_transcript import MerlinTranscript\nfrom merkle import MerkleTree\n\ndef fri(f, D, degree, query_num, transcript, debug=False):\n\n    assert len(f) == len(D), \"len(f) != len(D), {} != {}, f={}, D={}\".format(len(f), len(D), f, D)\n    assert isinstance(transcript, MerlinTranscript)\n\n    if debug: print(f\"f={f}, D={D}\")\n\n    # first tree\n    first_tree = MerkleTree(f)\n    first_oracle = f\n\n    # first random number\n    transcript.append_message(b'first_tree', bytes(first_tree.root, 'ascii'))\n    r = int.from_bytes(transcript.challenge_bytes(b'r', int(4)), 'big')\n\n    # first step (J mapping)\n    # for f in natural order, we just divide f into 2 parts from the middle\n    N = len(f)\n    assert N % 2 == 0, \"N must be even, N={}\".format(N)\n\n    left = f[:N//2]\n    right = f[:N//2-1:-1]\n    assert len(left) == len(right), \"len(left) != len(right), {} != {}, left={}, right={}\".format(len(left), len(right), left, right)\n    f = [None for _ in range(N//2)]\n    for i, (_, y) in enumerate(D[:N//2]):\n        f0 = (left[i] + right[i]) / C31(2)\n        f1 = (left[i] - right[i]) / (C31(2) * y)\n        f[i] = f0 + r * f1\n        if debug: print(f\"f[{i}] = {f[i]} = ({left[i]} + {right[i]})/2 + {r} * ({left[i]} - {right[i]})/(2 * {y})\")\n\n    print(f\"f={f}\")\n\n    D = [x for x, _ in D[:N//2]]\n\n    # fold\n    trees = [MerkleTree(f)]\n    oracles = [f[:]]\n    # log_2(degree) - 1 because we have already done the first step\n    for i in range(log_2(degree) - 1):\n        # random number\n        transcript.append_message(b'tree', bytes(trees[-1].root, 'ascii'))\n\n        r = int.from_bytes(transcript.challenge_bytes(b'r', int(4)), 'big')\n        f, D = fold(f, D, r, debug)\n\n        if debug: print(f\"f={f}, D={D}\")\n\n        # merkle tree\n        trees.append(MerkleTree(f))\n        oracles.append(f[:])\n\n    # query paths\n    max_size = N\n    get_q = lambda transcript: int.from_bytes(transcript.challenge_bytes(b'query', int(4)), 'big') % max_size\n    queries = [get_q(transcript) for _ in range(query_num)]\n\n    query_paths = []\n    for q in queries:\n        max_size = N\n        cur_path = []\n        indices = []\n        x0 = q\n        x1 = max_size - q - 1\n        if x1 < x0:\n            x0, x1 = x1, x0\n\n        if debug: print(f\"max_size={max_size}\")\n        if debug: print(f\"q={q}\")\n        if debug: print(f\"x0={x0}, x1={x1}\")\n        \n        cur_path.append((first_oracle[x0], first_oracle[x1]))\n        indices.append(x0)\n        q = x0\n        max_size >>= 1\n\n        for oracle in oracles:\n            x0 = q\n            x1 = max_size - q - 1\n            if x1 < x0:\n                x0, x1 = x1, x0\n\n            if debug: print(f\"q={q}\")\n            if debug: print(f\"x0={x0}, x1={x1}\")\n            \n            cur_path.append((oracle[x0], oracle[x1]))\n            indices.append(x0)\n            q = x0\n            max_size >>= 1\n\n        query_paths.append((cur_path, indices))\n\n    # merkle paths\n    merkle_paths = []\n    for _, indices in query_paths:\n        cur_query_paths = []\n        for i, idx in enumerate(indices):\n            if i == 0:\n                if debug: print(f\"i = {i}, idx = {idx}, first_oracle[idx] = {first_oracle[idx]}\")\n                cur_query_paths.append(first_tree.get_authentication_path(idx))\n            else:\n                cur_tree = trees[i - 1]\n                if debug: print(f\"i = {i}, idx = {idx}, oracles[i-1][idx] = {oracles[i-1][idx]}\")\n                cur_query_paths.append(cur_tree.get_authentication_path(idx))\n        merkle_paths.append(cur_query_paths)\n\n    return {\n        'query_paths': query_paths,\n        'merkle_paths': merkle_paths,\n        'first_tree': first_tree.root,\n        'intermediate_trees': [tree.root for tree in trees],\n        'degree_bound': degree,\n        'final_value': f[0],\n    }\n\nfrom merkle import verify_decommitment\n\ndef defri(fri_proof, degree_bound, T, query_num, transcript, debug=False):\n\n    assert degree_bound >= fri_proof['degree_bound']\n    degree_bound = fri_proof['degree_bound']\n\n    assert isinstance(transcript, MerlinTranscript)\n\n    first_tree = fri_proof['first_tree']\n    intermediate_trees = fri_proof['intermediate_trees']\n\n    transcript.append_message(b'first_tree', bytes(first_tree, 'ascii'))\n    r = int.from_bytes(transcript.challenge_bytes(b'r', int(4)), 'big')\n\n    fold_challenges = [r]\n    for i in range(log_2(int(degree_bound)) - 1):\n        transcript.append_message(b'tree', bytes(intermediate_trees[i], 'ascii'))\n        r = int.from_bytes(transcript.challenge_bytes(b'r', int(4)), 'big')\n        fold_challenges.append(r)\n\n    get_q = lambda transcript: int.from_bytes(transcript.challenge_bytes(b'query', int(4)), 'big') % degree_bound\n    queries = [get_q(transcript) for _ in range(query_num)]\n\n    for q, (cur_path, _), mps in zip(queries, fri_proof['query_paths'], fri_proof['merkle_paths']):\n        num_vars = degree_bound\n        for i, mp in enumerate(mps):\n\n            assert num_vars > 0, \"num_vars must be positive, num_vars={}\".format(num_vars)\n\n            x0 = q\n            x1 = num_vars - q - 1\n            if x1 < x0:\n                x0, x1 = x1, x0\n            \n            if debug: print(f\"num_vars={num_vars}\")\n            if debug: print(f\"q={q}\")\n            if debug: print(f\"x0={x0}, x1={x1}\")\n\n            code_left, code_right = F31(cur_path[i][0]), F31(cur_path[i][1])\n\n            table = T[i]\n            if debug: print(f\"i = {i}, table={table}\")\n            if i != len(mps) - 1:\n                alpha = fold_challenges[i]\n                f_code_folded = cur_path[i + 1][0 if x0 < num_vars / 4 else 1]\n                assert f_code_folded == (code_left + code_right)/2 + alpha * (code_left - code_right)/(2*table[x0]), f\"{f_code_folded} != ({code_left} + {code_right})/2 + {alpha} * ({code_left} - {code_right})/(2*{table[x0]})\"\n            else:\n                assert fri_proof['final_value'] == code_left, f\"{fri_proof['final_value']} != {code_left}\"\n\n            if i == 0:\n                assert verify_decommitment(x0, code_left, mp, fri_proof['first_tree']), f\"verify_decommitment(x0={x0}, code_left={code_left}, mp={mp}, fri_proof['first_tree']={fri_proof['first_tree']})\"\n            else:\n                assert verify_decommitment(x0, code_left, mp, fri_proof['intermediate_trees'][i - 1]), f\"verify_decommitment(x0={x0}, code_left={code_left}, mp={mp}, fri_proof['intermediate_trees'][i - 1]={fri_proof['intermediate_trees'][i - 1]})\"\n\n            num_vars >>= 1\n            q = x0\n\nf = [randint(0, 30) for _ in range(len(D_standard_coset_4))]\nquery_num = 4\nfri_proof = fri(f, D_standard_coset_4, len(D_standard_coset_4), query_num, MerlinTranscript(b'FRI'), debug=True)\nprint(\"---------------- fri ----------------\")\nT = []\ndomain = D_standard_coset_4[:]\nfor i in range(log_2(len(domain)) + 1):\n    if i != 0:\n        T.append([x for x, _ in domain])\n        domain = sq(domain)[:len(domain)//2]\n    else:\n        T.append([y for _, y in domain])\n        domain = domain[:len(domain)//2]\n\ndefri(fri_proof, len(D_standard_coset_4), T, query_num, MerlinTranscript(b'FRI'), debug=True)","type":"content","url":"/src/circle#circle-fri","position":13},{"hierarchy":{"lvl1":"Commit phase"},"type":"lvl1","url":"/src/deepfold","position":0},{"hierarchy":{"lvl1":"Commit phase"},"content":"F193 = GF(64*3+1)\nTWO_ADICITY = 6\nORDER = 64*3+1\nMULTIPLICATIVE_GENERATOR = F193.primitive_element()\nROOT_OF_UNITY = F193(MULTIPLICATIVE_GENERATOR**3)\nF193.primitive_element()\nR193.<X, A, B, X0, X1, X2, X3, X4, X5, A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15> = PolynomialRing(F193)\n\nfrom typing import Union, Optional, TypeVar\nfrom random import randint, Random\nfrom utils import prime_field_inv, is_power_of_two, log_2, next_power_of_two\n\nclass Fp:\n    field_modulus = ORDER\n\n    ROOT_OF_UNITY = ROOT_OF_UNITY\n    MULTIPLICATIVE_GENERATOR = MULTIPLICATIVE_GENERATOR\n    TWO_ADICITY = TWO_ADICITY\n\n    def __init__(self, val: Integer | int) -> None:\n        if self.field_modulus is None:\n            raise AttributeError(\"Field Modulus hasn't been specified\")\n\n        if isinstance(val, Fp):\n            self.n = val.n % self.field_modulus\n        elif isinstance(val, Integer):\n            self.n = val % self.field_modulus\n        elif Integer(val) == Integer(val):\n            self.n = Integer(val) % self.field_modulus\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(val))\n            )\n\n    def __add__(self, other: Union[Integer, \"Fp\"]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)((self.n + on) % self.field_modulus)\n\n    def __mul__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)((self.n * on) % self.field_modulus)\n\n    def __rmul__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        return self * other\n\n    def __radd__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        return self + other\n\n    def __rsub__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)((on - self.n) % self.field_modulus)\n\n    def __sub__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)((self.n - on) % self.field_modulus)\n\n    def __div__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)(\n            self.n * prime_field_inv(on, self.field_modulus) % self.field_modulus\n        )\n\n    def __truediv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        return self.__div__(other)\n\n    def __rdiv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        if isinstance(other, Fp):\n            on = other.n\n        elif isinstance(other, Integer):\n            on = other\n        elif Integer(other) == Integer(other):\n            on = Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n        return type(self)(\n            prime_field_inv(self.n, self.field_modulus) * on % self.field_modulus\n        )\n\n    def __rtruediv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n        return self.__rdiv__(other)\n\n    # def __pow__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n    #     if other == 0:\n    #         return type(self)(1)\n    #     elif other == 1:\n    #         return type(self)(self.n)\n    #     elif other % 2 == 0:\n    #         return (self * self) ** (other // 2)\n    #     else:\n    #         return ((self * self) ** (other // 2)) * self\n\n    def __eq__(self, other: Union[\"Fp\", Integer]) -> bool:\n        if isinstance(other, Fp):\n            return self.n == other.n\n        elif isinstance(other, Integer):\n            return self.n == other\n        elif Integer(other) == Integer(other):\n            return self.n == Integer(other)\n        else:\n            raise TypeError(\n                \"Expected an int or Fp object, but got object of type {}\"\n                .format(type(other))\n            )\n\n    def __ne__(self, other: Union[\"Fp\", Integer]) -> bool:\n        return not self == other\n\n    def __neg__(self) -> \"Fp\":\n        return type(self)(-self.n)\n\n    def __str__(self) -> str:\n        return self.repr()\n        \n    def __repr__(self) -> str:\n        return self.repr()\n    \n    # Override the default (inefficient) __pow__ function in py_ecc.fields.field_elements.FQ\n    def __pow__(self: \"Fp\", other: int) -> \"Fp\":\n        return type(self)(pow(self.n, other, self.field_modulus))\n    \n    # def __repr__(self) -> str:\n    #     return repr(self.n)\n\n    def __int__(self) -> int:\n        return self.n\n\n    @classmethod\n    def one(cls) -> \"Fp\":\n        return cls(1)\n\n    @classmethod\n    def zero(cls) -> \"Fp\":\n        return cls(0)\n    \n    @classmethod\n    def neg_one(cls) -> \"Fp\":\n        return cls(cls.field_modulus - 1)\n\n    @classmethod\n    def rand(cls, rndg: Optional[Random] = None) -> \"Fp\":\n        if rndg is None:\n            return cls(randint(1, cls.field_modulus - 1))\n        return cls(rndg.randint(1, cls.field_modulus - 1))\n    \n    @classmethod\n    def random(cls) -> \"Fp\":\n        return cls.rand()\n    \n    @classmethod\n    def rands(cls, rndg: Random, n: int) -> list[\"Fp\"]:\n        return [cls(rndg.randint(1, cls.field_modulus - 1)) for _ in range(n)]\n    \n    @classmethod\n    def from_bytes(cls, b: bytes) -> \"Fp\":\n        i = int.from_bytes(b, \"big\")\n        return cls(i)\n    \n    def inv(self) -> \"Fp\":\n        return Fp(prime_field_inv(self.n, self.field_modulus))\n    \n    def repr(self) -> str:\n        k = self.field_modulus // 2\n        if self.n < k:\n            return f\"{self.n}\"\n        else:\n            return f\"-{self.field_modulus - self.n}\"\n        \n    def exp(self: \"Fp\", other: int) -> \"Fp\":\n        return type(self)(pow(self.n, other, self.field_modulus))\n    \n    @classmethod\n    def compute_root_of_unity(cls) -> \"Fp\":\n        return cls(pow(cls.MULTIPLICATIVE_GENERATOR, ((cls.field_modulus - 1) // 2 ** cls.TWO_ADICITY), cls.field_modulus))\n    \n    @classmethod\n    def root_of_unity(cls) -> \"Fp\":\n        return cls(cls.ROOT_OF_UNITY)\n    \n    @classmethod\n    def multiplicative_generator(cls) -> \"Fp\":\n        return cls(cls.MULTIPLICATIVE_GENERATOR)\n\n    @classmethod\n    def nth_root_of_unity(cls, n: int) -> \"Fp\":\n        assert is_power_of_two(n), \"n must be a power of two\"\n        return cls(pow(cls.ROOT_OF_UNITY, 2**(cls.TWO_ADICITY - log_2(n)), cls.field_modulus))\n\nfrom unipoly2 import UniPolynomial, UniPolynomialWithFft, bit_reverse_permutation\nfrom mle2 import MLEPolynomial\nUniPolynomialWithFft.set_field_type(Fp)\nMLEPolynomial.set_field_type(Fp)\nfrom merkle import MerkleTree\n\nf_mle = MLEPolynomial([Fp(1), Fp(3), Fp(2), Fp(1),\n                    Fp(2), Fp(-2), Fp(1), Fp(2)], 3)\nf_cm = MerkleTree(f_mle.evals)\nus = [Fp(2), Fp(-1), Fp(2)]\nv = f_mle.evaluate(us)\nv, f_cm.root\n\ndef rs_encode(f: list[Field], coset: Field, blowup_factor: int) -> list[Field]:\n    n = next_power_of_two(len(f))\n    N = n * blowup_factor\n\n    omega_Nth = Fp.nth_root_of_unity(N)\n    # print(f\"omega_Nth = {omega_Nth}\")\n    k = log_2(N)\n    # print(f\"n = {n}, N = {N}, k = {k}\")\n    vec = f + [Fp.zero()] * (N - len(f))\n    # print(f\"vec = {vec}, len(vec) = {len(vec)}\")\n    return UniPolynomialWithFft.fft_coset_rbo(vec, coset, k, omega=omega_Nth)\n     \n\nf = f_mle.evals\nalpha = Fp(2)\nf_len = len(f)\ng = Fp.multiplicative_generator()\nblowup_factor = 2\nf_code_len = f_len * blowup_factor\nprint(f\"f_code_len = {f_code_len}, g={g}, blowup_factor={blowup_factor}\")\n\n","type":"content","url":"/src/deepfold","position":1},{"hierarchy":{"lvl1":"Commit phase"},"type":"lvl1","url":"/src/deepfold#commit-phase","position":2},{"hierarchy":{"lvl1":"Commit phase"},"content":"\n\ndebug = 2\ntwiddles = UniPolynomialWithFft.precompute_twiddles_for_fft(f_code_len, is_bit_reversed=True)\ncoset = Fp.multiplicative_generator()\n\nf = f_mle.evals.copy()\n\n# Sumcheck Round 0\n\neq = MLEPolynomial.eqs_over_hypercube(us)\nn = len(f)\nhalf = n >> 1\nsum0 = v\nv, f, eq\n\nalpha = Fp(31)\n\ndef compute_alpha_powers(alpha: Fp, n: int) -> list[Fp]:\n    return [alpha**(2**i) for i in range(n)]\nalpha_powers = compute_alpha_powers(alpha, 3)\nalpha_powers\n\ndef fold(f: list[Fp], r: Fp) -> list[Fp]:\n    n = len(f)\n    assert n % 2 == 0, f\"n = {n}, n must be even\"\n    half = n >> 1\n    f_even = f[::2]\n    f_odd = f[1::2]\n    return [f_even[i] + r * (f_odd[i] - f_even[i]) for i in range(half)]\nfold([A0, A1, A2, A3, A4, A5, A6, A7], X0)\n\ndef expanded_partial_evaluate(f: list[Fp], us: list[Fp]) -> list[Fp]:\n    \"\"\"\n    Evaluate mle polynomial *partially* from x_{n-1}, x_{n-2}, ..., x_{n-k}\n    \"\"\"\n    k, n = len(us), len(f)\n    assert n == 2**k, f\"n = {n}, k = {k}\"\n\n    rs, e = [], f.copy()\n    half = n >> 1\n    for i in range(k):\n        e_low, e_high = e[:half], e[half:]\n        e = [e_low[j] + us[k-i-1] * (e_high[j] - e_low[j]) for j in range(half)]\n        rs += e\n        half >>= 1\n    return rs\n\nexpanded_partial_evaluate([A0, A1, A2, A3], [X0, X1])\n\nexpanded_partial_evaluate([Fp(1), Fp(2), Fp(3), Fp(4)], [Fp(3), Fp(2)])\n\nMLEPolynomial([Fp(1), Fp(2), Fp(3), Fp(4)], 2).evaluate([Fp(3), Fp(2)])\n\nexpanded_partial_evaluate([Fp(1), Fp(2), Fp(3), Fp(4), Fp(5), Fp(6), Fp(7), Fp(8)], [Fp(4), Fp(2), Fp(3)])\n\nMLEPolynomial([Fp(1), Fp(2), Fp(3), Fp(4), Fp(5), Fp(6), Fp(7), Fp(8)], 3).evaluate([Fp(4), Fp(2), Fp(3)])\n\nf_at_alpha = f_mle.evaluate(alpha_powers)\nf_at_alpha\n\n\n# FRI Round 0\n\nf0_code = rs_encode(f, coset, 2)\nif debug > 1:\n    print(f\"P> check f0_code\")\n    f_orig_evals = bit_reverse_permutation(f0_code)\n    f_orig_coeffs = UniPolynomialWithFft.ifft_coset(f_orig_evals, coset, log_2(f_code_len))\n    f_orig = UniPolynomial(f_orig_coeffs)\n    assert f_orig.coeffs == f, f\"f_orig != f0, f_orig = {f_orig.coeffs}, f = {f}\"\n    print(f\"P> check f0_code passed\")\n\nalpha0 = Fp(32)\nalpha0_powers = compute_alpha_powers(alpha0, 3)\nalpha0_powers\n\n# Sumcheck Round 1\n\nf0_even = f[::2]\nf0_odd = f[1::2]\nf0_even_mle = MLEPolynomial(f0_even, f_mle.num_var-1)\nf0_odd_mle = MLEPolynomial(f0_odd, f_mle.num_var-1)\n\n# construct hz(X): hz(X) = f(X, u1, u2,...)\nhz_at_0 = f0_even_mle.evaluate(us[1:])\nhz_at_1 = f0_odd_mle.evaluate(us[1:])\nhz = [hz_at_0, hz_at_1]\n\nassert (UniPolynomial.evaluate_from_evals(hz, us[0], [Fp(0), Fp(1)])) == v, \\\n    f\"hz(us[0]) = {UniPolynomial.evaluate_from_evals(hz, us[0])}, v = {v}\"\n\n# alpha0 = Fp.rand()\nr0 = Fp(-30)\nr0\n\n# Sumcheck fold \n\nf1 = [(Fp(1) - r0) * f0_even[i] + r0 * f0_odd[i] for i in range(half)]\n\n# compute the new sum = h(r0)\nhz_r0 = UniPolynomial.evaluate_from_evals(hz, r0, [Fp(0), Fp(1)])\n\nhalf >>= 1\n\n# Sumcheck Round 2\n\nf1_even = f1[::2]\nf1_odd = f1[1::2]\nf1_even_mle = MLEPolynomial(f1_even, f_mle.num_var-2)\nf1_odd_mle = MLEPolynomial(f1_odd, f_mle.num_var-2)\n\n# construct hz(X): hz(X) = f1(X, u1, u2,...)\nhz_at_0 = f1_even_mle.evaluate(us[2:])\nhz_at_1 = f1_odd_mle.evaluate(us[2:])\nhz = [hz_at_0, hz_at_1]\n\nassert (UniPolynomial.evaluate_from_evals(hz, us[1], [Fp(0), Fp(1)])) == hz_r0, \\\n    f\"hz(us[1]) = {UniPolynomial.evaluate_from_evals(hz, us[1])}, hz_r0 = {hz_r0}\"\n\n\n# alpha0 = Fp.rand()\nr1 = Fp(-32)\nr1\n\n# Sumcheck fold \n\nf2 = [(Fp(1) - r1) * f1_even[i] + r1 * f1_odd[i] for i in range(half)]\n\n# compute the new sum = h(r1)\nhz_r1 = UniPolynomial.evaluate_from_evals(hz, r1, [Fp(0), Fp(1)])\nhz_r1\n\nhalf >>= 1\n\n# Sumcheck Round 2\n\nf2_even = f2[::2]\nf2_odd = f2[1::2]\nf2_even_mle = MLEPolynomial(f2_even, f_mle.num_var-3)\nf2_odd_mle = MLEPolynomial(f2_odd, f_mle.num_var-3)\n\n# construct hz(X): hz(X) = f2(X) = f(r0, r1, X)\nhz_at_0 = f2_even_mle.evaluate(us[3:])\nhz_at_1 = f2_odd_mle.evaluate(us[3:])\nhz = [hz_at_0, hz_at_1]\n\nassert (UniPolynomial.evaluate_from_evals(hz, us[2], [Fp(0), Fp(1)])) == hz_r1, \\\n    f\"hz(us[2]) = {UniPolynomial.evaluate_from_evals(hz, us[2])}, hz_r1 = {hz_r1}\"\n\n# alpha0 = Fp.rand()\nr2 = Fp(-33)\nr2\n\n\n# Sumcheck fold \n\nf3 = [(Fp(1) - r2) * f2_even[i] + r2 * f2_odd[i] for i in range(half)]\n\n# compute the new sum = h(r2)\nhz_r2 = UniPolynomial.evaluate_from_evals(hz, r2, [Fp(0), Fp(1)])\nhz_r2\n\n# Check the final result: h(r0, r1, r2) = f(r0, r1, r2)\n\nhz_r2 == f_mle.evaluate([r0, r1,r2])\n\n","type":"content","url":"/src/deepfold#commit-phase","position":3},{"hierarchy":{"lvl1":"Simplified Sumcheck"},"type":"lvl1","url":"/src/deepfold#simplified-sumcheck","position":4},{"hierarchy":{"lvl1":"Simplified Sumcheck"},"content":"\n\nsum_checked = v\nhalf = n >> 1\nk = len(us)\n# f_code = f0_code\n# coset = self.coset_gen\nconstant = None\nr_vec = []\nsumcheck_h_vec = []\n# code_commitments = []\n# trees = [MerkleTree(f_code)]\n# codes = [f_code]\n\nf_partial_evals = expanded_partial_evaluate(f, us)\nf_partial_evals\n\n# Sumcheck Round 0\ni = 0 \n\nf_even = f[::2]\nf_odd = f[1::2]\nprint(f\"P> f = {f}\")\n\n# construct h_i(X) = f_i(r0,r1,...,X,u_{i+1},u_{i+2},...,u_k)\nh_at_0 = f_partial_evals[-3]\nh_at_1 = f_partial_evals[-2]\nh_at_u = f_partial_evals[-1]\nh_at_u_plus_1 = h_at_u + (h_at_1 - h_at_0)\n\nprint(f\"P> h_at_0 = {h_at_0}\")\nprint(f\"P> h_at_1 = {h_at_1}\")\nprint(f\"P> h_at_u = {h_at_u}\")\nprint(f\"P> h_plus_1_at_u = {h_at_u_plus_1}\")\n\nif debug > 0:\n    print(f\"P> check h(0), h(1), h(u), h(u+1)\")\n    partial_f_mle = MLEPolynomial(f, k-i)\n    assert h_at_0 == partial_f_mle.evaluate([Fp(0)]+us[i+1:]), \\\n        f\"h_at_0 = {h_at_0}, partial_f_mle.evaluate([Fp(0)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\"\n    assert h_at_1 == partial_f_mle.evaluate([Fp(1)]+us[i+1:]), \\\n        f\"h_at_1 = {h_at_1}, partial_f_mle.evaluate([Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(1)]+us[i+1:])}\"\n    assert h_at_u == partial_f_mle.evaluate(us[i:]), \\\n        f\"h_at_u = {h_at_u}, partial_f_mle.evaluate(us[i:]) = {partial_f_mle.evaluate(us[i:])}\"\n    assert h_at_u_plus_1 == partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]), \\\n        f\"h_plus_1_at_u = {h_at_u_plus_1}, partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\"\n    # print(f\"P> f(0) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\")\n    # print(f\"P> f(u) = {partial_f_mle.evaluate(us[i:])}\")\n    # print(f\"P> f(u+1) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\")\n    assert h_at_u == sum_checked, \\\n        f\"h_at_u = {h_at_u}, sum_checked = {sum_checked}\"\n    print(f\"P> check h_at_u passed, {h_at_u} = {sum_checked}\")\nsumcheck_h_vec.append(h_at_u)\n\n# tr.absorb(b\"h(X)\", h_eval_at_u_plus_1)\n\n# check sum\n# assert UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)]) == sum_checked, \\\n#     f\"h(us[{i}]) = {UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)])}, sum_checked = {sum_checked}\"\n\n# Receive a random number from the verifier\nr = Fp(-12)\n\nif debug > 0:\n    print(f\"P> r[{i}] = {r}\")\n\n\n# NOTE: The verifier compute f_i(r), which is *equal to* f_{i+1}(u_i). And then in \n#   the next round, the prover will only need to send f_{i+1}(u_i+1) to the verifier.\n#   The computation of f_{i+1}(r) at the verifier's side costs only one multiplication.\nh_at_r = h_at_u + (h_at_u_plus_1 - h_at_u) * (r - us[i])\n\nif debug > 0:\n    print(f\"P> check new sumcheck passed\")\n    assert h_at_r == MLEPolynomial(f, k-i).evaluate([r]+us[i+1:]), \\\n        f\"h_at_r = {h_at_r}, f(r) = {MLEPolynomial(f, k-i).evaluate([r]+us[i+1:])}\"\n    new_sum = UniPolynomial.evaluate_from_evals([h_at_u, h_at_u_plus_1], r, [us[i], us[i]+Fp(1)])\n    assert h_at_r == new_sum\n    print(f\"P> check new sumcheck passed, {h_at_r} = {new_sum}\")\n\n# fold f\n\n# f_folded = [(Fp(1) - r) * f_even[i] + r * f_odd[i] for i in range(half)]\nf_folded = fold(f, r)\n\nf_parital_evals_trimmed = f_partial_evals[:-1]\nf_parital_even = f_parital_evals_trimmed[::2]\nf_parital_odd = f_parital_evals_trimmed[1::2]\nprint(f\"P> f_parital_evals_trimmed = {f_parital_evals_trimmed}\")\nf_parital_evals_folded = fold(f_parital_evals_trimmed, r)\n\n\n# # update parameters for the next round\nsum_checked = h_at_r\nr_vec.append(r)\nf = f_folded\nf_partial_evals = f_parital_evals_folded\n# f_code = f_code_folded\nhalf >>= 1\ncoset *= coset\n\nf_partial_evals, half\n\n# Sumcheck Round 1\ni = 1\n\nf_even = f[::2]\nf_odd = f[1::2]\nprint(f\"P> f = {f}\")\n\n# construct h(X)\nh_eval_at_0 = f_partial_evals[-3]\nh_eval_at_1 = f_partial_evals[-2]\nh_eval_at_u = f_partial_evals[-1]\nh_eval_at_u_plus_1 = h_eval_at_u + (h_eval_at_1 - h_eval_at_0)\n\nprint(f\"P> h_eval_at_0 = {h_eval_at_0}\")\nprint(f\"P> h_eval_at_1 = {h_eval_at_1}\")\nprint(f\"P> h_eval_at_u = {h_eval_at_u}\")\nprint(f\"P> h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}\")\n\nif debug > 0:\n    print(f\"P> check h(0), h(1), h(u), h(u+1)\")\n    partial_f_mle = MLEPolynomial(f, k-i)\n    assert h_eval_at_0 == partial_f_mle.evaluate([Fp(0)]+us[i+1:]), \\\n        f\"h_eval_at_0 = {h_eval_at_0}, partial_f_mle.evaluate([Fp(0)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\"\n    assert h_eval_at_1 == partial_f_mle.evaluate([Fp(1)]+us[i+1:]), \\\n        f\"h_eval_at_1 = {h_eval_at_1}, partial_f_mle.evaluate([Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(1)]+us[i+1:])}\"\n    assert h_eval_at_u == partial_f_mle.evaluate(us[i:]), \\\n        f\"h_eval_at_u = {h_eval_at_u}, partial_f_mle.evaluate(us[i:]) = {partial_f_mle.evaluate(us[i:])}\"\n    assert h_eval_at_u_plus_1 == partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]), \\\n        f\"h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}, partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\"\n    # print(f\"P> f(0) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\")\n    # print(f\"P> f(u) = {partial_f_mle.evaluate(us[i:])}\")\n    # print(f\"P> f(u+1) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\")\n    assert h_eval_at_u == sum_checked, \\\n        f\"h_eval_at_u = {h_eval_at_u}, sum_checked = {sum_checked}\"\n    print(f\"P> check h_eval_at_u passed, {h_eval_at_u} = {sum_checked}\")\nsumcheck_h_vec.append(h_eval_at_u_plus_1)\n\n# tr.absorb(b\"h(X)\", h_eval_at_u_plus_1)\n\n# check sum\n# assert UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)]) == sum_checked, \\\n#     f\"h(us[{i}]) = {UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)])}, sum_checked = {sum_checked}\"\n\n# Receive a random number from the verifier\nr = Fp(-13)\n\nif debug > 0:\n    print(f\"P> r[{i}] = {r}\")\n\nh_eval_at_r = h_eval_at_u + (h_eval_at_u_plus_1 - h_eval_at_u) * (r - us[i])\n\nif debug > 0:\n    print(f\"P> check new sumcheck passed\")\n    assert h_eval_at_r == MLEPolynomial(f, k-i).evaluate([r]+us[i+1:]), \\\n        f\"h_eval_at_r = {h_eval_at_r}, f(r) = {MLEPolynomial(f, k-i).evaluate([r]+us[i+1:])}\"\n    new_sum = UniPolynomial.evaluate_from_evals([h_eval_at_u, h_eval_at_u_plus_1], r, [us[i], us[i]+Fp(1)])\n    assert h_eval_at_r == new_sum\n    print(f\"P> check new sumcheck passed, {h_eval_at_r} = {new_sum}\")\n\n# fold f\n\n# f_folded = [(Fp(1) - r) * f_even[i] + r * f_odd[i] for i in range(half)]\nf_folded = fold(f, r)\n\nf_parital_evals_trimmed = f_partial_evals[:-1]\nf_parital_even = f_parital_evals_trimmed[::2]\nf_parital_odd = f_parital_evals_trimmed[1::2]\nprint(f\"P> f_parital_evals_trimmed = {f_parital_evals_trimmed}\")\nf_parital_evals_folded = fold(f_parital_evals_trimmed, r)\nprint(f\"P> f_parital_evals_folded = {f_parital_evals_folded}\")\n\n\n# # update parameters for the next round\nsum_checked = h_eval_at_r\nr_vec.append(r)\nf = f_folded\nf_partial_evals = f_parital_evals_folded\n# f_code = f_code_folded\nhalf >>= 1\ncoset *= coset\n\n# Sumcheck Round 2 (final round)\ni = 2\n\nf_even = f[::2]\nf_odd = f[1::2]\nprint(f\"P> f = {f}\")\n\n# construct h(X)\nh_eval_at_0 = f_partial_evals[0]\nh_eval_at_1 = f_partial_evals[0]\nh_eval_at_u = f_partial_evals[0]\nh_eval_at_u_plus_1 = h_eval_at_u + (h_eval_at_1 - h_eval_at_0)\n\nprint(f\"P> h_eval_at_0 = {h_eval_at_0}\")\nprint(f\"P> h_eval_at_1 = {h_eval_at_1}\")\nprint(f\"P> h_eval_at_u = {h_eval_at_u}\")\nprint(f\"P> h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}\")\n\nif debug > 0:\n    print(f\"P> check h(0), h(1), h(u), h(u+1)\")\n    partial_f_mle = MLEPolynomial(f, k-i)\n    assert h_eval_at_0 == partial_f_mle.evaluate([Fp(0)]+us[i+1:]), \\\n        f\"h_eval_at_0 = {h_eval_at_0}, partial_f_mle.evaluate([Fp(0)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\"\n    assert h_eval_at_1 == partial_f_mle.evaluate([Fp(1)]+us[i+1:]), \\\n        f\"h_eval_at_1 = {h_eval_at_1}, partial_f_mle.evaluate([Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(1)]+us[i+1:])}\"\n    assert h_eval_at_u == partial_f_mle.evaluate(us[i:]), \\\n        f\"h_eval_at_u = {h_eval_at_u}, partial_f_mle.evaluate(us[i:]) = {partial_f_mle.evaluate(us[i:])}\"\n    assert h_eval_at_u_plus_1 == partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]), \\\n        f\"h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}, partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\"\n    # print(f\"P> f(0) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\")\n    # print(f\"P> f(u) = {partial_f_mle.evaluate(us[i:])}\")\n    # print(f\"P> f(u+1) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\")\n    assert h_eval_at_u == sum_checked, \\\n        f\"h_eval_at_u = {h_eval_at_u}, sum_checked = {sum_checked}\"\n    print(f\"P> check h_eval_at_u passed, {h_eval_at_u} = {sum_checked}\")\nsumcheck_h_vec.append(h_eval_at_u_plus_1)\n\n# tr.absorb(b\"h(X)\", h_eval_at_u_plus_1)\n\n# check sum\n# assert UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)]) == sum_checked, \\\n#     f\"h(us[{i}]) = {UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)])}, sum_checked = {sum_checked}\"\n\n# Receive a random number from the verifier\nr = Fp(-13)\n\nif debug > 0:\n    print(f\"P> r[{i}] = {r}\")\n\n# NOTE: The verifier compute f_i(r), which is *equal to* f_{i+1}(u_i). And then in \n#   the next round, the prover will only need to send f_{i+1}(u_i+1) to the verifier.\nh_eval_at_r = h_eval_at_u + (h_eval_at_u_plus_1 - h_eval_at_u) * (r - us[i])\n\nif debug > 0:\n    print(f\"P> check new sumcheck passed\")\n    assert h_eval_at_r == MLEPolynomial(f, k-i).evaluate([r]+us[i+1:]), \\\n        f\"h_eval_at_r = {h_eval_at_r}, f(r) = {MLEPolynomial(f, k-i).evaluate([r]+us[i+1:])}\"\n    new_sum = UniPolynomial.evaluate_from_evals([h_eval_at_u, h_eval_at_u_plus_1], r, [us[i], us[i]+Fp(1)])\n    assert h_eval_at_r == new_sum\n    print(f\"P> check new sumcheck passed, {h_eval_at_r} = {new_sum}\")\n\n# fold f\n\n# f_folded = [(Fp(1) - r) * f_even[i] + r * f_odd[i] for i in range(half)]\nf_folded = fold(f, r)\n\nf_parital_evals_trimmed = f_partial_evals[:-1]\nf_parital_even = f_parital_evals_trimmed[::2]\nf_parital_odd = f_parital_evals_trimmed[1::2]\nprint(f\"P> f_parital_evals_trimmed = {f_parital_evals_trimmed}\")\nf_parital_evals_folded = fold(f_parital_evals_trimmed, r)\nprint(f\"P> f_parital_evals_folded = {f_parital_evals_folded}\")\n\n\n# # Sumcheck Round 1\n# i = 0 \n\n# f_even = f[::2]\n# f_odd = f[1::2]\n# print(f\"P> f = {f}\")\n\n# # construct h(X)\n# h_eval_at_0 = MLEPolynomial(f_even, k-i-1).evaluate(us[i+1:])\n# h_eval_at_1 = MLEPolynomial(f_odd, k-i-1).evaluate(us[i+1:])\n# h_eval_at_u = (Fp(1) - us[i]) * h_eval_at_0 + us[i] * h_eval_at_1\n# h_eval_at_u_plus_1 = h_eval_at_u + (h_eval_at_1 - h_eval_at_0)\n\n# print(f\"P> h_eval_at_0 = {h_eval_at_0}\")\n# print(f\"P> h_eval_at_1 = {h_eval_at_1}\")\n# print(f\"P> h_eval_at_u = {h_eval_at_u}\")\n# print(f\"P> h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}\")\n\n# if debug > 0:\n#     print(f\"P> check h(0), h(1), h(u), h(u+1)\")\n#     partial_f_mle = MLEPolynomial(f, k-i)\n#     assert h_eval_at_0 == partial_f_mle.evaluate([Fp(0)]+us[i+1:]), \\\n#         f\"h_eval_at_0 = {h_eval_at_0}, partial_f_mle.evaluate([Fp(0)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\"\n#     assert h_eval_at_1 == partial_f_mle.evaluate([Fp(1)]+us[i+1:]), \\\n#         f\"h_eval_at_1 = {h_eval_at_1}, partial_f_mle.evaluate([Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([Fp(1)]+us[i+1:])}\"\n#     assert h_eval_at_u == partial_f_mle.evaluate(us[i:]), \\\n#         f\"h_eval_at_u = {h_eval_at_u}, partial_f_mle.evaluate(us[i:]) = {partial_f_mle.evaluate(us[i:])}\"\n#     assert h_eval_at_u_plus_1 == partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]), \\\n#         f\"h_eval_at_u_plus_1 = {h_eval_at_u_plus_1}, partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:]) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\"\n#     # print(f\"P> f(0) = {partial_f_mle.evaluate([Fp(0)]+us[i+1:])}\")\n#     # print(f\"P> f(u) = {partial_f_mle.evaluate(us[i:])}\")\n#     # print(f\"P> f(u+1) = {partial_f_mle.evaluate([us[i]+Fp(1)]+us[i+1:])}\")\n#     assert h_eval_at_u == sum_checked, \\\n#         f\"h_eval_at_u = {h_eval_at_u}, sum_checked = {sum_checked}\"\n#     print(f\"P> check h_eval_at_u passed, {h_eval_at_u} = {sum_checked}\")\n# sumcheck_h_vec.append(h_eval_at_u_plus_1)\n\n# # tr.absorb(b\"h(X)\", h_eval_at_u_plus_1)\n\n# # check sum\n# # assert UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)]) == sum_checked, \\\n# #     f\"h(us[{i}]) = {UniPolynomial.evaluate_from_evals(h, us[i], [Field(0), Field(1)])}, sum_checked = {sum_checked}\"\n\n# # Receive a random number from the verifier\n# r = Fp(-12)\n\n# if debug > 0:\n#     print(f\"P> r[{i}] = {r}\")\n\n# h_eval_at_r = h_eval_at_u + (h_eval_at_u_plus_1 - h_eval_at_u) * (r - us[i])\n\n# if debug > 0:\n#     print(f\"P> check new sumcheck passed\")\n#     assert h_eval_at_r == MLEPolynomial(f, k-i).evaluate([r]+us[i+1:]), \\\n#         f\"h_eval_at_r = {h_eval_at_r}, f(r) = {MLEPolynomial(f, k-i).evaluate([r]+us[i+1:])}\"\n#     new_sum = UniPolynomial.evaluate_from_evals([h_eval_at_u, h_eval_at_u_plus_1], r, [us[i], us[i]+Fp(1)])\n#     assert h_eval_at_r == new_sum\n#     print(f\"P> check new sumcheck passed, {h_eval_at_r} = {new_sum}\")\n\n# # fold f\n\n# f_folded = [(Fp(1) - r) * f_even[i] + r * f_odd[i] for i in range(half)]\n\n\n","type":"content","url":"/src/deepfold#simplified-sumcheck","position":5},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/src/kzg10","position":0},{"hierarchy":{"lvl1":""},"content":"from group import DummyGroup\nfrom utils import log_2, pow_2\nfrom unipolynomial import UniPolynomial\nfrom mle2 import MLEPolynomial\nfrom kzg10 import Commitment, KZG10Commitment\n\nF193.<a> = GF(193)\nR193.<X> = F193[]\nR193\nFp=F193\nFp2 = Fp.extension(2, 'b')\nFp, Fp2\n\nR_2.<X0, X1, X2, X3, X4, X5, X6, X7, X8, X9, Y0, Y1, Y2, Y3, Y4, Y5, Y6, Y7, A0, A1, A2, B0, B1, B2> = Fp[]; R_2\n\nG1 = DummyGroup(Fp)\nG2 = DummyGroup(Fp2)\n\nUniPolynomial.set_scalar(Fp(0), Fp)\nCommitment.set_scalar(Fp(0))\n\nf = UniPolynomial([2, 3, 2, 1])\nkzg10 = KZG10Commitment(G1, G2, 10)\nkzg10.setup(secret_symbol=X0, g1_generator=1, g2_generator=1)\nf_cm = kzg10.commit([2, 3, 2, 1])\nprint(f_cm)\n\nf.evaluate(1) == 8\n\nprf = kzg10.prove_eval_and_degree(f_cm, f.coeffs, 1, 8, 4)\n\nkzg10.verify_eval_and_degree(f_cm, prf, 1, 8, 4)","type":"content","url":"/src/kzg10","position":1},{"hierarchy":{"lvl1":"Ligerito protocol"},"type":"lvl1","url":"/src/ligerito","position":0},{"hierarchy":{"lvl1":"Ligerito protocol"},"content":"F193.<a> = GF(193) # 193 = 64 * 3 + 1\nR193.<X> = F193[]\nR193\nFp=F193\n\nfrom utils import log_2, next_power_of_two, is_power_of_two, bits_le_with_width, inner_product, reverse_bits\n\ndef nth_root_of_unity(n):\n    k = log_2(n)\n    assert k <= 6, \"k is greater than 6\"\n    \n    return Fp(5)**(3*2**(6-k))\n\nFp.nth_root_of_unity = nth_root_of_unity\nFp.TWO_ADICITY = 6\nFp.MULTIPLICATIVE_GENERATOR = Fp.primitive_element()\nFp.ROOT_OF_UNITY = Fp(5)**3\n\ndef neg_one():\n    return Fp(-1)\nFp.neg_one = neg_one\n\nR_2.<X0, X1, X2, X3, X4, X5, X6, X7, X8, X9, Y0, Y1, Y2, Y3, Y4, Y5, Y6, Y7, A0, A1, A2, B0, B1, B2> = Fp[]; R_2\n\nfrom mle2 import MLEPolynomial\nfrom unipoly2 import UniPolynomial, UniPolynomialWithFft\nfrom transcript import MerlinTranscript\nfrom merkle import MerkleTree\n\nMLEPolynomial.set_field_type(Fp)\ntranscript = MerlinTranscript(\"test\")\nUniPolynomial.set_field_type(Fp)\nUniPolynomialWithFft.set_field_type(Fp)\n\n\ndef rs_encode(f: list[Fp], coset: Fp, blowup_factor: int) -> list[Fp]:\n    n = next_power_of_two(len(f))\n    N = n * blowup_factor\n\n    omega_Nth = Fp.nth_root_of_unity(N)\n    k = log_2(N)\n    vec = f + [Fp.zero()] * (N - len(f))\n    return UniPolynomialWithFft.fft_coset_rbo(vec, coset, k, omega=omega_Nth)\n\n\ndef rs_generator_matrix(g, k, c):\n    assert is_power_of_two(k), \"k: %d is not a power of two\" % k\n    assert is_power_of_two(c), \"c: %d is not a power of two\" % c\n\n    n = k * c\n    w = Fp.nth_root_of_unity(n)\n    G = []\n    for i in range(n):\n        G.append([w**(i*j) for j in range(k)])\n    return G\n\nG = rs_generator_matrix(Fp(1), 8, 2)\nG[1]\n\ndef matrix_mul_vec(G, m):\n    assert len(G[0]) == len(m), \"len(G[0]) != len(m)\"\n    return [inner_product(G[i], m, Fp(0)) for i in range(len(G))]\nmatrix_mul_vec(G, [1, 2, 3, 4, 5, 6, 7, 8])\n\ndef matrix_transpose(M):\n    if type(M[0]) == type([]):\n        return [[M[j][i] for j in range(len(M))] for i in range(len(M[0]))]\n    elif type(M) == type([Fp(0)]):\n        return [[M[i]] for i in range(len(M))]\nmatrix_transpose([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), matrix_transpose([1, 2, 3])\n\n\ndef matrix_mul_matrix(G, M):\n    m = len(G)\n    n = len(G[0])\n    k = len(M[0])\n    assert n == len(M), \"len(G[0]) != len(M[0])\"\n    A = []\n    for i in range(m):\n        A.append([inner_product(G[i], [M[l][j] for l in range(n)], Fp(0)) for j in range(k)])\n    return A\nmatrix_mul_matrix(G, matrix_transpose(G))\n\n# Test matrix_mul_matrix()\nmatrix_mul_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], [[1, 2, 3], [5, 6, 7], [9, 10, 11]])\n\n# Test matrix_mul_matrix()\nmatrix_mul_matrix([[X0, X1, X2], [X3, X4, X5]], [[Y0], [Y1], [Y2]])\n\n# Test rs_encode() and matrix_mul()\n\n# rs_encode([1, 2, 3, 4, 5, 6, 7, 8], Fp(1), 8, 2) == matrix_mul(G, [1, 2, 3, 4, 5, 6, 7, 8])\n\n","type":"content","url":"/src/ligerito","position":1},{"hierarchy":{"lvl1":"Ligerito protocol"},"type":"lvl1","url":"/src/ligerito#ligerito-protocol","position":2},{"hierarchy":{"lvl1":"Ligerito protocol"},"content":"blowup factor is 2\n\nnumber of queries is 2\n\ncoset generator is Fp(1)\n\nblowup_factor = 2\nnum_queries = 2\ncoset_gen = Fp(1)\n\n\n# An MLE polynomial example, with length 32\nevals_over_hypercube = [Fp(2), Fp(3), Fp(4), Fp(5), Fp(6), Fp(7), Fp(8), Fp(9), \\\n             Fp(10), Fp(11), Fp(12), Fp(13), Fp(14), Fp(15), Fp(16), Fp(17), \\\n             Fp(2), Fp(3), Fp(4), Fp(5), Fp(6), Fp(7), Fp(8), Fp(9), \\\n             Fp(10), Fp(11), Fp(12), Fp(13), Fp(14), Fp(15), Fp(16), Fp(17), \\\n            ]\n\npoint = [Fp(-1), Fp(2), Fp(1), Fp(2), Fp(2)]\n\nf_mle = MLEPolynomial(evals_over_hypercube, 5)\n\nevaluation = f_mle.evaluate(point)\nevaluation\n\n\ntranscript = MerlinTranscript(\"test\")\n\n# Commit to the MLE polynomial\n\nclass Commitment:\n\n    def __init__(self, tree: MerkleTree):\n        self.tree = tree\n        self.cm = tree.root\n        self.root = tree.root\n\n    def __repr__(self):\n        return f\"Commitment(len={len(self.tree.data)}, root={self.cm})\"\n    \ndef commit(f_mle: MLEPolynomial) -> Commitment:\n    evals = f_mle.evals\n    k = f_mle.num_var\n    k2 = k // 2\n    k1 = k - k2\n    n1 = 2**k1\n    n2 = 2**k2\n    c_mat = []\n    for i in range(n1):\n        c_row = [evals[i*n2+j] for j in range(n2)]\n        c_row_code = rs_encode(c_row, coset_gen, blowup_factor)\n        c_mat.append(c_row_code)\n\n    c_mat_T = matrix_transpose(c_mat)\n    print(f\"c_mat_T={c_mat_T}\")\n    cm_vec = [MerkleTree(c_mat_T[i]) for i in range(len(c_mat_T))]\n    # print(f\"cm_vec={[cm_vec[i].root for i in range(len(cm_vec))]})\")\n    return Commitment(MerkleTree([cm.root for cm in cm_vec]))\n\nf_cm = commit(f_mle)\nf_cm\n\n\n","type":"content","url":"/src/ligerito#ligerito-protocol","position":3},{"hierarchy":{"lvl1":"Ligerito protocol","lvl2":"Prove the evaluation argument"},"type":"lvl2","url":"/src/ligerito#prove-the-evaluation-argument","position":4},{"hierarchy":{"lvl1":"Ligerito protocol","lvl2":"Prove the evaluation argument"},"content":"\n\n[Fp(-3)*Fp(65), Fp(64)*Fp(65)]\n\n\n","type":"content","url":"/src/ligerito#prove-the-evaluation-argument","position":5},{"hierarchy":{"lvl1":"Misc"},"type":"lvl1","url":"/src/ligerito#misc","position":6},{"hierarchy":{"lvl1":"Misc"},"content":"\n\nblowup_factor = 2\n\ndef prove_eval(mle: MLEPolynomial, point: list[Fp], transcript: MerlinTranscript):\n    assert mle.num_var == len(point), \"mle.num_var != len(point)\"\n\n    evals = mle.evals\n    k = mle.num_var\n    k1_prime = 2\n    k1 = k - k1_prime\n\n    G = rs_generator_matrix(Fp(1), 2**k1, blowup_factor)\n    X = []\n    for i in range(2**k1):\n        X.append([evals[i+j*2**k1] for j in range(2**k1_prime)])\n\n    r_vec = [Fp.random_element() for _ in range(k1_prime)]\n\n    eq_r = MLEPolynomial.eqs_over_hypercube(r_vec)\n\n    X_code = matrix_mul_matrix(G, X)\n    \n    y = matrix_mul_vec(X, eq_r)\n\n    return X, X_code, y, r_vec\n\ndef verify_eval(X_code, y, point: list[Fp], r_vec: list[Fp],transcript: MerlinTranscript):\n\n    k = len(point)\n    k1_prime = 2\n    k1 = k - k1_prime\n\n    G = rs_generator_matrix(Fp(1), 2**k1, blowup_factor)\n\n    eq_r = MLEPolynomial.eqs_over_hypercube(r_vec)\n\n    lhs = matrix_mul_vec(X_code, eq_r)\n    rhs = matrix_mul_vec(G, y)\n    return lhs, rhs\n\n\ndef matrix_geometric(M):\n    return len(M), len(M[0])\n\nX, X_code, y, r_vec = prove_eval(MLEPolynomial([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 4), [1, 2, 3, 4], transcript)\n\nr_vec, matrix_geometric(X_code), matrix_geometric(X), y\n\nverify_eval(X_code, y, [1, 2, 3, 4], r_vec,transcript)\n\nX, X_code, y, r_vec  = prove_eval(MLEPolynomial([X1, X2, X3, X4, X5, X6, X7, X8], 3), [1, 2, 3], transcript)\n\nX, X_code, y, r_vec\n\nverify_eval(X_code, y, [1, 2, 3], r_vec, transcript)\n\n","type":"content","url":"/src/ligerito#misc","position":7},{"hierarchy":{"lvl1":"Section 5: Matrix-vector product with partial sumcheck"},"type":"lvl1","url":"/src/ligerito#section-5-matrix-vector-product-with-partial-sumcheck","position":8},{"hierarchy":{"lvl1":"Section 5: Matrix-vector product with partial sumcheck"},"content":"\n\nblowup_factor = 2\ndebug = 2\ndef prove_eval(mle: MLEPolynomial, point: list[Fp], transcript: MerlinTranscript):\n    assert mle.num_var == len(point), \"mle.num_var != len(point)\"\n\n    v = mle.evaluate(point)\n    print(f\"P> v = {v}\")\n    evals = mle.evals\n    k = mle.num_var\n    k1_prime = 2\n    k1 = k - k1_prime\n\n    G = rs_generator_matrix(Fp(1), 2**k1, blowup_factor)\n    X = []\n    for i in range(2**k1):\n        X.append([evals[i+j*2**k1] for j in range(2**k1_prime)])\n    if debug > 0:\n        print(f\"P> X = {X}\")\n\n    f = mle.evals\n    eq = MLEPolynomial.eqs_over_hypercube(point)\n    sumcheck_h_vec = []\n    sum_checked = v\n    r_vec = []\n    for i in range(k1_prime):\n        if debug > 0:\n            print(f\"P> sumcheck round {i}\")\n        half = len(f) // 2\n        f_low = f[:half]\n        f_high = f[half:]\n        eq_low = eq[:half]\n        eq_high = eq[half:]\n\n        h_eval_at_0 = sum([f_low[j] * eq_low[j] for j in range(half)], Fp(0))\n        h_eval_at_1 = sum([f_high[j] * eq_high[j] for j in range(half)], Fp(0))\n        h_eval_at_2 = sum([ (2 * f_high[j] - f_low[j]) * (2 * eq_high[j] - eq_low[j]) for j in range(half)], Fp(0))\n\n        h = [h_eval_at_0, h_eval_at_1, h_eval_at_2]\n        sumcheck_h_vec.append(h)\n\n        transcript.absorb(b\"h(X)\", h)\n\n        if debug > 0:\n            print(f\"P> h = {h}\")\n        \n        assert h_eval_at_0 + h_eval_at_1 == sum_checked, \\\n            f\"h_eval_at_0 + h_eval_at_1 = {h_eval_at_0 + h_eval_at_1}, sum_checked = {sum_checked}\"\n\n        r = Fp.random_element()\n\n        r_vec.append(r)\n\n        # fold f\n\n        f_folded = [(Fp(1) - r) * f_low[i] + r * f_high[i] for i in range(half)]\n        eq_folded = [(Fp(1) - r) * eq_low[i] + r * eq_high[i] for i in range(half)]\n\n        f = f_folded\n        eq = eq_folded\n\n        sum_checked = UniPolynomial.evaluate_from_evals(h, \n                r, [Fp(0), Fp(1), Fp(2)])\n\n    X_code = matrix_mul_matrix(G, X)\n\n    X_code_T = matrix_transpose(X_code)\n    \n    cm_vec = [MerkleTree(X_code_T[i]) for i in range(len(X_code_T))]\n\n    print(cm_vec)\n\n    eq_r = MLEPolynomial.eqs_over_hypercube(r_vec[::-1])\n\n    y = matrix_mul_vec(X, eq_r)\n    assert y == f_folded, f\"y != f_folded, y = {y}, f_folded = {f_folded}\"\n    \n\n    if debug > 1: \n        print(f\"P> check folded code\")\n        X_code_folded = matrix_mul_vec(X_code, eq_r)\n        print(X_code_folded)\n        assert X_code_folded == matrix_mul_vec(G, f_folded), f\"X_code_folded != X_code\"\n        print(f\"P> check folded code passed\")\n\n    return X, X_code, y, r_vec\n\ndef verify_eval(X_code, y, point: list[Fp], r_vec: list[Fp], transcript: MerlinTranscript):\n\n    k = len(point)\n    k1_prime = 2\n    k1 = k - k1_prime\n\n    G = rs_generator_matrix(Fp(1), 2**k1, blowup_factor)\n\n    eq_r = MLEPolynomial.eqs_over_hypercube(r_vec)\n\n    lhs = matrix_mul_vec(X_code, eq_r)\n    rhs = matrix_mul_vec(G, y)\n    return lhs, rhs\n\n\n\nX, X_code, y, r_vec  = prove_eval(MLEPolynomial([X1, X2, X3, X4, X5, X6, X7, X8], 3), [1, 2, 3], transcript)\n\ndef reverse_bits(n: int, bit_length: int) -> int:\n    \"\"\"\n    Reverse the bits of an integer.\n\n    Args:\n        n (int): The input integer.\n        bit_length (int): The number of bits to consider.\n\n    Returns:\n        int: The integer with its bits reversed.\n    \"\"\"\n    result = 0\n    for i in range(bit_length):\n        result = (result << 1) | (n & 1)\n        n >>= 1\n    return result\n\nreverse_bits(4, 3)\n\ndef tensor_vector(index: int, k1: int, blowup_factor: int):\n    N = 2**k1 * blowup_factor\n    omega_Nth = Fp.nth_root_of_unity(N)\n    index_rev = reverse_bits(index, log_2(N))\n    omega = omega_Nth**index_rev\n    print(f\"omega = {omega}\")\n    vec = [omega**(2**i) for i in range(k1)]\n    print(f\"vec = {vec}\")\n    return vec\n\ndef rs_generator_matrix_at_row(index:int, f_len: int, coset:Fp, blowup_factor:int) -> list[Fp]:\n    n = next_power_of_two(f_len)\n    N = n * blowup_factor\n    omega_Nth = Fp.nth_root_of_unity(N)\n    index_rev = reverse_bits(index, log_2(N))\n    omega = omega_Nth**index_rev\n    return [coset* omega**i for i in range(n)]\n\ntensor_vector(1, 3, 2)\n\nomega_Nth = Fp.nth_root_of_unity(16)\nindex_rev = reverse_bits(1, 4)\nomega = omega_Nth**index_rev\nindex_rev, omega\n\nomega, omega**2, omega**4\n\n[63 * Fp(93), -2 * Fp(93)]\n\neq0 = [Fp(-83), Fp(28)]\nrs_w = [Fp(1), Fp(-81)]\nbeta = Fp(93)\neq = [eq0[j] + beta * rs_w[j] for j in range(len(eq0))]\neq\n\n\npoint = [Fp(3), Fp(2), Fp(2)]\nk_last = 1\nr_vec_all = [Fp(76), Fp(70)]\n\nscalar = MLEPolynomial.evaluate_eq_polynomial(point[k_last:], r_vec_all)\neq_0 = MLEPolynomial.eqs_over_hypercube(point[:k_last])\neq_0 = [eq_0[j] * scalar for j in range(2**k_last)]\neq_0\n\nq=7\nk1_prime = 1\nk1 = 2\n\nrs_generator_matrix_at_row(q, 2**k1, Fp(1), 4)\n\nr_vec = r_vec_all[:k1-k_last]\nr_vec\n\n\ndef tensor_vector_with_monomial_basis(vec: list[Fp]) -> list[Fp]:\n    if len(vec) == 0:\n        return [Fp(1)]\n    v = vec[-1]\n    vec_expanded = tensor_vector_with_monomial_basis(vec[:-1])\n    vec_right = [vec_expanded[i] * v for i in range(len(vec_expanded))]\n    return vec_expanded + vec_right\ntensor_vector_with_monomial_basis([X0, X1, X2])\n\ndef tensor_vector_with_multilinear_basis(vec: list[Fp]) -> list[Fp]:\n    if len(vec) == 0:\n        return [Fp(1)]\n    v = vec[-1]\n    vec_expanded = tensor_vector_with_multilinear_basis(vec[:-1])\n    vec_left = [vec_expanded[i] * (Fp(1)-v) for i in range(len(vec_expanded))]\n    vec_right = [vec_expanded[i] * v for i in range(len(vec_expanded))]\n    return vec_left + vec_right\ntensor_vector_with_multilinear_basis([X0, X1, X2])\n\nMLEPolynomial.eqs_over_hypercube([X0, X1, X2])\n\n# iteration 0\n\nfrom functools import reduce\nfrom operator import mul\n\nbeta=Fp(86)\nq=7\nk1_prime = 1\nk1 = 2\nr_vec = r_vec_all[:k1-k_last]\n\nw_rev = tensor_vector(q, k1, 2)\nprint(f\"w_rev = {w_rev}\")\nw_ex = tensor_vector_with_monomial_basis(w_rev[k_last:])\nprint(f\"w_ex = {w_ex}\")\nr_ex = tensor_vector_with_multilinear_basis(r_vec)\nprint(f\"r_ex = {r_ex}\")\nscalar = inner_product(r_ex, w_ex, Fp(0))\nscalar_2 = reduce(mul, [inner_product([Fp(1)-r_vec[j], r_vec[j]], [1, w_rev[k_last:][j]], Fp(0)) for j in range(len(r_vec))], Fp(1))\nprint(f\"scalar = {scalar}, scalar_2 = {scalar_2}\")\nassert scalar == scalar_2, f\"scalar != scalar_2, {scalar} != {scalar_2}\"\n\nnew_eq = tensor_vector_with_monomial_basis(w_rev[:k_last])\nnew_eq = [new_eq[j] * scalar for j in range(2**k_last)]\neq_2 = [eq_0[j] + beta * new_eq[j] for j in range(2**k_last)]\neq_2, new_eq, scalar\n\n\n# iteration 1\n\nq=0\nk1_prime = 1\nk1 = 1\nr_vec = r_vec_all[:k1-k_last]\nbeta = Fp(12)\n\nw_rev = tensor_vector(q, k1, 2)\nprint(f\"w_rev = {w_rev}\")\nw_ex = tensor_vector_with_monomial_basis(w_rev[k_last:])\nprint(f\"w_ex = {w_ex}\")\nr_ex = tensor_vector_with_multilinear_basis(r_vec)\nprint(f\"r_ex = {r_ex}\")\nscalar = inner_product(r_ex, w_ex, Fp(0))\nscalar_2 = reduce(mul, [inner_product([Fp(1)-r_vec[j], r_vec[j]], [1, w_rev[k_last:][j]], Fp(0)) for j in range(len(r_vec))], Fp(1))\nprint(f\"scalar = {scalar}, scalar_2 = {scalar_2}\")\nassert scalar == scalar_2, f\"scalar != scalar_2, {scalar} != {scalar_2}\"\n\nnew_eq = tensor_vector_with_monomial_basis(w_rev[:k_last])\nnew_eq = [new_eq[j] * scalar for j in range(2**k_last)]\neq_3 = [eq_2[j] + beta * new_eq[j] for j in range(2**k_last)]\neq_3, new_eq, scalar","type":"content","url":"/src/ligerito#section-5-matrix-vector-product-with-partial-sumcheck","position":9},{"hierarchy":{"lvl1":"2. Prove evaluation"},"type":"lvl1","url":"/src/mercury-pcs","position":0},{"hierarchy":{"lvl1":"2. Prove evaluation"},"content":"from curve import Fr as Field, ec_mul, G1Point as G1\nfrom merlin.merlin_transcript import MerlinTranscript\nfrom mle2 import MLEPolynomial\nfrom utils import log_2, log_2_ceiling, is_power_of_two, prime_field_inv, Scalar\nfrom kzg10_non_hiding2 import Commitment, KZG10_PCS\nfrom curve import Fr as Field, G1Point as G1, G2Point as G2, ec_pairing_check\nfrom unipoly2 import UniPolynomial, UniPolynomialWithFft\nfrom typing import TypeVar, Union, Optional\nfrom random import Random, randint\n\nBN254_Fr = GF(Field.field_modulus); BN254_Fr\n\nRp.<X> = BN254_Fr[]\nB_mono = [X^i for i in range(0, 1024)]\ndef poly(cof, B=B_mono):\n    return reduce(lambda acc, e: acc + e, [BN254_Fr(int(a)) * b for a, b in zip(cof, B)])\n    \ndef powers(u, k):\n    return [u^i for i in range(0, 2**k)]\n    \ndef eval(f, D):\n    n = len(D)\n    k = log_2_ceiling(n)\n    return [poly(f, powers(D[i], k)) for i in range(0, n)]\nF = Field\n\n\nkzg_pcs = KZG10_PCS(G1, G2, Field, 20, debug=True)\n\ndef commit(f: MLEPolynomial):\n    evals = f.evals\n    f_cm = kzg_pcs.commit(UniPolynomial(evals))\n    return f_cm\n\ntr = MerlinTranscript(b\"test-mercury-pcs\")\n\nf_mle = MLEPolynomial([F(int(1)), F(int(3)), F(int(2)), F(int(1)),\n                    F(int(2)), F(int(-2)), F(int(1)), F(int(0)),\n                    F(int(-1)), F(int(2)), F(int(3)), F(int(1)),\n                    F(int(3)), F(int(1)), F(int(-2)), F(int(3))], 4)\nf_cm = commit(f_mle)\nus = [F(int(2)), F(int(-1)), F(int(1)), F(int(-2))]\nv = f_mle.evaluate(us)\nv\n\n\nk = f_mle.num_var\nus_l = us[:k//2]\nus_r = us[k//2:]\nvec_eq_l = MLEPolynomial.eqs_over_hypercube(us_l)\nvec_eq_r = MLEPolynomial.eqs_over_hypercube(us_r)\nn_l = len(vec_eq_l) # column\nn_r = len(vec_eq_r) # row\nvec_eq_l, vec_eq_r\n\n\nalpha = F(int(3))\nalpha_exp_4 = F(int(3^4))\n\n\\tilde{f}(X_0, X_1, X_2, X_3) = \n\\begin{bmatrix}\n1 & X_2 & X_3 & X_2X_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 & c_1 & c_2 & c_3 \\\\\nc_4 & c_5 & c_6 & c_7 \\\\\nc_8 & c_9 & c_{10} & c_{11} \\\\\nc_{12} & c_{13} & c_{14} & c_{15}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\nX_0 \\\\\nX_1 \\\\\nX_0X_1 \\\\\n\\end{bmatrix}\n\nf_mle.evals\n\n[f_mle.evals[i:i+4] for i in range(0, len(f_mle.evals), 4)]\n\n\nh_evals = []\nfor j in range(n_l):\n    h_evals.append([f_mle.evals[j+i*n_l] for i in range(n_r)] )\nh_evals\n\n\nf0_mle = MLEPolynomial(h_evals[0], 2)\nf1_mle = MLEPolynomial(h_evals[1], 2)\nf2_mle = MLEPolynomial(h_evals[2], 2)\nf3_mle = MLEPolynomial(h_evals[3], 2)\nf0_mle.evals, f1_mle.evals, f2_mle.evals, f3_mle.evals\n\n\nf0_poly = poly(f0_mle.evals)\nf1_poly = poly(f1_mle.evals)\nf2_poly = poly(f2_mle.evals)\nf3_poly = poly(f3_mle.evals)\n\n\nf0_uni = UniPolynomial(f0_mle.evals)\nf1_uni = UniPolynomial(f1_mle.evals)\nf2_uni = UniPolynomial(f2_mle.evals)\nf3_uni = UniPolynomial(f3_mle.evals)\nf_uni = UniPolynomial(f_mle.evals)\nf_uni, f_uni.evaluate(F(int(3)))\n\nf0_uni.evaluate(F(int(3^4))) + F(int(3)) * f1_uni.evaluate(F(int(3^4))) + F(int(3^2)) * f2_uni.evaluate(F(int(3^4))) + F(int(3^3)) * f3_uni.evaluate(F(int(3^4)))\n\nf_poly = (f0_poly(X^4) + X*f1_poly(X^4) + X^2*f2_poly(X^4) + X^3*f3_poly(X^4))\nf_poly, f_poly(BN254_Fr(int(3)))\n\n\nq_poly = f_poly // (X^4-3)\nr_poly = f_poly % (X^4-3)\nq_poly, r_poly\n\n\nq_poly(BN254_Fr(int(alpha))), r_poly(BN254_Fr(int(alpha)))\n\n# Check if f[0..3] are correct\nx_uni = UniPolynomial([F(int(0)), F(int(1))])\n\nf_alt_eval_at_alpha = f0_uni.evaluate(alpha_exp_4) \\\n    + (f1_uni.evaluate(alpha_exp_4)*alpha) \\\n    + (f2_uni.evaluate(alpha_exp_4)*alpha*alpha) \\\n    + (f3_uni.evaluate(alpha_exp_4) * alpha*alpha*alpha)\nf_alt_eval_at_alpha\n\nq0_uni, r0 = f0_uni.div_by_linear_divisor(alpha)\nq1_uni, r1 = f1_uni.div_by_linear_divisor(alpha)\nq2_uni, r2 = f2_uni.div_by_linear_divisor(alpha)\nq3_uni, r3 = f3_uni.div_by_linear_divisor(alpha)\nq0_uni, r0, q1_uni, r1, q2_uni, r2, q3_uni, r3\n\n\nr0 == f0_uni.evaluate(alpha), r1 == f1_uni.evaluate(alpha), r2 == f2_uni.evaluate(alpha), r3 == f3_uni.evaluate(alpha)\n\n\n# Construct q(X)\n#\n#   q(X) = q0(X^4) + X*q1(X^4) + X^2*q2(X^4) + X^3*q3(X^4)\n#   \n#  Or, the coefficients of q(X) are:\n#\n#   [q0[0], q1[0], q2[0], q3[0], \n#    q0[1], q1[1], q2[1], q3[1], \n#    q0[2], q1[2], q2[2], q3[2]]\n#\n#  Please note that deg(q) < 12\n\nq_coeffs = []\nfor i in range(len(q0_uni.coeffs)):\n    q_coeffs.append(q0_uni.coeffs[i])\n    q_coeffs.append(q1_uni.coeffs[i])\n    q_coeffs.append(q2_uni.coeffs[i])\n    q_coeffs.append(q3_uni.coeffs[i])\nq_uni = UniPolynomial(q_coeffs)\nq_uni.evaluate(alpha), q_coeffs\n\ndef compute_quotient(coeffs, k):\n    num_col = 2**k\n    num_row = len(coeffs) // num_col\n    print(f\"num_col: {num_col}, num_row: {num_row}\")\n\n    q_coeffs = [F.zero()] * (num_col * (num_row-1))\n    for j in range(num_col):\n        fi = coeffs[j::num_row]\n\n        qi, ri = UniPolynomial(fi).div_by_linear_divisor(alpha)\n        for i in range(num_row-1):\n            q_coeffs[j+i*num_row] = qi.coeffs[i]\n    return q_coeffs\n\n\n\ncompute_quotient(f_uni.coeffs, 2)\n\nq_eval_at_alpha = q0_uni.evaluate(alpha_exp_4) \\\n    + (q1_uni.evaluate(alpha_exp_4)*alpha) \\\n    + (q2_uni.evaluate(alpha_exp_4)*alpha*alpha) \\\n    + (q3_uni.evaluate(alpha_exp_4) * alpha*alpha*alpha)\nq_eval_at_alpha\n\ndef compute_product_poly(a: list[Field], b: list[Field]):\n    assert len(a) == len(b), \"Length of a and b must be the same\"\n    l = len(a)\n    s = [F.zero()] * (l-1)\n    dot_product = F.zero()\n    for i in range(l):\n        for j in range(l):\n            if abs(i-j) == 0:\n                dot_product += a[i] * b[j]\n            else:\n                s[abs(i-j)-1] += a[i] * b[j]\n    return s, dot_product\n\ndef inner_product(a, b, z):\n    return sum([a[i] * b[i] for i in range(len(a))], z)\n\na = [F(int(1)), F(int(2)), F(int(3)), F(int(4))]\nb = [F(int(4)), F(int(2)), F(int(3)), F(int(1))]\na_uni = UniPolynomial(a)\nb_uni = UniPolynomial(b)\ns, v = compute_product_poly(a, b)\ns_uni = UniPolynomial(s)\ns, v, v == inner_product(a, b, F(int(0)))\n\n# Check if s(X) is correct\n#\n#    a(X)b(1/X) + a(1/X)b(X) = 2v + X*s(X) + (1/X)*s(1/X)\n#\nz = F.rand()\nz_inv = int(1)/z\n# a_uni.evaluate(z) * b_uni.evaluate(z_inv), F(int(2)) * v\na_uni.evaluate(z) * b_uni.evaluate(z_inv) + a_uni.evaluate(z_inv) * b_uni.evaluate(z) \\\n    == F(int(2))*v + z*s_uni.evaluate(z) + z_inv*s_uni.evaluate(z_inv)\n\nq_coeffs = []\nfor i in range(len(q0_uni.coeffs)):\n    q_coeffs.append(q0_uni.coeffs[i])\n    q_coeffs.append(q1_uni.coeffs[i])\n    q_coeffs.append(q2_uni.coeffs[i])\n    q_coeffs.append(q3_uni.coeffs[i])\nq_coeffs\n\nq, r = UniPolynomial.polynomial_division_with_remainder(f_uni.coeffs, \n                        [-alpha, F(int(0)), F(int(0)), F(int(0)), F(int(1))])\nq, r\n\nr_uni = UniPolynomial(r)\n\nr_uni.evaluate(F(int(3)))\n\nf0_uni + f1_uni, Scalar(F(int(3))) * f0_uni\n\n\nh = Scalar(vec_eq_l[0]) * f0_uni + Scalar(vec_eq_l[1]) * f1_uni  \\\n    + Scalar(vec_eq_l[2]) * f2_uni + Scalar(vec_eq_l[3]) * f3_uni\nh\n\nf0_mle, -f0_mle\n\nf0_mle - f1_mle, f0_mle, f1_mle\n\nv = f_mle.evaluate(us)\nv\n\n","type":"content","url":"/src/mercury-pcs","position":1},{"hierarchy":{"lvl1":"2. Prove evaluation"},"type":"lvl1","url":"/src/mercury-pcs#id-2-prove-evaluation","position":2},{"hierarchy":{"lvl1":"2. Prove evaluation"},"content":"\n\ndebug = 1\ntranscript = MerlinTranscript(b\"test-mercury-pcs\")\ntranscript.absorb(b\"commitment\", f_cm.cm)\ntranscript.absorb(b\"point\", us)\ntranscript.absorb(b\"value\", v)\n\n\n","type":"content","url":"/src/mercury-pcs#id-2-prove-evaluation","position":3},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 1"},"type":"lvl2","url":"/src/mercury-pcs#step-1","position":4},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 1"},"content":"compute h(X)\n\ncommit h(X)\n\n\nk = f_mle.num_var\nus_l = us[:k//2]\nus_r = us[k//2:]\nvec_eq_l = MLEPolynomial.eqs_over_hypercube(us_l)\nvec_eq_r = MLEPolynomial.eqs_over_hypercube(us_r)\nn_l = len(vec_eq_l) # column\nn_r = len(vec_eq_r) # row\nvec_eq_l, vec_eq_r\n\n# Compute h(X)\nh_coeffs = []\nfor i in range(n_r):\n    h_coeffs.append(sum([f_mle.evals[i*n_l+j] * vec_eq_l[j] for j in range(n_r)], F(int(0))))\nh_uni = UniPolynomial(h_coeffs)\nh_cm = kzg_pcs.commit(h_uni)\ntranscript.absorb(b\"h_cm\", h_cm.cm)\nh_cm\n\nif debug > 0:\n    print(f\"P> check: folded polynomial h(X)\")\n    coeffs = f_mle.evals\n    eqs = MLEPolynomial.eqs_over_hypercube(us)\n    lhs = inner_product(h_coeffs, vec_eq_r, F(int(0))) \n    rhs = inner_product(coeffs, eqs, F(int(0)))\n    assert lhs == rhs, f\"lhs: {lhs}, rhs: {rhs}\"\n    print(f\"P> check: folded polynomial h(X) passed\")\n\n\n","type":"content","url":"/src/mercury-pcs#step-1","position":5},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 2"},"type":"lvl2","url":"/src/mercury-pcs#step-2","position":6},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 2"},"content":"(a). get \\alpha\\in\\mathbb{F}_p from the verifier\n\n(b). compute q(X) and g(X), s.t.f(X) = (X^b - \\alpha) * q(X) + g(X)\n\n# Step 2\n\nalpha = transcript.squeeze(Field, b\"alpha\", 4)\nalpha\n\ndef div_by_quadratic_binomial(coeffs, alpha, k):\n    num_col = 2**k\n    num_row = len(coeffs) // num_col\n    print(f\"num_col: {num_col}, num_row: {num_row}\")\n\n    q_coeffs = [F.zero()] * (num_col * (num_row-1))\n    r_coeffs = [F.zero()] * num_col\n    for j in range(num_col):\n        fi = coeffs[j::num_row]\n        qi, ri = UniPolynomial(fi).div_by_linear_divisor(alpha)\n        for i in range(num_row-1):\n            q_coeffs[j+i*num_row] = qi.coeffs[i]\n        r_coeffs[j] = ri\n    return q_coeffs, r_coeffs\n\n# Compute q(X) and g(X)\n\nq_coeffs, g_coeffs = div_by_quadratic_binomial(f_mle.evals, alpha, 2)\nq_uni = UniPolynomial(q_coeffs)\ng_uni = UniPolynomial(g_coeffs)\nq_cm = kzg_pcs.commit(q_uni)\ng_cm = kzg_pcs.commit(g_uni)\nq_cm, g_cm\n\nif debug > 0:\n    print(f\"P> check: quotient polynomial q(X) and remainder polynomial g(X)\")\n    q, r = UniPolynomial.polynomial_division_with_remainder(f_mle.evals, \n                            [-alpha, F(int(0)),  F(int(0)), F(int(0)), F(int(1))])\n    assert q == q_coeffs, f\"q: {q}, q_coeffs: {q_coeffs}\"\n    print(f\"P> check: quotient polynomial q(X) passed\")\n    assert r == g_coeffs, f\"r: {r}, r_coeffs: {g_coeffs}\"\n    print(f\"P> check: remainder polynomial g(X) passed\")\n\n# Commit and send q(X) and g(X)\n\ntranscript.absorb(b\"q_cm\", q_cm.cm)\ntranscript.absorb(b\"g_cm\", g_cm.cm)\n\n","type":"content","url":"/src/mercury-pcs#step-2","position":7},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 3"},"type":"lvl2","url":"/src/mercury-pcs#step-3","position":8},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 3"},"content":"(a). Get \\gamma\\in\\mathbb{F}_p from the verifier\n\n(b). Compute s(X) and send its commitment C_s\n\n(c). Compute d(X) and send its commitment C_d\n\ngamma = transcript.squeeze(Field, b\"gamma\", 4)\ngamma, type(gamma)\n\ndef compute_product_poly(a: list[Field], b: list[Field]):\n    assert len(a) == len(b), \"Length of a and b must be the same\"\n    l = len(a)\n    s = [F.zero()] * (l-1)\n    dot_product = F.zero()\n    for i in range(l):\n        for j in range(l):\n            if abs(i-j) == 0:\n                dot_product += a[i] * b[j]\n            else:\n                s[abs(i-j)-1] += a[i] * b[j]\n    return s, dot_product\n\n# Compute s1(X) and s2(X), and send [s(X)]\n#\n#  where s(X) = s1(X) + gamma * s2(X)\n\ns1_coeffs, v1 = compute_product_poly(g_coeffs, vec_eq_l)\ns2_coeffs, v2 = compute_product_poly(h_coeffs, vec_eq_r)\ns1_uni = UniPolynomial(s1_coeffs)\ns2_uni = UniPolynomial(s2_coeffs)\ns_uni = s1_uni + Scalar(gamma) * s2_uni\ns_cm = kzg_pcs.commit(s_uni)\ns_cm\n\nif debug > 0:\n    print(f\"P> check: s1(X) and s2(X)\")\n    assert v2 == v, f\"v2: {v2}, v: {v}\"\n    assert v1 == h_uni.evaluate(alpha), f\"v1: {v1}, h_uni.evaluate(alpha): {h_uni.evaluate(alpha)}\"\n    z = F.rand()\n    z_inv = int(1)/z\n    g_uni = UniPolynomial(g_coeffs)\n    p1_uni = UniPolynomial(vec_eq_l)\n    h_uni = UniPolynomial(h_coeffs)\n    p2_uni = UniPolynomial(vec_eq_r)\n    s1_uni = UniPolynomial(s1_coeffs)\n    s2_uni = UniPolynomial(s2_coeffs)\n    assert g_uni.evaluate(z) * p1_uni.evaluate(z_inv) + g_uni.evaluate(z_inv) * p1_uni.evaluate(z) \\\n        == F(int(2))*v1 + z*s1_uni.evaluate(z) + z_inv*s1_uni.evaluate(z_inv)\n    print(f\"P> check: s1(X) passed\")\n    assert h_uni.evaluate(z) * p2_uni.evaluate(z_inv) + h_uni.evaluate(z_inv) * p2_uni.evaluate(z) \\\n        == F(int(2))*v2 + z*s2_uni.evaluate(z) + z_inv*s2_uni.evaluate(z_inv)\n    print(f\"P> check: s2(X) passed\")\n\n\n\n# Compute d(X) where d(X) = X^{l-1} * g(1/X)\n\nd_coeffs = g_coeffs[::-1]\nd_uni = UniPolynomial(d_coeffs)\nd_cm = kzg_pcs.commit(d_uni)\nd_cm\n\nif debug > 0:\n    print(f\"P> check: d(X)\")\n    z = F.rand()\n    z_inv = int(1)/z\n    d_uni = UniPolynomial(d_coeffs)\n    g_uni = UniPolynomial(g_coeffs)\n    lhs = d_uni.evaluate(z)\n    assert lhs == z^(int(n_l)-int(1)) * g_uni.evaluate(z_inv)\n    print(f\"P> check: d(X) passed\")\n\ntranscript.absorb(b\"s_cm\", s_cm.cm)\ntranscript.absorb(b\"d_cm\", d_cm.cm)\n\n","type":"content","url":"/src/mercury-pcs#step-3","position":9},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 4"},"type":"lvl2","url":"/src/mercury-pcs#step-4","position":10},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 4"},"content":"(a). Get \\zeta\\in\\mathbb{F}_p from the verifier\n\n(b). Compute and send g(\\zeta), g(1/\\zeta), h(\\zeta), h(1/\\zeta), s(\\zeta), s(1/\\zeta) and send them to the verifier\n\n(c). Compute the quotient polynomial t(X) and send its commitment C_tt(X) = \\frac{f(X) - q(X)(z^b - \\alpha) - g(\\zeta)}{X - \\zeta}\n\nzeta = transcript.squeeze(Field, b\"zeta\", 4)\nzeta_inv = zeta.inv()\n\ng_at_zeta = g_uni.evaluate(zeta)\ng_at_zeta_inv = g_uni.evaluate(zeta_inv)\nh_at_zeta = h_uni.evaluate(zeta)\nh_at_zeta_inv = h_uni.evaluate(zeta_inv)\ns_at_zeta = s_uni.evaluate(zeta)\ns_at_zeta_inv = s_uni.evaluate(zeta_inv)\n\nu = f_uni - q_uni * Scalar(zeta^n_l - alpha) - Scalar(g_at_zeta)\nu\n\nt_uni, t_eval = u.div_by_linear_divisor(zeta)\nt_cm = kzg_pcs.commit(t_uni)\nt_cm, t_eval\n\n\nif debug > 0:\n    print(f\"P> check: quotient polynomial t(X)\")\n    assert t_eval == Field.zero(), f\"t_eval: {t_eval}\"\n    z = F.rand()\n    t_z = t_uni.evaluate(z)\n    f_z = f_uni.evaluate(z)\n    q_z = q_uni.evaluate(z)\n    assert t_z * (z - zeta) == f_z - q_z * (zeta^n_l - alpha) - g_at_zeta\n    print(f\"P> check: quotient polynomial t(X) passed\")\n\n\nif debug > 0:\n    print(f\"P> check: C_t\")\n    g_G1 = kzg_pcs.params['g']\n    h_G2 = kzg_pcs.params['h']\n    h_tau_G2 = kzg_pcs.params['tau_h']\n    lhs1 = f_cm.cm + t_cm.cm.ec_mul(zeta) - q_cm.cm.ec_mul(zeta^n_l - alpha) - g_G1.ec_mul(g_at_zeta)\n    lhs2 = h_G2\n    rhs1 = t_cm.cm\n    rhs2 = h_tau_G2\n    print(f\"type(lhs1)= {type(lhs1)}, type(lhs2)= {type(lhs2)}, type(rhs1)= {type(rhs1)}, type(rhs2)= {type(rhs2)}\")\n    checked = ec_pairing_check([lhs1, rhs1], [-lhs2, rhs2])\n    assert checked, \"C_t is not valid\"\n    print(f\"P> check: C_t passed\")\n\n\ntranscript.absorb(b\"t_cm\", t_cm.cm)\ntranscript.absorb(b\"g_at_zeta\", g_at_zeta)\ntranscript.absorb(b\"g_at_zeta_inv\", g_at_zeta_inv)\ntranscript.absorb(b\"h_at_zeta\", h_at_zeta)\ntranscript.absorb(b\"h_at_zeta_inv\", h_at_zeta_inv)\ntranscript.absorb(b\"s_at_zeta\", s_at_zeta)\ntranscript.absorb(b\"s_at_zeta_inv\", s_at_zeta_inv)\n\n","type":"content","url":"/src/mercury-pcs#step-4","position":11},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 5"},"type":"lvl2","url":"/src/mercury-pcs#step-5","position":12},{"hierarchy":{"lvl1":"2. Prove evaluation","lvl2":"Step 5"},"content":"Aggregate the evaluation proofs\n\n# Aggregate the evaluation proofs\n\neta = transcript.squeeze(Field, b\"eta\", 4)\n\na_uni = g_uni + Scalar(eta) * h_uni + Scalar(eta*eta) * s_uni\na_at_zeta_inv, a_at_zeta_inv_arg = kzg_pcs.prove_evaluation(a_uni, zeta_inv)\n\na_uni += Scalar(eta*eta*eta) * d_uni\na_at_zeta, a_at_zeta_arg = kzg_pcs.prove_evaluation(a_uni, zeta)\n\nh_uni_at_alpha, h_at_alpha_arg = kzg_pcs.prove_evaluation(h_uni, alpha)\na_at_zeta_arg, a_at_zeta_inv_arg, h_at_alpha_arg\n\n\n\nif debug > 0:\n    print(f\"P> check: h(alpha)\")\n    p1_uni = UniPolynomial(vec_eq_l)\n    p2_uni = UniPolynomial(vec_eq_r)\n    p1_at_zeta = p1_uni.evaluate(zeta)\n    p2_at_zeta = p2_uni.evaluate(zeta)\n    p1_at_zeta_inv = p1_uni.evaluate(zeta_inv)\n    p2_at_zeta_inv = p2_uni.evaluate(zeta_inv)\n    h_alpha = (h_at_zeta * p2_at_zeta_inv) + (h_at_zeta_inv * p2_at_zeta) - v - v\n    h_alpha *= gamma \n    h_alpha += (g_at_zeta * p1_at_zeta_inv) + (g_at_zeta_inv * p1_at_zeta)\n    h_alpha -= zeta * s_at_zeta + zeta_inv * s_at_zeta_inv\n\n    d_at_zeta = zeta^(int(n_l-1)) * g_at_zeta_inv\n    assert d_at_zeta == d_uni.evaluate(zeta), f\"d_at_zeta: {d_at_zeta}, d_uni.evaluate(zeta): {d_uni.evaluate(zeta)}\"\n    assert h_uni_at_alpha * Field(int(2)) == h_alpha, f\"h_uni_at_alpha: {h_uni_at_alpha}, h_alpha: {h_alpha}\"\n    print(f\"P> check: h(alpha) passed\")\n\n\nif debug > 0:\n    print(f\"P> check: aggregate evaluation proofs\")\n    C_a = g_cm.cm + h_cm.cm.ec_mul(eta) + s_cm.cm.ec_mul(eta*eta) \n    checked1 = kzg_pcs.verify_evaluation(Commitment(C_a), zeta_inv, a_at_zeta_inv, a_at_zeta_inv_arg)\n    C_a += d_cm.cm.ec_mul(eta*eta*eta)\n    checked2 = kzg_pcs.verify_evaluation(Commitment(C_a), zeta, a_at_zeta, a_at_zeta_arg)\n    assert checked1 and checked2, f\"checked1: {checked1}, checked2: {checked2}\"\n    checked3 = kzg_pcs.verify_evaluation(h_cm, alpha, h_uni_at_alpha, h_at_alpha_arg)\n    assert checked3, f\"checked3: {checked3}\"\n    print(f\"P> check: aggregate evaluation proofs passed\")\n\nff = UniPolynomial([F(int(2)), F(int(6)), F(int(10)), F(int(14))])\nff.evaluate(F(int(3194597718)))\n","type":"content","url":"/src/mercury-pcs#step-5","position":13},{"hierarchy":{"lvl1":"STIR"},"type":"lvl1","url":"/src/stir","position":0},{"hierarchy":{"lvl1":"STIR"},"content":"This implementation is referenced from the following link: \n\nhttps://​github​.com​/WizardOfMenlo​/stir\n\n","type":"content","url":"/src/stir","position":1},{"hierarchy":{"lvl1":"STIR","lvl2":"引入库"},"type":"lvl2","url":"/src/stir#id","position":2},{"hierarchy":{"lvl1":"STIR","lvl2":"引入库"},"content":"\n\nimport math\nfrom merlin.merlin_transcript import MerlinTranscript\nfrom merkle import MerkleTree\nfrom typing import List\nimport numpy as np\nfrom hashlib import sha256\n# 导入SageMath库\nfrom sage.all import *\nfrom enum import Enum\nfrom typing import Generic, TypeVar, List\n\n","type":"content","url":"/src/stir#id","position":3},{"hierarchy":{"lvl1":"STIR","lvl2":"Define Class"},"type":"lvl2","url":"/src/stir#define-class","position":4},{"hierarchy":{"lvl1":"STIR","lvl2":"Define Class"},"content":"\n\n","type":"content","url":"/src/stir#define-class","position":5},{"hierarchy":{"lvl1":"STIR","lvl3":"Domain Class","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#domain-class","position":6},{"hierarchy":{"lvl1":"STIR","lvl3":"Domain Class","lvl2":"Define Class"},"content":"\n\nimport numpy as np\nfrom hashlib import sha256\n# 导入SageMath库\nfrom sage.all import *\n\n\n# 定义域类\nclass Domain:\n    def __init__(self, root_of_unity, root_of_unity_inv, offset, backing_domain):\n        self.root_of_unity = root_of_unity\n        self.root_of_unity_inv = root_of_unity_inv\n        self.offset = offset\n        self.backing_domain = backing_domain\n    \n    def __repr__(self):\n        return (f\"Domain(root_of_unity={self.root_of_unity}, \"\n                f\"root_of_unity_inv={self.root_of_unity_inv}, \"\n                f\"backing_domain={self.backing_domain})\")\n    \n    def new(self, degree, log_rho_inv):\n        pass\n\n    def generate_elements(self):\n        # 生成域的元素（这里简化为整数序列）\n        return np.arange(self.size)\n    \n    def size(self):\n        return len(self.backing_domain)\n    \n    # o^power * <w^power> \n    def scale_generator_by(self, power):\n        root_of_unity = self.root_of_unity ** power\n        root_of_unity_inv = self.root_of_unity_inv ** power\n        offset = self.offset ** power\n        size = len(self.backing_domain) // power\n        backing_domain = [offset * (root_of_unity ** i) for i in range(size)]\n        return Domain(root_of_unity, root_of_unity_inv, offset, backing_domain)\n    \n    # L_0 = o * <w> then L_1 = w * o^power * <w^power>\n    def scale_with_offset(self, power):\n        root_of_unity = self.root_of_unity ** power\n        root_of_unity_inv = self.root_of_unity_inv ** power\n        offset = self.root_of_unity * self.offset ** power\n        size = len(self.backing_domain) // power\n        backing_domain = [self.root_of_unity * (self.offset ** power) * (root_of_unity ** i) for i in range(size)]\n        return Domain(root_of_unity, root_of_unity_inv, offset, backing_domain)\n\n","type":"content","url":"/src/stir#domain-class","position":7},{"hierarchy":{"lvl1":"STIR","lvl3":"多项式类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#id-1","position":8},{"hierarchy":{"lvl1":"STIR","lvl3":"多项式类","lvl2":"Define Class"},"content":"\n\n# 定义多项式类\nclass DensePolynomial:\n    def __init__(self, coefficients):\n        self.coefficients = coefficients\n\n    def evaluate(self, x):\n        # 使用 Horner 法则计算多项式在x处的值\n        result = 0\n        for coefficient in reversed(self.coefficients):\n            result = result * x + coefficient\n        return Fp(result)\n\n    def degree(self):\n        return len(self.coefficients) - 1\n    \n    def evaluate_over_domain(self, domain):\n        result = []\n        for point in domain:\n            result.append(self.evaluate(point))\n        return result\n    \n    def mul(self, Fp, poly: 'DensePolynomial') -> 'DensePolynomial':\n        R.<X> = Fp[]\n        f_X = R(self.coefficients)\n        g_X = R(poly.coefficients)\n        # print(\"f_X = \", f_X)\n        # print(\"g_X = \", g_X)\n        # print(\"f_X * g_X = \", f_X * g_X)\n        return DensePolynomial(list(f_X * g_X))\n    \n    def __repr__(self) -> str:\n        return (f\"Denspolynomial(coefficients={self.coefficients})\")\n\n","type":"content","url":"/src/stir#id-1","position":9},{"hierarchy":{"lvl1":"STIR","lvl3":"Witness 类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#witness","position":10},{"hierarchy":{"lvl1":"STIR","lvl3":"Witness 类","lvl2":"Define Class"},"content":"\n\n# 定义 Witness 类\nclass Witness:\n    def __init__(self, domain: Domain, polynomial: DensePolynomial, merkle_tree, folded_evals):\n        self.domain = domain\n        self.polynomial = polynomial\n        self.merkle_tree = merkle_tree\n        self.folded_evals = folded_evals\n\n    def __repr__(self):\n        return (f\"Witness(domain={self.domain}, polynomial={self.polynomial}, \"\n                f\"merkle_tree={self.merkle_tree}, folded_evals={self.folded_evals}) \")\n\n# 定义WitnessExtended类\nclass WitnessExtended:\n    def __init__(self, domain, polynomial: DensePolynomial, merkle_tree, folded_evals, num_round, folding_randomness):\n        self.domain = domain\n        self.polynomial = polynomial\n        self.merkle_tree = merkle_tree\n        self.folded_evals = folded_evals\n        # 轮数\n        self.num_round = num_round\n        # 下一轮进行 fold 的随机数\n        self.folding_randomness = folding_randomness\n    def __repr__(self):\n        return (f\"WitnessExtended(domain={self.domain}, polynomial={self.polynomial}, \"\n                f\"merkle_tree={self.merkle_tree}, folded_evals={self.folded_evals}, \"\n                f\"num_round={self.num_round}, folding_randomness={self.folding_randomness})\")\n\n","type":"content","url":"/src/stir#witness","position":11},{"hierarchy":{"lvl1":"STIR","lvl3":"Commitment 类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#commitment","position":12},{"hierarchy":{"lvl1":"STIR","lvl3":"Commitment 类","lvl2":"Define Class"},"content":"\n\nclass Commitment:\n    def __init__(self, root):\n        self.root = root\n\n    def __repr__(self):\n        return f\"Commitment(root={self.root})\"\n\n","type":"content","url":"/src/stir#commitment","position":13},{"hierarchy":{"lvl1":"STIR","lvl3":"Proof 类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#proof","position":14},{"hierarchy":{"lvl1":"STIR","lvl3":"Proof 类","lvl2":"Define Class"},"content":"\n\nclass RoundProof:\n    def __init__(self, g_root, betas, queries_to_prev, ans_polynomial: DensePolynomial, shake_polynomial: DensePolynomial):\n        self.g_root = g_root\n        self.betas = betas\n        self.queries_to_prev = queries_to_prev\n        self.ans_polynomial = ans_polynomial\n        self.shake_polynomial = shake_polynomial\n\n    def __repr__(self):\n        return (f\"RoundProof(g_root={self.g_root}, betas={self.betas}, \"\n                f\"queries_to_prev={self.queries_to_prev}, ans_polynomial={self.ans_polynomial}, \"\n                f\"shake_polynomial={self.shake_polynomial})\")\n\nclass Proof:\n    def __init__(self, round_proofs: List[RoundProof], final_polynomial, queries_to_final):\n        self.round_proofs = round_proofs\n        self.final_polynomial = final_polynomial\n        self.queries_to_final = queries_to_final\n    \n    def __repr__(self):\n        return (f\"Proof(round_proofs={self.round_proofs}, final_polynomial={self.final_polynomial}, \"\n            f\"queries_to_final={self.queries_to_final})\")\n\n","type":"content","url":"/src/stir#proof","position":15},{"hierarchy":{"lvl1":"STIR","lvl3":"参数类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#id-2","position":16},{"hierarchy":{"lvl1":"STIR","lvl3":"参数类","lvl2":"Define Class"},"content":"\n\nfrom enum import Enum\n# 枚举 Soundness 类型\nclass SoundnessType(Enum):\n    Provable = 1\n    Conjecture = 2\n\nimport math\n\n# 定义参数类\nclass Parameters():\n    def __init__(self, \n                 security_level: int,\n                 protocol_security_level: int,\n                 starting_degree: int,\n                 stopping_degree: int,\n                 folding_factor: int,\n                 starting_rate: int,\n                 soundness_type: SoundnessType,\n                 fiat_shamir_config):\n        self.security_level = security_level\n        self.protocol_security_level = protocol_security_level\n        self.starting_degree = starting_degree\n        self.stopping_degree = stopping_degree\n        self.folding_factor = folding_factor\n        self.starting_rate = starting_rate\n        self.soundness_type = soundness_type\n        self.fiat_shamir_config = fiat_shamir_config\n\n    def __repr__(self):\n        return (f\"Parameters(security_level={self.security_level}, \"\n                f\"protocol_security_level={self.protocol_security_level}, \"\n                f\"starting_degree={self.starting_degree}, \"\n                f\"stopping_degree={self.stopping_degree}, \"\n                f\"folding_factor={self.folding_factor}, \"\n                f\"starting_rate={self.starting_rate}, \"\n                f\"soundness_type={self.soundness_type},\"\n                f\"fiat_shamir_config={self.fiat_shamir_config})\")\n    \n    def repetitions(self, log_inv_rate: int) -> int:\n        constant = 0\n        if self.soundness_type == SoundnessType.Provable:\n            constant = 2\n        elif self.soundness_type == SoundnessType.Conjecture:\n            constant = 1\n        \n        return math.ceil(constant * (self.security_level / log_inv_rate))\n         \n\n\nfrom typing import Generic, TypeVar, List\n\n# F = TypeVar('F')\n# MerkleConfig = TypeVar('MerkleConfig')\n# FSConfig = TypeVar('FSConfig')\n\nclass FullParameters:\n    def __init__(self, \n                 parameters: Parameters,\n                 num_rounds: int,\n                 degrees: List[int],\n                 rates: List[int],\n                 repetitions: List[int],\n                 ood_samples: int):\n        self.parameters = parameters\n        self.num_rounds = num_rounds\n        self.rates = rates\n        self.repetitions = repetitions\n        # self.pow_bits = pow_bits\n        self.ood_samples = ood_samples\n        self.degrees = degrees\n\n    def __repr__(self):\n        return (f\"FullParameters(parameters={self.parameters}, \"\n                f\"num_rounds={self.num_rounds}, \"\n                f\"rates={self.rates}, \"\n                f\"repetitions={self.repetitions}, \"\n                f\"ood_samples={self.ood_samples}, \"\n                f\"degrees={self.degrees})\")\n    \n    @classmethod\n    def from_parameters(cls, parameters: Parameters):\n        assert is_power_of_two(parameters.folding_factor) == True\n        assert is_power_of_two(parameters.starting_degree) == True\n        assert is_power_of_two(parameters.stopping_degree) == True\n        \n\n        d = parameters.starting_degree\n        degrees = [d]\n        num_rounds = 0\n        while d > parameters.stopping_degree:\n            # 保证能除尽\n            assert d % parameters.folding_factor == 0\n            d /= parameters.folding_factor\n            degrees.append(d)\n            num_rounds += 1\n        \n        degrees.pop()\n        num_rounds -= 1\n\n        rates = [parameters.starting_rate]\n        log_folding_factor = math.log(parameters.folding_factor, 2)\n        # parameters.starting_rate = - log_2(\\rho_0)\n        for i in range(1, num_rounds + 1):\n            rates.append(parameters.starting_rate + i * (log_folding_factor - 1))\n        \n        repetitions = [parameters.repetitions(rate) for rate in rates]\n\n        # 预防泄漏知识，repetions[i] = min(repetions[i], d_i / k)\n        # 这里不包括最后一轮\n        ood_samples = 2\n        for i in range(0, num_rounds):\n            repetitions[i] = min(repetitions[i], degrees[i] / parameters.folding_factor - ood_samples - 1)\n        \n        assert num_rounds + 1 == len(rates)\n        assert num_rounds + 1 == len(repetitions)\n\n        return FullParameters(\n            parameters=parameters,\n            num_rounds=num_rounds,\n            degrees=degrees,\n            rates=rates,\n            repetitions=repetitions,\n            ood_samples=ood_samples)\n\n","type":"content","url":"/src/stir#id-2","position":17},{"hierarchy":{"lvl1":"STIR","lvl3":"VerificationState 类","lvl2":"Define Class"},"type":"lvl3","url":"/src/stir#verificationstate","position":18},{"hierarchy":{"lvl1":"STIR","lvl3":"VerificationState 类","lvl2":"Define Class"},"content":"\n\n# 枚举 Soundness 类型\nclass OracleType(Enum):\n    Initial = 1\n    Virtual = 2\n    \nclass VerificationState:\n    def __init__(self, \n                 oracle: OracleType,\n                 domain: Domain,\n                 folding_randomness,\n                 num_round: int,\n                 comb_randomness,\n                 quotient_set,\n                 ans_polynomial):\n        self.oracle = oracle\n        self.domain = domain\n        self.folding_randomness = folding_randomness\n        self.num_round = num_round  # 表示第几轮\n        self.comb_randomness = comb_randomness\n        self.quotient_set = quotient_set\n        self.ans_polynomial = ans_polynomial\n\n    def __repr__(self):\n        return (f\"oracle{self.oracle}, \"\n                f\"domain{self.domain}, \"\n                f\"folding_randomness={self.folding_randomness}, \"\n                f\"num_round={self.num_round}\",\n                f\"comb_randomness={self.comb_randomness})\")\n\n","type":"content","url":"/src/stir#verificationstate","position":19},{"hierarchy":{"lvl1":"STIR","lvl2":"工具函数"},"type":"lvl2","url":"/src/stir#id-3","position":20},{"hierarchy":{"lvl1":"STIR","lvl2":"工具函数"},"content":"\n\n# 定义一些工具函数\ndef squeeze_field_elements(Fp, num_elements):\n    odd_randomness = [Fp.random_element() for i in range(num_elements)]\n    return odd_randomness\n\n# 将多项式的值进行分组\ndef stack_evaluations(evals, folding_factor):\n    assert len(evals) % folding_factor == 0\n    size_of_new_domain = len(evals) // folding_factor \n\n    stacked_evaluations = []\n    for i in range(size_of_new_domain):\n        new_evals = []\n        for j in range(folding_factor):\n            new_evals.append(evals[i + j * size_of_new_domain])\n        stacked_evaluations.append(new_evals)\n    \n    return stacked_evaluations\n\n# 多项式折叠函数\ndef poly_fold(f: DensePolynomial, folding_randomness, folding_factor) -> DensePolynomial:\n    degree = f.degree() + 1\n    coefficients = f.coefficients\n    coefficients_array = np.array(coefficients)\n    matrix_coefficients = coefficients_array.reshape(degree // folding_factor, folding_factor)\n    # print(\"矩阵：\")\n    # print(matrix_coefficients)\n\n    transposed_matrix = matrix_coefficients.T\n    # print(\"转置后的矩阵：\")\n    # print(transposed_matrix)\n\n    size = len(transposed_matrix[0])\n    # print(size)\n    folded_coefficients = [0] * size\n\n    pow = 1\n    for row in transposed_matrix:\n        for i in range(len(row)):\n            folded_coefficients[i] += pow * row[i]\n        pow *=  folding_randomness\n    \n    return DensePolynomial(folded_coefficients)\n\ndef interpolation(Fp, points):\n    n = len(points)\n\n    # 先计算 L_i(X) 的分母\n    L_denominator = [1] * n\n\n    for i in range(0, n):\n        for j in range(0, n):\n            if i != j:\n                xi = points[i][0]\n                xj = points[j][0]\n                # print(\"xi = \", xi, \"xj = \", xj)\n                # print(L_denominator[i])\n                L_denominator[i] = (xi - xj) * L_denominator[i]\n    \n    # print(\"L_denominator = \", L_denominator)\n\n    # 声明多项式自变量 X 在 GF 中\n    R.<X> = Fp[]\n\n    # 计算 L_i(X)\n    L_X = [1] * n\n    for i in range(0, n):\n        for j in range(0, n):\n            if i != j:\n                xj = points[j][0]\n                L_X[i] = (X - xj) * L_X[i]\n        L_X[i] = Fp(1 / L_denominator[i]) * L_X[i]\n    \n    # 计算 f(X)\n    f_X = 0\n    for i in range(0, n):\n        yi = points[i][1]\n        f_X = f_X + yi * L_X[i]\n\n    print(\"f_X = \", f_X)\n\n    return DensePolynomial(list(f_X))\n\ndef poly_quotient(Fp, poly: DensePolynomial, ans: DensePolynomial, points) -> DensePolynomial:\n    R.<X> = Fp[]\n    ans_X = R(ans.coefficients)\n    poly_X = R(poly.coefficients)\n    vanish_X = R(1)\n    for point in points:\n        vanish_X = vanish_X * (X - point[0])\n    \n    quotient, remainder = (poly_X - ans_X).quo_rem(vanish_X)\n    assert remainder == 0\n    \n    # return DensePolynomial(quotient.coefficients())\n    return DensePolynomial(list(quotient))\n\ndef verify_decommitment(leaf_id, leaf_data, decommitment, root):\n    leaf_num = 2 ** len(decommitment)\n    node_id = leaf_id + leaf_num\n    cur = sha256(str(leaf_data).encode()).hexdigest()\n    for bit, auth in zip(bin(node_id)[3:][::-1], decommitment[::-1]):\n        if bit == '0':\n            h = cur + auth\n        else:\n            h = auth + cur\n        cur = sha256(h.encode()).hexdigest()\n    return cur == root\n\ndef verify_multi_path(root, queries_to_prev):\n    indexs = queries_to_prev[0]\n    leaves = queries_to_prev[1]\n    multi_path = queries_to_prev[2]\n    for i in range(len(indexs)):\n        if verify_decommitment(indexs[i], leaves[i], multi_path[i], root) == False:\n            return False\n    return True\n\n","type":"content","url":"/src/stir#id-3","position":21},{"hierarchy":{"lvl1":"STIR","lvl2":"StirProver"},"type":"lvl2","url":"/src/stir#stirprover","position":22},{"hierarchy":{"lvl1":"STIR","lvl2":"StirProver"},"content":"\n\n# 定义Prover类\nclass StirProver:\n    def __init__(self, parameters: FullParameters):\n        self.parameters = parameters\n    \n    def commit(self, domain: Domain, witness_polynomial: DensePolynomial):\n        # domain = Domain(stir_prover.parameters.parameters.starting_degree, stir_prover.parameters.parameters.starting_rate)\n        evals = [witness_polynomial.evaluate(x) for x in domain.backing_domain]\n        folded_evals = stack_evaluations(evals, self.parameters.parameters.folding_factor)\n        merkle_tree = MerkleTree(folded_evals)\n        initial_commitment = merkle_tree.root\n        # return initial_commitment, Witness(domain, witness_polynomial, merkle_tree, folded_evals)\n        return {\n            \"commitment\": Commitment(initial_commitment),\n            \"witness\": Witness(domain, witness_polynomial, merkle_tree, folded_evals)\n        }\n\n    def round(self, Fp, transcript: MerlinTranscript, witness: WitnessExtended, debug=False):\n        if debug: print(\"--------- prove round begin ---------\")\n        R.<X> = Fp[]\n        # if debug: R.<X> = Fp[]\n        if debug: print(\"\\n1. Send folded function \")\n        if debug: print(\"Before folded, f(X) = \", R(witness.polynomial.coefficients))\n        # folded witness.polynomial\n        g_poly = poly_fold(witness.polynomial, witness.folding_randomness, self.parameters.parameters.folding_factor)\n        if debug: print(\"After folded, g(X) = \", R(g_poly.coefficients))\n\n        g_domain = witness.domain.scale_with_offset(2)\n        if debug: print(\"evaluation domain L_i : \", g_domain.backing_domain)\n\n        # get evaluations of polynomial g_poly\n        g_evaluations = g_poly.evaluate_over_domain(g_domain.backing_domain)\n        # if debug: print(\"g_domain.backing_domain L_i: \", g_domain.backing_domain)\n        if debug: print(\"evaluations of g(X) on L_i: \", g_evaluations)\n\n\n        g_folded_evaluations = stack_evaluations(g_evaluations, self.parameters.parameters.folding_factor)\n        if debug: print(\"Group the values of g(X) on L_i: \")\n        if debug: print(g_folded_evaluations)\n\n        if debug: print(\"commit the Merkle root of values of g(X):\")\n        g_merkle = MerkleTree(g_folded_evaluations)\n        g_root = g_merkle.root\n        if debug: print(\"Merkle root :\", g_root)\n\n        transcript.append_message(b\"merkle_root\", g_root.encode('ascii'))\n\n        # Out of domain sample\n        # odd_randomness = transcript.challenge_bytes(b\"odd_randomness\", stir_prover.parameters.ood_samples)\n        if debug: print(\"\\n2. Out-of-domain samples\")\n        odd_randomness = []\n        for _ in range(self.parameters.ood_samples):\n            random = transcript.challenge_bytes(b\"odd_randomness\", 1)[0] % Fp.order()\n            odd_randomness.append(random)\n        odd_randomness = list(set(odd_randomness))\n        # odd_randomness = [transcript.challenge_bytes(b\"odd_randomness\", 1)[0] % Fp.order() for _ in range(self.parameters.ood_samples)]\n        # odd_randomness = squeeze_field_elements(Fp, stir_prover.parameters.ood_samples)\n        if debug: print(\"out-of-domain randomness:\", odd_randomness)\n        \n        # Out of domain reply\n        if debug: print(\"\\n3. Out-of-domain reply\")\n        betas = g_poly.evaluate_over_domain(odd_randomness)\n        if debug: print(\"betas: \", betas)\n        for i in range(len(betas)):\n            transcript.append_message(f'betas_at_{i}'.encode('ascii'), str(betas[i]).encode('ascii'))\n\n        # STIR message\n        if debug: print(\"\\n4. STIR message\")\n        comb_randomness = transcript.challenge_bytes(b\"comb_randomness\", 1)[0] % Fp.order()\n        if debug: print(\"comb_randomness:\", comb_randomness)\n\n        folding_randomness = transcript.challenge_bytes(b\"folding_randomness\", 1)[0] % Fp.order()\n        if debug: print(\"folding_randomness:\", folding_randomness)\n\n        scaling_factor = witness.domain.size() / self.parameters.parameters.folding_factor\n        if debug: print(\"scaling_factor:\", scaling_factor)\n\n        num_repetitions = self.parameters.repetitions[witness.num_round]\n        stir_randomness_indexes = []\n        for i in range(num_repetitions):\n            index = transcript.challenge_bytes(b\"stir_randomness_indexes\", 1)[0] % scaling_factor\n            stir_randomness_indexes.append(index)\n\n        # 去重\n        if debug: print(\"remove duplicates from the stir_randomness_indexes\")\n        stir_randomness_indexes = list(set(stir_randomness_indexes))\n        L_k = witness.domain.scale_generator_by(folding_factor)\n        for odd_random in odd_randomness:\n            for index in stir_randomness_indexes:\n                if L_k.backing_domain[index] == odd_random:\n                    stir_randomness_indexes.pop(index)\n        if debug: print(\"stir_randomness_index: \", stir_randomness_indexes)\n\n        if debug: print(\"\\nsample shake randomness:\")\n        _shake_randomness = transcript.challenge_bytes(b\"_shake_randomness\", 1)[0] % Fp.order()\n        if debug: print(\"shake_randomness: \", _shake_randomness)\n\n        if debug: print(\"\\nquery to prev f value at stir_randomness_indexes:\")\n        queries_to_prev_index = stir_randomness_indexes\n        queries_to_prev_ans = []\n        for index in stir_randomness_indexes:\n            queries_to_prev_ans.append(witness.folded_evals[index])\n        if debug: print(\"queries_to_prev_index: \", queries_to_prev_index)\n        if debug: print(\"queries_to_prev_ans: \", queries_to_prev_ans)\n\n        queries_to_prev_proof = []\n        for index in stir_randomness_indexes:\n            queries_to_prev_proof.append(witness.merkle_tree.get_authentication_path(index))\n        if debug: print(\"queries_to_prev_proof: \", queries_to_prev_proof)\n        queries_to_prev = [queries_to_prev_index, queries_to_prev_ans, queries_to_prev_proof]\n        if debug: print(\"queries_to_prev: \", queries_to_prev)\n\n        if debug: print(\"\\nfrom L_{i-1}^k to get r_{i}^{shift}:\")\n        if debug: print(\"L_{i-1} :\", witness.domain.backing_domain)\n        # L_k = witness.domain.scale_generator_by(folding_factor)\n        if debug: print(\"L_{i-1}^k :\", L_k.backing_domain)\n        stir_randomness = [L_k.backing_domain[index] for index in stir_randomness_indexes]\n        if debug: print(\"stir_randomness: \", stir_randomness)\n\n        # compute the set we are quotienting by\n        # ODD samples + stir_randomness\n        if debug: print(\"\\n5. Define next polynomial\")\n        quotient_set = odd_randomness + stir_randomness\n        if debug: print(\"quotient set：\", quotient_set)\n\n        quotient_answers = [[x, g_poly.evaluate(x)] for x in quotient_set]\n        print(\"values of g(X) on quotient set \", quotient_answers)\n\n        # Ans(X)\n        if debug: print(\"\\ncompute Ans(X) by interpolation:\")\n        ans_polynomial = interpolation(Fp, quotient_answers)\n        if debug: print(\"Ans(X)'s coefficients: \", ans_polynomial.coefficients)\n\n        # compute Shake(X)\n        if debug: print(\"\\ncompute Shake(X) (by Ans(X) and quotient set) :\")\n        R.<X> = Fp[]\n        ans_X = R(ans_polynomial.coefficients)\n        if debug: print(\"Ans(X) = \", ans_X)\n        shake_X = 0 * X\n        for point in quotient_answers:\n            num_polynomial = R(ans_X - point[1])\n            den_polynomial = R(X - point[0])\n            # 进行多项式除法，获取商和余数\n            quotient, remainder = num_polynomial.quo_rem(den_polynomial)\n            # print(\"quotient = \", quotient)\n            # print(\"remainder = \", remainder)\n            assert remainder == 0 * X\n            shake_X = shake_X + quotient\n            # print(\"shake_X = \", shake_X)\n        if debug: print(\"Shake(X) = \", shake_X)\n\n        # print(shake_X.coefficients())\n        # shake_polynomial = DensePolynomial(shake_X.coefficients()) # 此方法返回非零系数\n        shake_polynomial = DensePolynomial(list(shake_X))\n\n\n        if debug: print(\"\\ncompute quotient polynomial\")\n        # compute quotient polynomial\n        quotient_polynomial = poly_quotient(Fp, g_poly, ans_polynomial, quotient_answers)\n        if debug: print(\"quotient_polynomial = \" , R(quotient_polynomial.coefficients))\n\n\n        # Deg Correction\n        if debug: print(\"\\nDegree Correction \")\n        coefficients_vec = []\n        for i in range(len(quotient_set) + 1):\n            coefficients_vec.append(comb_randomness ** i)\n\n        scaling_polynomial = DensePolynomial(coefficients_vec)\n        if debug: print(\"scaling_polynomial = \", R(scaling_polynomial.coefficients))\n\n        # next round witness polynomial\n        witness_polynomial = scaling_polynomial.mul(Fp, quotient_polynomial)\n        if debug: print(\"witness_polynomial = \", R(witness_polynomial.coefficients))\n\n        return {\n            \"witness_extended\": WitnessExtended (\n                domain = g_domain,\n                polynomial = witness_polynomial,\n                merkle_tree = g_merkle,\n                folded_evals = g_folded_evaluations,\n                num_round = witness.num_round + 1,\n                folding_randomness = folding_randomness\n            ),\n            \"round_proof\": RoundProof (\n                g_root = g_root,\n                betas = betas,\n                queries_to_prev = queries_to_prev,\n                ans_polynomial= ans_polynomial,\n                shake_polynomial= shake_polynomial\n            )\n        }\n    \n    def prove(self, witness: Witness, debug=False) -> Proof:\n       R.<X> = Fp[]\n       # 保证多项式的次数小于参数 starting_degree\n       assert witness.polynomial.degree() < self.parameters.parameters.starting_degree\n\n       transcript = MerlinTranscript(b\"STIR\")\n       transcript.append_message(b\"merkle_root\", witness.merkle_tree.root.encode('ascii'))\n       folding_randomness = transcript.challenge_bytes(b\"folding_randomness\", 1)[0] % Fp.order()\n       print(\"folding_randomness: \", folding_randomness)\n\n       witness = WitnessExtended (\n              domain= witness.domain,\n              polynomial= witness.polynomial,\n              merkle_tree= witness.merkle_tree,\n              folded_evals= witness.folded_evals,\n              num_round= 0,\n              folding_randomness= folding_randomness,\n       )\n\n       round_proofs = []\n       if debug: print(\"---------------------- begin proof ----------------------\")\n       if debug: print(\"num_rounds = \", self.parameters.num_rounds)\n       for _ in range(0,self.parameters.num_rounds):\n              result = self.round(Fp, transcript, witness, debug)\n              witness = result[\"witness_extended\"]\n              if debug: print(\"witness:\", witness)\n              if debug: print(\"round_proof:\", result[\"round_proof\"])\n              round_proofs.append(result[\"round_proof\"])\n              if debug: print(\" \")\n       \n       if debug: print(\"------------- final round ------------------\")\n       if debug: print(\"witness: \", witness)\n       if debug: print(R(witness.polynomial.coefficients))\n       final_polynomial = poly_fold(witness.polynomial, witness.folding_randomness, self.parameters.parameters.folding_factor)\n       \n       \n       final_randomness_indexs = []\n       num_rounds = self.parameters.num_rounds\n       if debug: print(\"num_rounds: \", num_rounds)\n       repetitions = self.parameters.repetitions\n       if debug: print(\"repetitions: \", repetitions)\n       final_repetitions = repetitions[num_rounds]\n       scaling_factor = witness.domain.size() / self.parameters.parameters.folding_factor\n       for i in range(final_repetitions):\n              index = transcript.challenge_bytes(b\"final_randomness_index\", 1)[0] % scaling_factor\n              final_randomness_indexs.append(index)\n       final_randomness_indexs = list(set(final_randomness_indexs))\n\n       queries_to_final_index = final_randomness_indexs\n       queries_to_final_ans = [witness.folded_evals[index] for index in final_randomness_indexs]\n       queries_to_final_proof = [witness.merkle_tree.get_authentication_path(index) for index in final_randomness_indexs]\n       queries_to_final = [queries_to_final_index, queries_to_final_ans, queries_to_final_proof]\n\n       if debug: print(\"round proofs: \", round_proofs)\n       if debug: print(\"final polynomial: \", R(final_polynomial.coefficients))\n       if debug: print(\"queries to final: \", queries_to_final)\n\n       return Proof(\n              round_proofs= round_proofs,\n              final_polynomial= final_polynomial,\n              queries_to_final= queries_to_final\n       )\n\n","type":"content","url":"/src/stir#stirprover","position":23},{"hierarchy":{"lvl1":"STIR","lvl2":"StirVerifier 类"},"type":"lvl2","url":"/src/stir#stirverifier","position":24},{"hierarchy":{"lvl1":"STIR","lvl2":"StirVerifier 类"},"content":"\n\nclass StirVerifier:\n    def __init__(self, parameters: FullParameters):\n        self.parameters = parameters\n    \n    def verify(self, commitment: Commitment, proof: Proof, domain: Domain, debug=False):\n        result = True\n        if debug: print(\"--------- verifier decision phase ---------\")\n        # 如果证明的最后的多项式的次数 + 1  > stopping_degree ，那么直接返回 false\n        if debug: print(\"\\n1. chekck final polynomial degree\")\n        if proof.final_polynomial.degree() + 1 > self.parameters.parameters.stopping_degree:\n            result = False\n            if debug: print(\"final polynomial degree not correct\", result)\n            return False\n        if debug: print(\"check answer: \", result)\n\n        # 验证 Merkle Tree 路径\n        current_root = commitment.root\n        if debug: print(\"\\n2. verify merkle path\")\n        for round_proof in proof.round_proofs:\n            if debug: print(\"current root: \", current_root)\n            if debug: print(\"round_proof: \", round_proof)\n            if debug: print(\"queries_to_prev\", round_proof.queries_to_prev)\n            if verify_multi_path(current_root, round_proof.queries_to_prev) == False: \n                result = False\n                if debug: print(\"Merkle Tree is not correct\")\n                return False\n            if debug: print(verify_multi_path(current_root, round_proof.queries_to_prev))\n            current_root = round_proof.g_root\n        \n        # 验证最后路径\n        if verify_multi_path(current_root, proof.queries_to_final) == False:\n            result = False\n            return False\n        if debug: print(verify_multi_path(current_root, proof.queries_to_final))\n        if debug: print(\"check answer: \", result)\n\n        if debug: print(\"\\n3. recompute randomness\")\n        transcript = MerlinTranscript(b\"STIR\")\n        transcript.append_message(b\"merkle_root\", commitment.root.encode('ascii'))\n        if debug: print(\"commitment root: \", commitment.root)\n        folding_randomness = transcript.challenge_bytes(b\"folding_randomness\", 1)[0] % Fp.order()\n        print(\"folding_randomness: \", folding_randomness)\n\n        verification_state = VerificationState(OracleType.Initial, domain, folding_randomness,0, 0, [],DensePolynomial([]))\n        \n        if debug: print(\"\\n4. verify round proof\")\n        for round_proof in proof.round_proofs:\n            result_verifier_round = self.verifier_round(transcript, round_proof, verification_state, debug)\n            if result_verifier_round[\"result\"] == False: \n                result = False\n                # return False\n            if debug: print(\"result: \", result_verifier_round[\"result\"])\n            verification_state = result_verifier_round[\"VerificationState\"]\n        \n        # 最后一轮检查\n        if debug: print(\"\\n5. verify final polynomial\")\n        final_repetitions = self.parameters.repetitions[self.parameters.num_rounds]\n        scaling_factor = verification_state.domain.size() / self.parameters.parameters.folding_factor\n        print(\"self.parameters: \", self.parameters)\n        final_randomness_indexs = []\n        for i in range(final_repetitions):\n                index = transcript.challenge_bytes(b\"final_randomness_index\", 1)[0] % scaling_factor\n                final_randomness_indexs.append(index)\n        final_randomness_indexs = list(set(final_randomness_indexs))\n        print(\"final_randomness_indexs: \", final_randomness_indexs)\n\n        oracle_answers = proof.queries_to_final[1]\n        print(\"oracle_answers: \", oracle_answers)\n\n        # compute folded answers\n        if debug: print(\"\\ncompute folded answers:\")\n        folded_answers = []\n        L_i_mins_1 = verification_state.domain.backing_domain\n        domain_stacks = stack_evaluations(L_i_mins_1, self.parameters.parameters.folding_factor)\n        print(\"domain_stacks: \", domain_stacks)\n\n        oracle_points = [domain_stacks[index] for index in final_randomness_indexs]\n        print(\"oracle_points: \", oracle_points)\n\n        # compute random_points\n        L_k = verification_state.domain.scale_generator_by(self.parameters.parameters.folding_factor)\n        final_randomness = [L_k.backing_domain[index] for index in final_randomness_indexs]\n        print(\"final_randomness: \", final_randomness)\n\n        new_oracle_points = []\n        new_oracle_answers = []\n        for i in range(len(oracle_points)):\n            new_oracle_point = []\n            new_oracle_answer = []\n            add_flag = True\n            for j in range(len(oracle_points[i])):\n                for quotient_point in verification_state.quotient_set:\n                    if quotient_point == oracle_points[i][j]: add_flag = False\n            if add_flag == True:\n                new_oracle_points.append(oracle_points[i])\n                new_oracle_answers.append(oracle_answers[i])\n\n        print(\"new_oracle_points: \", new_oracle_points)\n        print(\"new_oracle_answers: \", new_oracle_answers)\n\n\n        print(\"quotient_set: \", verification_state.quotient_set)\n        oracle_answers = compute_oracle_final(verification_state, proof, new_oracle_points, new_oracle_answers)\n        print(\"oracle_answers: \", oracle_answers)\n\n        if debug: print(\"folding_randomness: \", verification_state.folding_randomness)\n\n        if debug: print(\"\\ncompute Ans(X):\")\n        for i in range(len(new_oracle_points)):\n            interpolate_points = [[new_oracle_points[i][j], oracle_answers[i][j]] for j in range(len(new_oracle_points[i]))]    \n            print(\"interpolate_points: \", interpolate_points)\n            interpolation_polynomial = interpolation(Fp, interpolate_points)\n            ans = interpolation_polynomial.evaluate(verification_state.folding_randomness)\n            print(\"Ans(X): \", ans)\n            folded_answers.append([final_randomness[i],ans])\n        if debug: print(\"get folded_answers:\")\n        print(\"folded_answers: \", folded_answers)\n\n\n        for i in range(len(folded_answers)):\n            if proof.final_polynomial.evaluate(folded_answers[i][0]) != folded_answers[i][1]:\n                result = False\n        return result\n    \n    def verifier_round(self, transcript: MerlinTranscript, round_proof: RoundProof, verification_state: VerificationState, debug=False):\n        if debug: print(\"--------- begin verifier round ------------\")\n        flag = True\n        if debug: print(\"\\nrecompute randomness:\")\n        transcript.append_message(b\"merkle_root\", round_proof.g_root.encode('ascii'))\n        if debug: print(\"round_proof.g_root: \", round_proof.g_root)\n        odd_randomness = [transcript.challenge_bytes(b\"odd_randomness\", 1)[0] % Fp.order() for _ in range(self.parameters.ood_samples)]\n        odd_randomness = list(set(odd_randomness))\n        if debug: print(\"odd_randomness: \", odd_randomness)\n\n\n        for i in range(len(round_proof.betas)):\n            transcript.append_message(f'betas_at_{i}'.encode('ascii'), str(round_proof.betas[i]).encode('ascii'))\n        comb_randomness = transcript.challenge_bytes(b\"comb_randomness\", 1)[0] % Fp.order()\n        if debug: print(\"comb_randomness: \", comb_randomness)\n        new_folding_randomness = transcript.challenge_bytes(b\"folding_randomness\", 1)[0] % Fp.order()\n        if debug: print(\"new_folding_randomness: \", new_folding_randomness)\n\n        scaling_factor = verification_state.domain.size() / self.parameters.parameters.folding_factor\n        print(\"scaling_factor: \", scaling_factor)\n\n        num_repetitions = self.parameters.repetitions[verification_state.num_round]\n        print(\"num_repetitions: \", num_repetitions)\n        stir_randomness_indexes = []\n        for i in range(num_repetitions):\n            index = transcript.challenge_bytes(b\"stir_randomness_indexes\", 1)[0] % scaling_factor\n            stir_randomness_indexes.append(index)\n        print(\"stir_randomness_indexes: \", stir_randomness_indexes)\n\n        # 去重\n        if debug: print(\"\\nremove duplicates from the stir_randomness_indexes: \")\n        stir_randomness_indexes = list(set(stir_randomness_indexes))\n        L_k = verification_state.domain.scale_generator_by(self.parameters.parameters.folding_factor)\n        for odd_random in odd_randomness:\n            for index in stir_randomness_indexes:\n                if L_k.backing_domain[index] == odd_random:\n                    stir_randomness_indexes.pop(index)\n        print(\"stir_randomness_index: \", stir_randomness_indexes)\n\n        _shake_randomness = transcript.challenge_bytes(b\"_shake_randomness\", 1)[0] % Fp.order()\n        print(\"shake_randomness: \", _shake_randomness)\n\n        print(verification_state.domain.backing_domain)\n        L_i_mins_1 = verification_state.domain.backing_domain\n\n        domain_stacks = stack_evaluations(L_i_mins_1, self.parameters.parameters.folding_factor)\n        print(\"domain_stacks: \", domain_stacks)\n\n        oracle_points = [domain_stacks[index] for index in stir_randomness_indexes]\n        print(\"oracle_points: \", oracle_points)\n\n        # compute oracle answers\n        # oracle_answers = round_proof.queries_to_prev[1]\n        print(\"round_proof.queries_to_prev[1] = \", round_proof.queries_to_prev[1])\n        # if debug: print(\"oracle_answers\", oracle_answers)\n\n        oracle_answers = compute_oracle_answers(verification_state, round_proof, oracle_points)\n        \n        if debug: print(\"\\ncompute oracle answers:\")\n        if debug: print(\"oracle_answers\", oracle_answers)\n\n        L_k = verification_state.domain.scale_generator_by(self.parameters.parameters.folding_factor)\n        stir_randomness = [L_k.backing_domain[index] for index in stir_randomness_indexes]\n        print(\"stir_randomness: \", stir_randomness)\n\n        if debug: print(\"\\ncompute folded answers by oracle answers\")\n        folded_answers = []\n        for i in range(len(oracle_points)):\n            interpolate_points = [[oracle_points[i][j], oracle_answers[i][j]] for j in range(len(oracle_points[i]))]    \n            if debug: print(\"interpolate_points: \", interpolate_points)\n            interpolation_polynomial = interpolation(Fp, interpolate_points)\n            ans = interpolation_polynomial.evaluate(verification_state.folding_randomness)\n            if debug: print(\"interpolate polynomial: \", ans)\n            folded_answers.append([stir_randomness[i],ans])\n        if debug: print(\"folded_answers: \", folded_answers)\n\n        if debug: print(\"\\nverifier get quotient set and quotient answers:\")\n        quotient_answers = []\n        for i in range(len(round_proof.betas)):\n            quotient_answers.append([odd_randomness[i],round_proof.betas[i]])\n        for folded_answer in folded_answers: quotient_answers.append(folded_answer)\n        if debug: print(\"quotient_answers: \", quotient_answers)\n\n        quotient_set = []\n        for odd_random in odd_randomness: quotient_set.append(odd_random)\n        for stir_random in stir_randomness: quotient_set.append(stir_random)\n        if debug: print(\"quotient_set: \", quotient_set)\n\n\n        if debug: print(\"\\nverifier evaluate round_proof.ans(_shake_randomness) and round_proof.shake(_shake_randomness):\")\n        interpolating_polynomial = round_proof.ans_polynomial\n        print(\"interpolating_polynomial: \", interpolating_polynomial)\n        ans_eval = interpolating_polynomial.evaluate(_shake_randomness)\n        print(\"ans_eval: \", ans_eval)\n        shake_eval = round_proof.shake_polynomial.evaluate(_shake_randomness)\n        print(\"shake_eval: \", shake_eval)\n\n        if debug: print(\"\\nverifier compute Ans(X) and check Ans(shake_randomness) == proof.ans(shake_randomness):\")\n        ans_polynomial = interpolation(Fp, quotient_answers)\n        print(\"Ans(X)'s coefficients: \", ans_polynomial.coefficients)\n        verifier_compute_ans_eval = ans_polynomial.evaluate(_shake_randomness)\n        if ans_eval != verifier_compute_ans_eval: flag = False\n        if debug: print(\"check result: \", flag)\n\n        # compute Shake(X)\n        if debug: print(\"\\nnow veriifier compute Shake(X) by own\")\n        R.<X> = Fp[]\n        ans_X = R(ans_polynomial.coefficients)\n        if debug: print(\"ans(X) = \", ans_X)\n        shake_X = 0 * X\n        for point in quotient_answers:\n            num_polynomial = R(ans_X - point[1])\n            den_polynomial = R(X - point[0])\n            # 进行多项式除法，获取商和余数\n            quotient, remainder = num_polynomial.quo_rem(den_polynomial)\n            # print(\"quotient = \", quotient)\n            # print(\"remainder = \", remainder)\n            assert remainder == 0 * X\n            shake_X = shake_X + quotient\n            # print(\"shake_X = \", shake_X)\n        print(\"Shake(X) = \", shake_X)\n\n        # print(shake_X.coefficients())\n        # shake_polynomial = DensePolynomial(shake_X.coefficients()) # 此方法返回非零系数\n        shake_polynomial = DensePolynomial(list(shake_X))\n\n        if debug: print(\"\\nverifier check Shake(_shake_randomness) == proof.shake(_shake_randomness)\")\n        verifier_compute_shake_eval = shake_polynomial.evaluate(_shake_randomness)\n        if debug: print(\"verifier_compute_shake_eval: \", verifier_compute_shake_eval)\n        if verifier_compute_shake_eval != shake_eval: flag = False\n        if debug: print(\"check answer: \", flag)\n\n        return {\n            \"VerificationState\": VerificationState(\n            oracle=OracleType.Virtual,\n            domain=verification_state.domain.scale_with_offset(2),\n            folding_randomness=new_folding_randomness,\n            num_round=verification_state.num_round + 1,\n            comb_randomness=comb_randomness,\n            quotient_set=quotient_set,\n            ans_polynomial=ans_polynomial\n            ),\n            \"result\": flag\n        }\n\n","type":"content","url":"/src/stir#stirverifier","position":25},{"hierarchy":{"lvl1":"STIR","lvl3":"Verifier 需要的工具函数","lvl2":"StirVerifier 类"},"type":"lvl3","url":"/src/stir#verifier","position":26},{"hierarchy":{"lvl1":"STIR","lvl3":"Verifier 需要的工具函数","lvl2":"StirVerifier 类"},"content":"\n\ndef compute_g_prime_evals(Fp, quotient_set, g_hat_evaluations, ans_polynomial: DensePolynomial, points):\n    '''\n    quotient_set = G_i\n    g_hat_evaluations = \\hat{g_i}\n    '''\n    g_prime_evaluations = []\n    for i in range(len(points)):\n        product = 1\n        point = points[i] # x\n        for g in quotient_set:\n            product *= point - g\n            # print(\"product: \", product)\n        ans_eval_point = ans_polynomial.evaluate(point)\n        g_prime_evaluations.append(Fp((g_hat_evaluations[i] - ans_eval_point)/product))\n    return g_prime_evaluations\n\ndef compute_oracle_answers(verification_state: VerificationState, round_proof: RoundProof, oracle_points):\n    oracle_answers = []\n    match verification_state.oracle:\n        case OracleType.Initial:\n            # print(\"OracleType.Initial\")\n            oracle_answers = round_proof.queries_to_prev[1]\n            # print(\"oracle_answers: \", oracle_answers)\n        case OracleType.Virtual:\n            # print(\"OracleType.Virtual\")\n            # queries_to_prev_ans = [56, 149, 106, 255]\n            # 先测试一个\n            queries_to_prev = round_proof.queries_to_prev[1]\n            # print(\"queries_to_prev: \", queries_to_prev)\n            for j in range(len(queries_to_prev)):\n                oracle_answer = []\n                g_hat_evaluations = round_proof.queries_to_prev[1][j]\n                g_prime_evals = compute_g_prime_evals(Fp, verification_state.quotient_set, g_hat_evaluations, verification_state.ans_polynomial, oracle_points[j])\n                # queries_to_prev_ans = round_proof.queries_to_prev[1][0]\n                print(\"\\ncompute g'(x) evals: \")\n                print(\"g_prime_evals: \", g_prime_evals)\n                \n                correct_degree = len(verification_state.quotient_set)\n                # print(\"correct_degree: \", correct_degree)\n                for i in range(len(oracle_points[j])):\n                    point_x = oracle_points[j][i]\n                    r = verification_state.comb_randomness\n                    # print(\"combine randomness: \", r)\n                    f_x = g_prime_evals[i]\n                    if Fp(point_x * r) == Fp(1):\n                        oracle_answer.append(f_x*(correct_degree+1))\n                    else:\n                        common_factor_inverse = Fp(1 - r * point_x).inverse()\n                        e_plus_one = correct_degree + 1\n                        r_times_x = Fp(r * point_x)\n                        a = Fp(f_x) * (Fp(1) - Fp(r_times_x)**Fp(e_plus_one))\n                        answer = Fp(a * common_factor_inverse)\n                        # print(\"answer: \", answer)\n                        oracle_answer.append(answer)\n                oracle_answers.append(oracle_answer)\n        case _:\n            print(\"Wrong case!\")\n    return oracle_answers\n\n\ndef compute_oracle_final(verification_state: VerificationState, proof: Proof, oracle_points, new_oracle_answers):\n    oracle_answers = []\n    queries_to_prev = new_oracle_answers\n    # print(\"queries_to_final: \", queries_to_prev)\n    for j in range(len(queries_to_prev)):\n        oracle_answer = []\n        g_hat_evaluations = new_oracle_answers[j]\n        g_prime_evals = compute_g_prime_evals(Fp, verification_state.quotient_set, g_hat_evaluations, verification_state.ans_polynomial, oracle_points[j])\n        # queries_to_prev_ans = round_proof.queries_to_prev[1][0]\n        print(\"\\ncompute g'(x) evals: \")\n        print(\"g_prime_evals: \", g_prime_evals)\n        \n        correct_degree = len(verification_state.quotient_set)\n        # print(\"correct_degree: \", correct_degree)\n        for i in range(len(oracle_points[j])):\n            point_x = oracle_points[j][i]\n            r = verification_state.comb_randomness\n            # print(\"combine randomness: \", r)\n            f_x = g_prime_evals[i]\n            if Fp(point_x * r) == Fp(1):\n                oracle_answer.append(f_x*(correct_degree+1))\n            else:\n                common_factor_inverse = Fp(1 - r * point_x).inverse()\n                e_plus_one = correct_degree + 1\n                r_times_x = Fp(r * point_x)\n                a = Fp(f_x) * (Fp(1) - Fp(r_times_x)**Fp(e_plus_one))\n                answer = Fp(a * common_factor_inverse)\n                # print(\"answer: \", answer)\n                oracle_answer.append(answer)\n        oracle_answers.append(oracle_answer)\n    return oracle_answers\n\n","type":"content","url":"/src/stir#verifier","position":27},{"hierarchy":{"lvl1":"STIR","lvl2":"Example 1 ：Test StirProver prove function"},"type":"lvl2","url":"/src/stir#example-1-test-stirprover-prove-function","position":28},{"hierarchy":{"lvl1":"STIR","lvl2":"Example 1 ：Test StirProver prove function"},"content":"\n\np = 257\nFp = GF(p)\ng = Fp.multiplicative_generator() # 生成元\norder = g.multiplicative_order()  # Fp 生成元的阶\nsubgroup_generator = g^(order // 64) # 一个大小为 64 的群\nroot_of_unity = subgroup_generator\nroot_of_unity_inv =  subgroup_generator.inverse()\nbacking_domain =  [subgroup_generator^i for i in range(64)]\noffset = 1\n\ndomain = Domain(root_of_unity, root_of_unity_inv, offset, backing_domain)\nprint(\"domain: \", domain)\n\ncoefficients = squeeze_field_elements(Fp, 64)\nf0 = DensePolynomial(coefficients)\nf0.coefficients\n\nf0_evals = f0.evaluate_over_domain(domain.backing_domain)\nprint(f0_evals)\n\nfolding_factor = 4\nfolded_evals = stack_evaluations(f0_evals, folding_factor)\nprint(\"folded_evals for f_0 = \", folded_evals)\n\nf0_merkle_tree = MerkleTree(folded_evals)\nprint(f0_merkle_tree.root)\n\nparameters = Parameters(security_level=10, \n                        protocol_security_level=128,\n                        starting_degree=64,\n                        stopping_degree=1,\n                        folding_factor=4,\n                        starting_rate=2,\n                        soundness_type=SoundnessType.Conjecture,\n                        fiat_shamir_config=MerlinTranscript(b\"initial\")\n                        )\nfull_parameters = FullParameters.from_parameters(parameters)\nprint(\"full_parameters: \", full_parameters)\nstir_prover = StirProver(full_parameters)   \n\n\nans = stir_prover.commit(domain, f0)\ncommitment = ans[\"commitment\"]\nprint(commitment.root)\n\n# stir_prover.round(Fp, transcript, witness_extend, True)\nwitness = Witness(domain, f0, f0_merkle_tree, folded_evals)\nproof = stir_prover.prove(witness, True)\n\nstir_verifier = StirVerifier(full_parameters)\nresult = stir_verifier.verify(commitment, proof, domain, debug=True)\nprint(\"result: \", result)","type":"content","url":"/src/stir#example-1-test-stirprover-prove-function","position":29},{"hierarchy":{"lvl1":"Proving demo"},"type":"lvl1","url":"/src/whir-pcs","position":0},{"hierarchy":{"lvl1":"Proving demo"},"content":"F193 = GF(64*3+1)\nTWO_ADICITY = 6\nORDER = 64*3+1\nMULTIPLICATIVE_GENERATOR = F193.primitive_element()\nROOT_OF_UNITY = F193(MULTIPLICATIVE_GENERATOR**3) # 125\nF193.primitive_element()\nR193.<X, A, B, X0, X1, X2, X3, X4, X5, A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15> = PolynomialRing(F193)\n\nfrom typing import Union, Optional, TypeVar\nfrom random import randint, Random\nfrom utils import prime_field_inv, is_power_of_two, log_2, next_power_of_two, bit_reverse, Scalar, inner_product\n\n\nfrom ff.tiny import F193\nFp = F193\n\n# class Fp:\n#     field_modulus = ORDER\n\n#     ROOT_OF_UNITY = ROOT_OF_UNITY\n#     MULTIPLICATIVE_GENERATOR = MULTIPLICATIVE_GENERATOR\n#     TWO_ADICITY = TWO_ADICITY\n\n#     def __init__(self, val: Integer | int) -> None:\n#         if self.field_modulus is None:\n#             raise AttributeError(\"Field Modulus hasn't been specified\")\n\n#         if isinstance(val, Fp):\n#             self.n = val.n % self.field_modulus\n#         elif isinstance(val, Integer):\n#             self.n = val % self.field_modulus\n#         elif Integer(val) == Integer(val):\n#             self.n = Integer(val) % self.field_modulus\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(val))\n#             )\n\n#     def __add__(self, other: Union[Integer, \"Fp\"]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)((self.n + on) % self.field_modulus)\n\n#     def __mul__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)((self.n * on) % self.field_modulus)\n\n#     def __rmul__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         return self * other\n\n#     def __radd__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         return self + other\n\n#     def __rsub__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)((on - self.n) % self.field_modulus)\n\n#     def __sub__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)((self.n - on) % self.field_modulus)\n\n#     def __div__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)(\n#             self.n * prime_field_inv(on, self.field_modulus) % self.field_modulus\n#         )\n\n#     def __truediv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         return self.__div__(other)\n\n#     def __rdiv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         if isinstance(other, Fp):\n#             on = other.n\n#         elif isinstance(other, Integer):\n#             on = other\n#         elif Integer(other) == Integer(other):\n#             on = Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#         return type(self)(\n#             prime_field_inv(self.n, self.field_modulus) * on % self.field_modulus\n#         )\n\n#     def __rtruediv__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#         return self.__rdiv__(other)\n\n#     # def __pow__(self, other: Union[\"Fp\", Integer]) -> \"Fp\":\n#     #     if other == 0:\n#     #         return type(self)(1)\n#     #     elif other == 1:\n#     #         return type(self)(self.n)\n#     #     elif other % 2 == 0:\n#     #         return (self * self) ** (other // 2)\n#     #     else:\n#     #         return ((self * self) ** (other // 2)) * self\n\n#     def __eq__(self, other: Union[\"Fp\", Integer]) -> bool:\n#         if isinstance(other, Fp):\n#             return self.n == other.n\n#         elif isinstance(other, Integer):\n#             return self.n == other\n#         elif Integer(other) == Integer(other):\n#             return self.n == Integer(other)\n#         else:\n#             raise TypeError(\n#                 \"Expected an int or Fp object, but got object of type {}\"\n#                 .format(type(other))\n#             )\n\n#     def __ne__(self, other: Union[\"Fp\", Integer]) -> bool:\n#         return not self == other\n\n#     def __neg__(self) -> \"Fp\":\n#         return type(self)(-self.n)\n\n#     def __str__(self) -> str:\n#         return self.repr()\n        \n#     def __repr__(self) -> str:\n#         return self.repr()\n    \n#     # Override the default (inefficient) __pow__ function in py_ecc.fields.field_elements.FQ\n#     def __pow__(self: \"Fp\", other: int) -> \"Fp\":\n#         return type(self)(pow(self.n, other, self.field_modulus))\n    \n#     # def __repr__(self) -> str:\n#     #     return repr(self.n)\n\n#     def __int__(self) -> int:\n#         return self.n\n\n#     @classmethod\n#     def one(cls) -> \"Fp\":\n#         return cls(1)\n\n#     @classmethod\n#     def zero(cls) -> \"Fp\":\n#         return cls(0)\n    \n#     @classmethod\n#     def neg_one(cls) -> \"Fp\":\n#         return cls(cls.field_modulus - 1)\n\n#     @classmethod\n#     def rand(cls, rndg: Optional[Random] = None) -> \"Fp\":\n#         if rndg is None:\n#             return cls(randint(1, cls.field_modulus - 1))\n#         return cls(rndg.randint(1, cls.field_modulus - 1))\n    \n#     @classmethod\n#     def random(cls) -> \"Fp\":\n#         return cls.rand()\n    \n#     @classmethod\n#     def rands(cls, rndg: Random, n: int) -> list[\"Fp\"]:\n#         return [cls(rndg.randint(1, cls.field_modulus - 1)) for _ in range(n)]\n    \n#     @classmethod\n#     def from_bytes(cls, b: bytes) -> \"Fp\":\n#         i = int.from_bytes(b, \"big\")\n#         return cls(i)\n    \n#     def inv(self) -> \"Fp\":\n#         return Fp(prime_field_inv(self.n, self.field_modulus))\n    \n#     def repr(self) -> str:\n#         k = self.field_modulus // 2\n#         if self.n < k:\n#             return f\"{self.n}\"\n#         else:\n#             return f\"-{self.field_modulus - self.n}\"\n        \n#     def exp(self: \"Fp\", other: int) -> \"Fp\":\n#         return type(self)(pow(self.n, other, self.field_modulus))\n    \n#     @classmethod\n#     def compute_root_of_unity(cls) -> \"Fp\":\n#         return cls(pow(cls.MULTIPLICATIVE_GENERATOR, ((cls.field_modulus - 1) // 2 ** cls.TWO_ADICITY), cls.field_modulus))\n    \n#     @classmethod\n#     def root_of_unity(cls) -> \"Fp\":\n#         return cls(cls.ROOT_OF_UNITY)\n    \n#     @classmethod\n#     def multiplicative_generator(cls) -> \"Fp\":\n#         return cls(cls.MULTIPLICATIVE_GENERATOR)\n\n#     @classmethod\n#     def nth_root_of_unity(cls, n: int) -> \"Fp\":\n#         assert is_power_of_two(n), \"n must be a power of two\"\n#         return cls(pow(cls.ROOT_OF_UNITY, 2**(cls.TWO_ADICITY - log_2(n)), cls.field_modulus))\n\nfrom unipoly2 import UniPolynomial, UniPolynomialWithFft, bit_reverse_permutation\nfrom mle2 import MLEPolynomial\nUniPolynomial.set_field_type(Fp)\nUniPolynomialWithFft.set_field_type(Fp)\nMLEPolynomial.set_field_type(Fp)\nfrom merkle import MerkleTree\nfrom transcript import MerlinTranscript\n\nf_mle = MLEPolynomial([Fp(1), Fp(3), Fp(2), Fp(1),\n                    Fp(2), Fp(-2), Fp(1), Fp(0),\n                    Fp(-1), Fp(2), Fp(3), Fp(1),\n                    Fp(3), Fp(1), Fp(-2), Fp(3),\n                    Fp(0), Fp(-1), Fp(-2), Fp(3),\n                    ], \n                    5)\nf_coeffs = MLEPolynomial.compute_coeffs_from_evals(f_mle.evals)\nprint(f\"len(f)={len(f_coeffs)}, f_coeffs={f_coeffs}\")\n\nus = [Fp(2), Fp(-1), Fp(2), Fp(-2), Fp(3)]\nv = f_mle.evaluate(us)\nv\n\ndef get_order_of_omega(omega: Fp, N: int) -> Fp:\n    for i in range(log_2(N)+1):\n        if omega**(2**i) == Fp.one():\n            return 2**i\n    return None\n\nfrom functools import reduce\nfrom operator import mul\n\ndef eq_eval(r_vec, u_vec):\n    assert len(r_vec) == len(u_vec), f\"len(r_vec) = {len(r_vec)}, len(u_vec) = {len(u_vec)}\"\n    factors = [(Fp(1) - r) * (1 - u) + r * u for r, u in zip(r_vec, u_vec)]\n\n    return reduce(mul, factors, Fp(1))\n\ndef rs_encode(f: list[Fp], coset: Fp, k:int, N: int) -> list[Fp]:\n    assert len(f) <= k, f\"len(f) = {len(f)}, k = {k}\"\n    assert is_power_of_two(N), f\"N = {N} is not a power of two\"\n    assert is_power_of_two(k), f\"k = {k} is not a power of two\"\n    assert N >= k, f\"N = {N} must be greater than k = {k}\"\n\n    omega = Fp.nth_root_of_unity(N)\n    print(f\"encode> k={k}, N={N}, omega = {omega}\")\n    k = log_2(N)\n    vec = f + [Fp.zero()] * (N - len(f))\n    return UniPolynomialWithFft.fft_coset_rbo(vec, coset, k, omega)\n\ndef compute_power_of_2_powers(alpha: Fp, n: int) -> list[Fp]:\n        return [alpha**(2**i) for i in range(n)]\nz0 = Fp(78)\npowers = compute_power_of_2_powers(z0, 3)\npowers\n\n\n\ndef compute_powers(alpha: Fp, n: int) -> list[Fp]:\n        return [alpha**i for i in range(n)]\nr0 = Fp(2)\npowers = compute_powers(r0, 8)\npowers\n\n\ndef eq_sum(eqs: list[list[Fp]], gamma: Fp) -> list[Fp]:\n    n = len(eqs[0])\n    new_eq = [Fp(0)] * n\n    for i, eq in enumerate(eqs):\n        assert len(eq) == n, f\"len(eq) = {len(eq)}, n = {n}\"\n        for j in range(n):\n            new_eq[j] += eq[j] * gamma**i\n\n    return new_eq\n\ndef fold_code(code: list[Fp], \n              r_vec: list[Fp], \n              coset: Fp, \n              twiddles, debug=0) -> list[Fp]:\n    k = len(r_vec)\n    assert is_power_of_two(len(code)), \"n must be a power of two\"\n    coseti = coset\n    \n    for round_idx in range(k):\n        code_folded = [(code[2*i] + code[2*i+1]) / 2 \n                        + r_vec[round_idx] * (code[2*i] - code[2*i+1]) / (2 * coseti * twiddles[i]) \n                        for i in range(len(code)//2)]\n        \n        if debug > 1:\n            print(\"P> show code folding\")\n            for i in range(len(code)//2):\n                left = code[2*i]\n                right = code[2*i+1]\n                print(f\"P> 📒: {left} + {right} => {code_folded[i]}, r={r_vec[round_idx]},w={twiddles[i]},coset={coseti}\")\n        coseti *= coseti\n        code = code_folded\n    return code_folded\n\ndef fold_code_chunk(code_chunk: list[Fp], \n              k: int,\n              r_vec: list[Fp], \n              coset: Fp, \n              start_idx: int, \n              size: int, \n              twiddles,\n              debug=0) -> list[Fp]:\n    assert len(code_chunk) == size, f\"len(code_chunk) = {len(code_chunk)}, size = {size}\"\n    if debug > 0: print(f\"P> start_idx = {start_idx}, size = {size}\")\n    if debug > 0: print(f\"P> twiddles = {twiddles}\")\n    coset_round = coset\n    twiddles_start_idx = start_idx // 2\n    for round_idx in range(k):\n        if debug > 0: print(f\"P> {round_idx}-th round, twiddles_start_idx = {twiddles_start_idx}\")\n        code_chunk_folded = [(code_chunk[2*i] + code_chunk[2*i+1]) / 2 \n                        + r_vec[round_idx] * (code_chunk[2*i] - code_chunk[2*i+1]) / (2 * coset_round * twiddles[twiddles_start_idx+i]) \n                        for i in range(len(code_chunk)//2)]\n\n        if debug > 1:\n            print(\"P> show code chunk folding\")\n            for i in range(len(code_chunk)//2):\n                left = code_chunk[2*i]\n                right = code_chunk[2*i+1]\n                print(f\"P> 📒: {left} + {right} => {code_chunk_folded[i]}, r={r_vec[round_idx]},w={twiddles[twiddles_start_idx+i]},coset={coset_round}\")\n\n        coset_round *= coset_round\n        code_chunk = code_chunk_folded\n        twiddles_start_idx = twiddles_start_idx // 2\n    return code_chunk_folded\n\n","type":"content","url":"/src/whir-pcs","position":1},{"hierarchy":{"lvl1":"Proving demo"},"type":"lvl1","url":"/src/whir-pcs#proving-demo","position":2},{"hierarchy":{"lvl1":"Proving demo"},"content":"\n\n","type":"content","url":"/src/whir-pcs#proving-demo","position":3},{"hierarchy":{"lvl1":"Proving demo","lvl2":"1. parameters setup"},"type":"lvl2","url":"/src/whir-pcs#id-1-parameters-setup","position":4},{"hierarchy":{"lvl1":"Proving demo","lvl2":"1. parameters setup"},"content":"\n\nf0_len = len(f_mle.evals)\ndebug = 2\n# coset = Fp(1)\ncoset = Fp.multiplicative_generator()\nk = 2\nfolding_factor = 1\ndomain_folding_factor = 1\nblowup_factor = 2\nt = 2  # number of shift queries, now it is fixed to 1\n\nf0_code_len = f0_len * blowup_factor\ntwiddles = UniPolynomialWithFft.precompute_twiddles_for_fft(f0_code_len, is_bit_reversed=True)\n\n\nomega = Fp.nth_root_of_unity(f0_code_len)\n\nomega^f0_code_len == Fp(1), omega^(f0_code_len//2) == Fp(-1)\n\n\n","type":"content","url":"/src/whir-pcs#id-1-parameters-setup","position":5},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Commit to the polynomial f_0"},"type":"lvl2","url":"/src/whir-pcs#commit-to-the-polynomial-f-0","position":6},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Commit to the polynomial f_0"},"content":"\n\nf0 = f_mle.evals\nf0_code = rs_encode(f_coeffs, coset, len(f_coeffs), f0_code_len)\nf0_cm = MerkleTree(f0_code)\n\nif debug > 1:\n    print(f\"check rs_encode\")\n    f_uni = UniPolynomial(f_coeffs)\n    D = compute_powers(omega, len(f0_code))\n    D_rbo = bit_reverse_permutation(D)\n    for i in range(len(f0_code)):\n        w = D_rbo[i]\n        assert f_uni.evaluate(w * coset) == f0_code[i], \\\n            f\"f_uni.evaluate(D_rbo[{i}]) = {f_uni.evaluate(w)}, f0_code[{i}] = {f0_code[i]}\"\n        w_pow = compute_power_of_2_powers(w * coset, f_mle.num_var)\n        assert f_mle.evaluate(w_pow) == f0_code[i], \\\n            f\"f_mle.evaluate(D_rbo[{i}]) = {f_mle.evaluate(w_pow)}, f0_code[{i}] = {f0_code[i]}\"\n    print(f\"check rs_encode passed\")\nf0_code\n\neq0 = MLEPolynomial.eqs_over_hypercube(us)\neq0_mle = MLEPolynomial(eq0, log_2(f0_len))\neq0_mle\n\nstr(v).encode()\n\ntr = MerlinTranscript(b\"whir-rs-pcs\")\nprint(f\"P> tr.state = {tr.state}\")\n# tr = tr.fork(b\"fork\")\ntr.absorb(b\"f_code_merkle_root\", f0_cm.root)\nprint(f\"P> tr.state = {tr.state}\")\ntr.absorb(b\"point\", us)\nprint(f\"P> tr.state = {tr.state}\")\ntr.absorb(b\"value\", v)\nprint(f\"P> tr.state = {tr.state}\")\n\n\nall_r_vec = []\ntotal_k = log_2(f0_len)\nremain_k = total_k\nacc_k = 0\nall_z_y_vec = []\nall_gamma = []\n\n","type":"content","url":"/src/whir-pcs#commit-to-the-polynomial-f-0","position":7},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First iteration, Sumcheck round"},"type":"lvl2","url":"/src/whir-pcs#first-iteration-sumcheck-round","position":8},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First iteration, Sumcheck round"},"content":"Inputs:\n\ni: is the index of the current iteration, i=1, 2, 3, \\ldots, M-1\n\nf_{i-1}: the polynomial to be folded\n\neq^*_{i-1}: the polynomial to be folded, eq_0 if i=1\n\nOutputs:\n\nf_i: the folded polynomial\n\n\\vec{r}_i: the random numbers generated by the verifier\n\n\\{h_j\\}: the sumcheck results, j \\in [0, 2^k)\n\nK_A: the number of variables of the folded polynomial, acc_k\n\nK_R: the number of variables of the remaining polynomial, remain_kK = K_A + K_R\n\nProtocol Steps:\n\nFold f_{i-1} to f_{i}f_i = \\mathsf{fold}(f_{i-1}, \\vec{r}_i)\n\nFold eq^*_{i-1} to eq_{i}eq_i = \\mathsf{fold}(eq^*_{i-1}, \\vec{r}_i)\n\nGenerate a new sum_checked (v_i)v_i = \\sum_{\\vec{b}\\in\\{0,1\\}^{K_R}} \\tilde{f}_i(\\vec{b}) \\cdot eq_i(\\vec{b})\n\nsum_checked = v\nhalf = f0_len >> 1\nr_vec = []\nsumcheck_h_vec = []\nf1 = f0.copy()\neq1 = eq0.copy()\nfor i in range(k):\n    f_even = f1[::2]\n    f_odd = f1[1::2]\n    eq_even = eq1[::2]\n    eq_odd = eq1[1::2]\n    # construct h(X)\n    h_eval_at_0 = sum([f_even[j] * eq_even[j] for j in range(half)], Fp.zero())\n    h_eval_at_1 = sum([f_odd[j] * eq_odd[j] for j in range(half)], Fp.zero())\n    h_eval_at_2 = sum([ (2 * f_odd[j] - f_even[j]) * (2 * eq_odd[j] - eq_even[j]) for j in range(half)])\n    h = [h_eval_at_0, h_eval_at_1, h_eval_at_2]\n\n    sumcheck_h_vec.append(h)\n\n    tr.absorb(b\"h(X)\", h)\n\n    if debug > 0:\n        print(f\"P> h = {h}\")\n        \n    # check sum\n    assert h_eval_at_0 + h_eval_at_1 == sum_checked, \\\n        f\"h_eval_at_0 + h_eval_at_1 = {h_eval_at_0 + h_eval_at_1}, sum_checked = {sum_checked}\"\n    \n    # Receive a random number from the verifier\n    r = tr.squeeze(Fp, b\"r\", 4)\n    if debug > 0:\n        print(f\"P> r[{i}] = {r}\")\n    r_vec.append(r)\n    \n    # fold f\n\n    f_folded = [(Fp(1) - r) * f_even[i] + r * f_odd[i] for i in range(half)]\n    eq_folded = [(Fp(1) - r) * eq_even[i] + r * eq_odd[i] for i in range(half)]\n\n    f1 = f_folded\n    eq1 = eq_folded\n\n    # update sumcheck for the next round\n    sum_checked = UniPolynomial.evaluate_from_evals(h, \n                r, [Fp(0), Fp(1), Fp(2)])\n    half >>= 1\n\n\n## End of the big loop for `i`\n\nf1_len = len(f1)\nf1_mle = MLEPolynomial(f1, log_2(f1_len))\n\n# check sumcheck\nif debug > 0:\n    print(f\"P> check sum_checked = {sum_checked}\")\n    # eq_mle_post = MLEPolynomial(eq1, log_2(f_len)-k)\n    # eq_mle_post_at_r = eq_mle_post.evaluate(r_vec)\n    eval = Fp.zero()\n    for i in range(f0_len // (2**k)):\n        eval += f1[i] * eq1[i]\n    assert sum_checked == eval, \\\n        f\"sum_checked = {sum_checked}, eval = {eval}\"\n    print(f\"P> sum_checked = {eval} = Σ_b f1(b) * eq(b,us)\")\n    print(f\"P> check sum_checked passed\")\n\nprint(f\"P> sum_checked = {sum_checked}, r_vec = {r_vec}, f1 = {f1}, eq1 = {eq1}\")\n\n\n","type":"content","url":"/src/whir-pcs#first-iteration-sumcheck-round","position":9},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First iteration, FRI round"},"type":"lvl2","url":"/src/whir-pcs#first-iteration-fri-round","position":10},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First iteration, FRI round"},"content":"Inputs:\n\ni: is the index of the current iteration, i=1, 2, 3, \\ldots, M-1\n\n\\vec{r}_i: the random numbers generated by the verifier.\n\nC_{i-1}: the code of the polynomial f_{i-1} generated in the preview iteration.\n\ng: the coset factor for RS-codes, i.e. C_i = f_i(X)\\mid_{L_i}, L_i=gH_{i}\n\nOutputs:\n\nK_A: the number of variables of the folded polynomial, acc_k\n\nK_R: the number of variables of the remaining polynomial, remain_k\n\nC_i: the code of the polynomial f_i generated in the previous sumcheck round.K = K_A + K_R\n\n(z_0, y_0): the out-of-domain query and its evaluation\n\n\\{(z_1, y_1), (z_2, y_2), \\ldots, (z_t, y_t)\\}: the shift queries and their evaluations\n\n\\{\\vec{c}_1, \\vec{c}_2, \\ldots, \\vec{c}_t\\}: the query replies from C_{i-1}\n\nSteps:\n\nConstruct code C_i from f_i\n\nSample z_0\\leftarrow\\mathbb{F}_p and evaluate \\tilde{f}_i(\\mathsf{pow}(z_0, K_R))=y_0\n\nSample \\{z_1, z_2, \\ldots, z_t\\}\\leftarrow L^{2^k}_{i-1} and evaluate \\tilde{f}_i(\\mathsf{pow}(z_j, K_R))=y_j\n\nCollect query replies \\{\\vec{c}_1, \\vec{c}_2, \\ldots, \\vec{c}_t\\} from C_{i-1}, s.t.f_{i-1}(x_{i,j})=c_{i,j}\n\nwhere \\vec{x}_i are query-locations of \\vec{c}_i in C_{i-1}.\n\nThe challenged evaluations \\{y_0, y_1, \\ldots, y_t\\} will be computed by the query-replies \\{\\vec{c}_1, \\vec{c}_2, \\ldots, \\vec{c}_t\\} by the verifier:\\mathsf{fold}(\\vec{c}_j, \\vec{r}_i) = \\vec{y}_j\n\nTherefore, the prover doesn’t have to send \\{\\vec{y}_1, \\vec{y}_2, \\ldots, \\vec{y}_t\\} to the verifier.\n\nThe verifier needs to check every sampled evaluations are correct, specifically:\\tilde{f}_i(\\mathsf{pow}(z_j, K_R))=y_j, \\qquad j\\in\\{0, 1, \\ldots, t\\}\n\nThese jobs are done by K_R-fold sumchecks in the following iterations along with the initial evaluation wrt. \\tilde{f}_0(\\vec{u}).\n\nCreate f1_code, whose length is 16\n\nf1_coeffs = MLEPolynomial.compute_coeffs_from_evals(f1)\nf1_code = rs_encode(f1_coeffs, coset, len(f1_coeffs), f0_code_len//2)\nf1_code_cm = MerkleTree(f1_code)\n\nif debug > 0:\n    print(f\"P> Check f1_code \")\n    print(f\"P> f1_code = {f1_code}\")\n    new_omega = Fp.nth_root_of_unity(len(f1_code))\n    D = compute_powers(new_omega, len(f1_code))\n    D_rbo = bit_reverse_permutation(D)\n    print(f\"P> D = {D}\")\n    print(f\"P> D_rbo = {D_rbo}\")\n    \n    f1_uni = UniPolynomialWithFft(f1_coeffs)\n    evals = [f1_uni.evaluate(coset * d) for d in D_rbo]\n    assert evals == f1_code, f\"evals={evals}, f1_code={f1_code}\"\n\n    for i in range(len(D_rbo)):\n        w = D_rbo[i]\n        w_pow = compute_power_of_2_powers(w * coset, f1_mle.num_var)\n        assert f1_code[i] == f1_mle.evaluate(w_pow), \\\n            f\"f1_code[{i}] = {f1_code[i]}, f1_mle.evaluate(w_pow) = {f1_mle.evaluate(w_pow)}\"\n    print(f\"P> Check f1_code passed\")\n\ntr.absorb(b\"f_folded_code_cm\", f1_code_cm.root)\n\n# out-of-domain query, and its evaluation\n\nz0 = tr.squeeze(Fp, b\"z0\", 4)\ny0 = f1_mle.evaluate(compute_power_of_2_powers(z0, log_2(len(f1))))\nprint(f\"P> z0 = {z0}, y0 = {y0}\")\n\ntr.absorb(b\"y0\", y0)\n\n# construct query indices\nmax_queries_try = 1000\nz_idx_vec = []\nfor i in range(max_queries_try):\n    z_idx = tr.squeeze(int, b\"zi\", 4) % (len(f0_code)//2**k)\n    if z_idx not in z_idx_vec:\n        z_idx_vec.append(z_idx)\n    else:\n        continue\n    if len(z_idx_vec) >= t:\n        break\nassert len(z_idx_vec) == t, f\"len(z_idx_vec) = {len(z_idx_vec)}, t = {t}\"\nprint(f\"P> z_idx_vec = {z_idx_vec}\")\n\n# shift queries and their evaluations\nz_vec, y_vec = [], []\nfor i in range(t):\n    # new_omega is the root of unity for `f0_code_folded`, while omega is the root of unity for `f0_code`\n    omega_code_folded = omega**(2**k)\n\n    zi = omega_code_folded** bit_reverse(z_idx_vec[i], log_2(len(f0_code)//2**k))\n    # print(f\"P> zi = {zi}\")\n    z_vec.append(zi)\n    zi_pow = compute_power_of_2_powers(zi * coset**(2**k), f1_mle.num_var)\n    # print(f\"P> z_pow = {z_pow}\")\n    y_vec.append(f1_mle.evaluate(zi_pow))\n\nprint(f\"P> z_vec = {z_vec}, y_vec = {y_vec}\")\n\n# collect query replies $c_j$, which are the code chunks of $C_{i-1}$ and can be used to compute $y_i$\nquery_replies = []\nfor i in range(t):\n    idx = z_idx_vec[i]\n    start_idx = idx*2**k\n    end_idx = (idx+1)*2**k\n    query_reply = f0_code[start_idx: end_idx]\n    query_replies.append(query_reply)\n\n    if debug > 1:\n        print(f\"P> check query-reply-{i} = {query_reply}\")\n        # print(f\"P> query_reply = {query_reply}, r_vec = {r_vec}\")\n        start_idx = idx*2**k\n        end_idx = (idx+1)*2**k\n        # print(f\"P> start_idx = {start_idx}, end_idx = {end_idx}\")\n        query_reply_folded = fold_code_chunk(query_reply, k, r_vec, coset, start_idx, 2**k, twiddles)\n        # print(f\"P> query_reply_folded = {query_reply_folded}\")\n        assert len(query_reply_folded) == int(1), f\"len(code_folded) = {len(query_reply_folded)}, query_reply = {query_reply}, r_vec = {r_vec}, coseti = {coset}\"\n        assert query_reply_folded[0] == y_vec[i], f\"code_folded[0] = {query_reply_folded[0]}, y_vec[{i}] = {y_vec[i]}\"\n\ntr.absorb(b\"query_replies\", query_replies)\nprint(f\"P> query_replies = {query_replies}\")\n\n\n","type":"content","url":"/src/whir-pcs#first-iteration-fri-round","position":11},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Update global variables"},"type":"lvl2","url":"/src/whir-pcs#update-global-variables","position":12},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Update global variables"},"content":"\n\nall_r_vec += r_vec\nremain_k -= k\nacc_k += k\nall_z_y_vec.append((acc_k, z0, y0, z_vec, y_vec, remain_k))\nprint(f\"P> acc_k = {acc_k}, remain_k = {remain_k}\")\n\n","type":"content","url":"/src/whir-pcs#update-global-variables","position":13},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First Iteration: batching"},"type":"lvl2","url":"/src/whir-pcs#first-iteration-batching","position":14},{"hierarchy":{"lvl1":"Proving demo","lvl2":"First Iteration: batching"},"content":"Inputs :\n\ni: is the index of the current iteration\n\n\\gamma: is the random challenge for aggregating sumchecks from the verifier\n\nv_i: is the sum to be checked from the previous sumcheck rounds\n\n\\{(z_0, y_0), (z_1, y_1), \\ldots, (z_t, y_t)\\}: the queries and their evaluations\n\nK_R: the number of variables of the remaining polynomial\n\nOutputs:\n\nv^*_i: the sum aggregated with new targets from the i-th FRI round (the last round).\n\neq^*_{i}: the MLE polynomial (evaluations array over \\{0,1\\}^{K_R}) aggregated.\n\nSteps:\n\nConstruct array eq_{z_0} from z_0\\mathsf{pow}(z_0, K_{R}) = (z_0, z_0^2, \\ldots, z_0^{2^{K_R-1}})\n\nwhere K_R = K - K_Aeq_{z_0} = \\tilde{eq}(\\vec{b}, \\mathsf{pow}(z_0, K_R))\n\nConstruct array-array eq_{z_i} from \\{z_i\\}eq_{z_i} = \\tilde{eq}(\\vec{b}, \\mathsf{pow}(z_i, K_R))\n\nAccumulate the sumcheck targets on v_i into v^*_{i}v^*_{i} = v_i + \\gamma \\cdot y_0 + \\sum_{j=1}^{t} \\gamma^{j+1} \\cdot y_j\n\nConstruct the array eq^*_{(i)} , whose length is 2^{K_R}eq^*_{(i)} = eq_{(i)} + \\gamma\\cdot eq_{z_0} + \\sum_{j=1}^{t} \\gamma^{j+1} \\cdot eq_{z_j}\n\nwhich follows the equation below:\\sum_{\\vec{b}\\in\\{0,1\\}^{K_R}} \\tilde{f}_{i}(\\vec{b}) \\cdot eq^*_{(i)}(\\vec{b}) = v^*_i\n\nacc_k, remain_k, f1_mle.num_var\n\n# Double check `sum_checked`\n\nif debug > 0:\n    print(f\"P> check checked_sum\")\n    assert sum([f1[i] * eq1[i] for i in range(len(f1))]) == sum_checked, \\\n        f\"sum([f1[i] * eq1[i] for i in range(len(f1))]) = {sum([f1[i] * eq1[i] for i in range(len(f1))])}, sum_checked = {sum_checked}\"\n    print(f\"P> check checked_sum passed\")\n\ngamma = tr.squeeze(Fp, b\"gamma\", 4)\ngamma\n\n# initialize `new_eq_mle` and `new_sum_checked`\nnew_eq_mle = MLEPolynomial(eq1, remain_k)\nnew_sum_checked = sum_checked\n\n# aggregate `z0, y0`\nz0_pow = compute_power_of_2_powers(z0, remain_k)\neqz = MLEPolynomial.eqs_over_hypercube(z0_pow)\neqz_mle = MLEPolynomial(eqz, remain_k)\nnew_eq_mle += Scalar(gamma) * eqz_mle\n\nnew_sum_checked += gamma * y0\n\n\n# aggregate `z_vec, y_vec`\neq_list = []\nfor i, zi, yi in zip(range(t), z_vec, y_vec):\n    # NOTE: `zi` is actually in the coset domain\n    zi_pow = compute_power_of_2_powers(zi * (coset**(2**k)), remain_k)\n    eqzi = MLEPolynomial(MLEPolynomial.eqs_over_hypercube(zi_pow), remain_k)\n    eq_list.append(eqzi)\n    new_eq_mle += Scalar(gamma**(i+2)) * eqzi\n    new_sum_checked += gamma**(i+2) * yi\n\n# compute `new_eq1` alternatively\n# TODO: remove it after testing\nnew_eq1 = eq_sum([eq1, eqz] + eq_list, gamma)\n\nif debug > 1:\n    print(f\"P> check new_eq and new_sum\")\n    f_curr = f1\n    new_eq_curr = new_eq_mle.evals\n    eval = sum([f_curr[i] * new_eq_curr[i] for i in range(len(f_curr))])\n    eval_alt = sum([f_curr[i] * new_eq1[i] for i in range(len(f_curr))])\n    assert eval == eval_alt == new_sum_checked, \\\n        f\"f1(z) = {eval}, eval_alt = {eval_alt}, new_sum_checked = {new_sum_checked}\"\n    print(f\"P> check new_eq and new_sum passed\")\n\n\nall_gamma.append(gamma)\n\n","type":"content","url":"/src/whir-pcs#first-iteration-batching","position":15},{"hierarchy":{"lvl1":"Proving demo","lvl2":"2nd Iteration, Sumcheck Round"},"type":"lvl2","url":"/src/whir-pcs#id-2nd-iteration-sumcheck-round","position":16},{"hierarchy":{"lvl1":"Proving demo","lvl2":"2nd Iteration, Sumcheck Round"},"content":"Construct f2, eq2\n\nConstruct r_vec_2\n\nOutput sumcheck_h_vec_2\n\nOutput sum_checked_2\n\nsum_checked_2 = new_sum_checked\nhalf = len(f1) >> 1\nr_vec_2 = []\nsumcheck_h_vec_2 = []\nf2 = f1.copy()\neq2 = new_eq1.copy()\n# new_eq\nfor i in range(k):\n    f2_even = f2[::2]\n    f2_odd = f2[1::2]\n    eq2_even = eq2[::2]\n    eq2_odd = eq2[1::2]\n    # construct h(X)\n    h_eval_at_0 = sum([f2_even[j] * eq2_even[j] for j in range(half)], Fp.zero())\n    h_eval_at_1 = sum([f2_odd[j] * eq2_odd[j] for j in range(half)], Fp.zero())\n    h_eval_at_2 = sum([ (2 * f2_odd[j] - f2_even[j]) * (2 * eq2_odd[j] - eq2_even[j]) for j in range(half)])\n    h = [h_eval_at_0, h_eval_at_1, h_eval_at_2]\n    sumcheck_h_vec_2.append(h)\n\n    tr.absorb(b\"h(X)\", h)\n    print(f\"P> tr.state = {tr.state}\")\n    if debug > 0:\n        print(f\"P> h = {h}\")\n    # check sum\n    assert h_eval_at_0 + h_eval_at_1 == sum_checked_2, \\\n        f\"i={i}, h_eval_at_0 + h_eval_at_1 = {h_eval_at_0 + h_eval_at_1}, sum_checked = {sum_checked_2}\"\n    \n    # Receive a random number from the verifier\n    r = tr.squeeze(Fp, b\"r\", 4)\n    if debug > 0:\n        print(f\"P> r[{i}] = {r}\")\n    r_vec_2.append(r)\n    \n    # fold f\n\n    f2_folded = [(Fp(1) - r) * f2_even[i] + r * f2_odd[i] for i in range(half)]\n    eq2_folded = [(Fp(1) - r) * eq2_even[i] + r * eq2_odd[i] for i in range(half)]\n\n    f2 = f2_folded\n    eq2 = eq2_folded\n\n    # update sumcheck for the next round\n    sum_checked_2 = UniPolynomial.evaluate_from_evals(h, \n                r, [Fp(0), Fp(1), Fp(2)])\n    half >>= 1\n\n## End of the big loop for `i`\n\nf2_len = len(f2)\nf2_mle = MLEPolynomial(f2, log_2(f2_len))\n\n# check sumcheck\nif debug > 0:\n    print(f\"P> check sum_checked = {sum_checked}\")\n    # eq_mle_post = MLEPolynomial(eq1, log_2(f_len)-k)\n    # eq_mle_post_at_r = eq_mle_post.evaluate(r_vec)\n    eval = Fp.zero()\n    for i in range(f1_len // (2**k)):\n        eval += f2[i] * eq2[i]\n    assert sum_checked_2 == eval, \\\n        f\"sum_checked = {sum_checked_2}, eval = {eval}\"\n    print(f\"P> sum_checked = {eval} = Σ_b f2(b) * eq(b,us)\")\n    print(f\"P> check sum_checked passed\")\n\nprint(f\"P> sum_checked = {sum_checked_2}, r_vec_2 = {r_vec_2}, f2 = {f2}, eq2 = {eq2}\")\n\n\n\n","type":"content","url":"/src/whir-pcs#id-2nd-iteration-sumcheck-round","position":17},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Good luck!"},"type":"lvl2","url":"/src/whir-pcs#good-luck","position":18},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Good luck!"},"content":"\n\n# Check sum_checked in advance before the protocol ends. Good luck!\nfrom utils import inner_product\n\nprint(f\"P> total_r_vec = {r_vec + r_vec_2}\")\nif debug > 0:\n    print(f\"check sumcheck result\")\n    print(f\"P> acc_k = {acc_k}, remain_k = {remain_k}\")\n\n    num_vars = remain_k-k\n    # define an accumulated array `eval` for hypercube-evaluations\n    new_f_mle = f_mle.partial_evaluate(r_vec + r_vec_2)\n\n    print(f\"P> f_mle_at_r = {new_f_mle}\")\n    eq_mle = Scalar(eq_eval(r_vec+r_vec_2, us[:-num_vars])) \\\n            * MLEPolynomial(MLEPolynomial.eqs_over_hypercube(us[-num_vars:]), num_vars)\n    print(f\"P> eq_mle = {eq_mle}\")\n    \n    z0_pow = compute_power_of_2_powers(z0, remain_k)\n    print(f\"P> z0_pow = {z0_pow}\")\n    z0_mle = Scalar(gamma * eq_eval(r_vec_2, z0_pow[:-num_vars])) \\\n            * MLEPolynomial(MLEPolynomial.eqs_over_hypercube(z0_pow[-num_vars:]), num_vars)\n    eq_mle += z0_mle\n    print(f\"P> z0_mle = {z0_mle}\")\n\n    for i in range(t):\n        zi = z_vec[i]\n        print(f\"P> zi = {zi}\")\n        zi_pow = compute_power_of_2_powers(zi * coset**(2**k), remain_k)\n        print(f\"P> zi_pow = {zi_pow}\")\n        zi_mle = Scalar(gamma**(i+2) * eq_eval(r_vec_2, zi_pow[:-num_vars])) \\\n                * MLEPolynomial(MLEPolynomial.eqs_over_hypercube(zi_pow[-num_vars:]), num_vars)\n        eq_mle += zi_mle\n        print(f\"P> zi_mle = {zi_mle}\")\n\n    eval = inner_product(new_f_mle.evals, eq_mle.evals, Fp.zero())\n\n    assert sum_checked_2 == eval, \\\n        f\"sum_checked_2 = {sum_checked_2}, eval = {eval}\"\n    print(f\"P> final_sum = Σ_b f(b) * eq(b,us) checked = {eval}\")\n\n","type":"content","url":"/src/whir-pcs#good-luck","position":19},{"hierarchy":{"lvl1":"Proving demo","lvl2":"2nd Iteration, (Final) FRI Round"},"type":"lvl2","url":"/src/whir-pcs#id-2nd-iteration-final-fri-round","position":20},{"hierarchy":{"lvl1":"Proving demo","lvl2":"2nd Iteration, (Final) FRI Round"},"content":"f0\n\nk=2\n\nf1\n\nk=2\n\nf2 (curr)\n\nf0_code, 64\n\n2\n\nf1_code, 32\n\n2\n\nf2_code, 16\n\nf0_mle, 32\n\n2^2=4\n\nf1_mle, 8\n\n2^2=4\n\nf2_mle, 2\n\nf2_coeffs = MLEPolynomial.compute_coeffs_from_evals(f2)\nf2_mle = MLEPolynomial(f2, log_2(len(f2)))\n\nf2_code = rs_encode(f2_coeffs, coset, len(f2_coeffs), len(f1_code)//2)\nif debug > 1:\n    print(f\"P> check f2_code \")\n    f2_coeffs = MLEPolynomial.compute_coeffs_from_evals(f2)\n    print(f\"P> f2_coeffs = {f2_coeffs}\")\n    f2_uni = UniPolynomialWithFft(f2_coeffs)\n    D = compute_powers(Fp.nth_root_of_unity(len(f2_code)), len(f2_code))\n    print(f\"P> D = {D}\")\n    D_rbo = bit_reverse_permutation(D)\n    evals = [f2_uni.evaluate(coset * d) for d in D_rbo]\n    assert evals == f2_code, f\"evals={evals}, f2_code={f2_code}\"\n    print(f\"P> f2_code = {f2_code}\")\n    for i in range(len(D_rbo)):\n        w = D_rbo[i]\n        w_pow = compute_power_of_2_powers(w * coset, f2_mle.num_var)\n        # print(f\"P> w = {w}, w_pow = {w_pow}\")\n        assert f2_code[i] == f2_mle.evaluate(w_pow), \\\n            f\"f2_code[{i}] = {f2_code[i]}, f2_mle.evaluate(w_pow) = {f2_mle.evaluate(w_pow)}\"\n    print(f\"P> check f2_code passed\")\n\ntr.absorb(b\"final_f_evals\", f2)\n\nprint(f\"P> f2 = {f2}\")\n\n# Query-phase\n\nf1_code_folded_len = len(f1_code)//2**k\n\nz_idx_vec = []\nfor i in range(max_queries_try):\n    z_idx = tr.squeeze(int, b\"zi\", 4) % f1_code_folded_len\n    if z_idx not in z_idx_vec:\n        z_idx_vec.append(z_idx)\n    else:\n        continue\n    if len(z_idx_vec) >= t:\n        break\nassert len(z_idx_vec) == t, f\"len(z_idx_vec) = {len(z_idx_vec)}, t = {t}\"\n\nif debug > 1:\n    print(f\"P> FRI> z_idx_vec = {z_idx_vec}, from [0..{f1_code_folded_len})\")\n\n\nz_omega = Fp.nth_root_of_unity(f1_code_folded_len)\nprint(f\"P> FRI> z_omega = {z_omega}, ord(z_omega) = {get_order_of_omega(z_omega, f1_code_folded_len)}\")\n# D = compute_powers(z_omega, f1_code_folded_len)\n# D_rbo = bit_reverse_permutation(D)\n# print(f\"P> FRI> D_rbo = {D_rbo}\")\n\n# compute {(zi, yi)}, where yi = f_i(zi)\nz_vec, y_vec = [], []\nfor i in range(t):\n    zi = z_omega** bit_reverse(z_idx_vec[i], log_2(f1_code_folded_len))\n    z_vec.append(zi)\n    zi_pow = compute_power_of_2_powers(zi * coset**(2**k), f2_mle.num_var)\n    y_vec.append(f2_mle.evaluate(zi_pow))\n\n# if debug > 1:\n#     f2_code_A = rs_encode(f2_coeffs, coset**(2**k), len(f2_coeffs), len(f1_code)//(2**k))\n#     f2_code_B = fold_code(f1_code, r_vec_2, coset, twiddles)\n\n#     print(f\"P> FRI> f2_code_A = {f2_code_A}\")\n#     print(f\"P> FRI> f2_code_B = {f2_code_B}\")\n\nif debug > 1:\n    print(f\"P> FRI> z_idx_vec = {z_idx_vec}\")\n    print(f\"P> FRI> z_vec = {z_vec}, y_vec = {y_vec}\")\n\n# collect leaves of the code for constructing `yi=f(zi)`\nquery_replies = []\nfor i in range(t):\n    idx = z_idx_vec[i]\n    start_idx = idx*2**k\n    query_reply = f1_code[start_idx: start_idx + 2**k]\n    query_replies.append(query_reply)\nprint(f\"P> FRI> query_replies = {query_replies}\")\n\nif debug > 1:\n    print(f\"P> FRI> check query replies (num={t})\")\n    for j in range(t):\n        start_idx = z_idx_vec[j] * 2**k\n        query_reply_folded = fold_code_chunk(query_replies[j], k, r_vec_2, coset, start_idx, 2**k, twiddles)\n        print(f\"P> FRI> query_reply_folded = {query_reply_folded}\")\n        assert len(query_reply_folded) == int(1), \\\n            f\"len(code_folded) = {len(query_reply_folded)}, query_reply = {query_reply}, r_vec = {r_vec_2}, coset = {coset}\"\n        assert query_reply_folded[0] == y_vec[j], \\\n            f\"code_folded[0] = {query_reply_folded[0]}, y_vec[{j}] = {y_vec[j]}\"\n        print(f\"P> FRI> check query-reply-{j} passed\")\n\n\n\ntr.absorb(b\"query_replies\", query_replies)\n\nall_z_y_vec\n\n","type":"content","url":"/src/whir-pcs#id-2nd-iteration-final-fri-round","position":21},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Update global variables"},"type":"lvl2","url":"/src/whir-pcs#update-global-variables-1","position":22},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Update global variables"},"content":"\n\nall_r_vec += r_vec_2\nremain_k -= k\nacc_k += k\n\nfinal_k = remain_k\nprint(f\"P> acc_k = {acc_k}, remain_k = {remain_k}, final_k = {final_k}\")\n\n","type":"content","url":"/src/whir-pcs#update-global-variables-1","position":23},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Check the final SUM!"},"type":"lvl2","url":"/src/whir-pcs#check-the-final-sum","position":24},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Check the final SUM!"},"content":"\n\n\n\neq_mle = MLEPolynomial(MLEPolynomial.eqs_over_hypercube(us[acc_k:]), final_k)\neq_mle = Scalar(eq_eval(all_r_vec, us[:acc_k])) * eq_mle\nfor i, ((acc_k_i, z0_i, y0_i, z_vec_i, y_vec_i, remain_k_i), gamma_i) in enumerate(zip(all_z_y_vec, all_gamma)):\n    print(f\"i={i}, acc_k_i = {acc_k_i}, remain_k_i = {remain_k_i}, total_k = {total_k}\")\n    assert acc_k_i + remain_k_i == total_k, \\\n        f\"i={i}, acc_k_i = {acc_k_i}, remain_k_i = {remain_k_i}, total_k = {total_k}\"\n    print(f\"len(z0_pow)={remain_k_i-final_k}\")\n    print(f\"P> gamma_i = {gamma_i}\")\n    z0_pow = compute_power_of_2_powers(z0_i, remain_k_i)\n    z0_mle = MLEPolynomial(MLEPolynomial.eqs_over_hypercube(z0_pow[-final_k:]), final_k)\n    z0_mle = Scalar(gamma_i * eq_eval(all_r_vec[acc_k_i:], z0_pow[:-final_k])) * z0_mle\n    print(f\"P> z0_mle = {z0_mle}\")\n    eq_mle += z0_mle\n\n    for j in range(t):\n        zi = z_vec_i[j]\n        print(f\"P> zi = {zi}\")\n        zi_pow = compute_power_of_2_powers(zi * coset**(2**k), remain_k_i)\n        print(f\"P> zi_pow = {zi_pow}\")\n        zi_mle = MLEPolynomial(MLEPolynomial.eqs_over_hypercube(zi_pow[-final_k:]), final_k)\n        zi_mle = Scalar(gamma_i**(j+2) * eq_eval(all_r_vec[acc_k_i:], zi_pow[:-final_k])) * zi_mle\n        print(f\"P> zi_mle = {zi_mle}\")\n        eq_mle += zi_mle\neq_mle\n\nfinal_mle = MLEPolynomial(f2, final_k)\n\nfinal_eval = inner_product(final_mle.evals, eq_mle.evals, Fp.zero())\n\nassert final_eval == sum_checked_2, \\\n    f\"final_eval = {final_eval}, sum_checked_2 = {sum_checked_2}\"\n\ntype(int(21))\n\n","type":"content","url":"/src/whir-pcs#check-the-final-sum","position":25},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Great!"},"type":"lvl2","url":"/src/whir-pcs#great","position":26},{"hierarchy":{"lvl1":"Proving demo","lvl2":"Great!"},"content":"\n\nSample from [0, 1, 2, 3, ..., 7], which is the index set of f1_code/2^k. We want to sample\ncode chunk [c0, ..., c2^k-1] from f1_code and fold it into yi, which is equal to f2_mle(zi).L_1 = \\{g, g\\omega, g\\omega^2, ..., g\\omega^{N/2-1}\\}C_1 = \\{c_0, c_1, ..., c_{N/2-1}\\}\n\nThe polynomial \\tilde{f}_2 is the folded MlE polynomial from \\tilde{f}_1,\\tilde{f}_2(X_2, \\ldots, X_{s-1}) = \\tilde{f}_1(r_1, X_2, \\ldots, X_{s-1}) = \\tilde{f}_0(r_0, r_1, X_2, \\ldots, X_{s-1})\n\nIf the verifier samples \\{z_i\\} from L^{2^k}_1,z_i \\leftarrow \\big(g\\cdot\\omega^{2^k}\\big)^i \\in L^{2^k}_1\n\nthen we can have a corresponding evaluation of \\tilde{f}_2, as denoted by y_i\\tilde{f}_2(z_i, z_i^2, z_i^4, ..., z_i^{2^{k^*}}) = y_i\n\nIn fact, y_i happens to be the folded code chunk from C_1\\mathsf{fold}(c_0, c_1, ..., c_{2^k-1}) = y_i","type":"content","url":"/src/whir-pcs#great","position":27},{"hierarchy":{"lvl1":"Notes on Virgo-PCS"},"type":"lvl1","url":"/virgo-pcs/virgo-pcs-01","position":0},{"hierarchy":{"lvl1":"Notes on Virgo-PCS"},"content":"Virgo is a zkSNARK proof system based on the GKR protocol. Unlike Libra, Virgo adopts a different polynomial commitment scheme, referred to as zkVPD (Verifiable Polynomial Delegation) in the paper. Virgo-zkVPD is based on the FRI (Fast Reed-Solomon IOP) protocol derived from the STARK system, making it a proof system that doesn’t require a trusted setup. Its security assumptions are based on information theory and the collision resistance of hash functions.\n\nThis article introduces the protocol principles of Virgo-PCS, which differ slightly from the original paper. The PCS system in the original paper supports arbitrary multivariate polynomials, while this article only considers MLE polynomials.","type":"content","url":"/virgo-pcs/virgo-pcs-01","position":1},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"1. Protocol Principles"},"type":"lvl2","url":"/virgo-pcs/virgo-pcs-01#id-1-protocol-principles","position":2},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"1. Protocol Principles"},"content":"For any MLE polynomial, \\tilde{f}(X_0, X_1, \\cdots, X_{n-1}), it can be expressed in the following coefficient form (or Monomial form):\\tilde{f}(X_0, X_1, \\cdots, X_{n-1}) = \\sum_{i=0}^{2^{n}-1} c_i\\cdot X_0^{i_0}X_1^{i_1}\\cdots{}X_{n-1}^{i_{n-1}}\n\nWhere \\vec{c}=(c_0, c_1, \\cdots, c_{2^n-1}) is the coefficient vector, and \\mathsf{bits}(i)=(i_0, i_1, \\cdots, i_{n-1}) is the binary representation of i (Little Endian).\n\nIf we consider how to prove the evaluation of this polynomial at a given point \\vec{u}=(u_0, u_1, \\cdots, u_{n-1}), i.e., \\tilde{f}(\\vec{u})=v, we only need to prove the following “sum”:\\sum_{i=0}^{2^n - 1} c_i\\cdot u_0^{i_0}u_1^{i_1}\\cdots{}u_{n-1}^{i_{n-1}} = v\n\nThe approach of Virgo-PCS is similar to PH23-PCS, using a Univariate Sumcheck protocol to prove the above sum. The Prover first commits to \\vec{c}, then proves through the following Univariate Sumcheck formula:c(X)\\cdot{}m(X) = h(X)\\cdot v_{\\mathbb{H}}(X) + X\\cdot g(X) + \\frac{v}{N}\n\nHere, \\mathbb{H} is a multiplicative subgroup of the finite field \\mathbb{F}_{p}, with a size of N = |\\mathbb{H}| = 2^n. The polynomial v_{\\mathbb{H}}= \\prod_{x \\in \\mathbb{H}}(X - x) is the vanishing polynomial of \\mathbb{H}. The polynomial m(X) encodes the vector \\vec{m}=(m_0, m_1, \\cdots, m_{2^n-1}):m_i = u_0^{i_0}u_1^{i_1}\\cdots{}u_{n-1}^{i_{n-1}}\n\nThen the Prover calculates the FRI commitment of h(X) and sends it to the Verifier. The Prover and Verifier then use the FRI protocol to prove the existence of the following Low Degree quotient polynomial g(X):g(X) = \\frac{N\\cdot c(X)\\cdot m(X) - N\\cdot h(X)\\cdot v_{\\mathbb{H}}(X) - v}{N\\cdot X}\n\nClearly, if we can prove that \\deg(g)<N-1, then we have proven that \\sum_{i=0}^{2^n - 1} c_i\\cdot m_i = v.\n\nHere is an explanation of how to convert the proof of the above “sum” into proving \\deg(g) < N - 1. To prove \\sum_{i=0}^{2^n - 1} c_i \\cdot m_i = v, we can first encode c_i and m_i as polynomials c(X) and m(X) over \\mathbb{H}, and then convert it to proving\\sum_{x \\in \\mathbb{H}} c(x) \\cdot m(x) = v\n\nThe degree of c(X) \\cdot m(X) is N - 1 + N - 1 = 2N - 2. By decomposing c(X) \\cdot m(X), we getc(X) \\cdot m(X) = g'(X) + h(X) \\cdot v_{\\mathbb{H}}(X)\n\nwhere \\deg(h(X)) < 2N - 1 - N = N - 1 and \\deg(g'(X)) < N. Therefore, the proof of the “sum” can be converted into proving\\sum_{x \\in \\mathbb{H}} c(x) \\cdot m(x) = \\sum_{x \\in \\mathbb{H}} (g'(x) + h(x) \\cdot v_{\\mathbb{H}}(x)) = \\sum_{x \\in \\mathbb{H}} g'(x) = g'(0) \\cdot N = v\n\nThe second-to-last equality above is obtained from the following lemma.\n\nLemma ([BC99]) Let \\mathbb{H} be a multiplicative coset of \\mathbb{F}, and g(X) be a univariate polynomial over \\mathbb{F} with degree strictly less than |\\mathbb{H}|. Then \\sum_{x \\in \\mathbb{H}} g(x) = g(0) \\cdot |\\mathbb{H}|.\n\nNow we only need to prove \\deg(g') < N and g'(0) = v/N. This can be converted into proving that the polynomial\\frac{g'(X) - g'(0)}{X - 0} = \\frac{g'(X) - v/N}{X - 0} = \\frac{N \\cdot g'(X) - v}{N \\cdot X}\n\nhas a degree strictly less than N - 1. The above polynomial isg(X) = \\frac{N \\cdot c(X) \\cdot m(X) - N \\cdot h(X) \\cdot v_{\\mathbb{H}}(X) - v}{N \\cdot X}\n\nThus, proving the “sum” is converted into proving \\deg(g) < N - 1.\n\nThis approach is generally correct, but the Verifier needs O(N) work because they need to calculate the values of m(X) at \\kappa sampling points. And m(X) must be a publicly computable vector from \\vec{u}.\n\nThe Virgo paper suggests that computing the value of m(X) at a point can utilize a GKR protocol, delegating the Verifier’s computation to the Prover while ensuring the correctness of the computation through the GKR protocol. This way, the Verifier only needs O(\\log^2(N)) work.\n\nThis GKR circuit is divided into four parts:\n\nCalculate the value of \\vec{m} based on \\vec{u}\n\nCalculate the coefficient vector of m(X) using the IFFT algorithm based on \\vec{m}\n\nCalculate the values of m(X) on the Extended Domain \\mathbb{L} based on the coefficient vector of m(X)\n\nFilter out the values of m(X) on \\{\\mathbb{L}_i\\}_{i\\in Q} according to the FRI-Query index set Q provided by the Verifier\n\nSince all calculations in this GKR protocol are based on public values, and the input length of the circuit is n=\\log(N), the depth of the circuit is \\log(N), and the width of the circuit is |\\mathbb{L}|=N / \\rho, the Verifier’s computational complexity is only O(\\log^2(N)) to complete the verification.","type":"content","url":"/virgo-pcs/virgo-pcs-01#id-1-protocol-principles","position":3},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"2. Simplified Protocol"},"type":"lvl2","url":"/virgo-pcs/virgo-pcs-01#id-2-simplified-protocol","position":4},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"2. Simplified Protocol"},"content":"","type":"content","url":"/virgo-pcs/virgo-pcs-01#id-2-simplified-protocol","position":5},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Protocol Parameters","lvl2":"2. Simplified Protocol"},"type":"lvl3","url":"/virgo-pcs/virgo-pcs-01#protocol-parameters","position":6},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Protocol Parameters","lvl2":"2. Simplified Protocol"},"content":"Domain \\mathbb{H} is a multiplicative subgroup of the prime field \\mathbb{F}_p, with size N=2^n.\n\nExtended Domain \\mathbb{L}\\subset\\mathbb{F}_p is a multiplicative subgroup Coset of size |\\mathbb{L}|=\\rho\\cdot N, where \\rho represents the code rate.","type":"content","url":"/virgo-pcs/virgo-pcs-01#protocol-parameters","position":7},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Commitment Calculation","lvl2":"2. Simplified Protocol"},"type":"lvl3","url":"/virgo-pcs/virgo-pcs-01#commitment-calculation","position":8},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Commitment Calculation","lvl2":"2. Simplified Protocol"},"content":"The Prover calculates the commitment value C_{\\tilde{f}} of \\tilde{f}(X_0, X_1, \\cdots, X_{n-1}) similar to the general FRI protocol, calculating the values of the corresponding Univariate polynomial c(X) on \\mathbb{L}.C_{\\tilde{f}} = \\mathsf{MerkleTree.Commit}(\\vec{c})","type":"content","url":"/virgo-pcs/virgo-pcs-01#commitment-calculation","position":9},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl3","url":"/virgo-pcs/virgo-pcs-01#evaluation-proof","position":10},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"","type":"content","url":"/virgo-pcs/virgo-pcs-01#evaluation-proof","position":11},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Public Input","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#public-input","position":12},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Public Input","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"C_{\\tilde{f}}\n\n\\vec{u}\n\nv=\\tilde{f}(\\vec{u})","type":"content","url":"/virgo-pcs/virgo-pcs-01#public-input","position":13},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 1.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-1","position":14},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 1.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"Calculate \\vec{m}, and construct m(X), whose Evaluation is \\vec{m}m(X) = m_0\\cdot L_{0}(X) + m_1\\cdot L_{1}(X) + \\cdots + m_{N-1}\\cdot L_{N-1}(X)\n\nConstruct h(X), and calculate its commitment C_h, where h(X) satisfies the following equation:h(X) = \\frac{c(X)\\cdot m(X) - X\\cdot g(X) - v/N}{v_{\\mathbb{H}}(X)}C_h = \\mathsf{MerkleTree.Commit}(h|_{\\mathbb{L}})","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-1","position":15},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-2","position":16},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"The Prover and Verifier use the FRI protocol to prove the existence of g(X). In the Query phase of the protocol, the Verifier provides an index set Q, and the Prover calculates the values of c(X) and h(X) on \\{x_i\\}_{i\\in Q}:\\{(c(x_i), \\pi_{c}(x_i))\\} \\leftarrow \\mathsf{MerkleTree.Open}(i, c(X)|_{\\mathbb{L}}), \\quad \\forall i\\in Q\\{(h(x_i), \\pi_{h}(x_i))\\} \\leftarrow \\mathsf{MerkleTree.Open}(i, h(X)|_{\\mathbb{L}}), \\quad \\forall i\\in Q\n\nHere, all x_i are elements in \\mathbb{L}.","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-2","position":17},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-3","position":18},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"The Verifier checks the correctness of \\{c(x_i), h(x_i)\\}_{i\\in Q}.\\mathsf{MerkleTree.Verify}(C_f, i, c(x_i), \\pi_{c}(x_i)) \\overset{?}{=} 1, \\quad \\forall i\\in Q\\mathsf{MerkleTree.Verify}(C_h, i, h(x_i), \\pi_{h}(x_i)) \\overset{?}{=} 1, \\quad \\forall i\\in Q","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-3","position":19},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 4.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-4","position":20},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 4.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"The Prover and Verifier run the GKR protocol to calculate the values of m|_{\\mathbb{L}}, and output the values of \\{m|_{x_i}\\}_{i\\in Q}, where x_i is the i-th element in the multiplicative subgroup \\mathbb{L}.","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-4","position":21},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 5.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-5","position":22},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 5.","lvl3":"Evaluation Proof","lvl2":"2. Simplified Protocol"},"content":"The Verifier verifies the correctness of each folding step in the FRI protocol using \\{c(x_i), h(x_i), m(x_i)\\}_{i\\in Q}.","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-5","position":23},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl2","url":"/virgo-pcs/virgo-pcs-01#id-3-supporting-zero-knowledge","position":24},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"3. Supporting Zero-Knowledge"},"content":"To support the Zero-Knowledge property, Virgo introduces random numbers in two places:\n\nA Mask polynomial r(X) is added to the commitment of c(X)\n\nIn the Univariate Sumcheck protocol, a random polynomial s(X) is introduced. When verifying \\sum_{x_i\\in\\mathbb{H}}c(x_i)m(x_i)=v, it simultaneously proves \\sum_{x_i\\in\\mathbb{H}}s(x_i)=v'.","type":"content","url":"/virgo-pcs/virgo-pcs-01#id-3-supporting-zero-knowledge","position":25},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Commitment Calculation","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl3","url":"/virgo-pcs/virgo-pcs-01#commitment-calculation-1","position":26},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Commitment Calculation","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Prover samples a random polynomial r(X) with Degree \\kappa-1, i.e., containing \\kappa random numbers.c^*(X) = c(X) + v_{\\mathbb{H}}(X)\\cdot r(X)C_{\\tilde{f}} = \\mathsf{MerkleTree.Commit}(c^*(X)|_{\\mathbb{L}})\n\nClearly, \\deg(c^*(X)) = N + \\kappa - 1.","type":"content","url":"/virgo-pcs/virgo-pcs-01#commitment-calculation-1","position":27},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl3","url":"/virgo-pcs/virgo-pcs-01#evaluation-proof-1","position":28},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"","type":"content","url":"/virgo-pcs/virgo-pcs-01#evaluation-proof-1","position":29},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Public Input","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#public-input-1","position":30},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Public Input","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"C_{\\tilde{f}}\n\n\\vec{u}\n\nv=\\tilde{f}(\\vec{u})","type":"content","url":"/virgo-pcs/virgo-pcs-01#public-input-1","position":31},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Witness","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#witness","position":32},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Witness","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"Coefficient vector \\vec{c} of the MLE polynomial \\tilde{f}(X_0, X_1, \\cdots, X_{n-1})\n\nRandom polynomial r(X)","type":"content","url":"/virgo-pcs/virgo-pcs-01#witness","position":33},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 1.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-1-1","position":34},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 1.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Prover calculates \\vec{m}, and constructs m(X), whose Evaluation is \\vec{m}m(X) = m_0\\cdot L_{0}(X) + m_1\\cdot L_{1}(X) + \\cdots + m_{N-1}\\cdot L_{N-1}(X)\n\nThe Prover samples a polynomial s(X) with Degree 2N + \\kappa - 1, whose sum on \\mathbb{H} is v'\\sum_{a\\in\\mathbb{H}}s(a) = v'\n\nThe Prover calculates the commitment of s(X)C_s = \\mathsf{MerkleTree.Commit}(s|_{\\mathbb{L}})","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-1-1","position":35},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-2-1","position":36},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 2.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Verifier provides a random number \\alpha to aggregate the sums of two different Sumcheck protocols.v^* = v + \\alpha\\cdot v'\n\nThe Prover constructs h(X), and calculates its commitment C_h, where h(X) satisfies the following equation:h(X) = \\frac{c^*(X)\\cdot m(X) + \\alpha\\cdot s(X) - X\\cdot g(X) - v^*/N}{v_{\\mathbb{H}}(X)}C_h = \\mathsf{MerkleTree.Commit}(h|_{\\mathbb{L}})","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-2-1","position":37},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-3-1","position":38},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 3.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Prover and Verifier use the FRI protocol to prove that the Degree of g(X) is less than N-1. This includes O(\\log(N)) rounds of Split-and-fold.","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-3-1","position":39},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 4.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-4-1","position":40},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 4.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Verifier samples \\kappa random indices, Q, and requires the Prover to provide the values of c^*(X), s(X), and h(X) on \\{a_i\\}_{i\\in Q}. Here a_i\\in\\mathbb{L} is the i-th element in \\mathbb{L}.\n\nThe Prover sends the values of c^*(X), s(X), and h(X) on \\{a_i\\}_{i\\in Q}, along with the Merkle paths.\\{(c^*(a_i), \\pi_{c^*}(a_i))\\} \\leftarrow \\mathsf{MerkleTree.Open}(i, c^*(X)|_{\\mathbb{L}}), \\quad \\forall i\\in Q\\{(s(a_i), \\pi_{s}(a_i))\\} \\leftarrow \\mathsf{MerkleTree.Open}(i, s(X)|_{\\mathbb{L}}), \\quad \\forall i\\in Q\\{(h(a_i), \\pi_{h}(a_i))\\} \\leftarrow \\mathsf{MerkleTree.Open}(i, h(X)|_{\\mathbb{L}}), \\quad \\forall i\\in Q","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-4-1","position":41},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 5.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#round-5-1","position":42},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Round 5.","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Prover and Verifier run the GKR protocol to calculate the values of m|_{\\mathbb{L}}, and output the values of \\{m|_{a_i}\\}_{i\\in Q}, where a_i is the i-th element in the multiplicative subgroup \\mathbb{L}.","type":"content","url":"/virgo-pcs/virgo-pcs-01#round-5-1","position":43},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Verification","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"type":"lvl4","url":"/virgo-pcs/virgo-pcs-01#verification","position":44},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl4":"Verification","lvl3":"Evaluation Proof","lvl2":"3. Supporting Zero-Knowledge"},"content":"The Verifier checks the correctness of \\{c^*(a_i), s(a_i), h(a_i)\\}_{i\\in Q}.\\mathsf{MerkleTree.Verify}(C_f, i, c^*(a_i), \\pi_{c^*}(a_i)) \\overset{?}{=} 1, \\quad \\forall i\\in Q\\mathsf{MerkleTree.Verify}(C_s, i, s(a_i), \\pi_{s}(a_i)) \\overset{?}{=} 1, \\quad \\forall i\\in Q\\mathsf{MerkleTree.Verify}(C_h, i, h(a_i), \\pi_{h}(a_i)) \\overset{?}{=} 1, \\quad \\forall i\\in Q\n\nThe Verifier verifies the correctness of each folding step in the FRI protocol using \\{c^*(a_i), s(a_i), h(a_i), m(a_i)\\}_{i\\in Q}.","type":"content","url":"/virgo-pcs/virgo-pcs-01#verification","position":45},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"4. Summary"},"type":"lvl2","url":"/virgo-pcs/virgo-pcs-01#id-4-summary","position":46},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"4. Summary"},"content":"Virgo-PCS is one of the earliest protocols to use the MLE-to-Univariate approach to construct polynomial commitments. It is also one of the earliest protocols to use Small Field, Mersenne-61 prime field to improve performance. Although the Virgo-PCS protocol requires the MLE polynomial to be given in Coefficient form, if we only consider the commitment of the MLE polynomial, we can directly use the Evaluation (Lagrange Basis) form of the MLE polynomial for proof without converting the MLE polynomial to Coefficient (Monomial Basis) form.","type":"content","url":"/virgo-pcs/virgo-pcs-01#id-4-summary","position":47},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"References"},"type":"lvl2","url":"/virgo-pcs/virgo-pcs-01#references","position":48},{"hierarchy":{"lvl1":"Notes on Virgo-PCS","lvl2":"References"},"content":"[ZXZS19] Jiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent Polynomial Delegation and Its Applications to Zero Knowledge Proof”. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 859-876. IEEE, 2020. \n\nhttps://​eprint​.iacr​.org​/2019​/1482.\n\n[PH23] Papini, Shahar, and Ulrich Haböck. “Improving logarithmic derivative lookups using GKR.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/1284.\n\n[BCRSVW19] Eli Ben-Sasson, Alessandro Chiesa, Michael Riabzev, Nicholas Spooner, Madars Virza, and Nicholas P. Ward. “Aurora: Transparent succinct arguments for R1CS.” Advances in Cryptology–EUROCRYPT 2019: 38th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Darmstadt, Germany, May 19–23, 2019, Proceedings, Part I 38. Springer International Publishing, 2019. \n\nhttps://​eprint​.iacr​.org​/2018​/828.\n\n[BC99] Byott, Nigel P., and Robin J. Chapman. “Power sums over finite subspaces of a field.” Finite Fields and Their Applications 5, no. 3 (1999): 254-265.","type":"content","url":"/virgo-pcs/virgo-pcs-01#references","position":49},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification"},"type":"lvl1","url":"/whir-pcs/whir","position":0},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification"},"content":"Jade Xie  \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nThis article mainly introduces the WHIR (Weights Help Improving Rate) protocol [ACFY24b]. Like the FRI [BBHR18], STIR [ACFY24a], and BaseFold [ZCF24] protocols, WHIR is also an IOPP protocol, but it has a smaller query complexity and a faster verification time. The paper [ACFY24b] mentions that WHIR’s verifier typically runs in hundreds of microseconds (1 microsecond = \n\n10-6 seconds), while other protocols’ verifiers take a few milliseconds (1 millisecond = \n\n10-3 seconds). Additionally, WHIR is an IOPP protocol for constrained Reed-Solomon codes (CRS), which allows WHIR to support queries for both multivariate linear polynomials and univariate polynomials, which is why WHIR can be compared simultaneously with BaseFold, FRI, and STIR [ACFY24b]. Overall, WHIR combines the ideas of BaseFold and STIR, enabling the WHIR protocol to support multivariate linear polynomials without sacrificing Prover efficiency and argument size, while also having a smaller query complexity.","type":"content","url":"/whir-pcs/whir","position":1},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"From Univariate Polynomials to Multivariate Linear Polynomials"},"type":"lvl2","url":"/whir-pcs/whir#from-univariate-polynomials-to-multivariate-linear-polynomials","position":2},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"From Univariate Polynomials to Multivariate Linear Polynomials"},"content":"For a finite field \\mathbb{F}, evaluation domain \\mathcal{L} \\subseteq \\mathbb{F}, and Reed-Solomon encoding of degree d \\in \\mathbb{N}, it represents the set of evaluations of all univariate polynomials over \\mathbb{F} with degree strictly less than d on \\mathcal{L}, denoted as \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d]. Assuming \\mathcal{L} is a multiplicative coset of \\mathbb{F}^*, and its order is a power of 2 (called “smooth” \\mathcal{L}), and also assuming the degree d = 2^m is in the form of a power of 2, then we can view the univariate polynomial as a multivariate linear polynomial with m variables. (From [ACFY24b, 1.1 Constrained Reed-Solomon codes])\n\nLet’s first give a simple example with d = 2^3, letf(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7\n\nLet X_1 = x, X_2 = x^2 , X_3 = x^4, then f(x) can be represented as:\\begin{aligned}\n    f(x) & = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7 \\\\\n    & = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1X_2 + a_4 X_3 + a_5 X_1 X_2 + a_6 X_1 X_3 + a_7 X_1 X_2 X_3\n\\end{aligned}\n\nDenote the new multivariate linear polynomial as\\hat{f}(X_1, X_2, X_3) = a_0 + a_1 X_1 + a_2 X_2 + a_3 X_1X_2 + a_4 X_3 + a_5 X_1 X_2 + a_6 X_1 X_3 + a_7 X_1 X_2 X_3\n\nIn this way, f(x) can be viewed as a univariate polynomial, or as a multivariate linear polynomial after variable substitution X_1 = x, X_2 = x^2 , X_3 = x^4.\n\nFor univariate polynomials in the RS code \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d], it’s similar, they can be viewed from the perspective of multivariate linear polynomials, i.e.,\\begin{aligned}\n    \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, d] & := \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{g} \\in \\mathbb{F}^{< 2^m}[X] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{g}(x)\\} \\\\\n    & = \\{f: \\mathcal{L} \\rightarrow \\mathbb{F}: \\exists \\hat{f} \\in \\mathbb{F}^{< 2}[X_1, \\ldots, X_m] \\text{ s.t. } \\forall x \\in \\mathcal{L}, f(x) = \\hat{f}(x^{2^0}, x^{2^1},\\ldots, x^{2^{m-1}})\\}\n\\end{aligned}\n\nIn the above equation, \\hat{g}(x) is the univariate polynomial, while \\hat{f}(X_1, \\ldots, X_m) is the multivariate linear polynomial with m variables. The idea used here appears in BaseFold. (From [ACFY24b, 1.1 Constrained Reed-Solomon codes])\n\nFurthermore, consistent with the FRI protocol, folding a univariate polynomial with a random number \\alpha_1 can be equivalently viewed as substituting \\alpha_1 for one of the variables in the multivariate linear polynomial.\n\nFor example, for the above f(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5 + a_6 x^6 + a_7 x^7, first fold with \\alpha_1, then\\begin{aligned}\n    f(x) & = a_0 + a_2 x^2 + a_4 x^4 + a_6 x^6 + x (a_1 + a_3 x^2 + a_5 x^4 + a_7 x^6) \\\\\n    & := f_1(x^2) + x f_2(x^2)\n\\end{aligned}\n\nThe folded polynomial is\\begin{aligned}\n    f^{(1)}(x) & = f_1(x) + \\alpha_1 f_2(x) \\\\\n    & = a_0 + a_2 x + a_4 x^2 + a_6 x^3 + \\alpha_1 (a_1 + a_3 x + a_5 x^2 + a_7 x^3) \\\\\n    & = a_0 + a_2 X_1 + a_4 X_2 + a_6 X_1X_2 + \\alpha_1 (a_1 + a_3 X_1 + a_5 X_2 + a_7 X_1X_2)\n\\end{aligned}\n\nThis is equivalent to directly substituting values and replacing variables in the original multivariate polynomial \\hat{f}(X_1,X_2,X_3), specifically:\n\nFirst, substitute X_1 with \\alpha_1, we get\\begin{aligned}\n    \\hat{f}(\\alpha_1,X_2,X_3) & = a_0 + a_1 \\cdot \\alpha_1 + a_2 X_2 + a_3 \\cdot  \\alpha_1 X_2 + a_4 X_3 + a_5 \\cdot \\alpha_1 X_2 + a_6 X_2 X_3 + a_7 \\cdot \\alpha_1 X_2 X_3 \\\\\n    & = a_0 + a_2 X_2 + a_4 X_3 + a_6 X_2 X_3 + \\alpha_1 (a_1 + a_3 X_2 + a_5 X_2 + a_7 X_2 X_3) \n\\end{aligned}\n\nLet the new variables X_1 = X_2, and X_2 = X_3, we get the folded polynomial as\\begin{aligned}\n    \\hat{f}^{(1)}(X_1, X_2) & = a_0 + a_2 X_1 + a_4 X_2 + a_6 X_1 X_2 + \\alpha_1 (a_1 + a_3 X_1 + a_5 X_2 + a_7 X_1 X_2) \\\\\n    & = f^{(1)}(x)\n\\end{aligned}\n\nWe can see that the polynomials obtained by the two folding methods are equivalent, except that f^{(1)}(x) is in the form of a univariate polynomial, while \\hat{f}^{(1)}(X_1, X_2) is in the form of a multivariate linear polynomial.\n\nIf we want to perform a 4-fold on the original polynomial f(x), from the perspective of univariate polynomials, we can perform a 2-fold on the polynomial f^{(1)}(x) after the 2-fold, i.e.,\\begin{aligned}\n    f^{(1)}(x) & = a_0 + a_2 x + a_4 x^2 + a_6 x^3 + \\alpha_1 (a_1 + a_3 x + a_5 x^2 + a_7 x^3) \\\\\n    & = (a_0 + \\alpha_1 a_1) + (a_2 + \\alpha_1 a_3) \\cdot x + (a_4 + \\alpha_1 a_5) \\cdot x^2 + (a_6 + \\alpha_1 a_7) \\cdot x^3 \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) \\cdot x^2) + x \\cdot ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7) \\cdot x^2) \\\\\n    & := f_1^{(1)}(x^2) + x f_2^{(1)}(x^2)\n\\end{aligned}\n\nFolding with a random number \\alpha_2, we get the folded polynomial as\\begin{aligned}\n    f^{(2)}(x) & = f_1^{(1)}(x) + \\alpha_2 f_2^{(1)}(x) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5)  x) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7) x) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_1) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_1)\n\\end{aligned}\n\nFrom the perspective of multivariate linear polynomials, we can perform a 2-fold on the multivariate linear polynomial \\hat{f}^{(1)}(X_1, X_2) after the 2-fold, i.e.,\n\nSubstitute X_1 with \\alpha_2, we get\\begin{aligned}\n    \\hat{f}^{(1)}(\\alpha_2, X_2) & = a_0 + a_2 \\alpha_2 + a_4 X_2 + a_6 \\alpha_2 X_2 + \\alpha_1 (a_1 + a_3 \\alpha_2 + a_5 X_2 + a_7 \\alpha_2 X_2) \\\\\n    & = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_2) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_2)\n\\end{aligned}\n\nLet the new variable X_1 = X_2, we get the folded polynomial as\\hat{f}^{(2)}(X_1) = ((a_0 + \\alpha_1 a_1) + (a_4 + \\alpha_1 a_5) X_1) + \\alpha_2 ((a_2 + \\alpha_1 a_3) + (a_6 + \\alpha_1 a_7)  X_1)\n\nWe can find that for multiple folds, folding using univariate polynomials and directly folding using multivariate linear polynomials are equivalent. The process of folding a multivariate linear polynomial with random numbers (\\alpha_1, \\alpha_2) is just the process of direct variable substitution, i.e., we get \\hat{f}^{(2)}(X_1) = \\hat{f}(\\alpha_1, \\alpha_2, X_1).\n\nBelow we introduce the definition of the folding function given in the paper [ACFY24b], which is consistent with the folding method in the FRI protocol.\n\nDefinition 1 [ACFY24b, Definition 4.14] Let f: \\mathcal{L} \\rightarrow \\mathbb{F} be a function, \\alpha \\in \\mathbb{F}. Define \\mathrm{Fold}(f, \\alpha): \\mathcal{L}^2 \\rightarrow \\mathbb{F} as follows:\\forall x \\in \\mathcal{L}^2, \\; \\mathrm{Fold}(f, \\alpha)(x^2) = \\frac{f(x) + f(-x)}{2} + \\alpha \\cdot \\frac{f(x) - f(-x)}{2 \\cdot x}\n\nTo calculate \\mathrm{Fold}(f, \\alpha)(x^2), it’s sufficient to query the values of f at x and -x.\n\nFor k \\le m and \\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_k) \\in \\mathbb{F}^k, define \\mathrm{Fold}(f, \\boldsymbol{\\alpha}) : \\mathcal{L}^{(2^k)} \\rightarrow \\mathbb{F}, denote \\mathrm{Fold}(f, \\boldsymbol{\\alpha}) := f_k, recursively define: f_0 := f and f_i := \\mathrm{Fold}(f_{i-1}, \\alpha_i).\n\nThe following proposition tells us that folding a Reed-Solomon code on any set of points still results in a Reed-Solomon code. ([ACFY24b])\n\nProposition 1 [ACFY24b, Claim 4.15] Let f: \\mathcal{L} \\rightarrow \\mathbb{F} be a function, \\boldsymbol{\\alpha} \\in \\mathbb{F}^k represent folding random numbers, let g:= \\mathrm{Fold}(f, \\boldsymbol{\\alpha}). If f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] and k \\le m, then g \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}^{(2^k)}, m-k], and the multilinear extension of g is \\hat{g}(X_k, \\ldots, X_m) := \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m), where \\hat{f} is the multilinear extension of f.\n\nThe \\hat{g}(X_k, \\ldots, X_m) := \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m) given in the proposition is consistent with the folding of the univariate polynomial f and the direct folding of the multivariate linear polynomial \\hat{f} with random numbers mentioned above. From the perspective of multivariate linear polynomials, it is just direct variable substitution with random numbers \\boldsymbol{\\alpha}, i.e., \\hat{f}(\\boldsymbol{\\alpha}, X_k, \\ldots, X_m).\n\nRecalling the FRI protocol, it continuously folds the univariate polynomial f with random numbers (\\alpha_1, \\ldots, \\alpha_m) until finally obtaining a constant polynomial. From the perspective of multivariate linear polynomials, we would eventually get \\hat{f}(\\alpha_1, \\ldots, \\alpha_m) as a constant. Connecting to the Sumcheck protocol, the last step also requires obtaining the value of a multivariate polynomial at a certain random point, and the verifier needs to obtain this value for verification. This step is usually implemented using an oracle. Now the FRI protocol can also provide the value of \\hat{f}(\\alpha_1, \\ldots, \\alpha_m) at a random point at the end. If the Sumcheck protocol and the FRI protocol choose the same random point (\\alpha_1, \\ldots, \\alpha_m), then the FRI protocol can directly provide the value needed for the last step of the Sumcheck protocol when it reaches the end. Combining the FRI protocol and the Sumcheck protocol in this way is the idea of the BaseFold protocol [ZCF24].","type":"content","url":"/whir-pcs/whir#from-univariate-polynomials-to-multivariate-linear-polynomials","position":3},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"CRS: Constrained Reed-Solomon codes"},"type":"lvl2","url":"/whir-pcs/whir#crs-constrained-reed-solomon-codes","position":4},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"CRS: Constrained Reed-Solomon codes"},"content":"Below is the definition of constrained Reed-Solomon codes given in the WHIR paper [ACFY24b]. It is a subset of Reed-Solomon codes, but with an additional constraint similar to Sumcheck.\n\nDefinition 2 [ACFY24b, Definition 1] For a field \\mathbb{F}, smooth evaluation domain \\mathcal{L} \\subseteq \\mathbb{F}, number of variables m \\in \\mathbb{N}, weight polynomial \\hat{w} \\in \\mathbb{F}[Z, X_1, \\ldots, X_m], and target \\sigma \\in \\mathbb{F}, the constrained Reed-Solomon code is defined as\\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma] := \\left\\{ f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m]: \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma \\right\\}.\n\nFrom the definition, we can see that CRS (constrained Reed-Solomon code) is first a RS code, i.e., f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] in the definition, but on top of this, it needs to satisfy a summation constraint similar to Sumcheck \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma.\n\nThe paper [ACFY24b] mentions that the weight polynomial \\hat{w} in the definition can be defined by oneself and has wide applications. The paper gives such an example: an evaluation constraint \\hat{f}(\\mathbf{z}) = \\sigma, which constrains the value of the multivariate polynomial \\hat{f} at point \\mathbf{z} \\in \\mathbb{F}^m to be the target value \\sigma. First, perform a multilinear extension on f \\in \\mathrm{RS}[\\mathbb{F}, \\mathcal{L}, m] to get\\hat{f}(\\mathbf{X}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} f(\\mathbf{b}) \\cdot \\mathrm{eq}(\\mathbf{b}, \\mathbf{X})\n\nwhere \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = \\prod_{i=1}^m (b_i X_i + (1 - b_i) \\cdot (1 - X_i)). Therefore, when \\mathbf{b},\\mathbf{X} \\in \\{0,1\\}^m, if \\mathbf{b} = \\mathbf{X}, then \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = 1, if \\mathbf{b} \\neq \\mathbf{X}, then \\mathrm{eq}(\\mathbf{b}, \\mathbf{X}) = 0. Thus\\hat{f}(\\mathbf{z}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} f(\\mathbf{b}) \\cdot \\mathrm{eq}(\\mathbf{b}, \\mathbf{z}) = \\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b})\n\nThe weight polynomial \\hat{w}(Z, \\mathbf{X}) can be defined as\\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}).\n\nIn this way, an evaluation constraint can be represented using the weight polynomial. Based on this, the corresponding PCS can be constructed (from [ACFY24b, 1.1 Hash-based PCS from CRS codes]), in two cases:\n\nConstrain the value of the multivariate linear polynomial \\hat{f} at \\mathbf{z} \\in \\mathbb{F}^m to be \\sigma, let the weight polynomial be\n\\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}).\n\nConstrain the value of a univariate polynomial f at z \\in \\mathbb{F} to be \\sigma, convert this case to the case of multivariate linear polynomials, consider the evaluation point as \\mathbf{z} = (z^{2^0}, \\ldots, z^{2^{m-1}}), then the weight polynomial is\n\\hat{w}(Z, X) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, (z^{2^0}, \\ldots, z^{2^{m-1}})).","type":"content","url":"/whir-pcs/whir#crs-constrained-reed-solomon-codes","position":5},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"One Iteration of WHIR"},"type":"lvl2","url":"/whir-pcs/whir#one-iteration-of-whir","position":6},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"One Iteration of WHIR"},"content":"As mentioned earlier, BaseFold combined the Sumcheck and FRI protocols, while the WHIR protocol combines the ideas of BaseFold and STIR, replacing the FRI protocol in BaseFold with the STIR protocol. Compared to the FRI protocol, the STIR protocol has a smaller query complexity. The core idea of the STIR protocol is to reduce the rate of each iteration, increasing the redundancy in the messages sent by the Prover, thereby reducing the Verifier’s query complexity.\n\nLet’s delve into one iteration of the WHIR protocol (from [ACFYb, 2.1.3 WHIR protocol]) to see how WHIR specifically combines BaseFold and the STIR protocol. After one iteration, the problem of testing the proximity of f \\in \\mathcal{C} := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma] is transformed into testing f' \\in \\mathcal{C}' := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'].\n\nSumcheck rounds. Prover and Verifier interact for k rounds of Sumcheck for the constraint in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma]\\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma\n\nwhere \\hat{f} is the multivariate linear polynomial corresponding to f.\n\na. Prover sends a univariate polynomial \\hat{h}_1(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-1}} \\hat{w}(\\hat{f}(X, \\mathbf{b}), X, \\mathbf{b}) to Verifier, Verifier checks \\hat{h}_1(0) + \\hat{h}_1(1) = \\sigma, selects a random number \\alpha_1 \\leftarrow \\mathbb{F} and sends it, the sumcheck claim becomes \\hat{h}_1(\\alpha_1) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-1}} \\hat{w}(\\hat{f}(\\alpha_1, \\mathbf{b}), \\alpha_1, \\mathbf{b}).\nb. For the i-th round, i from 2 to k, Prover sends a univariate polynomial\\hat{h}_i(X) := \\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-i}} \\hat{w}(\\hat{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, X, \\mathbf{b})\n\nVerifier checks \\hat{h}_{i}(0) + \\hat{h}_{i}(1) = \\hat{h}_{i-1}(\\alpha_{i-1}), selects a random number \\alpha_i \\leftarrow \\mathbb{F}, the sumcheck claim becomes\\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-i}} \\hat{w}(\\hat{f}(\\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}), \\alpha_1, \\ldots, \\alpha_{i - 1}, \\alpha_i, \\mathbf{b}) = \\hat{h}_i(\\alpha_i)\n\nTherefore, after the above k rounds of sumcheck, prover has sent polynomials (\\hat{h}_1, \\ldots, \\hat{h}_k), verifier has selected random numbers \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k) \\in \\mathbb{F}^k. The initial claim becomes the following statement\\sum_{\\mathbf{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(\\hat{f}(\\boldsymbol{\\alpha}, \\mathbf{b}), \\boldsymbol{\\alpha}, \\mathbf{b}) = \\hat{h}_k(\\alpha_k)\n\nSend folded function. Prover sends function g: \\mathcal{L}^{(2)} \\rightarrow \\mathbb{F}. In the case of an honest Prover, \\hat{g} \\equiv \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), g is defined as the evaluation of \\hat{g} on domain \\mathcal{L}^{(2)}.\n\nThis means first folding \\hat{f} 2^k times with random numbers \\boldsymbol{\\alpha} to get \\hat{g} = \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), at this point \\hat{g} : \\mathcal{L}^{(2^k)} \\rightarrow \\mathbb{F}, with its domain range as \\mathcal{L}^{(2^k)}. Since \\hat{g} is essentially a polynomial, we can change the domain of its variables to \\mathcal{L}^{(2)}, the function g is consistent with the evaluation of \\hat{g} on \\mathcal{L}^{(2)}.\n\nOut-of-domain sample. Verifier selects a random number z_0 \\leftarrow \\mathbb{F} and sends it to Prover. Let \\boldsymbol{z}_0 := (z_0^{2^0}, \\ldots, z_0^{2^{m-k - 1}}).\n\nOut-of-domain answers. Prover sends y_0 \\in \\mathbb{F}. In the honest case, y_0 := \\hat{g}(\\boldsymbol{z}_0).\n\nShift queries and combination randomness. For Verifier, for each i \\in [t], select random numbers z_i \\leftarrow \\mathcal{L}^{(2^k)} and send, obtain y_i := \\mathrm{Fold}(f, \\boldsymbol{\\alpha})(z_i) by querying f. Let \\boldsymbol{z}_i := (z_i^{2^0}, \\ldots, z_i^{2^{m- k - 1}}). Verifier also selects a random number \\gamma \\leftarrow \\mathbb{F} and sends.\n\nRecursive claim. Prover and Verifier define new weight polynomial and target value:\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\\sigma' := \\hat{h}_k(\\alpha_k) + \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i,\n\nThen, recursively test g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'].\n\nFirst, let’s explain that the constraint in g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'] is correct, i.e., prove\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}'(g(\\boldsymbol{b}), \\boldsymbol{b}) = \\sigma'\n\nSubstituting \\hat{w}' and \\sigma', we get\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) + \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = \\hat{h}_k(\\alpha_k) + \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\nWe prove this in two parts:\n\nProve\n\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) = \\hat{h}_k(\\alpha_k)\n\nFrom step 2 of the protocol, we know g(\\boldsymbol{b}) = \\hat{f}(\\boldsymbol{\\alpha}, \\boldsymbol{b}), therefore\\begin{aligned}\n    \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(g(\\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) & = \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} \\hat{w}(\\hat{f}(\\boldsymbol{\\alpha}, \\boldsymbol{b}), \\boldsymbol{\\alpha}, \\boldsymbol{b}) \\\\\n    & = \\hat{h}_k(\\alpha_k)\n\\end{aligned}\n\nThe last equation is obtained from the final claim of the sumcheck in step 1 of the protocol.\n\nProve\\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\nProof:\\begin{aligned}\n    \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}}  g(\\boldsymbol{b}) \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) & =  \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} g(\\boldsymbol{b}) \\cdot  \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) \\\\\n    & = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot g(\\boldsymbol{z}_i) \\\\\n    & = \\sum_{i = 0}^t \\gamma^{i+1} \\cdot y_i\n\\end{aligned}\n\nWhere \\sum_{\\boldsymbol{b} \\in \\{0,1\\}^{m-k}} g(\\boldsymbol{b}) \\cdot  \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{b}) = g(\\boldsymbol{z}_i) is precisely what we mentioned earlier about using the weight polynomial \\hat{w}(Z, \\mathbf{X}) = Z \\cdot \\mathrm{eq}(\\mathbf{X}, \\mathbf{z}) to constrain the value of a multivariate linear polynomial at a certain point.\n\nThis also explains that the constraint definition in g \\in \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'] is correct.\n\nThe definition of the new weight polynomial \\hat{w}' is\\hat{w}'(Z, \\boldsymbol{X}) := \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) + Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X})\n\nIt consists of two parts:\n\nThe first part \\hat{w}(Z, \\boldsymbol{\\alpha}, \\boldsymbol{X}) constrains the correctness of k rounds of sumcheck in step 1 of the protocol.\n\nThe second part Z \\cdot \\sum_{i = 0}^t \\gamma^{i+1} \\cdot \\mathrm{eq}(\\boldsymbol{z}_i, \\boldsymbol{X}) constrains that the values of g at \\boldsymbol{z}_i are correct, and uses random number \\gamma to linearly combine these t + 1 constraints.\na. The constraint g(\\boldsymbol{z}_0) = y_0 is actually verifying the correctness of out-of-domain answers.\nb. For i \\in [t], the constraint g(\\boldsymbol{z}_i) = y_i is requiring the correctness of shift queries.\n\nThis also shows the flexibility of the weight polynomial definition, which can implement multiple constraints at once.","type":"content","url":"/whir-pcs/whir#one-iteration-of-whir","position":7},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and BaseFold","lvl2":"One Iteration of WHIR"},"type":"lvl3","url":"/whir-pcs/whir#connection-between-whir-and-basefold","position":8},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and BaseFold","lvl2":"One Iteration of WHIR"},"content":"WHIR adopts the idea of BaseFold, and the definition of CRS itself introduces a constraint similar to sumcheck. In step 1 of the protocol, it first performs k rounds of sumcheck, where the random numbers \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_k) selected for sumcheck are completely consistent with the random numbers used for folding \\hat{f} later, i.e., in step 2 of the protocol \\hat{g} = \\hat{f}(\\boldsymbol{\\alpha}, \\cdot), where \\hat{f} is folded 2^k times.","type":"content","url":"/whir-pcs/whir#connection-between-whir-and-basefold","position":9},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and STIR","lvl2":"One Iteration of WHIR"},"type":"lvl3","url":"/whir-pcs/whir#connection-between-whir-and-stir","position":10},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl3":"Connection between WHIR and STIR","lvl2":"One Iteration of WHIR"},"content":"After using sumcheck in step 1 of the protocol, the subsequent steps 2-5 are similar to the STIR protocol. The following figure shows one iteration of the STIR protocol.\n\nThe core idea of the STIR protocol is to reduce the rate in each iteration. Specifically, in the next iteration, the folded polynomial \\hat{g} is not evaluated on \\mathcal{L}^{(2^k)}, but instead chooses to evaluate on a domain \\mathcal{L}^{(2)} that is only half the size of the original domain \\mathcal{L}. This corresponds to step 2 of the WHIR protocol. The benefit of doing this is that it greatly increases the redundancy of the sent messages, reducing the query complexity of the verifier.\n\nFor f \\in \\mathcal{C} := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}, m, \\hat{w}, \\sigma], its rate is \\rho = \\frac{2^m}{|\\mathcal{L}|}, and after one WHIR iteration f' \\in \\mathcal{C}' := \\mathrm{CRS}[\\mathbb{F}, \\mathcal{L}^{(2)}, m - k, \\hat{w}', \\sigma'], its rate is\\rho' = \\frac{2^{m - k}}{|\\mathcal{L}^{(2)}|} = \\frac{2^{m - k}}{\\frac{|\\mathcal{L}|}{2}} = \\frac{2^{m - k + 1}}{|\\mathcal{L}|} = 2^{1 - k} \\cdot \\rho = \\left(\\frac{1}{2}\\right)^{k - 1} \\cdot \\rho\n\nWhen k \\ge 2, we can see that \\rho' will be smaller than \\rho, the rate decreases.","type":"content","url":"/whir-pcs/whir#connection-between-whir-and-stir","position":11},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Mutual correlated agreement"},"type":"lvl2","url":"/whir-pcs/whir#mutual-correlated-agreement","position":12},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Mutual correlated agreement"},"content":"The correlated agreement theorem given in the [BCIKS20] paper is a key theorem for proving the security of FRI and STIR protocols, which ensures that the process of folding the original function with random numbers in the FRI protocol or STIR protocol is secure. In the security analysis of WHIR, a new concept of mutual correlated agreement is introduced, which has a stronger conclusion than correlated agreement.\n\n[ACFYb, 1.2 Mutual correlated agreement] gives the related definitions of correlated agreement and mutual correlated agreement. A code \\mathcal{C} := \\text{RS}[\\mathbb{F}, \\mathcal{L}, m] has (\\delta, \\varepsilon)-correlated agreement means: for every f_1, \\ldots, f_\\ell, under the uniform selection of \\alpha \\leftarrow \\mathbb{F}, with probability 1 - \\varepsilon: if there exists a set S \\subseteq \\mathcal{L}, where |S| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|, f^*_\\alpha := \\sum_{i=1}^\\ell \\alpha^{i-1} \\cdot f_i is consistent with \\mathcal{C} on S, then there exists a set T \\subseteq \\mathcal{L}, where |T| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|, each f_i is consistent with \\mathcal{C} on T.\n\nIn the above definition, describing a function f as “consistent” with a code \\mathcal{C} on a set S means that there exists a codeword u \\in \\mathcal{C} in the encoding space such that for any x \\in S, f(x) = u(x).\n\nThe definition of correlated agreement is shown in the following figure (refer to the video \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification).\n\nThe [BCIKS20] paper shows that a Reed-Solomon code with rate \\rho has (\\delta, \\varepsilon)-correlated agreement, where \\delta \\in (0, 1 - \\sqrt{\\rho}), \\varepsilon := \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}. In other words, if \\delta \\in (0, 1 - \\sqrt{\\rho}) and\\Pr_{\\alpha \\in \\mathbb{F}} \\left[ \\Delta(f^*_\\alpha, \\mathcal{C}) \\le \\delta \\right] > \\varepsilon = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}\n\nthen, there exist sets T \\subseteq \\mathcal{L}, and codes c_0, \\ldots, c_l \\in \\mathcal{C} such that\n\n|T| \\geq (1 - \\delta) \\cdot |\\mathcal{L}|\n\nEach f_i is consistent with c_i on T\n\nIt can be found that the definition of (\\delta, \\varepsilon)-correlated agreement does not require the sets S and T to be the same set, while in WHIR, a concept stronger than correlated agreement is introduced, called mutual correlated agreement, which requires the sets S and T to be the same set. As shown in the following figure (refer to the video \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification):\n\nThe WHIR paper gives the following conjecture about mutual correlated agreement.\n\nConjecture 1 [ACFY24b, Conjecture 1] (informal). For every Reed-Solomon code \\mathcal{C} = \\text{RS}[\\mathbb{F}, \\mathcal{L}, m], if it has (\\delta, \\varepsilon)-correlated agreement, where \\varepsilon = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}, then it has (\\delta, \\varepsilon')-mutual correlated agreement, where \\varepsilon' = \\frac{\\text{poly}(2^m, 1/\\rho)}{|\\mathbb{F}|}.\n\nThe WHIR paper proves that in the case of unique decoding, i.e., when \\delta \\in (0, \\frac{1 - \\rho}{2}), Conjecture 1 holds with \\varepsilon' = \\varepsilon. This also connects correlated agreement with mutual correlated agreement.","type":"content","url":"/whir-pcs/whir#mutual-correlated-agreement","position":13},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Summary"},"type":"lvl2","url":"/whir-pcs/whir#summary","position":14},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"Summary"},"content":"The WHIR protocol combines the ideas of BaseFold and STIR. First, for univariate polynomials in RS encoding, they can be viewed as equivalent multivariate linear polynomials through variable substitution, and the folding of univariate polynomials is also equivalent to folding the corresponding multivariate linear polynomials. This allows WHIR to support both univariate polynomials and multivariate linear polynomials.\n\nSecondly, a new CRS encoding definition is given, adding a constraint similar to sumcheck on the basis of RS encoding, which is a constraint similar to sumcheck on the weight polynomial \\hat{w}. The flexibility of the weight polynomial definition allows multiple constraints to be required at once in the protocol, including constraining the correctness of sumcheck, the correctness of out-of-domain answers, and the correctness of shift queries.\n\nThen, by delving into one iteration of the WHIR protocol, we can see its connection with BaseFold and STIR protocols. The key here is still to build a bridge between univariate polynomials and multivariate linear polynomials, allowing free switching between the univariate function f and the multivariate linear polynomial \\hat{f}. Through the introduction of CRS, the goal of the protocol is increased to verify the correctness of a constraint similar to sumcheck,\\sum_{\\mathbf{b} \\in \\{0,1\\}^m} \\hat{w}(\\hat{f}(\\mathbf{b}), \\mathbf{b}) = \\sigma\n\nTherefore, combining the idea of BaseFold, first perform k rounds of sumcheck, replacing k variables in the multivariate linear polynomial \\hat{f} with the random numbers \\boldsymbol{\\alpha} from the sumcheck protocol. The folding of \\hat{f} still uses the same random numbers \\boldsymbol{\\alpha}. Combining the idea of STIR, to reduce the rate, the folded function is evaluated on a larger domain \\mathcal{L}^{(2)}. The subsequent steps of out-of-domain sample and shift queries in the WHIR protocol are similar to the STIR protocol.\n\nFinally, we introduced the mutual correlated agreement conclusion used in the security proof of the WHIR protocol, which is stronger than the correlated agreement conclusion.","type":"content","url":"/whir-pcs/whir#summary","position":15},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"References"},"type":"lvl2","url":"/whir-pcs/whir#references","position":16},{"hierarchy":{"lvl1":"Note on WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","lvl2":"References"},"content":"[ACFY24a] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “STIR: Reed-Solomon proximity testing with fewer queries.” In Annual International Cryptology Conference, pp. 380-413. Cham: Springer Nature Switzerland, 2024.\n\n[ACFY24b] Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon Proximity Testing with Super-Fast Verification.” Cryptology ePrint Archive (2024).\n\n[BBHR18] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: Proceedings of the 45th International Colloquium on Automata, Languages and Programming (ICALP), 2018.\n\n[BCIKS20] Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. Proximity Gaps for Reed–Solomon Codes. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, pages 900–909, 2020.\n\n[ZCF24] Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: efficient field-agnostic polynomial commitment schemes from foldable codes.” Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2024.\n\nBlog: \n\nWHIR: Reed–Solomon Proximity Testing with Super-Fast Verification\n\nvideo: \n\nZK12: WHIR: Reed-Solomon Proximity Testing with Super-Fast Verification","type":"content","url":"/whir-pcs/whir#references","position":17},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)"},"type":"lvl1","url":"/zeromorph/zeromorph-02","position":0},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)"},"content":"","type":"content","url":"/zeromorph/zeromorph-02","position":1},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"1. Optimization Ideas"},"type":"lvl2","url":"/zeromorph/zeromorph-02#id-1-optimization-ideas","position":2},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"1. Optimization Ideas"},"content":"","type":"content","url":"/zeromorph/zeromorph-02#id-1-optimization-ideas","position":3},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"2. Protocol Description"},"type":"lvl2","url":"/zeromorph/zeromorph-02#id-2-protocol-description","position":4},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"2. Protocol Description"},"content":"","type":"content","url":"/zeromorph/zeromorph-02#id-2-protocol-description","position":5},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl3","url":"/zeromorph/zeromorph-02#evaluation-proof-protocol","position":6},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"","type":"content","url":"/zeromorph/zeromorph-02#evaluation-proof-protocol","position":7},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#common-inputs","position":8},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Commitment \\mathsf{cm}(f) of the MLE polynomial \\tilde{f} mapped to the univariate polynomial f(X)=[[\\tilde{f}]]_n\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/zeromorph/zeromorph-02#common-inputs","position":9},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#witness","position":10},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Evaluation vector \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1}) of the MLE polynomial \\tilde{f}","type":"content","url":"/zeromorph/zeromorph-02#witness","position":11},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#round-1","position":12},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Prover computes n remainder MLE polynomials \\{\\tilde{q}_i\\}_{i=0}^{n-1}\n\nProver constructs univariate polynomials q_i=[[\\tilde{q}_i]]_i, \\quad 0 \\leq i < n mapped from remainder MLE polynomials\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{i=0}^{n-1} (X_i-u_i) \\cdot \\tilde{q}_i(X_0,X_1,\\ldots, X_{i-1})\n\nProver computes and sends their commitments: \\mathsf{cm}(q_0), \\mathsf{cm}(q_1), \\ldots, \\mathsf{cm}(q_{n-1})","type":"content","url":"/zeromorph/zeromorph-02#round-1","position":13},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#round-2","position":14},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Verifier sends a random number \\beta\\in \\mathbb{F}_q^*\n\nProver constructs g(X) as an aggregation polynomial of \\{q_i(X)\\}, satisfyingg(X^{-1}) = \\sum_{i=0}^{n-1} \\beta^i \\cdot X^{-2^i+1}\\cdot q_i(X)\n\nProver computes and sends the commitment \\mathsf{cm}(g) of g(X)","type":"content","url":"/zeromorph/zeromorph-02#round-2","position":15},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#round-3","position":16},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^* to challenge the polynomial evaluation at X=\\zeta\n\nProver computes g(\\zeta^{-1}) and calculates the quotient polynomial q_g(X)q_g(X) = \\frac{g(X) - g(\\zeta^{-1})}{X-\\zeta^{-1}}\n\nProver constructs linearization polynomials r_\\zeta(X) and s_\\zeta(X)\n\nComputes r_\\zeta(X):r_\\zeta(X) = f(X) - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot q_i(X)\n\nComputes s_\\zeta(X), which equals zero at X=\\zeta:s_\\zeta(X) = g(\\zeta^{-1}) - \\sum_i\\beta^i\\zeta^{2^i-1}\\cdot q_i(X)\n\nComputes quotient polynomials w_r(X) and w_s(X):w_r(X) = \\frac{r_\\zeta(X)}{X-\\zeta}, \\qquad w_s(X) = \\frac{s_\\zeta(X)}{X-\\zeta}\n\nComputes and sends commitment \\mathsf{cm}(q_g)","type":"content","url":"/zeromorph/zeromorph-02#round-3","position":17},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#round-4","position":18},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Verifier sends a random number \\alpha\\in \\mathbb{F}_p^* to aggregate w_r(X) and w_s(X)\n\nProver computes w(X) and sends its commitment \\mathsf{cm}(w):w(X) = w_r(X) + \\alpha\\cdot w_s(X)","type":"content","url":"/zeromorph/zeromorph-02#round-4","position":19},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#proof","position":20},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"Total of n+3 elements in \\mathbb{G}_1 and 1 element in \\mathbb{F}_q:\\pi= \\Big( \\mathsf{cm}(q_0), \\mathsf{cm}(q_1), \\ldots, \\mathsf{cm}(q_{n-1}), \\mathsf{cm}(g), \\mathsf{cm}(q_g), \\mathsf{cm}(w), g(\\zeta^{-1})\\Big)","type":"content","url":"/zeromorph/zeromorph-02#proof","position":21},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"type":"lvl4","url":"/zeromorph/zeromorph-02#verification","position":22},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"2. Protocol Description"},"content":"The Verifier:\n\nConstructs commitment \\mathsf{cm}(r_\\zeta):\\mathsf{cm}(r_\\zeta) = \\mathsf{cm}(f) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(q_i)\n\nConstructs commitment \\mathsf{cm}(s_\\zeta):\\mathsf{cm}(s_\\zeta) = g(\\zeta^{-1})\\cdot[1]_1 - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{-2^i+1}\\cdot \\mathsf{cm}(q_i)\n\nVerifies that r_\\zeta(\\zeta) = 0 and s_\\zeta(\\zeta) = 0:e(\\mathsf{cm}(r_\\zeta) + \\alpha\\cdot \\mathsf{cm}(s_\\zeta), \\ [1]_2) = e(\\mathsf{cm}(w),\\ [\\tau]_2 - \\zeta\\cdot [1]_2)\n\nWhich can be transformed into the following pairing equation:e(\\mathsf{cm}(r_\\zeta) + \\alpha\\cdot \\mathsf{cm}(s_\\zeta) + \\zeta\\cdot\\mathsf{cm}(w), \\ [1]_2) = e(\\mathsf{cm}(w),\\ [\\tau]_2)\n\nVerifies the correctness of g(\\zeta^{-1}):e(\\mathsf{cm}(g) - g(\\zeta^{-1})\\cdot [1]_1 + \\zeta^{-1}\\cdot\\mathsf{cm}(q_g),\\  [1]_2) = e(\\mathsf{cm}(q_g), \\ [\\tau]_2)","type":"content","url":"/zeromorph/zeromorph-02#verification","position":23},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"3. Performance Analysis"},"type":"lvl2","url":"/zeromorph/zeromorph-02#id-3-performance-analysis","position":24},{"hierarchy":{"lvl1":"Zeromorph-PCS (Part II)","lvl2":"3. Performance Analysis"},"content":"","type":"content","url":"/zeromorph/zeromorph-02#id-3-performance-analysis","position":25},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI"},"type":"lvl1","url":"/zeromorph/zeromorph-fri","position":0},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI"},"content":"Jade Xie \n\njade@secbit.io\n\nYu Guo \n\nyu.guo@secbit.io\n\nIn our previous article, we introduced how the zeromorph protocol can be integrated with KZG to create a Polynomial Commitment Scheme (PCS) for multilinear polynomials. This article discusses how the zeromorph protocol can be integrated with FRI to form another PCS.","type":"content","url":"/zeromorph/zeromorph-fri","position":1},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"Integration with FRI"},"type":"lvl2","url":"/zeromorph/zeromorph-fri#integration-with-fri","position":2},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"Integration with FRI"},"content":"As previously explained, the zeromorph protocol ultimately reduces to proving a key equation:\\hat{f}(X) - v\\cdot\\Phi_n(X) = \\sum_{k = 0}^{n - 1} \\Big(X^{2^k}\\cdot \\Phi_{n-k-1}(X^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(X^{2^k})\\Big)\\cdot \\hat{q}_k(X)\n\nAdditionally, to prevent cheating by the Prover, the quotient polynomials \\hat{q}_k(X) must have degrees less than 2^k.\n\nTo prove that the equation holds, the Verifier can randomly select a point X = \\zeta and ask the Prover to provide the values of \\hat{f}(\\zeta) and \\hat{q}_k(\\zeta), allowing the Verifier to check whether the following equation holds:\\hat{f}(\\zeta) - v\\cdot\\Phi_n(\\zeta) = \\sum_{k = 0}^{n - 1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta)\n\nWhen integrating zeromorph with the FRI protocol, we can use FRI’s PCS to provide the values of \\hat{f}(\\zeta) and \\hat{q}_k(\\zeta), and employ FRI’s low degree test to prove that \\deg(\\hat{q_k}) < 2^k.\n\nFor example, if the Prover wants to prove the correctness of the provided value \\hat{f}(\\zeta). Proving this value is correct is equivalent to proving that the quotient polynomial\\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta}\n\nexists. For this quotient polynomial to exist, its degree must be less than 2^{n} - 1. To integrate with the FRI protocol for low degree testing, the degree needs to be aligned to a power of 2, requiring a degree correction. The Verifier can provide a random number \\lambda, and we define:q_{\\hat{f}_{\\zeta}}(X) = \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta}\n\nThen we use the FRI protocol to prove that this quotient polynomial has degree less than 2^n.","type":"content","url":"/zeromorph/zeromorph-fri#integration-with-fri","position":3},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Commitment Phase","lvl2":"Integration with FRI"},"type":"lvl3","url":"/zeromorph/zeromorph-fri#commitment-phase","position":4},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Commitment Phase","lvl2":"Integration with FRI"},"content":"To commit to a multilinear extension (MLE) polynomial with n variables:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nWe first map its values on the hypercube (a_0, \\ldots, a_{N - 1}) to a univariate polynomial \\hat{f}(X):\\hat{f}(X) = a_0 + a_1 X + \\cdots + a_{N-1} X^{N - 1}\n\nFor the FRI protocol, we select a multiplicative subgroup D = D_0 in field \\mathbb{F} with size equal to a power of 2, where:D_n \\subseteq D_{n - 1} \\subseteq \\ldots \\subseteq D_0\n\nwith |D_{i - 1}|/|D_{i}| = 2, and rate parameter \\rho = N / |D_0|. In the following protocol, we describe folding n times, resulting in a constant polynomial. In practice, we might fold to a polynomial with a small degree, with slight protocol adjustments. The FRI commitment to function \\hat{f} is a commitment to the Reed-Solomon encoding of \\hat{f}(X) over D:\\mathsf{cm}(\\hat{f}(X)) = \\mathsf{cm}([\\hat{f}(x)|_{x \\in D}])\n\nIn implementation, a Merkle tree is typically used to commit to [\\hat{f}(x)|_{x \\in D}]:\\mathsf{cm}(\\hat{f}(X)) = \\mathsf{MT.Commit}([\\hat{f}(x)|_{x \\in D}])\n\nThe Prover sends the root hash of this Merkle tree as the commitment to [\\hat{f}(x)|_{x \\in D}].","type":"content","url":"/zeromorph/zeromorph-fri#commitment-phase","position":5},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl3","url":"/zeromorph/zeromorph-fri#evaluation-proof-protocol","position":6},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"","type":"content","url":"/zeromorph/zeromorph-fri#evaluation-proof-protocol","position":7},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#common-inputs","position":8},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Commitment to MLE polynomial \\tilde{f}: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})\n\nRate parameter: \\rho\n\nFRI low degree test query repetition parameter: l (in practice, l depends on security parameters, security assumptions, and rate)\n\nMultiplicative subgroups for FRI encoding: D, D^{(0)}, \\ldots, D^{(n - 1)}","type":"content","url":"/zeromorph/zeromorph-fri#common-inputs","position":9},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#witness","position":10},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Point value vector of MLE polynomial \\tilde{f} on the n-dimensional HyperCube: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/zeromorph/zeromorph-fri#witness","position":11},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-1","position":12},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Prover sends commitments to remainder polynomials:\n\nCompute n remainder MLE polynomials \\{\\tilde{q}_k\\}_{k=0}^{n-1}, satisfying:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{k=0}^{n-1} (X_k-u_k) \\cdot \\tilde{q}_k(X_0,X_1,\\ldots, X_{k-1})\n\nConstruct the univariate polynomials corresponding to the remainder MLE polynomials: \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n\n\nCompute and send their commitments: \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1}), where these commitments are FRI commitments to \\hat{q}_0, \\ldots, \\hat{q}_{n - 1} with multiplicative subgroup D^{(k)} = D^{(k)}_0 for \\hat{q}_k:\\mathsf{cm}(\\hat{q}_k(X)) = \\mathsf{cm}([\\hat{q}_k(x)|_{x \\in D^{(k)}}]) = \\mathsf{MT.commit}([\\hat{q}_k(x)|_{x \\in D^{(k)}}])\n\nwhere |D^{(k)}| = 2^k / \\rho.","type":"content","url":"/zeromorph/zeromorph-fri#round-1","position":13},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-2","position":14},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Verifier sends random value \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F} \\setminus D\n\nProver computes and sends \\hat{f}(\\zeta)\n\nProver computes and sends \\hat{q}_k(\\zeta), \\, 0 \\le k < n.","type":"content","url":"/zeromorph/zeromorph-fri#round-2","position":15},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-3","position":16},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Verifier sends random value \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver computes:q_{f_\\zeta}(X) = \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta}\n\nat points in D:[q_{f_\\zeta}(x)|_{x \\in D}] = \\big[\\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta} + \\lambda \\cdot x \\cdot \\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta}\\big|_{x \\in D} \\big]\n\nFor 0 \\le k < n, Prover computes:q_{\\hat{q}_k}(X) = \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta}\n\nat points in D^{(k)}.","type":"content","url":"/zeromorph/zeromorph-fri#round-3","position":17},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-4","position":18},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Prover and Verifier engage in the FRI protocol’s low degree test to prove that q_{f_\\zeta}(X) has degree less than 2^n:\\pi_{q_{f_\\zeta}} \\leftarrow \\mathsf{FRI.LDT}(q_{f_\\zeta}(X), 2^n)\n\nThis involves n rounds of interaction, folding the original polynomial to a constant polynomial:\n\nDefine q_{f_\\zeta}^{(0)}(x)|_{x \\in D} := q_{f_\\zeta}(x)|_{x \\in D}\n\nFor i = 1,\\ldots, n:\n\nVerifier sends random value \\alpha^{(i)}\n\nFor any y \\in D_i, find x \\in D_{i - 1} such that y = x^2, Prover computes:q_{f_\\zeta}^{(i)}(y) = \\frac{q_{f_\\zeta}^{(i - 1)}(x) + q_{f_\\zeta}^{(i - 1)}(-x)}{2} + \\alpha^{(i)} \\cdot \\frac{q_{f_\\zeta}^{(i - 1)}(x) - q_{f_\\zeta}^{(i - 1)}(-x)}{2x}\n\nIf i < n, Prover sends the Merkle Tree commitment to [q_{f_\\zeta}^{(i)}(x)|_{x \\in D_{i}}]:\\mathsf{cm}(q_{f_\\zeta}^{(i)}(X)) = \\mathsf{MT.commit}([q_{f_\\zeta}^{(i)}(x)|_{x \\in D_{i}}])\n\nIf i = n, Prover selects any x_0 \\in D_{n} and sends the value q_{f_\\zeta}^{(i)}(x_0).\n\n📝 Notes\n\nIf the folding count r < n, the final result won’t be a constant polynomial. In this case, the Prover would send a Merkle Tree commitment in round r rather than a single value.","type":"content","url":"/zeromorph/zeromorph-fri#round-4","position":19},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-5","position":20},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"This round continues the FRI protocol’s low degree test query phase. The Verifier performs l repeated queries, each time randomly selecting a value from D_0 and asking the Prover to send the values from each folding round along with their Merkle Paths to verify the correctness of each folding:\n\nRepeat l times:\n\nVerifier randomly selects s^{(0)} \\stackrel{\\$}{\\leftarrow} D_0\n\nProver sends \\hat{f}(s^{(0)}), \\hat{f}(- s^{(0)}) with their Merkle Paths:\\{(\\hat{f}(s^{(0)}), \\pi_{\\hat{f}}(s^{(0)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], s^{(0)})\\{(\\hat{f}(-s^{(0)}), \\pi_{\\hat{f}}(-s^{(0)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], -s^{(0)})\n\nProver calculates s^{(1)} = (s^{(0)})^2\n\nFor i = 1, \\ldots, n - 1:\n\nProver sends q_{f_\\zeta}^{(i)}(s^{(i)}), q_{f_\\zeta}^{(i)}(-s^{(i)}) with their Merkle Paths:\\{(q_{f_\\zeta}^{(i)}(s^{(i)}), \\pi_{q_{f_\\zeta}^{(i)}}(s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q_{f_\\zeta}^{(i)}(x)|_{x \\in D_i}], s^{(i)})\\{(q_{f_\\zeta}^{(i)}(-s^{(i)}), \\pi_{q_{f_\\zeta}}^{(i)}(-s^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q_{f_\\zeta}^{(i)}(x)|_{x \\in D_i}], -s^{(i)})\n\nProver calculates s^{(i + 1)} = (s^{(i)})^2\n\nIf the folding count r < n, in the final step the Prover would send q_{f_\\zeta}^{(r)}(s^{(r)}) with its Merkle Path.","type":"content","url":"/zeromorph/zeromorph-fri#round-5","position":21},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-6","position":22},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 6","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"Prover and Verifier engage in the FRI protocol’s low degree test to prove that for 0 \\le k < n, q_{\\hat{q}_k}(X) has degree less than 2^k:\\pi_{q_{\\hat{q}_k}} \\leftarrow \\mathsf{FRI.LDT}(q_{\\hat{q}_k}(X), 2^k)\n\nThis involves folding rounds until reaching a constant polynomial:\n\nDefine q_{\\hat{q}_k}^{(0)}(x)|_{x \\in D^{(k)}} := q_{f_\\zeta}(x)|_{x \\in D^{(k)}}\n\nFor i = 1,\\ldots, k:\n\nVerifier sends random value \\beta_k^{(i)}\n\nFor any y \\in D_i^{(k)}, find x \\in D_{i - 1}^{(k)} such that y = x^2, Prover computes:q_{\\hat{q}_k}^{(i)}(y) = \\frac{q_{\\hat{q}_k}^{(i - 1)}(x) + q_{\\hat{q}_k}^{(i - 1)}(-x)}{2} + \\beta_k^{(i)} \\cdot \\frac{q_{\\hat{q}_k}^{(i - 1)}(x) - q_{\\hat{q}_k}^{(i - 1)}(-x)}{2x}\n\nIf i < k, Prover sends the Merkle Tree commitment to [q_{\\hat{q}_k}^{(i)}(x)|_{x \\in D_i^{(k)}}]:\\mathsf{cm}(q_{\\hat{q}_k}^{(i)}(X)) = \\mathsf{MT.commit}([q_{\\hat{q}_k}^{(i)}(x)|_{x \\in D_{i}^{(k)}}])\n\nIf i = k, Prover selects any y_0^{(k)} \\in D_{n} and sends the value q_{\\hat{q}_k}^{(i)}(y_0^{(k)}).\n\n📝 Notes\n\nIf the folding count r < k, the final result won’t be a constant polynomial. In this case, the Prover would send a Merkle Tree commitment in round r rather than a single value.","type":"content","url":"/zeromorph/zeromorph-fri#round-6","position":23},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-7","position":24},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 7","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"This round continues the FRI protocol’s low degree test query phase for each quotient polynomial. For each k, the Verifier performs l repeated queries:\n\nFor k = 0, \\ldots, n - 1, repeat l times:\n\nVerifier randomly selects s_k^{(0)} \\stackrel{\\$}{\\leftarrow} D_0^{(k)}\n\nProver sends \\hat{q}_k(s_k^{(0)}), \\hat{q}_k(- s_k^{(0)}) with their Merkle Paths:\\{(\\hat{q}_k(s_k^{(0)}), \\pi_{\\hat{q}_k}(s_k^{(0)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{q}_k(x)|_{x \\in D_0^{(k)}}], s^{(0)})\\{(\\hat{q}_k(-s_k^{(0)}), \\pi_{\\hat{q}_k}(-s_k^{(0)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{q}_k(x)|_{x \\in D_0^{(k)}}], -s^{(0)})\n\nProver calculates s_k^{(1)} = (s_k^{(0)})^2\n\nFor i = 1, \\ldots, k - 1:\n\nProver sends q_{\\hat{q}_k}^{(i)}(s_k^{(i)}), q_{\\hat{q}_k}^{(i)}(-s_k^{(i)}) with their Merkle Paths:\\{(q_{\\hat{q}_k}^{(i)}(s_k^{(i)}), \\pi_{q_{\\hat{q}_k}^{(i)}}(s_k^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q_{\\hat{q}_k}^{(i)}(x)|_{x \\in D_i^{(k)}}], s_k^{(i)})\\{(q_{\\hat{q}_k}^{(i)}(-s_k^{(i)}), \\pi_{q_{\\hat{q}_k}^{(i)}}(-s_k^{(i)}))\\} \\leftarrow \\mathsf{MT.open}([q_{\\hat{q}_k}^{(i)}(x)|_{x \\in D_i^{(k)}}], -s_k^{(i)})\n\nProver calculates s_k^{(i + 1)} = (s_k^{(i)})^2\n\nIf the folding count r < k, in the final step the Prover would send q_{\\hat{q}_k}^{(r)}(s^{(r)}) with its Merkle Path.","type":"content","url":"/zeromorph/zeromorph-fri#round-7","position":25},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#proof","position":26},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"The complete proof sent by the Prover is:\\begin{aligned}\n  \\pi = \\left(\\mathsf{cm}(\\hat{q}_0(X)), \\ldots \\mathsf{cm}(\\hat{q}_{n - 1}(X)), \\hat{f}(\\zeta), \\hat{q}_0(\\zeta), \\ldots, \\hat{q}_{n - 1}(\\zeta), \\pi_{q_{f_\\zeta}}, \\pi_{q_{\\hat{q}_0}}, \\ldots, \\pi_{q_{\\hat{q}_{n - 1}}}\\right)\n\\end{aligned}\n\nUsing the notation \\{\\cdot\\}^l to represent the proofs generated by the l repeated queries in the FRI low degree test query phase, the n + 1 proofs for the FRI low degree tests are:\\begin{aligned}\n  \\pi_{q_{f_\\zeta}} = &  ( \\mathsf{cm}(q_{f_\\zeta}^{(1)}(X)), \\ldots, \\mathsf{cm}(q_{f_\\zeta}^{(n - 1)}(X)),q_{f_\\zeta}^{(n)}(x_0),  \\\\\n  & \\, \\{\\hat{f}(s^{(0)}), \\pi_{\\hat{f}}(s^{(0)}), \\hat{f}(- s^{(0)}), \\pi_{\\hat{f}}(-s^{(0)}), \\\\\n  & \\quad q_{f_\\zeta}^{(1)}(s^{(1)}), \\pi_{q_{f_\\zeta}^{(1)}}(s^{(1)}),q_{f_\\zeta}^{(1)}(-s^{(1)}), \\pi_{q_{f_\\zeta}^{(1)}}(-s^{(1)}), \\ldots, \\\\\n  & \\quad q_{f_\\zeta}^{(n - 1)}(s^{(n - 1)}), \\pi_{q_{f_\\zeta}^{(n - 1)}}(s^{(n - 1)}),q_{f_\\zeta}^{(n - 1)}(-s^{(n - 1)}), \\pi_{q_{f_\\zeta}^{(i)}}(-s^{(n - 1)})\\}^l)\n\\end{aligned}\n\nFor k = 0, \\ldots, n - 1:\\begin{aligned}\n  \\pi_{q_{\\hat{q}_k}} = &  ( \\mathsf{cm}(q_{\\hat{q}_k}^{(1)}(X)), \\ldots, \\mathsf{cm}(q_{\\hat{q}_k}^{(k - 1)}(X)),q_{\\hat{q}_k}^{(k)}(y_0^{(k)}),  \\\\\n  & \\, \\{\\hat{q}_k(s_k^{(0)}), \\pi_{\\hat{q}_k}(s_k^{(0)}), \\hat{q}_k(-s_k^{(0)}), \\pi_{\\hat{q}_k}(-s_k^{(0)}),\\\\\n  & \\quad q_{\\hat{q}_k}^{(1)}(s_k^{(1)}), \\pi_{q_{\\hat{q}_k}^{(1)}}(s_k^{(1)}), q_{\\hat{q}_k}^{(1)}(-s_k^{(1)}), \\pi_{q_{\\hat{q}_k}^{(1)}}(-s_k^{(1)}) \\ldots, \\\\\n  & \\quad q_{\\hat{q}_k}^{(k - 1)}(s_k^{(k-1)}), \\pi_{q_{\\hat{q}_k}^{(k - 1)}}(s_k^{(k - 1)}), q_{\\hat{q}_k}^{(k - 1)}(-s_k^{(k - 1)}), \\pi_{q_{\\hat{q}_k}^{(k - 1)}}(-s_k^{(k - 1)})\\}^l)\n\\end{aligned}","type":"content","url":"/zeromorph/zeromorph-fri#proof","position":27},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#verification","position":28},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Integration with FRI"},"content":"The Verifier performs the following steps:\n\nVerify the low degree test proof for q_{f_\\zeta}(X):\\mathsf{FRI.LDT.verify}(\\pi_{q_{f_\\zeta}}, 2^n) \\stackrel{?}{=} 1\n\nThe specific verification process repeats l times:\n\nVerify the correctness of \\hat{f}(s^{(0)}), \\hat{f}(-s^{(0)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X)), \\hat{f}(s^{(0)}), \\pi_{\\hat{f}}(s^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X)), \\hat{f}(-s^{(0)}), \\pi_{\\hat{f}}(-s^{(0)})) \\stackrel{?}{=} 1\n\nVerifier calculates:q_{f_\\zeta}^{(0)}(s^{(0)}) = (1 + \\lambda \\cdot s^{(0)}) \\cdot \\frac{\\hat{f}(s^{(0)}) - \\hat{f}(\\zeta)}{s^{(0)} - \\zeta}q_{f_\\zeta}^{(0)}(- s^{(0)}) = (1 - \\lambda \\cdot s^{(0)}) \\cdot\\frac{\\hat{f}(-s^{(0)}) - \\hat{f}(\\zeta)}{-s^{(0)} - \\zeta}\n\nVerify the correctness of q_{f_\\zeta}^{(1)}(s^{(1)}), q_{f_\\zeta}^{(1)}(-s^{(1)}):\\mathsf{MT.verify}(\\mathsf{cm}(q_{f_\\zeta}^{(1)}(X)), q_{f_\\zeta}^{(1)}(s^{(1)}), \\pi_{q_{f_\\zeta}^{(1)}}(s^{(1)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{f_\\zeta}^{(1)}(X)), q_{f_\\zeta}^{(1)}(-s^{(1)}), \\pi_{q_{f_\\zeta}^{(1)}}(-s^{(1)})) \\stackrel{?}{=} 1\n\nVerify the correctness of the first folding:q_{f_\\zeta}^{(1)}(s^{(1)}) \\stackrel{?}{=} \\frac{q_{f_\\zeta}^{(0)}(s^{(0)}) + q_{f_\\zeta}^{(0)}(- s^{(0)})}{2} + \\alpha^{(1)} \\cdot \\frac{q_{f_\\zeta}^{(0)}(s^{(0)}) - q_{f_\\zeta}^{(0)}(- s^{(0)})}{2 \\cdot s^{(0)}}\n\nFor i = 2, \\ldots, n - 1:\n\nVerify the correctness of q_{f_\\zeta}^{(i)}(s^{(i)}), q_{f_\\zeta}^{(i)}(-s^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(q_{f_\\zeta}^{(i)}(X)), q_{f_\\zeta}^{(i)}(s^{(i)}), \\pi_{q_{f_\\zeta}^{(i)}}(s^{(i)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{f_\\zeta}^{(i)}(X)), q_{f_\\zeta}^{(i)}(-s^{(i)}), \\pi_{q_{f_\\zeta}^{(i)}}(-s^{(i)})) \\stackrel{?}{=} 1\n\nVerify the correctness of the i-th folding:\nq_{f_\\zeta}^{(i)}(s^{(i)}) \\stackrel{?}{=} \\frac{q_{f_\\zeta}^{(i-1)}(s^{(i - 1)}) + q_{f_\\zeta}^{(i - 1)}(- s^{(i - 1)})}{2} + \\alpha^{(i)} \\cdot \\frac{q_{f_\\zeta}^{(i - 1)}(s^{(i - 1)}) - q_{f_\\zeta}^{(i - 1)}(- s^{(i - 1)})}{2 \\cdot s^{(i - 1)}}\n\nVerify that the final polynomial is constant:q_{f_\\zeta}^{(n)}(x_0) \\stackrel{?}{=} \\frac{q_{f_\\zeta}^{(n-1)}(s^{(n - 1)}) + q_{f_\\zeta}^{(n - 1)}(- s^{(n - 1)})}{2} + \\alpha^{(n)} \\cdot \\frac{q_{f_\\zeta}^{(n - 1)}(s^{(n - 1)}) - q_{f_\\zeta}^{(n - 1)}(- s^{(n - 1)})}{2 \\cdot s^{(n - 1)}}\n\nFor k = 0, \\ldots, n - 1, verify the low degree test proof for q_{\\hat{q}_k}(X):\\mathsf{FRI.LDT.verify}(\\pi_{q_{\\hat{q}_k}}, 2^k) \\stackrel{?}{=} 1\n\nThe specific verification process repeats l times:\n\nVerify the correctness of \\hat{q}_k(s_k^{(0)}), \\hat{q}_k(-s_k^{(0)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{q}_k(X)), \\hat{q}_k(s_k^{(0)}), \\pi_{\\hat{q}_k}(s_k^{(0)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{q}_k(X)), \\hat{q}_k(-s_k^{(0)}), \\pi_{\\hat{q}_k}(-s_k^{(0)})) \\stackrel{?}{=} 1\n\nVerifier calculates:q_{\\hat{q}_k}^{(0)}(s_k^{(0)}) = (1 + \\lambda \\cdot s_k^{(0)}) \\cdot \\frac{\\hat{q}_k(s_k^{(0)})- \\hat{q}_k(\\zeta)}{s_k^{(0)} - \\zeta}q_{\\hat{q}_k}^{(0)}(-s_k^{(0)}) = (1 - \\lambda \\cdot s_k^{(0)}) \\cdot \\frac{\\hat{q}_k(-s_k^{(0)})- \\hat{q}_k(\\zeta)}{-s_k^{(0)} - \\zeta}\n\nVerify the correctness of q_{\\hat{q}_k}^{(1)}(s_k^{(1)}),q_{\\hat{q}_k}^{(1)}(-s_k^{(1)}):\\mathsf{MT.verify}(\\mathsf{cm}(q_{\\hat{q}_k}^{(1)}(X)), q_{\\hat{q}_k}^{(1)}(s_k^{(1)}), \\pi_{q_{\\hat{q}_k}^{(1)}}(s_k^{(1)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{\\hat{q}_k}^{(1)}(X)), q_{\\hat{q}_k}^{(1)}(-s_k^{(1)}), \\pi_{q_{\\hat{q}_k}^{(1)}}(-s_k^{(1)})) \\stackrel{?}{=} 1\n\nVerify the correctness of the first folding:q_{\\hat{q}_k}^{(1)}(s_k^{(1)}) \\stackrel{?}{=} \\frac{q_{\\hat{q}_k}^{(0)}(s_k^{(0)}) + q_{\\hat{q}_k}^{(0)}(- s_k^{(0)})}{2} + \\beta_k^{(1)} \\cdot \\frac{q_{\\hat{q}_k}^{(0)}(s_k^{(0)}) - q_{\\hat{q}_k}^{(0)}(- s_k^{(0)})}{2 \\cdot s_k^{(0)}}\n\nFor i = 2, \\ldots, k - 1:\n\nVerify the correctness of q_{\\hat{q}_k}^{(i)}(s_k^{(i)}), q_{\\hat{q}_k}^{(i)}(-s_k^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(q_{\\hat{q}_k}^{(i)}(X)), q_{\\hat{q}_k}^{(i)}(s_k^{(i)}), \\pi_{q_{\\hat{q}_k}^{(i)}}(s_k^{(i)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(q_{\\hat{q}_k}^{(i)}(X)), q_{\\hat{q}_k}^{(i)}(-s_k^{(i)}), \\pi_{q_{\\hat{q}_k}^{(i)}}(-s_k^{(i)})) \\stackrel{?}{=} 1\n\nVerify the correctness of the i-th folding:q_{\\hat{q}_k}^{(i)}(s_k^{(i)}) \\stackrel{?}{=} \\frac{q_{\\hat{q}_k}^{(i - 1)}(s_k^{(i - 1)}) + q_{\\hat{q}_k}^{(i - 1)}(- s_k^{(i - 1)})}{2} + \\beta_k^{(i)} \\cdot \\frac{q_{\\hat{q}_k}^{(i - 1)}(s_k^{(i - 1)}) - q_{\\hat{q}_k}^{(i - 1)}(- s_k^{(i - 1)})}{2 \\cdot s_k^{(i - 1)}}\n\nVerify that the final polynomial is constant:q_{\\hat{q}_k}^{(k)}(y_0^{(k)}) \\stackrel{?}{=} \\frac{q_{\\hat{q}_k}^{(k - 1)}(s_k^{(k - 1)}) + q_{\\hat{q}_k}^{(k - 1)}(- s_k^{(k - 1)})}{2} + \\beta_k^{(k)} \\cdot \\frac{q_{\\hat{q}_k}^{(k - 1)}(s_k^{(k - 1)}) - q_{\\hat{q}_k}^{(k - 1)}(- s_k^{(k - 1)})}{2 \\cdot s_k^{(k - 1)}}\n\nCalculate \\Phi_n(\\zeta) and \\Phi_{n - k}(\\zeta^{2^k})(0 \\le k < n):\\Phi_n(\\zeta) = 1 + \\zeta + \\zeta^2 + \\ldots + \\zeta^{2^n-1}\\Phi_{n-k}(\\zeta^{2^k}) = 1 + \\zeta^{2^k} + \\zeta^{2\\cdot 2^k} + \\ldots + \\zeta^{(2^{n-k}-1)\\cdot 2^k}\n\nVerify the correctness of the following equation:\\hat{f}(\\zeta) - v\\cdot\\Phi_n(\\zeta) \\stackrel{?}{=} \\sum_{k = 0}^{n - 1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta)","type":"content","url":"/zeromorph/zeromorph-fri#verification","position":29},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl2","url":"/zeromorph/zeromorph-fri#optimized-zeromorph-integration-with-fri","position":30},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"In the protocol described above, we commit to n univariate polynomials \\hat{q}_k(X) and provide separate FRI low degree test proofs for each. In practice, since the degree bounds of consecutive polynomials \\hat{q}_{k}(X) and \\hat{q}_{k - 1}(X) differ by a factor of 2, we can use a rolling batch technique to perform only one low degree test for all n polynomials. Additionally, since the degree bounds of \\hat{f}(X) and \\hat{q}_{n-1}(X) also differ by a factor of 2, we can use the rolling batch technique from [ZLGSCLD24] to perform just one low degree test for all n + 1 polynomials: \\hat{f}(X), \\hat{q}_{n-1}(X), \\ldots, \\hat{q}_{0}(X).\n\nWhen committing to the n univariate polynomials \\hat{q}_k(X), since the sizes of D^{(k)} and D^{(k - 1)} also differ by a factor of 2, we can borrow the \n\nmmcs structure from plonky3 to make just one commitment for all n polynomials.\n\nLet’s first explain the mmcs commitment process using the example of 3 polynomials: \\hat{q}_2(X), \\hat{q}_1(X), \\hat{q}_0(X). Setting \\rho = \\frac{1}{2}, the Prover commits to:\\mathsf{cm}(\\hat{q}_2(X)) = [\\hat{q}_2(x)|_{x\\in D^{(2)}}] = \\{\\hat{q}_2(\\omega_2^0), \\hat{q}_2(\\omega_2^1), \\hat{q}_2(\\omega_2^2), \\ldots, \\hat{q}_2(\\omega_2^7)\\}\\mathsf{cm}(\\hat{q}_1(X)) = [\\hat{q}_1(x)|_{x\\in D^{(1)}}] = \\{\\hat{q}_1(\\omega_1^0), \\hat{q}_1(\\omega_1^1), \\hat{q}_1(\\omega_1^2), \\hat{q}_1(\\omega_1^3)\\}\\mathsf{cm}(\\hat{q}_0(X)) = [\\hat{q}_0(x)|_{x\\in D^{(0)}}] = \\{\\hat{q}_0(\\omega_0^0), \\hat{q}_0(\\omega_0^1)\\}\n\nwhere \\omega_2, \\omega_1, \\omega_0 are generators of D^{(2)}, D^{(1)}, D^{(0)} satisfying:(\\omega_2)^8 = 1, (\\omega_1)^4 = 1, (\\omega_0)^2 = 1\n\nIn practice, we can choose a generator g of \\mathbb{F}_p^* and set:\\omega_2 = g^{\\frac{p - 1}{8}}, \\omega_1 = g^{\\frac{p - 1}{4}},\\omega_0 = g^{\\frac{p - 1}{2}}\n\nBy Fermat’s Little Theorem, we can verify that (\\omega_2)^8 = 1, (\\omega_1)^4 = 1, (\\omega_0)^2 = 1 holds, with the relationships \\omega^2 = \\omega_1, \\omega_1^2 = \\omega_0.\n\nWe can see that \\mathsf{cm}(\\hat{q}_2(X)) commits to 8 values, \\mathsf{cm}(\\hat{q}_1(X)) commits to 4 values, and \\mathsf{cm}(\\hat{q}_0(X)) commits to 2 values. Using separate Merkle Trees would require 3 trees with heights 3, 2, and 1 respectively. With the mmcs structure, we can place all 14 values in a single tree of height 6.\n\nThis commitment method is denoted:\\mathsf{cm}(\\hat{q}_2(X), \\hat{q}_1(X), \\hat{q}_0(X)) = \\mathsf{MMCS.commit}(\\hat{q}_2(X), \\hat{q}_1(X), \\hat{q}_0(X))\n\nNow let’s illustrate the rolling batch technique with n = 3 as an example. For quotient polynomials q_{\\hat{q}_2}(X), q_{\\hat{q}_1}(X), q_{\\hat{q}_0}(X), instead of providing 3 separate FRI low degree test proofs, we can use the rolling batch technique to prove their degree bounds with just one low degree test, as shown in the following diagram.\n\nAfter folding, we add the value with the next q_{\\hat{q}_{i - 1}} and continue with FRI folding until reaching a constant polynomial.","type":"content","url":"/zeromorph/zeromorph-fri#optimized-zeromorph-integration-with-fri","position":31},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Commitment Phase","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl3","url":"/zeromorph/zeromorph-fri#commitment-phase-1","position":32},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Commitment Phase","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"The commitment phase is the same as in the non-optimized protocol. To commit to an MLE polynomial with n variables:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i=0}^{N-1} a_i \\cdot \\overset{\\sim}{eq}(\\mathsf{bits}(i), (X_0, X_1, \\ldots, X_{n-1}))\n\nFirst map it to a univariate polynomial \\hat{f}(X):\\hat{f}(X) = a_0 + a_1 X + \\cdots + a_{N-1} X^{N - 1}\n\nwith commitment:\\mathsf{cm}(\\hat{f}(X)) = [\\hat{f}(x)|_{x \\in D}]\n\nusing a Merkle Tree:\\mathsf{cm}(\\hat{f}(X)) = \\mathsf{MT.Commit}([\\hat{f}(x)|_{x \\in D}])","type":"content","url":"/zeromorph/zeromorph-fri#commitment-phase-1","position":33},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl3","url":"/zeromorph/zeromorph-fri#evaluation-proof-protocol-1","position":34},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"","type":"content","url":"/zeromorph/zeromorph-fri#evaluation-proof-protocol-1","position":35},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#common-inputs-1","position":36},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Common Inputs","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Commitment to MLE polynomial \\tilde{f}: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})\n\nRate parameter: \\rho\n\nFRI low degree test query repetition parameter: l\n\nMultiplicative subgroups for FRI encoding: D, D^{(0)}, \\ldots, D^{(n - 1)}","type":"content","url":"/zeromorph/zeromorph-fri#common-inputs-1","position":37},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#witness-1","position":38},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Point value vector of MLE polynomial \\tilde{f} on the n-dimensional HyperCube: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/zeromorph/zeromorph-fri#witness-1","position":39},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-1-1","position":40},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Prover sends commitments to remainder polynomials:\n\nCompute n remainder MLE polynomials \\{\\tilde{q}_k\\}_{k=0}^{n-1}, satisfying:\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{k=0}^{n-1} (X_k-u_k) \\cdot \\tilde{q}_k(X_0,X_1,\\ldots, X_{k-1})\n\nConstruct the univariate polynomials corresponding to the remainder MLE polynomials: \\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n\n\nCompute and send their commitments using the mmcs structure. First calculate the values of these polynomials on their respective domains:\\{[\\hat{q}_k(x)|_{x \\in D^{(k)}}]\\}_{k = 0}^{n - 1}\n\nwhere |D^{(k)}| = 2^k / \\rho, then commit to these (2^{n - 1} + 2^{n - 2} + \\ldots + 2^0)/\\rho values all at once using mmcs:\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0) = \\mathsf{MMCS.commit}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0)","type":"content","url":"/zeromorph/zeromorph-fri#round-1-1","position":41},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-2-1","position":42},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Verifier sends random value \\zeta \\stackrel{\\$}{\\leftarrow} \\mathbb{F} \\setminus D\n\nProver computes and sends \\hat{f}(\\zeta)\n\nProver computes and sends \\{\\hat{q}_k(\\zeta)\\}_{k = 0}^{n - 1}.","type":"content","url":"/zeromorph/zeromorph-fri#round-2-1","position":43},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-3-1","position":44},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Verifier sends random value \\lambda \\stackrel{\\$}{\\leftarrow} \\mathbb{F}\n\nProver computes:q_{f_\\zeta}(X) = \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{f}(X) - \\hat{f}(\\zeta)}{X - \\zeta}\n\nat points in D:[q_{f_\\zeta}(x)|_{x \\in D}] = \\big[\\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta} + \\lambda \\cdot x \\cdot \\frac{\\hat{f}(x) - \\hat{f}(\\zeta)}{x - \\zeta}\\big|_{x \\in D} \\big]\n\nFor 0 \\le k < n, Prover computes:q_{\\hat{q}_k}(X) = \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta} + \\lambda \\cdot X \\cdot \\frac{\\hat{q_k}(X) - \\hat{q}_k(\\zeta)}{X - \\zeta}\n\nat points in D^{(k)}.","type":"content","url":"/zeromorph/zeromorph-fri#round-3-1","position":45},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-4-1","position":46},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"Prover and Verifier engage in the FRI protocol’s low degree test using the rolling batch technique to prove all quotient polynomials’ degree bounds in one test. For convenience, denote:q_{\\hat{q}_n}(X) := q_{f_\\zeta}(X)\n\nso the low degree test proof is:\\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}} \\leftarrow \\mathsf{OPFRI.LDT}(q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}, 2^{n})\n\nThis involves n + 1 rounds of interaction until reaching a constant polynomial:\n\nInitialize i = n and D^{(n)} := D. For x \\in D^{(n)}, initialize:\\mathsf{fold}^{(i)}(x) = q_{\\hat{q}_{n}}(x)\n\nFor i = n - 1, \\ldots, 0:\n\nVerifier sends random value \\beta^{(i)}\n\nFor y \\in D^{(i)}, find x \\in D^{(i + 1)} such that x^2 = y, Prover computes:\\mathsf{fold}^{(i)}(y) = \\frac{\\mathsf{fold}^{(i + 1)}(x) + \\mathsf{fold}^{(i + 1)}(-x)}{2} + \\beta^{(i)} \\cdot \\frac{\\mathsf{fold}^{(i + 1)}(x) - \\mathsf{fold}^{(i + 1)}(-x)}{2x}\n\nFor x \\in D^{(i)}, Prover updates \\mathsf{fold}^{(i)}(x):\\mathsf{fold}^{(i)}(x) = \\mathsf{fold}^{(i)}(x) + q_{\\hat{q}_{i}}(x)\n\nIf i > 0:\n\nProver sends commitment to \\mathsf{fold}^{(i)}(x):\\mathsf{cm}(\\mathsf{fold}^{(i)}(X)) = \\mathsf{MT.commit}([\\mathsf{fold}^{(i)}(x)|_{x \\in D^{(i)}}])\n\nIf i = 0:\n\nAs the final result is a constant polynomial, Prover selects any point y_0 \\in D^{(0)} and sends the folded value \\mathsf{fold}^{(0)}(y_0).","type":"content","url":"/zeromorph/zeromorph-fri#round-4-1","position":47},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#round-5-1","position":48},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Round 5","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"This round continues the FRI protocol’s low degree test query phase. The Verifier performs l repeated queries:\n\nVerifier randomly selects t^{(n)} \\stackrel{\\$}{\\leftarrow} D^{(n)}\n\nProver sends \\hat{f}(t^{(n)}), \\hat{f}(- t^{(n)}) with their Merkle Paths:\\{(\\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], t^{(n)})\\{(\\hat{f}(-t^{(n)}), \\pi_{\\hat{f}}(-t^{(n)}))\\} \\leftarrow \\mathsf{MT.open}([\\hat{f}(x)|_{x \\in D_0}], -t^{(n)})\n\nFor i = n - 1, \\ldots, 1:\n\nProver calculates t^{(i)} = (t^{(i + 1)})^2\n\nProver sends \\hat{q}_{i}(t^{(i)}) and its Merkle Path:\\{(\\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)}))\\} \\leftarrow \\mathsf{MMCS.open}(\\hat{q}_{i}, t^{(i)})\n\nProver sends \\mathsf{fold}^{(i)}(-t^{(i)}) and its Merkle Path:\\{(\\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)}))\\} \\leftarrow \\mathsf{MT.open}(\\mathsf{fold}^{(i)}, -t^{(i)})\n\nFor i = 0:\n\nProver calculates t^{(0)} = (t^{(1)})^2\n\nProver sends \\hat{q}_0(t^{(0)}) and its Merkle Path:\n\\{(\\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)}))\\} \\leftarrow \\mathsf{MMCS.open}(\\hat{q}_0, t^{(0)})\n\n📝 Notes\n\nFor example, when querying 3 polynomials with the selected point being the last element of q_{\\hat{q}_2}(X), \\omega_2^7, the Prover needs to send the values and Merkle Paths for the green portions in the diagram. The orange-bordered elements indicate that what’s sent is not the value and Merkle Path of the quotient polynomial itself, but the Merkle Path of \\hat{q}_k(X). The Prover sends:> \\{\\hat{q_2}(\\omega_2^7), \\hat{q_2}(\\omega_2^3), \\hat{q}_1(\\omega_1^3), \\mathsf{fold}^{(1)}(\\omega_1^1),  \\hat{q}_0(\\omega_0^1)\\}\n>\n\nand their corresponding Merkle Paths.","type":"content","url":"/zeromorph/zeromorph-fri#round-5-1","position":49},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#proof-1","position":50},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Proof","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"The complete proof sent by the Prover is:\\begin{aligned}\n  \\pi = \\left(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{f}(\\zeta), \\hat{q}_0(\\zeta), \\ldots, \\hat{q}_{n - 1}(\\zeta), \\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n - 1}}, \\ldots, q_{\\hat{q}_{0}}}\\right)\n\\end{aligned}\n\nUsing the notation \\{\\cdot\\}^l to represent the proofs generated by the l repeated queries in the FRI low degree test query phase, the FRI low degree test proof is:\\begin{aligned}\n  \\pi_{q_{\\hat{q}_{n}}, \\ldots, q_{\\hat{q}_{0}}} = &  ( \\mathsf{cm}(\\mathsf{fold}^{(n - 1)}(X)), \\ldots, \\mathsf{cm}(\\mathsf{fold}^{(1)}(X)),\\mathsf{fold}^{(0)}(y_0),  \\\\\n  & \\, \\{\\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)}), \\hat{f}(- t^{(n)}), \\pi_{\\hat{f}}(- t^{(n)}),\\\\\n  & \\quad \\hat{q}_{n - 1}(t^{(n - 1)}), \\pi_{\\hat{q}_{n - 1}}(t^{(n - 1)}), \\mathsf{fold}^{(n - 1)}(-t^{(n - 1)}), \\pi_{\\mathsf{fold}^{(n - 1)}}(-t^{(n - 1)}), \\ldots, \\\\\n  & \\quad \\hat{q}_{1}(t^{(1)}), \\pi_{\\hat{q}_{1}}(t^{(1)}), \\mathsf{fold}^{(1)}(-t^{(1)}), \\pi_{\\mathsf{fold}^{(1)}}(-t^{(1)}), \\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)})\\}^l)\n\\end{aligned}","type":"content","url":"/zeromorph/zeromorph-fri#proof-1","position":51},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"type":"lvl4","url":"/zeromorph/zeromorph-fri#verification-1","position":52},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Zeromorph Integration with FRI"},"content":"The Verifier performs the following steps:\n\nVerify the optimized FRI low degree test for q_{f_{\\zeta}}(X) and the n quotient polynomials \\{q_{\\hat{q}_k}\\}_{k = 0}^{n - 1} all at once:\\mathsf{OPFRI.verify}( \\pi_{q_{\\hat{q}_{n}},q_{\\hat{q}_{n-1}}, \\ldots, q_{\\hat{q}_{0}}}, 2^{n}) \\stackrel{?}{=} 1\n\nThe specific verification process repeats l times:\n\nVerify the correctness of \\hat{f}(t^{(n)}), \\hat{f}(-t^{(n)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X), \\hat{f}(t^{(n)}), \\pi_{\\hat{f}}(t^{(n)})) \\stackrel{?}{=} 1\\mathsf{MT.verify}(\\mathsf{cm}(\\hat{f}(X), \\hat{f}(-t^{(n)}), \\pi_{\\hat{f}}(-t^{(n)})) \\stackrel{?}{=} 1\n\nVerifier calculates:q_{\\hat{q}_{n}}(t^{(n)}) = (1 + \\lambda \\cdot t^{(n)}) \\cdot \\frac{\\hat{f}(t^{(n)}) - \\hat{f}(\\zeta)}{t^{(n)} - \\zeta}q_{\\hat{q}_{n}}(-t^{(n)}) = (1 - \\lambda \\cdot t^{(n)}) \\cdot \\frac{\\hat{f}(-t^{(n)}) - \\hat{f}(\\zeta)}{-t^{(n)} - \\zeta}\n\nInitialize the fold value:\\mathsf{fold} = \\frac{q_{\\hat{q}_{n}}(t^{(n)}) + q_{\\hat{q}_{n}}(-t^{(n)})}{2} + \\beta^{(n - 1)} \\cdot \\frac{q_{\\hat{q}_{n}}(t^{(n)}) - q_{\\hat{q}_{n}}(-t^{(n)})}{2 \\cdot t^{(n)}}\n\nFor i = n - 1, \\ldots , 1:\n\nVerifier calculates t^{(i)} = (t^{(i + 1)})^2\n\nVerify the correctness of \\hat{q}_{i}(t^{(i)}):\\mathsf{MMCS.verify}(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{q}_{i}(t^{(i)}), \\pi_{\\hat{q}_{i}}(t^{(i)})) \\stackrel{?}{=} 1\n\nVerifier calculates:q_{\\hat{q}_{i}}(t^{(i)}) = (1 + \\lambda \\cdot t^{(i)}) \\cdot \\frac{\\hat{q}_{i}(t^{(i)}) - \\hat{q}_{i}(\\zeta)}{t^{(i)} - \\zeta}\n\nUpdate the fold value:\\mathsf{fold} = \\mathsf{fold} + q_{\\hat{q}_{i}}(t^{(i)})\n\nVerify the correctness of \\mathsf{fold}^{(i)}(-t^{(i)}):\\mathsf{MT.verify}(\\mathsf{cm}(\\mathsf{fold}^{(i)}(X)), \\mathsf{fold}^{(i)}(-t^{(i)}), \\pi_{\\mathsf{fold}^{(i)}}(-t^{(i)})) \\stackrel{?}{=} 1\n\nUpdate the fold value:\\mathsf{fold} = \\frac{\\mathsf{fold}^{(i)}(-t^{(i)}) + \\mathsf{fold}}{2} + \\beta^{(i - 1)} \\cdot \\frac{\\mathsf{fold}^{(i)}(-t^{(i)}) - \\mathsf{fold}}{2 \\cdot t^{(i)}}\n\nFor i = 0:\n\nVerifier calculates t^{(0)} = (t^{(1)})^2\n\nVerify the correctness of \\hat{q}_0(t^{(0)}):\\mathsf{MMCS.verify}(\\mathsf{cm}(\\hat{q}_{n - 1}, \\hat{q}_{n - 2}, \\ldots, \\hat{q}_0), \\hat{q}_0(t^{(0)}), \\pi_{\\hat{q}_0}(t^{(0)})) \\stackrel{?}{=} 1\n\nVerifier calculates:q_{\\hat{q}_0}(t^{(0)}) = (1 + \\lambda \\cdot t^{(0)}) \\cdot \\frac{\\hat{q}_0(t^{(0)}) - \\hat{q}_0(\\zeta)}{t^{(0)} - \\zeta}\n\nVerify the correctness of the final equation:\\mathsf{fold}^{(0)}(y_0) \\stackrel{?}{=} \\mathsf{fold} + q_{\\hat{q}_0}(t^{(0)})\n\n📝 Notes\n\nFor the example query above, the Verifier uses the values sent by the Prover to calculate the purple values in the diagram and verifies the Merkle Tree proofs for the orange portions. Finally, the Verifier checks if the calculated final purple value equals the value previously sent by the Prover.\n\nCalculate \\Phi_n(\\zeta) and \\Phi_{n - k}(\\zeta^{2^k})(0 \\le k < n):\\Phi_n(\\zeta) = 1 + \\zeta + \\zeta^2 + \\ldots + \\zeta^{2^n-1}\\Phi_{n-k}(\\zeta^{2^k}) = 1 + \\zeta^{2^k} + \\zeta^{2\\cdot 2^k} + \\ldots + \\zeta^{(2^{n-k}-1)\\cdot 2^k}\n\nVerify the correctness of the following equation:\\hat{f}(\\zeta) - v\\cdot\\Phi_n(\\zeta) \\stackrel{?}{=} \\sum_{k = 0}^{n - 1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta)","type":"content","url":"/zeromorph/zeromorph-fri#verification-1","position":53},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"References"},"type":"lvl2","url":"/zeromorph/zeromorph-fri#references","position":54},{"hierarchy":{"lvl1":"Zeromorph-PCS: Integration with FRI","lvl2":"References"},"content":"[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[H22] Haböck, Ulrich. “A summary on the FRI low degree test.” Cryptology ePrint Archive (2022).\n\nPlonky3. \n\nhttps://​github​.com​/Plonky3​/Plonky3\n\n[ZLGSCLD24] Zhang, Zongyang, Weihan Li, Yanpei Guo, Kexin Shi, Sherman SM Chow, Ximeng Liu, and Jin Dong. “Fast {RS-IOP} Multivariate Polynomial Commitments and Verifiable Secret Sharing.” In 33rd USENIX Security Symposium (USENIX Security 24), pp. 3187-3204. 2024.","type":"content","url":"/zeromorph/zeromorph-fri#references","position":55},{"hierarchy":{"lvl1":"Notes on ZeroMorph"},"type":"lvl1","url":"/zeromorph/zeromorph","position":0},{"hierarchy":{"lvl1":"Notes on ZeroMorph"},"content":"Yu Guo \n\nyu.guo@secbit.io\n\nZeroMorph [KT23] is an MLE polynomial commitment scheme based on KZG10. In fact, the ZeroMorph scheme is a more general framework that can be based on different Univariate Polynomial Commitment schemes, such as the FRI-based ZeroMorph scheme.\n\nThe core idea of ZeroMorph is to use the Evaluations of MLE polynomials, i.e., the “point value vector”, as the “coefficient vector” of Univariate polynomials. This approach may seem strange, but the framework remains clear and concise.\n\nThe key to understanding ZeroMorph lies in understanding the transformations of values on high-dimensional Boolean HyperCubes and how they correspond to operations on Univariate polynomials.","type":"content","url":"/zeromorph/zeromorph","position":1},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomials"},"type":"lvl2","url":"/zeromorph/zeromorph#mle-polynomials","position":2},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomials"},"content":"An MLE (Multilinear Extension) polynomial \\tilde{f} is a class of Multivariate polynomials defined on the Boolean HyperCube. The degree of any variable in each term does not exceed 1. For example, \\tilde{f}=1 + 2X_0 + 3X_1X_0 is an MLE polynomial, while \\tilde{f}'=1 + 2X_0^2 + 3X_1X_0 + X_1 is not, because the degree of X_0^2 is greater than 1.\n\nAn MLE polynomial can correspond to a function from Boolean vectors to a finite field, i.e., f:\\{0,1\\}^n\\to \\mathbb{F}_q, and we call its dimension n. The following figure is an example of a three-dimensional MLE polynomial \\tilde{f}(X_0, X_1, X_2), which can be uniquely represented by the “point value vector” (a_0, a_1, \\ldots, a_7). This corresponds to the “point value form” representation in Univariate polynomials, i.e., the Evaluations form.\n\nOf course, an MLE polynomial can also be represented in “coefficient form”, i.e., Coefficients form, as follows:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{1}\\sum_{i_1=0}^{1}\\cdots \\sum_{i_{n-1}=0}^{1} f_{i_0i_1\\cdots i_{n-1}} X_0^{i_0}X_1^{i_1}\\cdots X_{n-1}^{i_{n-1}}\n\nFor the example of the three-dimensional MLE polynomial above, we can write it as:\\tilde{f}(X_0, X_1, X_2) = f_0 + f_1X_0 + f_2X_1 + f_3X_2 + f_4X_0X_1 + f_5X_0X_2 + f_6X_1X_2 + f_7X_0X_1X_2\n\nwhere (f_0, f_1, \\ldots, f_7) is the coefficient vector of the MLE polynomial. Note that because MLE polynomials belong to multivariate polynomials, any representation requires determining the ordering of terms in the polynomial in advance. In this article and subsequent discussions, we will base our approach on Lexicographic Order.\nFor the “point value form” representation of MLE polynomials, we can define it as:\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) = \\sum_{i_0=0}^{1}\\sum_{i_1=0}^{1}\\cdots \\sum_{i_{n-1}=0}^{1} a_{i_0i_1\\cdots i_{n-1}}\\cdot eq(i_0, i_1, \\ldots, i_{n-1}, X_0, X_1, \\ldots, X_{n-1})\n\nwhere eq is a set of Lagrange Polynomials for the n-dimensional Boolean HyperCube \\{0, 1\\}^n:eq(i_0, i_1, \\ldots, i_{n-1}, X_0, X_1, \\ldots, X_{n-1}) = \\prod_{j=0}^{n-1} \\Big((1-i_j)\\cdot (1-X_j)+ i_j\\cdot X_j\\Big)\n\nThere exists an N\\log(N) conversion algorithm between the “point value form” and “coefficient form” of MLE polynomials, which we will not discuss in depth here.\n\nWe can use ZeroMorph to map an MLE polynomial to a Univariate polynomial, more specifically, to map the “point value vector” of the MLE polynomial on the Boolean HyperCube to the “coefficient vector” of a Univariate polynomial.","type":"content","url":"/zeromorph/zeromorph#mle-polynomials","position":3},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomial to Univariate Polynomial"},"type":"lvl2","url":"/zeromorph/zeromorph#mle-polynomial-to-univariate-polynomial","position":4},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomial to Univariate Polynomial"},"content":"Let’s use a simple example to quickly understand this mapping relationship. Consider a MLE polynomial of dimension 2:\\tilde{f}(X_0, X_1) = 2 + X_1 + X_0X_1\n\nIt’s easy to verify that its point value representation on the Boolean HyperCube is:\\begin{split}\n\\tilde{f}(0,0) = 2 \\\\ \n\\tilde{f}(1,0) = 2 \\\\\n\\tilde{f}(0,1) = 3 \\\\\n\\tilde{f}(1,1) = 4 \\\\\n\\end{split}\n\nIf we adopt the ZeroMorph scheme, it can be mapped to the following Univariate polynomial:\\hat{f}(X) = 2 + 2\\cdot X + 3\\cdot X^2 + 4\\cdot X^3\n\nAssuming we have a Univariate polynomial commitment scheme, we can then calculate the commitment of the mapped Univariate polynomial. For example, suppose we have the following KZG10 commitment scheme SRS:SRS = ([1]_1, [\\tau]_1, [\\tau^2]_1, [\\tau^3]_1, \\ldots, [\\tau^{D}]_1, [1]_2, [\\tau]_2, [\\tau^2]_2, [\\tau^3]_2, \\ldots, [\\tau^D]_2)\n\nAccording to the KZG10 commitment algorithm, we calculate the commitment of \\hat{f}(X) as follows:\\mathsf{cm}(\\hat{f}) = 2\\cdot [1]_1 + 2\\cdot [\\tau]_1 + 3\\cdot [\\tau^2]_1 + 4\\cdot [\\tau^3]_1\n\nIn the following sections, we will use the symbol [[\\tilde{f}]] to represent the Univariate polynomial corresponding to the mapping of the MLE polynomial \\tilde{f}.","type":"content","url":"/zeromorph/zeromorph#mle-polynomial-to-univariate-polynomial","position":5},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Polynomial Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"type":"lvl3","url":"/zeromorph/zeromorph#polynomial-mapping","position":6},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Polynomial Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"content":"In this section, we will discuss more mapping situations. For simplicity, let’s first consider the case of three-dimensional MLE, i.e., \\tilde{f}\\in \\mathbb{F}_q[X_0, X_1, X_2]^{\\leq 1}.\n\nSuppose \\tilde{f} is just a constant polynomial, meaning its coefficient vector only has the first term non-zero, and all other elements are zero. The polynomial can be represented as:\\tilde{c}(X_0, X_1, X_2) = c_0\n\nLet’s consider what kind of Univariate polynomial this constant polynomial would map to. First, we need to convert it to point value form. Consider a three-dimensional Boolean HyperCube, regardless of how X_0,X_1,X_2\\in\\{0, 1\\} are valued, this polynomial always evaluates to c_0 on the Boolean HyperCube. This means its point value form is (c_0, c_0, c_0, \\ldots, c_0), so its corresponding Univariate polynomial is:\\begin{split}\n    [[\\tilde{c}]] &= c_0 + c_0X + c_0X^2 + c_0X^3 + \\ldots + c_0X^{7} \\\\\n    & = c_0 \\cdot (1 + X + X^2 + X^3 + \\ldots + X^{7}) \\\\\n\\end{split}\n\nNow let’s consider a two-dimensional MLE polynomial \\tilde{c}'(X_0, X_1), which is also a constant polynomial, i.e., \\tilde{c}'(X_0, X_1) = c_0. Its corresponding Univariate polynomial is:\\begin{split}\n    [[\\tilde{c}']] &= c_0 + c_0X + c_0X^2 + c_0X^3  \\\\\n    & = c_0 \\cdot (1 + X + X^2 + X^3) \\\\\n\\end{split}\n\nWe can see that although the coefficient form representations of the two MLE polynomials \\tilde{c} and \\tilde{c}' are completely the same, the Univariate polynomials they map to are different. This is because for both Univariate and Multivariate polynomials, their point value form representations implicitly include the selection of the Evaluation Domain. The Evaluation Domain of \\tilde{c} is a 3-dimensional Boolean HyperCube, while the Evaluation Domain of \\tilde{c}' is a 2-dimensional Boolean HyperCube. Therefore, when we calculate the point value form of polynomials, we need to clarify the choice of Evaluation Domain. For MLE polynomials, if their Evaluation Domain is an n-dimensional Boolean HyperCube, we modify the mapping notation by adding a subscript n to the mapping brackets, i.e., [[\\tilde{f}]]_n. Below are the two different Univariate polynomials produced by the mapping of \\tilde{c} on two different Evaluation Domains:\\begin{split}\n    [[\\tilde{c}]]_2 &= c_0 + c_0X + c_0X^2 + c_0X^3  \\\\\n    [[\\tilde{c}]]_3 &= c_0 + c_0X + c_0X^2 + c_0X^3 + c_0X^4 + c_0X^5 + c_0X^6 + c_0X^7 \\\\\n\\end{split}","type":"content","url":"/zeromorph/zeromorph#polynomial-mapping","position":7},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Additive Homomorphism of Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"type":"lvl3","url":"/zeromorph/zeromorph#additive-homomorphism-of-mapping","position":8},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Additive Homomorphism of Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"content":"For any two MLE polynomials, if they have the same dimension, such as \\tilde{f}_1(X_0, X_1) and \\tilde{f}_2(X_0, X_1), suppose the point value form representation of the former is\\tilde{f}_1(X_0, X_1) = v_0\\cdot eq(0,0, X_0, X_1) + v_1\\cdot eq(0,1, X_0, X_1) + v_2\\cdot eq(1,0, X_0, X_1) + v_3\\cdot eq(1,1, X_0, X_1)\\tilde{f}_2(X_0, X_1) = v_0'\\cdot eq(0,0, X_0, X_1) + v_1'\\cdot eq(0,1, X_0, X_1) + v_2'\\cdot eq(1,0, X_0, X_1) + v_3'\\cdot eq(1,1, X_0, X_1)\n\nThen their sum is: \\tilde{f}_1(X_0, X_1) + \\tilde{f}_2(X_0, X_1), and its point value form is:\\begin{split}\n    \\tilde{f}_1(X_0, X_1) + \\tilde{f}_2(X_0, X_1) &= (v_0+v_0')\\cdot eq(0,0, X_0, X_1) + (v_1+v_1')\\cdot eq(0,1, X_0, X_1) \\\\\n    & \\ + (v_2+v_2')\\cdot eq(1,0, X_0, X_1) + (v_3+v_3')\\cdot eq(1,1, X_0, X_1) \\\\\n\\end{split}\n\nThus, the following equation holds:[[\\tilde{f}_1(X_0, X_1) + \\tilde{f}_2(X_0, X_1)]]_2 = [[\\tilde{f}_1(X_0, X_1)]]_2 + [[\\tilde{f}_2(X_0, X_1)]]_2\n\nIt’s not hard to prove that the above equation holds for MLE polynomials of any same dimension. It’s also easy to prove:[[\\alpha\\cdot \\tilde{f}]]_n = \\alpha\\cdot [[\\tilde{f}]]_n,\\quad \\forall \\alpha \\in \\mathbb{F}_q\n\nTherefore, we say that the mapping [[\\tilde{f}]]_n has polynomial additive homomorphism and is a one-to-one mapping (Injective and Surjective).","type":"content","url":"/zeromorph/zeromorph#additive-homomorphism-of-mapping","position":9},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Low Dimension to High Dimension Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"type":"lvl3","url":"/zeromorph/zeromorph#low-dimension-to-high-dimension-mapping","position":10},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Low Dimension to High Dimension Mapping","lvl2":"MLE Polynomial to Univariate Polynomial"},"content":"Let’s consider a more general polynomial case. Suppose a two-dimensional MLE polynomial \\tilde{c}(X_0, X_1) has values (v_0, v_1, v_2, v_3) on a two-dimensional Boolean HyperCube. Then its corresponding Univariate polynomial is:\\begin{split}\n    [[\\tilde{c}]]_2 &= v_0 + v_1X + v_2X^2 + v_3X^3 \\\\\n\\end{split}\n\nAnd X_2\\cdot \\tilde{c}(X_0, X_1) is also an MLE polynomial, with dimension 3. Its values on the Boolean HyperCube are: (0, 0, 0, 0, v_0, v_1, v_2, v_3), i.e., the first four terms are zero, and the last four terms are equal to the values of \\tilde{c}(X_0, X_1) in the two-dimensional MLE polynomial, as shown in the following figure:\n\nThis is easy to explain because when X_2=0, the overall polynomial value is zero, so the values at the vertices of the square formed by X_0, X_1 are all zero. When X_2=1, the polynomial X_2\\cdot \\tilde{c}(X_0, X_1) equals \\tilde{c}(X_0, X_1). Therefore, the values at the vertices of the square plane where X_2=1 are equal to \\tilde{c}(X_0, X_1). Furthermore, we can draw the following conclusion:[[X_2\\cdot \\tilde{c}]]_3 = X^4 \\cdot [[\\tilde{c}]]_2\n\nQuick derivation as follows:[[X_2\\cdot \\tilde{c}]]_3 = v_0X^4 + v_1X^5 + v_2X^6 + v_3X^7 = X^4\\cdot (v_0 + v_1X + v_2X^2 + v_3X^3) = X^4 \\cdot [[\\tilde{c}]]_2\n\nHere, X^4 raises the degree of [[\\tilde{c}]]_2, making it fit perfectly in the high-bit region of the 3-dimensional HyperCube (i.e., the region where X_2=1).\n\nNext, let’s consider the values of \\tilde{c} on a three-dimensional HyperCube. We’ll find that regardless of whether the newly added variable X_2 is 0 or 1, the polynomial’s value only depends on X_0, X_1. Therefore, its point value form is equal to the two-dimensional point value vector copied once, filling up the 3-dimensional HyperCube, as shown in the following figure:\n\nIn other words, the point value form of \\tilde{c} on a three-dimensional HyperCube is (v_0, v_1, v_2, v_3, v_0, v_1, v_2, v_3), so the Univariate polynomial it maps to is:\\begin{split}\n    [[\\tilde{c}]]_3 &= v_0 + v_1X + v_2X^2 + v_3X^3 + v_0X^4 + v_1X^5 + v_2X^6 + v_3X^7 \\\\\n    & = (1 + X^4)\\cdot (v_0 + v_1X + v_2X^2 + v_3X^3) \\\\\n    & = (1 + X^4)\\cdot [[\\tilde{c}]]_2\n\\end{split}\n\nThe above equation can be explained as follows: the values on the three-dimensional HyperCube are composed of two parts, [[\\tilde{c}]]_2 and [[\\tilde{c}]]_2 with its degree raised by X^4.\n\nSimilarly, the values of \\tilde{c} on a four-dimensional HyperCube are (v_0, v_1, v_2, v_3, \\quad v_0, v_1, v_2, v_3, \\quad v_0, v_1, v_2, v_3, \\quad v_0, v_1, v_2, v_3), so the Univariate polynomial it maps to is:\\begin{split}\n    [[\\tilde{c}]]_4 &= v_0 + v_1X + v_2X^2 + v_3X^3 + v_0X^4 + v_1X^5 + v_2X^6 + v_3X^7 + v_0X^8 + v_1X^9 + v_2X^{10} + v_3X^{11} + v_0X^{12} + v_1X^{13} + v_2X^{14} + v_3X^{15} \\\\\n    & = (1 + X^4 + X^8 + X^{12})\\cdot (v_0 + v_1X + v_2X^2 + v_3X^3) \\\\\n    & = (1 + X^4 + X^8 + X^{12})\\cdot [[\\tilde{c}]]_2\n\\end{split}\n\nWhen raising a low-dimensional MLE to a high-dimensional HyperCube, we see the phenomenon of the low-dimensional HyperCube constantly copying itself. We can define a new polynomial function, \\Phi_k(X), to represent this repetitive operation:\\Phi_k(X^h) = 1 + X^h + X^{2h} + \\ldots + X^{(2^{k}-1)h}\n\nTherefore, we can define a general relation, i.e.[[\\tilde{c}]]_n=\\Phi_{n-k}(X^{2^k})\\cdot [[\\tilde{c}]]_k\n\nConsequently, it can be verified that :  [[\\tilde{c}]]_4=\\Phi_2(X^4)\\cdot [[\\tilde{c}]]_2.","type":"content","url":"/zeromorph/zeromorph#low-dimension-to-high-dimension-mapping","position":11},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomial Remainder Theorem"},"type":"lvl2","url":"/zeromorph/zeromorph#mle-polynomial-remainder-theorem","position":12},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"MLE Polynomial Remainder Theorem"},"content":"TODO: What’s the correct name for this remainder theorem?\n\nThe next question is how to use this MLE to Univariate polynomial mapping to implement the MLE Evaluation Argument protocol. Specifically, the problem is how to use \\mathsf{cm}(\\tilde{f}) to verify the correctness of \\tilde{f}'s value at a certain point, such as \\tilde{f}(u_0, u_1)? Although we already have an Evaluation Argument protocol based on KZG10, unfortunately it’s based on Univariate polynomials, not MLE polynomials. KZG10 uses the polynomial remainder theorem, as in the following formula:\\hat{f}(X) -  \\hat{f}(z) = q(X)\\cdot (X-z)\n\nIt uses the commitment \\mathsf{cm}(q) of the quotient polynomial q(X) as the proof for the Evaluation Argument. So how do we transform the problem of proving MLE’s evaluation at a multi-dimensional point, such as (u_0, u_1, \\ldots, u_{n-1}), into proving the evaluation of a Univariate polynomial at one or more points?\n\nThe paper [PST13] gives a multivariate polynomial version of the above theorem:f(X_0, X_1, \\ldots, X_{n-1}) - f(u_0, u_1, \\ldots, u_{n-1})= \\sum_{k=0}^{n-1}q_k(X_0, X_1, \\ldots, X_{n-1}) \\cdot (X_k - u_k)\n\nIf f(X_0, X_1, \\ldots, X_{n-1}) is an MLE polynomial, it can be simplified to the following formula:\\begin{split}\n\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) - \\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) & = \\tilde{q}_{n-1}(X_0, X_1, \\ldots, X_{\\color{red}n-2}) \\cdot (X_{n-1} - u_{n-1}) \\\\\n& + \\tilde{q}_{n-2}(X_0, X_1, \\ldots, X_{\\color{red}n-3}) \\cdot (X_{n-2} - u_{n-2}) \\\\\n& + \\cdots \\\\\n& + \\tilde{q}_{1}(X_{\\color{red}0}) \\cdot (X_{1} - u_{1}) \\\\\n& + \\tilde{q}_{0} \\cdot (X_{0} - u_{0}) \\\\\n\\end{split}\n\nThis is because in the MLE polynomial f(X_0, X_1, \\ldots, X_{n-1}), the highest degree of each variable is 1. For f(X_0, X_1, \\ldots, X_{k}), after dividing by the factor (X_k-u_k), the remainder polynomial will no longer contain the variable X_k. So when f(X_0, X_1, \\ldots, X_{n-1}) is divided by factors (X_{n-1} - u_{n-1}) to (X_0 - u_0) in sequence, the number of variables in the quotient polynomials and remainder polynomials keeps decreasing one by one, until we finally get a constant quotient polynomial \\tilde{q}_0, and of course a constant remainder polynomial, which is exactly the evaluation of the MLE polynomial at (u_0, u_1, \\ldots, u_{n-1}).\n\nLet’s assume this final evaluation is v, i.e.,\\tilde{f}(u_0, u_1, \\ldots, u_{n-1}) = v\n\nThen we apply the Zeromorph mapping to both sides of the remainder theorem equation (both viewed as MLE polynomials) to obtain the corresponding Univariate polynomials.[[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) - v]]_n= [[\\sum_{k=0}^{n-1}\\tilde{q}_k(X_0, X_1, \\ldots, X_{\\color{red}k-1}) \\cdot (X_k - u_k)]]_n\n\nDue to the additive homomorphism of the mapping, we can continue to simplify the above equation:\\begin{split}\n[[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n - [[v]]_n &= \\sum_{k=0}^{n-1}[[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1}) \\cdot (X_k - u_k)]]_n \\\\\n&= \\sum_{k=0}^{n-1}\\Big([[X_k\\cdot \\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_n - u_k[[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_n\\Big)\n\\end{split}\n\nFirst, look at the term [[\\tilde{f}(X_0,X_1,\\ldots, X_{n-1})]]_n on the left side of the equation, which directly maps to \\hat{f}(X). Then look at the term [[v]]_n, which maps to \\hat{v}(X),[[v]]_n  = \\hat{v}(X) = v + vX + vX^2 + \\ldots + vX^{2^n-1}\n\nOr we can use the \\Phi_n(X) function to represent it:[[v]]_n = v\\cdot\\Phi_n(X)\n\nLooking at the term [[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_n on the right side of the equation, this term is mapping a k-dimensional HyperCube to an n-dimensional HyperCube, and then performing the mapping. According to the previous discussion, we need to copy the k-dimensional HyperCube 2^{n-k} times consecutively to fill the n-dimensional HyperCube:[[f(X_0, X_1, \\ldots, X_{k-1})]]_n = \\Phi_{n-k}(X^{2^k})\\cdot [[f(X_0, X_1, \\ldots, X_{k-1})]]_{k}\n\nTo explain further, because \\Phi_{n-k}(X^{2^k}) represents a coefficient vector with an interval of 2^k, its definition expands as follows:\\Phi_{n-k}(X^{2^k}) = 1 + X^{2^k} + X^{2\\cdot 2^k} + \\ldots + X^{(2^{n-k}-1)\\cdot 2^k}\n\nIts coefficient vector is:(1, 0, 0 ,\\ldots, 0, \\quad 1, 0 ,\\ldots, 0, \\quad 1, 0 ,\\ldots, 0, \\quad 1)\n\nSuppose there is a degree-limited polynomial g(X)\\in\\mathbb{F}_q[X], satisfying \\deg(g)<2^k, then the polynomial \\Phi_{n-k}(X^{2^k})\\cdot g represents a polynomial g(X) of degree 2^k-1 repeated 2^{n-k} times with an interval of 2^k, ultimately resulting in a polynomial of degree 2^n-1.\n\nFinally, there’s the term [[X_k\\cdot \\tilde{f}(X_0, X_1, \\ldots, X_{k-1})]]_n, how do we continue to simplify it?\n\nWe can construct its mapping in two steps. First, look at \\tilde{f}(X_0, X_1, \\ldots, X_{k-1}) which can be represented by a k-dimensional Hypercube, then when multiplied by a new variable X_k, it becomes a (k+1)-dimensional HyperCube. This new Hypercube can be divided into two parts, one part is all zeros (when X_k=0), and the other part is exactly \\tilde{f}(X_0, X_1, \\ldots, X_{k-1}). So we first use the \\Phi_n(X) function to construct a repetition pattern of HyperCube with an interval of 2^{k+1}, then repeat the k-dimensional HyperCube 2^{n-k-1} times, so we get the following polynomial.\\Phi_{n-k-1}(X^{2^{k+1}})\\cdot [[\\tilde{f}(X_0, X_1, \\ldots, X_{k-1})]]_{k}\n\nHowever, this is only the first step. The above Univariate polynomial is not equal to [[X_k\\cdot \\tilde{f}(X_0, X_1, \\ldots, X_{k-1})]]_n, because in each repeated (k+1)-dimensional HyperCube of the former, the part where X_k=1 is zero, while the part where X_k=0 is filled with the k-dimensional HyberCube \\tilde{f}(X_0, X_1, \\ldots, X_{k-1}), which is opposite to the HyperCube we want. We need to add a shift factor like X^{2^k} to it, so that we can swap the position of the k-dimensional HyperCube corresponding to X_k (from the low-bit region to the high-bit region):[[X_k\\cdot \\tilde{f}(X_0, X_1, \\ldots, X_{k-1})]]_n = X^{2^k}\\cdot \\Phi_{n-k-1}(X^{2^{k+1}})\\cdot [[\\tilde{f}(X_0, X_1, \\ldots, X_{k-1})]]_{k}\n\nThe following figure demonstrates with a specific example where k=3, n=5. The left side is the 5-dimensional HyperCube before shifting, where the upper and lower half fields represent the fifth dimension, and each half field has two three-dimensional cubes representing the fourth dimension. We can see that only the three-dimensional cube when X_3=0 corresponds to \\tilde{f}(X_0, X_1, X_2), while when X_3=1, the three-dimensional cube is all zero. The right side of the figure below is the 5-dimensional HyperCube after shifting, where the \\tilde{f}(X_0, X_1, X_2) cube has been shifted to the right, that is, to the region corresponding to X_3=1.\n\nAt this point, we can obtain the key equation of the Zeromorph protocol:\\begin{split}\n[[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n - v\\cdot\\Phi_n(X) &=\\sum_{k=0}^{n-1}\\Big(X^{2^k}\\cdot \\Phi_{n-k-1}(X^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(X^{2^k})\\Big)\\cdot [[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_k\n\\end{split}","type":"content","url":"/zeromorph/zeromorph#mle-polynomial-remainder-theorem","position":13},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl2","url":"/zeromorph/zeromorph#kzg10-based-evaluation-argument","position":14},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"KZG10-based Evaluation Argument"},"content":"Note that the Zeromorph equation we derived in the previous section is an equation about Univariate polynomials. We can write it briefly as:\\hat{f}(X) - v\\cdot\\Phi_n(X) = \\sum_k \\Big(X^{2^k}\\cdot \\Phi_{n-k-1}(X^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(X^{2^k})\\Big)\\cdot \\hat{q}_k(X)\n\nHere \\hat{f}(X) and \\hat{q}_k(X) are defined as follows:\\begin{split}\n\\hat{f}(X) &= [[\\tilde{f}(X_0, X_1, \\ldots, X_{n-1})]]_n \\\\\n\\hat{q}_k(X) &= [[\\tilde{q}_k(X_0, X_1, \\ldots, X_{k-1})]]_k\\\\\n\\end{split}\n\nTo prove that the value of \\tilde{f}(X_0, X_1, \\ldots, X_{n-1}) at the point (u_0, u_1, \\ldots, u_{n-1}) is v, we only need to check if the above polynomials are equal. Here, we use the idea of the Schwartz-Zippel lemma: let the Verifier randomly choose a point X=\\zeta, and then let the Prover provide the values of \\hat{f}(\\zeta) and \\hat{q}_k(\\zeta), so that the Verifier can verify whether the following equation holds:\\hat{f}(\\zeta) - v\\cdot\\Phi_n(\\zeta) = \\sum_k \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot\\Phi_{n-k}(\\zeta^{2^k})\\Big)\\cdot \\hat{q}_k(\\zeta)\n\nHowever, this is not enough, because what the Prover actually commits to is \\hat{q}_k(X). To ensure that the MLE remainder polynomial relation holds, we must enforce that the degrees of all quotient polynomials \\hat{q}_k(X) are less than 2^k, i.e., \\deg(\\hat{q}_k)<2^k, to ensure that the Prover has no room for cheating.\n\nBoth FRI and KZG10 provide methods to prove \\deg(\\hat{q}_k)<2^k. In this article, we only consider the Zeromorph protocol based on KZG10. A simple Degree Bound proof protocol based on KZG10 is as follows:\n\nThe Prover provides \\mathsf{cm}(\\hat{q}_k) and additionally \\mathsf{cm}(X^{D-2^k+1}\\cdot \\hat{q}_k(X)) and sends them to the Verifier,\n\nThe Verifier verifies the following equation:e\\big(\\mathsf{cm}(\\hat{q}_k),\\ [\\tau^{D-2^k+1}]_2\\big) = e\\big(\\mathsf{cm}(X^{D-2^k+1}\\cdot \\hat{q}_k(X)),\\ [1]_2\\big)\n\nHere, the role of X^{D-2^k+1}\\cdot \\hat{q}_k(X) is to align the Degree of \\hat{q}_k(X) to D. Because in the KZG10 SRS, the maximum Degree of polynomials that can be committed is D, so if the Degree of \\hat{q}_k(X) exceeds 2^k, then \\deg(X^{D-2^k+1}\\cdot \\hat{q}) > D, making it impossible to commit using the KZG10 SRS. Conversely, if the Prover can correctly commit to X^{D-2^k+1}\\cdot \\hat{q}_k(X), it proves that \\deg(\\hat{q}_k)<2^k.","type":"content","url":"/zeromorph/zeromorph#kzg10-based-evaluation-argument","position":15},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl3","url":"/zeromorph/zeromorph#protocol-description","position":16},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Below, we first give a simple and naive protocol implementation for easy understanding.","type":"content","url":"/zeromorph/zeromorph#protocol-description","position":17},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Public Input","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl4","url":"/zeromorph/zeromorph#public-input","position":18},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Public Input","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Commitment of MLE polynomial \\tilde{f}: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/zeromorph/zeromorph#public-input","position":19},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Witness","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl4","url":"/zeromorph/zeromorph#witness","position":20},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Witness","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Point value vector of MLE polynomial \\tilde{f} on n-dimensional HyperCube \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/zeromorph/zeromorph#witness","position":21},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 1","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl4","url":"/zeromorph/zeromorph#round-1","position":22},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 1","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Prover sends commitments of remainder polynomials\n\nCalculate n remainder MLE polynomials, \\{\\tilde{q}_k\\}_{k=0}^{n-1}\n\nConstruct Univariate polynomials mapped from remainder MLE polynomials\\hat{q}_k=[[\\tilde{q}_k]]_k, \\quad 0 \\leq k < n\n\nCalculate and send their commitments: \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1})\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{k=0}^{n-1} (X_k-u_k) \\cdot \\tilde{q}_k(X_0,X_1,\\ldots, X_{k-1})\n\nProver calculates, \\pi_k=\\mathsf{cm}(X^{D_{max}-2^k+1}\\cdot \\hat{q}_k), \\quad 0\\leq k<n, as the Degree Bound proof of \\deg(\\hat{q}_k)<2^k, and sends them to the Verifier","type":"content","url":"/zeromorph/zeromorph#round-1","position":23},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 2","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl4","url":"/zeromorph/zeromorph#round-2","position":24},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 2","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^*\n\nProver calculates auxiliary polynomial r(X) and quotient polynomial h(X), and sends \\mathsf{cm}(h)\n\nCalculate r(X),r(X) = [[\\tilde{f}]]_{n} - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{k=0}^{n-1} \\Big(\\zeta^{2^k}\\cdot \\Phi_{n-k-1}(\\zeta^{2^{k+1}}) - u_k\\cdot \\Phi_{n-k}(\\zeta^{2^{k}})\\Big)\\cdot \\hat{q}_k(X)\n\nCalculate h(X) and its commitment \\mathsf{cm}(h), as proof that r(X) takes the value zero at X=\\zetah(X) = \\frac{r(X)}{X-\\zeta}","type":"content","url":"/zeromorph/zeromorph#round-2","position":25},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Verification","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl4","url":"/zeromorph/zeromorph#verification","position":26},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Verification","lvl3":"Protocol Description","lvl2":"KZG10-based Evaluation Argument"},"content":"Verifier verifies the following equations\n\nConstruct the commitment of \\mathsf{cm}(r):\\mathsf{cm}(r) = \\mathsf{cm}([[\\tilde{f}]]_{n}) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nVerify r(\\zeta) = 0e(\\mathsf{cm}(r), \\ [1]_2) = e(\\mathsf{cm}(h), [\\tau]_2 - \\zeta\\cdot [1]_2)\n\nVerify if (\\pi_0, \\pi_1, \\ldots, \\pi_{n-1}) are correct, i.e., verify the Degree Bound of all remainder polynomials: \\deg(\\hat{q}_i)<2^i, for 0\\leq i<ne(\\mathsf{cm}(\\hat{q}_i), [\\tau^{D_{max}-2^i+1}]_2) = e(\\pi_i, [1]_2), \\quad 0\\leq i<n","type":"content","url":"/zeromorph/zeromorph#verification","position":27},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Efficiency Overview","lvl2":"KZG10-based Evaluation Argument"},"type":"lvl3","url":"/zeromorph/zeromorph#efficiency-overview","position":28},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Efficiency Overview","lvl2":"KZG10-based Evaluation Argument"},"content":"Proof size: (2n+1)\\mathbb{G}_1\n\nVerifier computation: (2n +2) P, (n+2)\\mathsf{EccMul}^{\\mathbb{G}_1}","type":"content","url":"/zeromorph/zeromorph#efficiency-overview","position":29},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Optimized Protocol"},"type":"lvl2","url":"/zeromorph/zeromorph#optimized-protocol","position":30},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Optimized Protocol"},"content":"In the naive protocol, there are n quotient polynomials, and their Degree Bound proofs have 2n \\mathbb{G}_1 elements, which is obviously not efficient enough. However, we can prove these n degree bounds in batch. Here’s the traditional batch proof approach:\n\nVerifier first sends a random number \\beta\n\nProver aggregates the n quotient polynomials together to get \\bar{q}(X), and when aggregating, aligns the Degree of these quotient polynomials to the same value, which is  2^{n} - 1:\\bar{q}(X) = \\sum_{k=0}^{n-1} \\beta^k \\cdot X^{2^n-2^k}\\cdot \\hat{q}_k(X)\n\nProver sends the commitment of \\bar{q}(X), \\mathsf{cm}(\\bar{q})\n\nVerifier sends a random number \\zeta\n\nProver constructs polynomial s(X), which takes the value zero at X=\\zeta, i.e., s(\\zeta)=0s(X) = \\bar{q}(X) - \\sum_{k=0}^{n-1} \\beta^k \\cdot \\zeta^{2^n-2^k}\\cdot \\hat{q}_k(X)\n\nProver constructs quotient polynomial h_1(X) and aligns its Degree to the maximum Degree Bound D, then proves s(\\zeta)=0, and sends the commitment \\mathsf{cm}(h_1)h_1(X) = \\frac{s(X)}{X-\\zeta}\\cdot X^{D-2^n+2}\n\nVerifier has \\mathsf{cm}(\\bar{q}) and \\mathsf{cm}(\\hat{q}_i), he can restore the commitment of \\mathsf{cm}(s) based on the following equation:\\mathsf{cm}(s) = \\mathsf{cm}(\\bar{q}) - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{2^n-2^k}\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nVerifier only needs two Pairing operations to verify s(\\zeta)=0, thus obtaining the proof that n Degree Bounds holde\\big(\\mathsf{cm}(s), \\ [\\tau^{D_{max}-2^n+2}]_2\\big) = e\\big(\\mathsf{cm}(h_1), [\\tau]_2 - \\zeta\\cdot [1]_2\\big)\n\nMoreover, Verifier can send a random number \\alpha to further aggregate the evaluation proofs of r(X) and s(X), because they both take the value zero at X=\\zeta.\n\nBelow is the optimized version of the Zeromorph protocol, refer to Zeromorph paper [KT23] Section 6. The main optimization technique is to aggregate multiple Degree Bound proofs together, and also aggregate the evaluation proof of R(X) together. This way, only two Pairing operations are needed for verification (this version does not consider the Zero-knowledge property for now).","type":"content","url":"/zeromorph/zeromorph#optimized-protocol","position":31},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl3","url":"/zeromorph/zeromorph#evaluation-proof-protocol","position":32},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"","type":"content","url":"/zeromorph/zeromorph#evaluation-proof-protocol","position":33},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Public Input","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#public-input-1","position":34},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Public Input","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Commitment of MLE polynomial \\tilde{f} mapped to Univariate polynomial F(X)=[[\\tilde{f}]]_n: \\mathsf{cm}([[\\tilde{f}]]_n)\n\nEvaluation point \\mathbf{u}=(u_0, u_1, \\ldots, u_{n-1})\n\nEvaluation result v = \\tilde{f}(\\mathbf{u})","type":"content","url":"/zeromorph/zeromorph#public-input-1","position":35},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#witness-1","position":36},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Witness","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Evaluation vector of MLE polynomial \\tilde{f}: \\mathbf{a} = (a_0, a_1, \\ldots, a_{2^n-1})","type":"content","url":"/zeromorph/zeromorph#witness-1","position":37},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#round-1-1","position":38},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 1","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Round 1: Prover sends commitments of remainder polynomials\n\nCalculate n remainder MLE polynomials, \\{q_i\\}_{i=0}^{n-1}\n\nConstruct Univariate polynomials mapped from remainder MLE polynomials \\hat{q}_i=[[q_i]]_i, \\quad 0 \\leq i < n\n\nCalculate and send their commitments: \\mathsf{cm}(\\hat{q}_0), \\mathsf{cm}(\\hat{q}_1), \\ldots, \\mathsf{cm}(\\hat{q}_{n-1})\\tilde{f}(X_0,X_1,\\ldots, X_{n-1}) - v = \\sum_{i=0}^{n-1} (X_k-u_k) \\cdot q_i(X_0,X_1,\\ldots, X_{k-1})","type":"content","url":"/zeromorph/zeromorph#round-1-1","position":39},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#round-2-1","position":40},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 2","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Verifier sends a random number \\beta\\in \\mathbb{F}_p^* to aggregate multiple Degree Bound proofs\n\nProver calculates \\bar{q}(X) and sends its commitment  \\mathsf{cm}(\\bar{q})\n\nCalculate \\bar{q}(X),\\bar{q}(X) = \\sum_{i=0}^{n-1} \\beta^i \\cdot X^{2^n-2^i}\\hat{q}_i(X)","type":"content","url":"/zeromorph/zeromorph#round-2-1","position":41},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#round-3","position":42},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 3","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Verifier sends a random number \\zeta\\in \\mathbb{F}_p^* to challenge the polynomial evaluation at X=\\zeta\n\nProver calculates h_0(X) and h_1(X)\n\nCalculate r(X),r(X) = \\hat{f}(X) - v\\cdot \\Phi_{n}(\\zeta) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot\\hat{q}_i(X)\n\nCalculate s(X),s(X) = \\bar{q}(X) - \\sum_{k=0}^{n-1} \\beta^k \\cdot \\zeta^{2^n-2^k}\\cdot \\hat{q}_k(X)\n\nCalculate quotient polynomials h_0(X) and h_1(X)h_0(X) = \\frac{r(X)}{X-\\zeta}, \\qquad h_1(X) = \\frac{s(X)}{X-\\zeta}","type":"content","url":"/zeromorph/zeromorph#round-3","position":43},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#round-4","position":44},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Round 4","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Verifier sends a random number \\alpha\\in \\mathbb{F}_p^* to aggregate h_0(X) and h_1(X)\n\nProver calculates h(X) and sends its commitment \\mathsf{cm}(h)h(X)=(h_0(X) + \\alpha\\cdot h_1(X))\\cdot X^{D_{max}-2^n+2}","type":"content","url":"/zeromorph/zeromorph#round-4","position":45},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"type":"lvl4","url":"/zeromorph/zeromorph#verification-1","position":46},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl4":"Verification","lvl3":"Evaluation Proof Protocol","lvl2":"Optimized Protocol"},"content":"Verifier verifies the following equations\n\nRestore the commitment of \\mathsf{cm}(r):\\mathsf{cm}(r) = \\mathsf{cm}(f) - \\mathsf{cm}(v\\cdot \\Phi_{n}(\\zeta)) - \\sum_{i=0}^{n-1} \\Big(\\zeta^{2^i}\\cdot \\Phi_{n-i-1}(\\zeta^{2^{i+1}}) - u_i\\cdot \\Phi_{n-i}(\\zeta^{2^{i}})\\Big)\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nRestore the commitment of \\mathsf{cm}(s):\\mathsf{cm}(s) = \\mathsf{cm}(\\bar{q}) - \\sum_{i=0}^{n-1} \\beta^i \\cdot \\zeta^{2^n-2^i}\\cdot \\mathsf{cm}(\\hat{q}_i)\n\nVerify r(\\zeta) = 0 and s(\\zeta) = 0e(\\mathsf{cm}(r) + \\alpha\\cdot \\mathsf{cm}(s), \\ [\\tau^{D-2^n+2}]_2) = e(\\mathsf{cm}(h),\\ [\\tau]_2 - \\zeta\\cdot [1]_2)","type":"content","url":"/zeromorph/zeromorph#verification-1","position":47},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Summary"},"type":"lvl2","url":"/zeromorph/zeromorph#summary","position":48},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Summary"},"content":"Overall, Zeromorph is a concise protocol. It directly maps the point value form of MLE to the coefficients of Univariate polynomials, and then uses the KZG10 protocol to complete the Evaluation proof. Subsequent articles will discuss how to combine Zeromorph with the FRI protocol to implement MLE PCS.","type":"content","url":"/zeromorph/zeromorph#summary","position":49},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Reference:"},"type":"lvl2","url":"/zeromorph/zeromorph#reference","position":50},{"hierarchy":{"lvl1":"Notes on ZeroMorph","lvl2":"Reference:"},"content":"[KT23] Kohrita, Tohru, and Patrick Towa. “Zeromorph: Zero-knowledge multilinear-evaluation proofs from homomorphic univariate commitments.” Cryptology ePrint Archive (2023). \n\nhttps://​eprint​.iacr​.org​/2023​/917\n\n[PST13] Papamanthou, Charalampos, Elaine Shi, and Roberto Tamassia. “Signatures of correct computation.” Theory of Cryptography Conference. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. \n\nhttps://​eprint​.iacr​.org​/2011​/587","type":"content","url":"/zeromorph/zeromorph#reference","position":51},{"hierarchy":{"lvl1":"Select SageMath Kernel First"},"type":"lvl1","url":"/zeromorph/zeromorph-mapping-tutorial","position":0},{"hierarchy":{"lvl1":"Select SageMath Kernel First"},"content":"F193.<a> = GF(193)\nFp=F193\n\nvar_names = ['X' + str(i) for i in range(100)] + ['X']\nR = PolynomialRing(QQ, var_names)\nR.inject_variables()\nglobal_vars = R.gens()\n\nclass MLE:\n    def __init__(self, poly):\n        self.vars = global_vars\n        self.mle = None\n        # if type of poly is list, then it's a list of evals\n        if isinstance(poly, list):\n            if len(poly) == 0:\n                self.k = 0\n                return\n            self.k = log(len(poly), 2)\n            self.from_evals(poly)\n        else:\n            self.k = len(poly.variables())\n            self.mle = poly\n\n    def __repr__(self):\n        return str(self.mle)\n    \n    # fallback to mle's method\n    def __getattr__(self, name):\n        return getattr(self.mle, name)\n\n    # eval means f(0,0,...,0) = evals[0]\n    #            f(0,0,...,1) = evals[1]\n    #            ...\n    #            f(1,1,...,1) = evals[2^k-1]\n    # f(X0, X1) = (1-X0)(1-X1)evals[0] + X0(1-X1)evals[1] + (1-X0)X1*evals[2] + X0X1*evals[3]\n    def from_evals(self, evals):\n        vars = self.vars[:len(evals)]\n        bit_len = self.k\n        mle_poly = 0\n        for i in range(len(evals)):\n            mask = MLE.get_bit_mask(i, bit_len)\n            prods = prod([var if enabled else (1-var) for var, enabled in zip(self.vars, mask)])\n            mle_poly += evals[i] * prods\n        self.mle = mle_poly\n        return self\n\n    def get_evals(self):\n        vars = self.vars[:2**self.k]\n        evals = []\n        k = self.k\n        if k == 0:\n            return [self.mle]\n\n        for i in range(2**k):\n            mask = self.get_bit_mask(i, k)\n            mle_tmp = self.mle\n            for j in range(k):\n                mle_tmp = mle_tmp.subs({vars[j]: mask[j]})\n            evals.append(mle_tmp)\n        return evals\n\n    def get_coeffs(self):\n        coeffs = []\n        vars = self.vars[:2**self.k]\n        k = self.k\n        if k == 0:\n            return [self.mle]\n        for i in range(2**k):\n            mask = self.get_bit_mask(i, k)\n            monomial = R(prod([var if bit else 1 for var, bit in zip(vars, mask)]))\n            coeff = self.mle.monomial_coefficient(monomial)\n            coeffs.append(coeff)\n        return coeffs\n\n    def from_coeffs(self, coeffs):\n        self.k = log(len(coeffs), 2)\n        self.mle = 0\n\n        vars = self.vars[:2**self.k]\n        k = self.k\n        for i in range(2**k):\n            mask = self.get_bit_mask(i, k)\n            monomial = R(prod([var if bit else 1 for var, bit in zip(vars, mask)]))\n            coeff = coeffs[i]\n            self.mle += coeff * monomial\n        return self\n\n    @staticmethod\n    def get_bit_mask(num, bits):\n        mask = []\n        for i in range(bits):\n            mask.append(num & 1)\n            num >>= 1\n        return mask\n\n    @staticmethod\n    def evals_to_univar(evals):\n        uni_poly = 0\n        for i in range(len(evals)):\n            uni_poly += evals[i] * X^i\n        return uni_poly\n\n    def extend(self, n):\n        k = self.k\n        evals = self.get_evals()\n        # extend evals to n\n        for i in range(n - k):\n            evals = evals + evals\n        return MLE(evals)\n\n    def to_uni(self):\n        evals = self.get_evals()\n        return MLE.evals_to_univar(evals)\n\n\n","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial","position":1},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl2":"ZeroMorph"},"type":"lvl2","url":"/zeromorph/zeromorph-mapping-tutorial#zeromorph","position":2},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl2":"ZeroMorph"},"content":"While KZG is a powerful commitment scheme, it’s limited to univariate polynomials. However, for techniques like sumcheck, we require multivariate polynomials (hereafter referred to as MLE - Multilinear Extension).\n\nKZG proves the existence of:\nq(X) = (f(X) - f(a)) / (X - a)\n\nIn MLE, we have a similar quotient theorem. For example:\nf(X0, X1, X2) = (X0-a)*q0 + (X1-b)*q1(X0) + (X2-c)*q2(X0, X1)\n\nTo leverage the power of KZG for MLE, we need to find a way to treat the MLE as a univariate polynomial. This requires a proper mapping from MLE to a univariate form.\n\nZeroMorph provides this crucial mapping.\n\nBefore delving into the specifics of this mapping, let’s first examine how MLE encodes values.\n\nThe following image demonstrates MLE encoding of 8 numbers on a 3D cube.\n\nmle_evals = [1, 1, 2, 3, 5, 8, 13, 21]\n\nvertices = [\n    ([0,0,0], mle_evals[0]), ([0,0,1], mle_evals[1]), ([0,1,0], mle_evals[2]), ([0,1,1], mle_evals[3]),\n    ([1,0,0], mle_evals[4]), ([1,0,1], mle_evals[5]), ([1,1,0], mle_evals[6]), ([1,1,1], mle_evals[7])\n]\n\ng = Graphics()\n\nfor x in [0,1]:\n    for y in [0,1]:\n        g += line3d([(x,y,0), (x,y,1)], color='black')\nfor x in [0,1]:\n    for z in [0,1]:\n        g += line3d([(x,0,z), (x,1,z)], color='black')\nfor y in [0,1]:\n    for z in [0,1]:\n        g += line3d([(0,y,z), (1,y,z)], color='black')\n\nfor (x,y,z), value in vertices:\n    g += text3d(f\"({x},{y},{z})={value}\", (x,y,z), fontsize=15)\n\nnote = \"This 3D cube visualization represents a multilinear extension, with each vertex storing the corresponding function value.\"\ntext_position = (0.5, 0.5, -0.5)\ng += text3d(note, text_position, fontsize=15)\nshow(g, aspect_ratio=(1, 1, 1), ticks=[[0,2,1], [0,2,1], [0,2,1]], frame=False)\n\n\nThis cube shows the MLE evaluation form. Then we convert it into coefficient form and map it into the Uni-Variate polynomial.\n\nmle = MLE(mle_evals)\nprint(f\"mle: {mle}\")\nprint(f\"mle  evals: {mle.get_evals()}\")\nprint(f\"U(mle): {mle.to_uni()}\")\n\n\nThen we commit this polynomial. The quotient polynomial also applies the same procedure.\n\npoint = [1, 2, 3]\n\nq2, r2 = mle.quo_rem((X2-point[2]))\nprint(f\"q2: {q2}\")\nprint(f\"r2: {r2}\")\n\nq1, r1 = r2.quo_rem((X1-point[1]))\nprint(f\"q1: {q1}\")\nprint(f\"r1: {r1}\")\n\nq0, r0 = r1.quo_rem((X0-point[0]))\nprint(f\"q0: {q0}\")\nprint(f\"r0: {r0}\")\n\nv = r0\n\n# check if mle - v == q0*(X0-point[0]) + q1*(X1-point[1]) + q2*(X2-point[2])\nprint(f\"mle - v:           {mle - v}\")\nprint(f\"sigma((Xk-uk)*qi): {q0*(X0-point[0]) + q1*(X1-point[1]) + q2*(X2-point[2])}\")\n\nq2_uni = MLE(q2).to_uni()\nq1_uni = MLE(q1).to_uni()\nq0_uni = MLE(q0).to_uni()\n\nprint(f\"q2_uni: {q2_uni}\")\nprint(f\"q1_uni: {q1_uni}\")\nprint(f\"q0_uni: {q0_uni}\")\n\n","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#zeromorph","position":3},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl2":"Verifier’s Equation Check"},"type":"lvl2","url":"/zeromorph/zeromorph-mapping-tutorial#verifiers-equation-check","position":4},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl2":"Verifier’s Equation Check"},"content":"The verifier needs to check if the equation holds using the Commitments of {U}(mle), {U}(q_0), {U}(q_1), {U}(q_2), and the plain point, v.","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#verifiers-equation-check","position":5},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Equation Transformation","lvl2":"Verifier’s Equation Check"},"type":"lvl3","url":"/zeromorph/zeromorph-mapping-tutorial#equation-transformation","position":6},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Equation Transformation","lvl2":"Verifier’s Equation Check"},"content":"Let’s transform the equation:\n{U}(mle - v) = {U}(q_0 \\cdot (X_0 - point[0]) + q_1 \\cdot (X_1 - point[1]) + q_2 \\cdot (X_2 - point[2]))\n\n{U}(mle) - {U}(v) = {U}(q_0X_0 - q_0 \\cdot point[0] + q_1X_1 - q_1 \\cdot point[1] + q_2X_2 - q_2 \\cdot point[2])\n\n\\quad = {U}(q_0X_0) - {U}(q_0 \\cdot point[0]) + {U}(q_1X_1) - {U}(q_1 \\cdot point[1]) + {U}(q_2X_2) - {U}(q_2 \\cdot point[2])","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#equation-transformation","position":7},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Challenge: Aligning the Dimension of U(MLE) without the Original MLE","lvl2":"Verifier’s Equation Check"},"type":"lvl3","url":"/zeromorph/zeromorph-mapping-tutorial#challenge-aligning-the-dimension-of-u-mle-without-the-original-mle","position":8},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Challenge: Aligning the Dimension of U(MLE) without the Original MLE","lvl2":"Verifier’s Equation Check"},"content":"We face a significant challenge in this process: we need to find a method to increase the dimension of {U}(q), but the verifier only has access to {U}(q), not the original  q itself. This limitation requires us to develop a approach to upscale the dimension while working solely with the committed values.","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#challenge-aligning-the-dimension-of-u-mle-without-the-original-mle","position":9},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"ZeroMorph’s U Mapping","lvl2":"Verifier’s Equation Check"},"type":"lvl3","url":"/zeromorph/zeromorph-mapping-tutorial#zeromorphs-u-mapping","position":10},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"ZeroMorph’s U Mapping","lvl2":"Verifier’s Equation Check"},"content":"ZeroMorph’s U mapping defines all dimensions in F[X] 2^n. We need to extend any lower degree items from k-dim to the n-dim vector space.","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#zeromorphs-u-mapping","position":11},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl4":"Example: {U}(q_0X_0)","lvl3":"ZeroMorph’s U Mapping","lvl2":"Verifier’s Equation Check"},"type":"lvl4","url":"/zeromorph/zeromorph-mapping-tutorial#example-u-q-0x-0","position":12},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl4":"Example: {U}(q_0X_0)","lvl3":"ZeroMorph’s U Mapping","lvl2":"Verifier’s Equation Check"},"content":"Let’s examine what’s happening with {U}(q_0X_0):\n\nIt means a 0-dim q_0 multiplies a variable, making it 1-dim\n\nWhen X_0=0, {U}(q_0 \\cdot 0) = 0\n\nWhen X_0=1, {U}(q_0 \\cdot 1) = {U}(q_0)\nThis appears to be zero-padding before q_0 on 1-dim.","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#example-u-q-0x-0","position":13},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Extending to Higher Dimensions","lvl2":"Verifier’s Equation Check"},"type":"lvl3","url":"/zeromorph/zeromorph-mapping-tutorial#extending-to-higher-dimensions","position":14},{"hierarchy":{"lvl1":"Select SageMath Kernel First","lvl3":"Extending to Higher Dimensions","lvl2":"Verifier’s Equation Check"},"content":"To fulfill the space requirements, we need to extend the 1-dimensional representation into a 3-dimensional one.\n\nConsider the following scenario:\n\nThe 1-dimensional representation is augmented with two additional variables that do not affect the outcome.\n\nRegardless of the values chosen for X1 and X2, only X0 has an impact on the result.\n\nTo achieve this extension, we can:\n\nReplicate the 1-dimensional representation to create a 2-dimensional structure.\n\nSubsequently, expand the 2-dimensional structure into a 3-dimensional representation.\n\nThis approach allows us to maintain the integrity of the original information while extending it to the required higher-dimensional space.\n\n# To explain the expansion of the k-dim mle to n-dim mle by using evals\n# We can use a 1D example to explain\n# the evals is [0, 35] which lying on the 1D line\nmle_evals = [0, 35]\n\nvertices = [\n    ([0,0,0], mle_evals[0]), ([1,0,0], mle_evals[1]),\n]\n\ng = Graphics()\n\nfor x in [0,1]:\n    g += line3d([(0,0,0), (1,0,0)], color='black')\n\nfor (x,y,z), value in vertices:\n    g += text3d(f\"({x})={value}\", (x,y,z), fontsize=15)\n\nnote = \"This line visualization represents a multilinear extension, with each vertex storing the corresponding function value.\"\ntext_position = (0.5, 0.5, -0.5)  # x和y居中，z在立方体下方\ng += text3d(note, text_position, fontsize=15)\nshow(g, aspect_ratio=(1, 1, 1), ticks=[[0,2,1], [0,2,1], [0,2,1]], frame=False)\n\n# Then expand the 1D mle to 2D mle, copy the 1D evals to next dimension\nvertices = [\n    ([0,0,0], mle_evals[0]), ([1,0,0], mle_evals[1]),\n    ([0,1,0], mle_evals[0]), ([1,1,0], mle_evals[1]),\n]\n\ng = Graphics()\n\nfor x in [0,1]:\n    for y in [0,1]:\n        g += line3d([(1-x,y,0), (x,y,0)], color='black')\n\nfor x in [0,1]:\n    for y in [0,1]:\n        g += line3d([(x,1-y,0), (x,y,0)], color='red', linestyle='dotted')\n\nfor (x,y,z), value in vertices:\n    g += text3d(f\"({x},{y})={value}\", (x,y,z), fontsize=15)\n\nnote = \"The second line is the copy of the first line, now it's a 2D plane\"\ntext_position = (0.5, 0.5, -0.5)  # x和y居中，z在立方体下方\ng += text3d(note, text_position, fontsize=15)\nshow(g, aspect_ratio=(1, 1, 1), ticks=[[0,2,1], [0,2,1], [0,2,1]], frame=False)\n\n# Then expand the 2D plane to 3D cube by copying the 2D plane to next dimension\nvertices = [\n    ([0,0,0], mle_evals[0]), ([1,0,0], mle_evals[1]),\n    ([0,1,0], mle_evals[0]), ([1,1,0], mle_evals[1]),\n    ([0,0,1], mle_evals[0]), ([1,0,1], mle_evals[1]),\n    ([0,1,1], mle_evals[0]), ([1,1,1], mle_evals[1]),\n]\n\ng = Graphics()\n\nfor x in [0,1]:\n    for y in [0,1]:\n        g += line3d([(x,y,0), (x,y,1)], color='red')\nfor x in [0,1]:\n    for z in [0,1]:\n        g += line3d([(x,0,z), (x,1,z)], color='black')\nfor y in [0,1]:\n    for z in [0,1]:\n        g += line3d([(0,y,z), (1,y,z)], color='black')\n\n\nfor (x,y,z), value in vertices:\n    g += text3d(f\"({x},{y},{z})={value}\", (x,y,z), fontsize=15)\n\nnote = \"The top plane is the copy of the bottom plane, now it's a 3D cube\"\ntext_position = (0.5, 0.5, -0.5)  # x和y居中，z在立方体下方\ng += text3d(note, text_position, fontsize=15)\nshow(g, aspect_ratio=(1, 1, 1), ticks=[[0,2,1], [0,2,1], [0,2,1]], frame=False)\n\n\nLet’s examine q_0. The verifier has the commitment of U(q_0), but how can they obtain U(q_0X_0) independently?\n\nU(q_0) is a 0-dimensional number. Multiplying it by X_0 is equivalent to padding U(q_0) with a leading zero.\n\nAt this point, the verifier has [0, U(q_0)].\n\nNow, let’s extend this to 3 dimensions: [0, U(q_0), 0, U(q_0), 0, U(q_0), 0, U(q_0)]\nNotice the pattern? It’s constructed through padding and duplication. We’ll call this the period function \\Phi(X).\n\nThis function transforms 0 + U(q_0)X into 0 + U(q_0)X + 0 + U(q_0)X^3 + 0 + U(q_0)X^5 + 0 + U(q_0)X^7\nWe can rewrite this equation as: (0 + U(q_0)X) (1 + X^2 + X^4 + X^6)\n\nThe U(q_0)X term causes a right shift for U(q_0), resulting in a leading zero padding. For a k-dimensional polynomial, this shift would be represented by X^{2^k}.\n\nWe call this expression (1 + X^2 + X^4 + X^6) the period function, denoted as \\Phi(X).\n\ndef Phi(x, k, n):\n    result = 0\n    x = x^(2^(k))\n    for i in range(2**(n-k)):\n        result = result + x^(i)\n    return result\n\n\n\nq0X0 = MLE(q0*X0)\nq0X0_uni = q0X0.to_uni()\nprint(f\"U(q0X0)_k: {q0X0_uni}\")\nq0X0_ext_3_uni = q0X0.extend(3).to_uni()\nprint(f\"U(q0X0)_n: {q0X0_ext_3_uni}\")\nprint(f\"U(q0X0)_n/U(q0)_k: {q0X0_ext_3_uni/q0_uni}\")\nprint(f\"Phi(X, 1, 3): {Phi(X, 1, 3)}\")\nprint(f\"Shift(X) * Phi(X, 1, 3): {X * Phi(X, 1, 3)}\")\n\nq1X1 = MLE(q1*X1)\nq1X1_uni = q1X1.to_uni()\nprint(f\"U(q1X1)_k: {q1X1_uni}\")\nq1X1_ext_3_uni = q1X1.extend(3).to_uni()\nprint(f\"U(q1X1)_n: {q1X1_ext_3_uni}\")\nprint(f\"U(q1X1)_n/U(q1)_k: {q1X1_ext_3_uni/q1_uni}\")\nprint(f\"Phi(X, 2, 3): {Phi(X, 2, 3)}\")\nprint(f\"Shift(X^2) * Phi(X, 2, 3): {X^2 * Phi(X, 2, 3)}\")\n\n\nThe verifier can now construct the transformation by applying a shift of X^d and duplicating with Phi(X) on U(q).\n\nThis process mirrors the padding and duplication performed on the original Multi-Linear Extension (MLE). As a result, the verifier can recover the original extension by applying the shift with X^d and Phi(X) on U(q), which is equivalent to the original MLE.\n\nWith this understanding of the mapping from MLE to Univariate Polynomial and how the verifier can construct the dimension extension without knowing the original MLE, we can now proceed to apply the Kate-Zaverucha-Goldberg (KZG) commitment scheme to it.","type":"content","url":"/zeromorph/zeromorph-mapping-tutorial#extending-to-higher-dimensions","position":15}]}